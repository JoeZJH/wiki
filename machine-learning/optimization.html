<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>优化算法 - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;优化算法</div>
</div>
<div class="clearfix"></div>
<div id="title">优化算法</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">梯度下降</a><ul>
<li><a href="#_2">动量方法</a></li>
<li><a href="#nesterov-accelerated-gradient">Nesterov accelerated gradient</a></li>
<li><a href="#adagrad">Adagrad</a></li>
<li><a href="#adadelta-google-inc-new-york-univ-matthew-dzeiler">Adadelta (Google Inc, New York Univ, Matthew D.Zeiler)</a></li>
<li><a href="#rmsprop-hinton">RMSprop (Hinton)</a></li>
<li><a href="#adam-adaptive-moment-estimation">Adam, adaptive moment estimation.</a></li>
<li><a href="#additional-strategies-for-optimizing-sgd">Additional strategies for optimizing SGD</a></li>
<li><a href="#batch-normalization">Batch normalization</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
</ul>
</div>
<h2 id="_1">梯度下降</h2>
<ul>
<li>批量梯度下降</li>
<li>随机梯度下降 SGD</li>
<li>mini-batch 梯度下降</li>
</ul>
<p>mini-batch梯度下降的问题<br />
- 学习率难以选择，过小收敛太慢，过大会导致震荡<br />
- 自动降低学习率需要预先指定条件，不是自适应的<br />
- 算法对所有的参数采用相同的学习率<br />
- 难以跳出鞍点，这是致命的问题</p>
<p>优化算法</p>
<h3 id="_2">动量方法</h3>
<p>可以加速相关方向的收敛和抑制不相关方向的震荡。动量实际上是对梯度的指数平滑<br />
$$<br />
v_t = \gamma v_{t-1} + (1-\gamma) \nabla_\theta J(\theta). \<br />
\theta = \theta - \eta v_t.<br />
$$</p>
<h3 id="nesterov-accelerated-gradient">Nesterov accelerated gradient</h3>
<p>采用预测的点的梯度，而不是当前梯度。<br />
$$<br />
v_t = \gamma v_{t-1} + (1-\gamma) \nabla_\theta J(\theta - \gamma v_{t-1}). \<br />
\theta = \theta - \eta v_t.<br />
$$</p>
<h3 id="adagrad">Adagrad</h3>
<p>解决了两个问题，自适应学习率 和 对不同频次特征采用不同的学习率，适应于稀疏特征 。<br />
更新权值的方程为<br />
$$<br />
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t.<br />
$$<br />
$(G_t)$是对过去的梯度的平方的累积，因此学习率一直在减少。</p>
<h3 id="adadelta-google-inc-new-york-univ-matthew-dzeiler">Adadelta (Google Inc, New York Univ, Matthew D.Zeiler)</h3>
<p>改善Adagrad单调递减的学习率，它不是累计所有的梯度，而是设置了一个固定的时间窗$(w)$.<br />
平滑平方误差$(E[g^2]_t)$<br />
$$<br />
E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma) g_t^2. <br />
$$<br />
权值更新方程为<br />
$$<br />
\Delta \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t +\epsilon}} \odot g_t.<br />
$$<br />
平滑步长的平方<br />
$$<br />
E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1-\gamma) \Delta \theta^2_t.<br />
$$<br />
利用这个重新设计学习率使得单位一致<br />
$$<br />
\Delta \theta_t = - \frac{\sqrt{E[\Delta \theta^2] + \epsilon}}{\sqrt{E[\theta^2]_t + \epsilon}} \odot g_t.<br />
$$</p>
<h3 id="rmsprop-hinton">RMSprop (Hinton)</h3>
<p>第一种Adadelta。</p>
<h3 id="adam-adaptive-moment-estimation">Adam, adaptive moment estimation.</h3>
<p>算法<br />
$$<br />
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t. \<br />
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2.<br />
$$</p>
<p>$$<br />
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}. \<br />
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}.<br />
$$</p>
<p>更新方程<br />
$$<br />
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \hat{m}_t.<br />
$$</p>
<h3 id="additional-strategies-for-optimizing-sgd">Additional strategies for optimizing SGD</h3>
<p>每一次的循环都要打散数据的顺序。Zaremba and Sutskever训练LSTM的时候，<br />
发现按照一定顺序反而能提高性能？</p>
<h3 id="batch-normalization">Batch normalization</h3>
<ul>
<li>可以使用更高的学习率</li>
<li>移除或使用较低的dropout</li>
<li>降低L2权重</li>
</ul>
<h2 id="reference">Reference</h2>
<ol>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/">http://sebastianruder.com/optimizing-gradient-descent/</a></li>
<li><a href="https://arxiv.org/pdf/1502.03167v3.pdf">https://arxiv.org/pdf/1502.03167v3.pdf</a></li>
<li><a href="http://blog.csdn.net/happynear/article/details/44238541">http://blog.csdn.net/happynear/article/details/44238541</a></li>
</ol>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-07-02 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '优化算法',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2020 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>