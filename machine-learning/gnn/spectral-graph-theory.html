<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>谱图理论-Spectral Graph Theory - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;<a href="/wiki/#-gnn">gnn</a>&nbsp;»&nbsp;谱图理论-Spectral Graph Theory</div>
</div>
<div class="clearfix"></div>
<div id="title">谱图理论-Spectral Graph Theory</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#_2">导言</a></li>
<li><a href="#_3">拉普拉斯矩阵</a><ul>
<li><a href="#isoperimetry">Isoperimetry</a><ul>
<li><a href="#fracxt-l-xxt-xlambda_2">推导过程中为什么不能直接从$(\frac{x^T L x}{x^T x})$得到下界$(\lambda_2)$</a></li>
</ul>
</li>
<li><a href="#_4">举例</a></li>
</ul>
</li>
<li><a href="#_5">邻接矩阵和图染色</a></li>
<li><a href="#_6">特征值的界</a><ul>
<li><a href="#_7">图的近似</a></li>
<li><a href="#_8">完全二叉树</a></li>
</ul>
</li>
<li><a href="#_9">电导率与归一化拉普拉斯矩阵</a><ul>
<li><a href="#_10">电导率</a></li>
<li><a href="#_11">归一化拉普拉斯矩阵</a></li>
<li><a href="#cheeger">Cheeger不等式</a></li>
</ul>
</li>
<li><a href="#faq">FAQ</a><ul>
<li><a href="#0">为什么特征值0的代数重数代表图可以划分为互不联通的子图的数目</a></li>
<li><a href="#_12">对拉普拉斯矩阵做线性变换有什么特殊含义吗</a></li>
<li><a href="#_13">拉普拉斯矩阵的特征值和特征向量有什么意义</a></li>
<li><a href="#d-max-regular">为什么联通图就是d-max regular？</a></li>
<li><a href="#perron-frobenius">如何理解Perron-Frobenius</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">关于</h1>
<ul>
<li>课程 Spectral Graph Theory  的学习笔记</li>
<li><a href="http://www.cs.yale.edu/homes/spielman/561/">http://www.cs.yale.edu/homes/spielman/561/</a></li>
<li>python 图代码库 <a href="https://networkx.github.io/">https://networkx.github.io/</a> <a href="https://networkx.github.io/documentation/stable/">https://networkx.github.io/documentation/stable/</a></li>
</ul>
<h1 id="_2">导言</h1>
<ul>
<li>图的邻接矩阵 $(A_G(u, v))$</li>
<li>图的操作：<ul>
<li>diffusion operator，扩散操作：每一个时刻，某个顶点上的东西，均匀扩散到邻居顶点，不保留任何物质<ul>
<li>$(D_G)$表示顶点的度矩阵，那么考虑归一化（这是为了得到扩散概率）邻接矩阵$(W_G = D_G^{-1} A_G)$ 。它的每一行$(W_G(u, \cdot))$代表从顶点u通过一次扩散操作转移到其他顶点的物质的比例。<ul>
<li>另一种解释：W可以看做一个概率转移矩阵，从u转移到v的概率</li>
</ul>
</li>
<li>用向量p表示初始每个顶点物质的数量，那么进过一次扩散操作后，每个顶点物质的数量为 $(p W_G)$<ul>
<li>概率解释：一次扩散操作后，每个定点上的物质的数量是所有定点的量通过一次转移，到达该顶点的量的总和</li>
</ul>
</li>
</ul>
</li>
<li>拉普拉斯矩阵：$(L_G = D_G - A_G)$<ul>
<li>半正定矩阵 $(x^T L_G x  = \sum_{(u,v) \in E} (x(u) - x(v))^2)$</li>
<li>二次型的意义是图上数量分布的平滑性</li>
</ul>
</li>
</ul>
</li>
<li>谱论 <ul>
<li>实对称矩阵，有n个特征值和对应的特征向量。线性代数的基础知识，不多介绍</li>
<li>瑞利商 $(\frac{x^T M x}{x^T x})$</li>
</ul>
</li>
<li>拉普拉斯矩阵的特征向量，假设特征值从小到大排序 $(\lambda_1 \le \lambda_2 \le \cdot \lambda_n)$<ul>
<li>L的特征值大于等于0</li>
<li>0 是特征值，对应的特征向量是全1向量</li>
<li>特征值0的代数重数代表图可以划分为互不联通的子图的数目</li>
<li>$(\lambda_2 &gt; 0)$ 当且仅当图是联通的。<ul>
<li>个人理解，一个互相联通的图可以用一个拉普拉斯矩阵表示出来，且存在一个非0向量使得这个Lx=0</li>
<li>两个互不联通的图看做一个图的时候，由于两个图节点间没有任何连接，因此对应的总拉普拉斯矩阵应该是<br />
$$<br />
L = \begin{bmatrix}<br />
L_1 &amp; O \\<br />
O &amp; L_2<br />
\end{bmatrix}<br />
$$</li>
<li>因此，显然可以构造两个不同的向量$([v_1, O], [O, v_2])$ 使得Lv=0，其中v1和v2分别是子矩阵的0空间中的任意向量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="_3">拉普拉斯矩阵</h1>
<ul>
<li>全1向量是特征值0对应的特征向量</li>
<li>$(\lambda_2)$ 对应的特征向量是满足下列条件的向量</li>
</ul>
<p>$$<br />
\min_x x^T Lx \\<br />
s.t. ||x||^2 = 1 \\<br />
1^T x = 0<br />
$$</p>
<ul>
<li>$(\lambda_3)$ 对应的特征向量是满足于跟上述x正交且满足上述条件的解y</li>
<li>$(\lambda_2)$ 跟问题「通过切割最少的边，将图切分成两个子图」有密切关系</li>
</ul>
<h2 id="isoperimetry">Isoperimetry</h2>
<ul>
<li>顶点集合的子集S的边界(boundary)定义为S与剩下定点的所有相连的边的集合</li>
</ul>
<p>$$<br />
\partial S = \{(u,v) \in E| u \in S, v\notin S \}<br />
$$</p>
<ul>
<li>
<p>isoperimetric ratio 定义为S的边界集合大小与S顶点数目之比<br />
$$<br />
\theta(S) = \frac{|\partial S|}{|S|}<br />
$$</p>
</li>
<li>
<p>isoperimetric number 定义为上述比值的最小值，要求S的大小不超过所有顶点数的一半<br />
$$<br />
\theta_G = \min_{|S| \le n/2} \theta(S)<br />
$$</p>
</li>
<li>
<p>定理(下界)：对每一个子集 $(S \subset V)$， <br />
$$<br />
\theta(S) \ge \lambda_2 (1-s)<br />
$$<br />
s=|S|/|V|是顶点数目比率，因为S的数目不超过V的一半，所以$(s \le 1/2)$，所以<br />
$$<br />
\theta_G \ge \lambda_2 /2<br />
$$</p>
</li>
</ul>
<blockquote>
<p>定理的证明，关键是理解 $(x^T L x)$ 的几何意义。取一个特殊的向量x，x是子集S的示性函数，即xi代表第i个定点是否在S中，如果在则xi=1，否则xi=0。<br />
那么 $( (Lx)_i , i \in S)$ 的意义则为第i个顶点与S外的其他定点的边的数目，把S中的所有i求和即得到边界$(\partial S)$ 的大小了。<br />
所以 $(\theta(S) = \frac{x^T L x}{x^T x})$<br />
显然上式是不小于$(\lambda_2)$ 的，但是还有更准确的下界。<br />
将x按照两个子空间分解（0特征值的空间，即全1向量空间；正交补空间，即 $(x^T 1 = 0)$）<br />
$(x = y + s)$，$(y^T 1 = 0)$，s则为每一维都是s的向量，且$(s_i=|S|/|V| )$<br />
所以继续推导有（注意s是L的特征向量，Ls=0）<br />
$$<br />
\theta(S) = \frac{x^T L x}{x^T x} \\<br />
          = \frac{y^T L y}{x^T x}   \ge \lambda_2 \frac{y^T y}{x^T x} \\<br />
          = \lambda_2 \frac{y^T y}{y^T y + s^T s}  =  \lambda_2 (1-s)<br />
$$</p>
</blockquote>
<ul>
<li>定理的意义：这个定理说明了$(\lambda_2)$跟L的联通性的关系，$(\lambda_2)$越大，说明L联通性越好（任意子集与其补集间的边越多）</li>
</ul>
<h3 id="fracxt-l-xxt-xlambda_2">推导过程中为什么不能直接从$(\frac{x^T L x}{x^T x})$得到下界$(\lambda_2)$</h3>
<ul>
<li>因为这里的x不一定全部在全1向量的补空间中，所以只能缩放到0，而不能缩放到$(\lambda_2)$</li>
</ul>
<h2 id="_4">举例</h2>
<ul>
<li>完全图Kn，任意两个顶点都是互相连接的</li>
<li>星状图Sn，所有顶点跟且只跟第1个顶点相连</li>
<li>Kn的拉普拉斯矩阵特征值：0，代数重数为1；n，代数重数为n-1。$(L_{K_n} = n I - 1^T 1)$<ul>
<li>所以，任意跟全1向量正交的向量$(\phi)$，都有$(L_{K_n}\phi = n \phi )$。即都是特征值n的特征向量。这样的特征向量空间维度当然是n-1，对应代数重数等于n-1</li>
</ul>
</li>
<li>如果定点v和w都只跟同一个节点z相连，那么$(\delta_v - \delta_w)$ 是L的特征向量，对应的特征值为1；</li>
<li>同时，上述条件下，其他特征值对应的特征向量$(\phi)$必定与这个特征向量正交，所以$(\phi(v) = \phi(w))$</li>
<li>Sn的拉普拉斯矩阵特征值：0，代数重数为1，特征向量为全1向量；1，代数重数为n-2，特征向量分别为$(\delta_i - \delta_{i+1}, i=1,2,...,n-2)$；n，代数重数为1。（因为Sn的迹为2n-2，所以剩下的特征值必定为n）</li>
<li>Sn的特征值为n的特征向量求解：设为$(\phi)$，因为$(\phi(2)=\phi(3)=...\phi(n))，所以只有两个独立变量。假设$(\phi = [a, b, b, ..., b])$，因为跟全1向量正交，所以 a+(n-1)b=0，所以得到特征向量为 [n-1, -1, -1, ..., -1]</li>
<li>立方体：Let G = (V,E) and H = (W,F) be graphs. Then G×H is the graph with vertex set V ×W and edge set</li>
</ul>
<p>$$<br />
((v,w), (v', w)), (v,v') \in E \\<br />
((v,w), (v, w')), (w,w') \in F<br />
$$</p>
<ul>
<li>重要例子：G是只有两个顶点{0,1}和一条边的图,特征值为0和2。立方体$(H_n = H_{n-1} X G)$。如果用比特来表示立方体的每个定点，可以发现，如果两个顶点的比特只相差一位，那么他们之间就有一条边。例如n=2的时候，是一个正方向，顶点依次为00,01,11,10</li>
<li>立方体的特征值与特征向量，如果G和H的特征值分别为$(\lambda_i)$ 和 $(\mu_i)$</li>
</ul>
<h1 id="_5">邻接矩阵和图染色</h1>
<ul>
<li>邻接矩阵作用到一个向量上，相当于做了一次扩散操作：即每个节点对应向量的一个元素，一次扩散操作将邻居节点按照权重聚集到中间节点<br />
$$<br />
(Ax)(u) = \sum_{(u,v)\in E} w_{u,v} x(v)<br />
$$</li>
<li>假设A的特征值从大到小排列 $(\mu_1 \ge \mu_2 \ge ... \ge \mu_n)$</li>
<li>对于正规图（顶点的度都是d一样的）：因为$(L = I d - A)$，所以 $(\lambda_i = d - \mu_i)$</li>
<li>对于正规图：最大特征值是$(\mu_1 = d)$，对应全1向量</li>
<li>对于非正规图：$(d_{ave} \le \mu_1 \le d_{max})$</li>
<li>对mu1的下界进一步强化，设S是G的任意子图，有 $(d_{ave}(S) \le \mu_1)$</li>
<li>定理：实对称阵A的子对称矩阵（行下标和列下标完全一致的子矩阵）A(S)，其最大最小特征值被A的最大最小特征值夹着<br />
$$<br />
\lambda_{max}(A) \ge \lambda_{max}(A(S)) \ge \lambda_{min}(A(S)) \ge \lambda_{min}(A)<br />
$$</li>
<li>
<p>如果G是联通的，那么$(\mu_1 = d_{max})$，此时叫$(d_{max})$-regular</p>
</li>
<li>
<p>Perron-Frobenius, Symmetric Case：如果G是联通图，A是邻结矩阵，$(\mu_1 \ge \mu_2 \ge ... \ge \mu_n)$ 是特征值，有<br />
    a. $(\mu_1 \ge -\mu_n)$<br />
    b. $(\mu_1 &gt; \mu_2)$<br />
    c. $(\mu_1)$的存在特征向量$(\phi_1 \ge 0)$</p>
</li>
<li>
<p>$(\mu_1 = -\mu_n)$ 当且仅当G是二分图</p>
</li>
<li>
<p>染色问题：保证相邻两个顶点的颜色不同，最少要$(\chi(G))$种不同的颜色。</p>
</li>
<li>
<p>将顶点编号，从小到大依次给顶点染色，保证顶点颜色跟小号顶点颜色不同即可。假设<br />
$$<br />
|\{ v: v&lt;u, (u, v) \in E \}| \le k<br />
$$<br />
那么，最少可以用k+1中颜色对图进行染色。容易验证 $(k \le \lfloor \mu_1 \rfloor)$</p>
</li>
<li>
<p>Hoffman 界<br />
$$<br />
\chi(G) \ge \frac{\mu_1 -\mu_n}{- \mu_n}<br />
$$</p>
</li>
</ul>
<h1 id="_6">特征值的界</h1>
<ul>
<li>(Courant-Fischer Theorem). Let L be a symmetric matrix with eigenvalues λ1 ≤ λ2 ≤···≤λn. Then,<br />
$$<br />
\lambda_k = \min_{S \subset R^n, dim(S)=k} \max_{x \in S} \frac{x^T Ax}{x^T x} \\<br />
 \max_{T \subset R^n, dim(T)=n - k + 1} \min_{x \in T} \frac{x^T Ax}{x^T x}<br />
$$</li>
<li>这个定理是说，第k个特征值是前k个特征向量张成的子空间中所能取得的最大瑞丽商，是后n-k+1个特征向量张成的子空间中所得取得的最小瑞丽商</li>
<li>$(\lambda_2)$ 是除了全1向量外的最小瑞利商。所以每一个跟全1向量正交的向量v的瑞利商给出一个$(\lambda_2)$的上界</li>
<li>令$(x_i = (n+1) - 2i)$，那么x跟1正交，所以导出一个上界$(\lambda_2 \le \frac{12}{n(n+1)})$</li>
<li>从拉普拉斯矩阵的二次型来看，增加边和增加边的权重，会增加二次型的值，所以有H&gt;=G，H是在G增加边或者增加权重后构成的图</li>
</ul>
<h2 id="_7">图的近似</h2>
<ul>
<li>
<p>图的近似，G的c-近似图H，如果满足 <br />
$$<br />
c H \ge G \ge H /c<br />
$$</p>
</li>
<li>
<p>定理：对任意$(\epsilon &gt; 0)$，存在 d &gt;0，对充分大的n，有 d-regular 图Gn 是Kn的$(1+\epsilon)$近似</p>
</li>
<li>即如果n非常大的时候，可以用很少的边的图来近似很多的边的图！！（参考应用：NetSMF）</li>
<li>例1： $((n-1)P_n \ge G_{1,n})$，Pn表示n个顶点的路径图，即顶点依次相连的图；G1n表示只有第1，n个顶点之间相连的图</li>
<li>路径不等式，一般地，有$((j - i)P_{i,j} \ge G_{i,j})$</li>
<li>利用$(\lambda_2(K_n) = n)$可以推导出$(\lambda_2(P_n) \ge \frac{6}{(n+1)(n-1)} )$</li>
<li>如果图G和H有$(G \ge c H)$，那么$(\lambda_k(G) \ge c \lambda_k(H))$</li>
</ul>
<h2 id="_8">完全二叉树</h2>
<ul>
<li>深度d的完全二叉树Td，节点数目为$(n=2^{d+1} -1)$，边集合为(i, 2i), (i, 2i+1)</li>
<li>lambda2的上界，构造节点向量x，让根节点为0，左子树上的节点全为1，右子树上的节点券为-1.那么x跟1正交，且有<br />
$$<br />
\lambda_2(T_d) \le \frac{2}{n-1}<br />
$$</li>
<li>利用完全图，可以得到另一个上界<br />
$$<br />
\lambda_2(T_d) \le \frac{1}{(n-1)\log_2 n}<br />
$$</li>
</ul>
<h1 id="_9">电导率与归一化拉普拉斯矩阵</h1>
<h2 id="_10">电导率</h2>
<ul>
<li>d(S)代表S中所有顶点的度之和；d(V)等于V中边数目的2倍</li>
<li>定义S的电导率<br />
$$<br />
\phi(S) = \frac{\partial S}{min(d(S), d(V-S))}<br />
$$</li>
<li>图G的电导率<br />
$$<br />
\phi_G = \min_{S \subset V} \phi(S)<br />
$$</li>
</ul>
<h2 id="_11">归一化拉普拉斯矩阵</h2>
<ul>
<li>拉普拉斯矩阵归一化：$(N = D^)$</li>
<li>特征值$(0 = v_1 \le v_2 \le ... \le v_n)$</li>
<li>0特征值对应的特征向量是$(d^{1/2})$，d向量的每个元素是对应顶点的度</li>
<li>定理：$(v_2/2 \le \phi_G)$</li>
<li>cheeger不等式 $(\phi_G \le \sqrt{2v_2})$</li>
</ul>
<h2 id="cheeger">Cheeger不等式</h2>
<h1 id="faq">FAQ</h1>
<h2 id="0">为什么特征值0的代数重数代表图可以划分为互不联通的子图的数目</h2>
<p>两个互不联通的图看做一个图的时候，由于两个图节点间没有任何连接，因此对应的总拉普拉斯矩阵应该是<br />
$$<br />
L = \begin{bmatrix}<br />
L_1 &amp; O \\<br />
O &amp; L_2<br />
\end{bmatrix}<br />
$$</p>
<p>因此，显然可以构造两个不同的向量$([v_1, O], [O, v_2])$ 使得Lv=0，其中v1和v2分别是子矩阵的0空间中的任意向量。</p>
<p>显然，如果可以划分成k个子图，那么0特征值的代数重数就是k。</p>
<h2 id="_12">对拉普拉斯矩阵做线性变换有什么特殊含义吗</h2>
<h2 id="_13">拉普拉斯矩阵的特征值和特征向量有什么意义</h2>
<h2 id="d-max-regular">为什么联通图就是d-max regular？</h2>
<h2 id="perron-frobenius">如何理解Perron-Frobenius</h2>
<ul>
<li>注意A的元素都是非负的</li>
<li>A相当于一次扩散操作</li>
<li>如果特征向量1中的某个元素为0，那么说明他的邻居里面有一些是负数，但是特征向量1是放大倍数最大的向量，所以如果把负数改成正数放大倍数更大，所以矛盾。</li>
<li>mu1的放大倍数是最大的，不考虑方向也是</li>
</ul>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2019-11-05 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '谱图理论-Spectral Graph Theory',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2020 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>