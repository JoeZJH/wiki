<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>强化学习 - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;强化学习</div>
</div>
<div class="clearfix"></div>
<div id="title">强化学习</div>
<ins class="adsbygoogle"
 style="display:block; text-align:center;"
 data-ad-layout="in-article"
 data-ad-format="fluid"
 data-ad-client="ca-pub-6300557868920774"
 data-ad-slot="6882414849"></ins>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#markov-decision-process">Markov Decision Process</a></li>
<li><a href="#reinforcement-learning">Reinforcement Learning</a><ul>
<li><a href="#model-based-learning">model-based Learning</a></li>
</ul>
</li>
<li><a href="#tutorialicml2016">深度强化学习tutorial@ICML2016</a><ul>
<li><a href="#value-based-deep-rl">Value-Based Deep RL</a></li>
<li><a href="#q-learning">Q-Learning：</a></li>
<li><a href="#dqn-agent">DQN：利用 agent 自身经验构建样本！</a></li>
<li><a href="#double-dqn">Double DQN</a></li>
<li><a href="#prioritised-replay">Prioritised replay</a></li>
<li><a href="#duelling-network">Duelling network</a></li>
<li><a href="#deep-policy-networks">Deep Policy Networks</a></li>
</ul>
</li>
<li><a href="#_2">相关资料</a></li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>强化学习是未来很重要的方向！参考 Alpha Go！</p>
<h2 id="markov-decision-process">Markov Decision Process</h2>
<ul>
<li>A set of states $(s \in S)$</li>
<li>A set of actions $(a \in A)$</li>
<li>A transition function $(T(s, a, s'))$<ul>
<li>转移概率 $(P(s'| s, a))$</li>
<li>也叫 model 或者 dynamics</li>
</ul>
</li>
<li>A reward function $(R(s, a, s'))$<ul>
<li>通常只是状态的函数$(R(s), R(s'))$</li>
</ul>
</li>
<li>一个初始状态</li>
<li>一个终止状态</li>
</ul>
<p>MDP 不是一个确定性的搜索问题，一种方法是 <strong>期望值最大搜索</strong> ？！</p>
<p>马尔科夫性说的是未来的决策与过去无关，至于当前状态和策略有关！</p>
<p>（最优）策略：$(\pi^* : S \rightarrow A)$</p>
<p>状态转移函数 $(T(s, a, s') = p(s'| s, a))$</p>
<p>discounting reward: $(r_0 + \gamma r_1 + \gamma^2 r_2 + ...)$</p>
<p>当 $(\gamma &lt; 1)$时，discounting reward 有界 $(R_{max}/(1-\gamma))$！</p>
<p>或者采用 finite horizon，当迭代到 T 步时，停止！</p>
<ul>
<li>
<p>MDP quantities:</p>
<ul>
<li>Policy = 对每一个状态选择一个action</li>
<li>Utility = sum of (discounted) rewards</li>
</ul>
</li>
<li>
<p>状态 value function： $(V^* (s))$</p>
</li>
<li>q-state (s, a), $(Q^* (s,a))$</li>
<li>
<p>最优策略 $(\pi^* (s))$</p>
</li>
<li>
<p>递归定义：</p>
<ul>
<li>$(V^ * (s) = \max_a Q^ * (s, a))$</li>
<li>$(Q^ * (s, a) = \sum_{s'} (s, a, s') \left[ R(s, a, s') + \gamma V^* (s') \right])$</li>
<li>$(V^ * (s) = \max_a \sum_{s'} (s, a, s') \left[ R(s, a, s') + \gamma V^* (s') \right])$</li>
</ul>
</li>
<li>
<p>Time-limited value: 定义$(V_k(s))$ 为状态s下，最多k步下的最优value</p>
</li>
<li>Value iteration:<ol>
<li>$(V_0(s) = 0)$</li>
<li>$(V_{k+1}(s) \leftarrow \max_a \sum_{s'} (s, a, s') \left[ R(s, a, s') + \gamma V_k^* (s') \right]) )$</li>
<li>重复1-2直到收敛！</li>
<li>复杂度，每次迭代 $(O(S^2A))$</li>
<li>收敛到唯一最优值！贝尔曼算子在 $(\gamma&lt;1)$时时压缩算子，所以必收敛到不动点。</li>
</ol>
</li>
<li>Policy iteration<ol>
<li>随机化策略$(\pi)$</li>
<li>Policy evaluation：对给定的策略 $(\pi)$，利用迭代或者线性方程求解的方法计算该策略下的值函数，即求解下面第一个方程。因为该方程是一个关于值函数的线性方程，所以对于有限状态的情况可以直接求解线性方程，或者利用迭代求解。</li>
<li>Policy improvement：对上述值函数，利用贝尔曼方程求出最优策略。重复2-3多次直到收敛，收敛条件是策略不改变了。<br />
   实际上，它是在交替迭代策略和值函数。这种方法可以保证每次迭代值函数单调不减，又因为有界所以收敛。</li>
</ol>
</li>
</ul>
<p>$$<br />
V^{\pi_i}(s) = r(s, \pi_i(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi_i(s)) V^{\pi_i}(s') \\<br />
\pi_{i+1}(s) = \arg \max_a r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a) V^{\pi_i}(s')<br />
$$</p>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>仍然是一个MDP过程，仍然寻找最优决策！<br />
但是不知道状态转移函数T 和 回报函数 R！</p>
<ul>
<li>MDP：Offline</li>
<li>RL：Online</li>
</ul>
<h3 id="model-based-learning">model-based Learning</h3>
<p>通过经验，学习一个近似模型，然后求解！</p>
<ul>
<li>Step1：学习经验MDP模型：<ol>
<li>对每一个个(s, a)统计s'</li>
<li>归一化得到$(\hat{T}(s, a, s'))$<br />
3.</li>
</ol>
</li>
</ul>
<h2 id="tutorialicml2016">深度强化学习tutorial@ICML2016</h2>
<p>AI = RL + DL</p>
<ul>
<li>值函数</li>
<li>策略</li>
<li>状态转移模型</li>
</ul>
<p>使用深度学习建模值函数，策略和模型！</p>
<h3 id="value-based-deep-rl">Value-Based Deep RL</h3>
<p>Q-Networks</p>
<p>$$<br />
Q(s, a, w) \approx Q^* (s, a)<br />
$$</p>
<h3 id="q-learning">Q-Learning：</h3>
<p>最优 Q-value 应该遵循 Bellman 方程</p>
<p>$$<br />
Q^ * (s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q(s', a')^ * |s, a  \right]<br />
$$</p>
<p>其中 s 表示状态，a 表示agent对环境做出的 action！<br />
将方程右边当做目标，用神经网络学习！<br />
损失函数：</p>
<p>$$<br />
l = (r + \gamma \max_{a'} Q(s', a', w) - Q(s, a, w) )^2<br />
$$</p>
<p>问题：1. 训练样本不是 iid； 2. 目标不稳定！</p>
<h3 id="dqn-agent">DQN：利用 agent 自身经验构建样本！</h3>
<p>$$<br />
l = \left(r + \gamma \max_{a'} Q(s', a', w^-) - Q(s, a, w) \right)^2<br />
$$</p>
<p>在某一次replay 的更新中，$(w^-)$是固定的！replay结束后，将线上的权值$(w)$更新到$(w^-)$</p>
<h3 id="double-dqn">Double DQN</h3>
<ul>
<li>当前的 Q-network w 用来选择 action</li>
<li>老的 Q-network w- 用来评估 action</li>
</ul>
<p>$$<br />
a^* = \arg\max_{a'} Q(s', a', w) \\<br />
l = \left( r + \gamma Q( s', a^ * , w^- ) - Q(s, a, w)  \right)^2<br />
$$</p>
<h3 id="prioritised-replay">Prioritised replay</h3>
<p>按照 TD-error 对 replay memory 中的样本进行 importance sampling。</p>
<p>$$<br />
\delta = \left| r + \gamma \max_{a'} Q(s', a', w^-)  - Q(s, a, w) \right| \\<br />
P(i) = \frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}} \\<br />
p_i = \delta_i + \epsilon<br />
$$</p>
<h3 id="duelling-network">Duelling network</h3>
<p>将 Q 函数分解为状态值函数与 advantage function（不知道怎么翻译） 之和。</p>
<p>$$<br />
Q(s,a; \theta, \alpha, \beta) = V(s; \theta, \alpha) + A(s,a; \theta, \alpha)<br />
$$</p>
<p>上式V和A之间是不定的，可以相差一个任意常数，不影响结果。为此，有两种解决方案，减最大值和平均值。<br />
平均值方案更加稳定，因为V只需要跟踪平均波动，而不是最大波动。</p>
<p>$$<br />
Q(s,a; \theta, \alpha, \beta) = V(s; \theta, \alpha) + \left( A(s,a; \theta, \alpha) - \max_{a'\in \mathcal{A}} A(s,a'; \theta, \alpha) \right)  \\<br />
Q(s,a; \theta, \alpha, \beta) = V(s; \theta, \alpha) + \left( A(s,a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s,a'; \theta, \alpha) \right)<br />
$$</p>
<p><img alt="Duelling network" src="/wiki/static/images/duelling-net.png" /></p>
<h3 id="deep-policy-networks">Deep Policy Networks</h3>
<p>用神经网络建模策略函数</p>
<p>$$<br />
a = \pi(a| s, \mathbf{u}) = \pi( s, \mathbf{u})<br />
$$</p>
<p>目标函数为total discounted reward</p>
<p>$$<br />
J(u) = \mathbf{E}(r_1 + \gamma r_2 + \gamma^2 r_3 + ... | \pi(., u))<br />
$$</p>
<p>令$(\tao = (s_1, a_1, ..., s_t, a_t))$代表状态-动作路径，用$(r(\tao))$代表每个路径的折扣reward，那么期望回报函数</p>
<p>$$<br />
J(\theta) = \int \pi_{\theta}(\tao) r(\tao) d\tao<br />
$$</p>
<p>对参数$(\theta)$求导，由于回报函数与参数无关，所以梯度只作用与策略函数</p>
<p>$$<br />
\nabla_{\theta} J(\theta) = \int \nabla_{\theta} \pi_{\theta}(\tao) r(\tao) d\tao =\int  \pi_{\theta}(\tao) \nabla_{\theta} \log \pi_{\theta}(\tao) r(\tao) d\tao = E_{\tao ~ \pi_{\theta}(\tao)} \nabla_{\theta} \pi_{\theta}(\tao) r(\tao)<br />
$$</p>
<p>利用马尔科夫性，</p>
<p>$$<br />
\log \pi_{\theta}(\tao) = \log p(s_1) + \sum_{t=1}^T \left[\log \pi_{\theta}(a_t|s_t) + \log p(s_{t+1}|s_t, a_t) \right]<br />
$$</p>
<p>代入上式可得</p>
<p>$$<br />
\nabla_{\theta} J(\theta) = E_{\tao ~ \pi_{\theta}(\tao)} \sum_{t=1}^T  \nabla_{\theta}\log \pi_{\theta}(a_t|s_t)  r(\tao)<br />
$$</p>
<p>如果把策略函数看做在状态s下选择动作a的概率，回报是该样本的权重！即加权极大似然估计！</p>
<h2 id="_2">相关资料</h2>
<ul>
<li>强化学习书籍：<a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html</a></li>
<li>Tutorial: Deep Reinforcement Learning， ICML 2016. David Silver, Google Deepmind.</li>
<li>CS234: Reinforcement Learning <a href="http://web.stanford.edu/class/cs234/index.html">http://web.stanford.edu/class/cs234/index.html</a></li>
<li>Berkeley 课程：CS 294: Deep Reinforcement Learning. <a href="http://rll.berkeley.edu/deeprlcourse/">http://rll.berkeley.edu/deeprlcourse/</a></li>
<li><a href="http://ai.berkeley.edu/course_schedule.html">http://ai.berkeley.edu/course_schedule.html</a></li>
</ul>
</div>
<div id="income">
    <img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-09-04 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '强化学习',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>