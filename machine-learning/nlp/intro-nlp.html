<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>自然语言处理入门 - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;<a href="/wiki/#-nlp">nlp</a>&nbsp;»&nbsp;自然语言处理入门</div>
</div>
<div class="clearfix"></div>
<div id="title">自然语言处理入门</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#1">第1章 新手上路</a></li>
<li><a href="#2">第2章 词典分词</a></li>
<li><a href="#3">第3章 二元语法与中文分词</a></li>
<li><a href="#4">第4章 隐马尔科夫模型与序列标注</a><ul>
<li><a href="#_2">样本生成问题</a></li>
<li><a href="#_3">模型训练</a></li>
<li><a href="#_4">模型预测</a></li>
<li><a href="#hmm">HMM应用于中文分词</a></li>
<li><a href="#hmm_1">二阶HMM</a></li>
</ul>
</li>
<li><a href="#5">第5章 感知机分类与序列标注</a><ul>
<li><a href="#_5">基于结构化感知机的中文分词</a></li>
</ul>
</li>
<li><a href="#6">第6章 条件随机场与序列标注</a></li>
<li><a href="#7">第7章 词性标注</a></li>
<li><a href="#8">第8章 命名实体识别</a></li>
<li><a href="#faq">FAQ</a><ul>
<li><a href="#_6">为什么逆向最长匹配通常好于正向最长匹配</a></li>
<li><a href="#_7">为什么字典树相比普通的二叉树要快</a></li>
<li><a href="#_8">为什么二元语言模型要求切分的最小粒度要在词典中</a></li>
<li><a href="#_9">二元语法解决的根本问题是什么</a></li>
<li><a href="#hmmunk">HMM中对UNK的发射概率是如何处理的</a></li>
<li><a href="#_10">结构化维特比译码算法模型参数如何训练</a></li>
<li><a href="#_11">感知机模型如何跟词典融合</a></li>
<li><a href="#hmmcrf">HMM为什么不如结构化感知机和CRF</a></li>
<li><a href="#crfcrf">结构化感知机和CRF的区别是什么？为什么CRF效果更好？</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">关于</h1>
<ul>
<li>《自然语言处理入门》何晗，HanLP作者</li>
<li>主要内容<ul>
<li>中文分词</li>
<li>序列标注与分词</li>
<li>CRF</li>
<li>词性标注</li>
<li>命名实体识别</li>
<li>信息抽取</li>
<li>文本聚类</li>
<li>文本分类</li>
<li>依存句法分析</li>
<li>深度学习与NLP</li>
</ul>
</li>
<li>HanLP主要代码是通过Java实现的，方便集成到线上环境；同时支持python，其实底层还是通过python来调用Java包</li>
</ul>
<h1 id="1">第1章 新手上路</h1>
<ul>
<li>基于规则的专家系统<ul>
<li>波特词干算法，由一些列规则组成，规则有优先级之分</li>
</ul>
</li>
<li>基于统计的学习方法</li>
<li>语料库<ul>
<li>中文分词：人名日报</li>
</ul>
</li>
</ul>
<h1 id="2">第2章 词典分词</h1>
<ul>
<li>词典：HanLP自带了千万级词典，同时提供mini版</li>
<li>切分算法<ul>
<li>完全切分，即遍历所有连续字序列，跟词典中的词进行匹配。输出所有可能的词</li>
<li>正向最长匹配：即认为越长的词优先级越高，优先匹配长词。从前往后扫叫做正向最长匹配，从后往前扫叫逆向最长匹配</li>
<li>逆向最长匹配：总的而言，逆向最长匹配效果好于正向，孙茂松教授论文</li>
<li>双向最长匹配：汉语中单字词数量远小于非单字词<ol>
<li>同时执行正向和逆向最长匹配，若两者词数不一样，选择词数更好的那一个</li>
<li>否则返回单字最少的那个。</li>
<li>其他情况返回你想最长匹配</li>
</ol>
</li>
<li>性能：<ul>
<li>Python set 比用java 的treemap慢</li>
<li>正向和逆向性能差不多，但是java的不好说，可能因为垃圾回收机制，正向快一些？</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">fully_segment</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">dic</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    text: 文本序列</span>
<span class="sd">    dic: set 集合，包含词典中所有的词    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">word_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">j</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dic</span><span class="p">:</span>
                <span class="n">word_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">word_list</span>
<span class="k">def</span> <span class="nf">forward_segment</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">dic</span><span class="p">):</span>
  <span class="n">word_list</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
          <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
              <span class="n">word</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">j</span><span class="p">]</span>
              <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dic</span><span class="p">:</span>
                  <span class="n">word_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">word_list</span>
</pre></div>


<ul>
<li>字典树 trie 树，前缀树；避免了前缀的重复比较，比简单的二叉树匹配要快<ul>
<li>由于子节点较多，所以孩子节点集合要用字典或者map来实现</li>
</ul>
</li>
<li>首字散列其余二分的字典树：在逆向和双向匹配是，大约比Java的treemap快一倍的样子<ul>
<li>首字：用字符散列函数来查询孩子，子节点个数很大，例如65535</li>
<li>其余的节点，通过二分查找来查询孩子</li>
</ul>
</li>
<li>前缀树的妙用：优化到1000万字每秒<ul>
<li>出发点，如果「自然」不在词典中，那么所有以「自然」开头的词都不在词典中。比普通实现的全切分可以快5-7倍</li>
</ul>
</li>
<li>
<p>双数组字典树：略，进一步加速2-3倍</p>
</li>
<li>
<p>中文分词的评估</p>
<ol>
<li>将切分区间标记为起止下标[i,j]</li>
<li>统计预测的区间集合A跟标注样本的区间集合B的交集，计算precision和recall<br />
$$<br />
P = \frac{|A \cap B|}{|A|} \\<br />
R = \frac{|A \cap B|}{|B|}<br />
$$</li>
</ol>
</li>
<li>评估指标<ul>
<li>P，R，F1</li>
<li>Roov，即不在词典内的词召回率</li>
<li>Riv，即在词典内的词召回率，因为存在歧义的问题，所以无法做到100%</li>
</ul>
</li>
<li>
<p>最长匹配算法，在MSR语料上的P为89%，R为95%，F192%，Roov2.58%,Riv97% </p>
</li>
<li>
<p>字典树的其他应用</p>
<ul>
<li>停用词过滤，敏感词过滤</li>
<li>繁简体转化，需要按词进行转换</li>
<li>拼音转换，需要按词进行转换</li>
</ul>
</li>
</ul>
<h1 id="3">第3章 二元语法与中文分词</h1>
<ul>
<li>基本思想是，利用语言模型这个先验知识，来对分词的歧义情况消歧义。例如，可以解决「商品和服务」的两种分法「商品 和 服务」 「商品 和服 务」，前者的概率较高</li>
<li>语言模型通过以词为单位的二元语言模型来建模。举例<ol>
<li>p(商品 和 服务) = p(商品|BOS) p(和|商品) p(服务|和) p(EOS|服务)</li>
<li>p(商品 和服 务) = p(商品|BOS) p(和服|商品) p(务|和服) p(EOS|务)</li>
</ol>
</li>
<li>上述条件概率通过统计得到 p(w1 | w0) = #(w0 w1)/#(w0)</li>
<li>
<p>数据稀疏和平滑，高阶语言模型用低阶平滑，例如2元模型用1元平滑<br />
$$<br />
p(w1|w0) = \lambda p(w1|w0) + (1-\lambda) p(w1)<br />
$$</p>
</li>
<li>
<p>分词语料库：人民日报，MSR</p>
</li>
<li>分词的每一种切分（在这种方法中要求最小粒度要在词典中），都可以看做词图上的一条路径，所以分词就变成找到一条概率最大的路径。当用log处理概率后，就可以认为是一个最短路径问题。viterbi译码算法</li>
<li>与用户词典的融合<ul>
<li>低优先级，用户词典合并：例如，统计分词后结果「商品 和 服务 员」，如果「服务员」出现在用户词典中，那么会合并为「商品 和 服务员」</li>
<li>高优先级，干预词网生成，一元词频由用户提供，二元词频直接由一元词频伪造（搞成相同）。分出概率较高，一般只在一定要分出的场景下才用</li>
</ul>
</li>
<li>问题，OOV 新词；因为OOV和新词都不在词典中，所以新词无法召回</li>
</ul>
<h1 id="4">第4章 隐马尔科夫模型与序列标注</h1>
<ul>
<li>中文分词作为一种序列标注问题，每个字可以属于的类别<ul>
<li>B，词首</li>
<li>E，词尾</li>
<li>M，词中</li>
<li>S，单字成词</li>
</ul>
</li>
<li>隐马尔科夫模型<ul>
<li>状态：词的类别：BEMS</li>
<li>转移概率：即标注的类别转移概率矩阵A，可以通过事先从语料中统计出来</li>
<li>发射概率：从类别（隐状态）到字（观测值、显状态）的概率，也是可以从语料中统计出来</li>
</ul>
</li>
</ul>
<h2 id="_2">样本生成问题</h2>
<ul>
<li>医疗诊断<ol>
<li>初始化一个隐状态</li>
<li>根据发射矩阵采样一个观测状态</li>
<li>根据转移矩阵采样下一个隐状态</li>
<li>重复2-3</li>
</ol>
</li>
</ul>
<h2 id="_3">模型训练</h2>
<ul>
<li>估计三个概率：初始概率，转移概率，发射概率</li>
<li>极大似然估计：即简单的统计</li>
</ul>
<h2 id="_4">模型预测</h2>
<ul>
<li>维特比译码算法，回忆卷积码译码算法</li>
<li>最大化条件概率 P(观测序列|隐状态序列)，对给定的观测序列X，寻找隐序列Y最大概率 P(Y|X) = P(X, Y)/P(X)，也就是只要最大化联合概率 P(X, Y) 即可</li>
<li>算法的主要步骤<ol>
<li>从i=1到N，N是观测序列长度</li>
<li>用pj 表示最优隐状态序列是第i个个隐状态的联合概率，即 P([o1,o2,..,oi], [s1,s2,...,si])</li>
<li>那么有动态规划方程，即从第i个状态递归到第i+1个状态的递归方程。因此每一步只需要保留|S|个p和最优序列<br />
$$<br />
p_j^{i+1} = \max_k p_k A_{k, j} B_{j, o_i} <br />
$$</li>
</ol>
</li>
</ul>
<h2 id="hmm">HMM应用于中文分词</h2>
<ul>
<li>HanLP中的HMMSegmenter</li>
<li>对于非字典中的词统一映射到UNK</li>
<li>一阶HMM的F1值只有79%，不如词典分词，但Roov提高到41%</li>
</ul>
<h2 id="hmm_1">二阶HMM</h2>
<ul>
<li>转移概率从只依赖前一个tag，还依赖前二个tag；只有转移概率变了，发射概率不变</li>
<li>估计方式和预测逻辑基本类似，只需要改一下转移概率的计算方式</li>
<li>第二个时刻需要特殊处理，第二个时刻因为没有二阶转移概率，所以用的是一阶转移概率</li>
<li>效果：Roov相比一阶有少量提升，但是F1反而稍微下降。</li>
<li>结论：增加阶数，对提升效果不明显，无法改变F1值不如词典的问题</li>
</ul>
<h1 id="5">第5章 感知机分类与序列标注</h1>
<ul>
<li>感知机部分略，感知机的损失函数是 $(loss = - \hat{y} y = max(0, - y w^T x))$，没有间隔，SVM在感知机上加了个间隔参数</li>
<li>投票感知机和平均感知机：即多个模型融合和多模型只是将权重平均</li>
<li>基于感知机的人名性别分类：用字做特征</li>
<li>结构化预测问题：如序列预测，语法树。关键是对可能的结构进行打分（在分词中，对每一种标注序列进行打分）</li>
<li>预测，结构化维特比译码算法：将概率换成打分即可，打分等于前一步打分+新增的该步打分（模型建模得到）</li>
</ul>
<h2 id="_5">基于结构化感知机的中文分词</h2>
<ul>
<li>将分词当做多分类任务，依赖序列之前的信息和当前的信息</li>
<li>结构化感知机中的序列打分相当于联合概率<ul>
<li>score([x1,x2,...,xk+1], [y0,y1,y2,...,yk+1]) = score([x1,x2,...,xk], [y0,y1,y2,...,yk]) + score(xk+1, yk+1)</li>
<li>score(xk+1, yk+1) 用一个线性多分类模型来建模(用xk+1预测yk+1)，特征不仅仅包含当前位置的特征，还附近的字特征和上文的标签特征</li>
</ul>
</li>
<li>特征提取：<ul>
<li>转移特征：相当于转移概率矩阵的用途。前一个字的标签</li>
<li>状态特征：类似于发射矩阵的用户。当前字附近的1gram和2gram</li>
</ul>
</li>
<li>模型压缩：去掉权重很小的特征</li>
<li>模型的F1：96%，Roov：70%。集合了基于词典和HMM的优点，效果比二者都好。基本达到实用水平。</li>
<li>特征工程<ul>
<li>叠字，相邻两个字是否相等</li>
<li>四元语法，相邻4个字符为窗口的所有n-gram</li>
<li>词典特征，用户词典全切分后，当前字符位置最长词语的长度</li>
<li>偏旁部首，字的偏旁</li>
</ul>
</li>
</ul>
<h1 id="6">第6章 条件随机场与序列标注</h1>
<ul>
<li>性能比感知机强大</li>
<li>CRF，P(Y|X)</li>
<li>特征函数，$(f(y_{t-1}, y_t, [x_1, ...,x_t]))$</li>
<li>条件概率建模<br />
$$<br />
p(Y|X) = \frac{1}{Z(X)} \Pi \exp(\sum_k w_k f_k(y_{t-1}, y_t, X_t))<br />
$$</li>
<li>结构化感知机和CRF的打分函数完全相同</li>
<li>CRF的梯度是特征函数在经验分布上的期望与模型分布上的期望之差。当差变为0了，模型就拟合了经验数据（即训练完了）<br />
$$<br />
\frac{\partial l}{\partial w_k} = E_{\hat{p}}(f_k) - E_{w}(f_k)<br />
$$</li>
<li>
<p>上述期望是对所有的Y求（即所有可能的标签）和所有的X求（即所有样本）</p>
</li>
<li>
<p>对比结构化感知机</p>
<ul>
<li>相同点：<ul>
<li>特征函数相同</li>
<li>权重向量相同</li>
<li>打分函数相同</li>
<li>预测算法相同，都是用维特比译码。CRF的联合概率可以分解为多步的乘积</li>
<li>同属结构化学习</li>
</ul>
</li>
<li>不同点：即损失函数不同，本质上来讲，感知机损失相当于只有正样本，而CRF损失还加了负样本<ul>
<li>训练算法不同：感知机在线学习；CRF是批量学习。即使都使用在线学习也不一样</li>
<li>感知机：$(-\Delta w = \phi(x^i , y^i) - \phi(x^i, - \hat{y}))$。只惩罚当前这个分错的类别</li>
<li>CRF：$(-\Delta w = \phi(x^i , y^i) - E_w\phi(x^i , y) )$。期望是对所有可能的yi求的，所以是对所有的其他类别做惩罚</li>
</ul>
</li>
</ul>
</li>
<li>
<p>CRF工具包，CRF++ <a href="https://taku910.github.io/crfpp/">https://taku910.github.io/crfpp/</a></p>
</li>
<li>效果，超过结构化感知机，F1+0.1%，Roov+1%</li>
</ul>
<h1 id="7">第7章 词性标注</h1>
<ul>
<li>联合模型：需要分词和词性同时标注的语料，虽然效果好，但是语料难搞，特征倍数扩大。所以主流还是将分词作为上游任务（语料多），词性标注作为下游任务</li>
<li>语料：<ul>
<li>人民日报语料库与PKU标注集，43中词性</li>
<li>国家语委语料库与863标注集，20个一级，29个二级词性</li>
<li>诛仙语料库与CTB标注集</li>
</ul>
</li>
<li>标注准确率，HMM大约40-45%，CRF 80%+</li>
</ul>
<h1 id="8">第8章 命名实体识别</h1>
<ul>
<li>将词聚合成复合词，NER<ul>
<li>人名</li>
<li>机构名</li>
<li>商品名</li>
</ul>
</li>
<li>基于规则<ul>
<li>词典，</li>
</ul>
</li>
</ul>
<h1 id="faq">FAQ</h1>
<h2 id="_6">为什么逆向最长匹配通常好于正向最长匹配</h2>
<h2 id="_7">为什么字典树相比普通的二叉树要快</h2>
<ul>
<li>例如，比较「你好」，普通二叉树每个字都要比较多次；而字典树「你」只比较一次！</li>
</ul>
<h2 id="_8">为什么二元语言模型要求切分的最小粒度要在词典中</h2>
<ul>
<li>因为不在词典中，没法通过词频计算概率</li>
</ul>
<h2 id="_9">二元语法解决的根本问题是什么</h2>
<ul>
<li>利用语言模型，消除只用字典分词出现的切分的歧义</li>
<li>基本单元是词，在词典中的词</li>
<li>HMM处理的基本单元是字</li>
</ul>
<h2 id="hmmunk">HMM中对UNK的发射概率是如何处理的</h2>
<h2 id="_10">结构化维特比译码算法模型参数如何训练</h2>
<ul>
<li>直接当做监督学习的任务来学；只是译码的时候是，从左到右，利用维特比译码算法+打分函数进行预测</li>
</ul>
<h2 id="_11">感知机模型如何跟词典融合</h2>
<h2 id="hmmcrf">HMM为什么不如结构化感知机和CRF</h2>
<ul>
<li>因为HMM只建模了隐状态（即标签）的前后关联关系，而没有建模显状态（字）的前后关联关系</li>
<li>结构化感知机通过状态特征，建模字的前后关联关系（即语言模型）</li>
<li>CRF直接建模标签的前后关联，字的前后关联，字根标签的发射关系</li>
</ul>
<h2 id="crfcrf">结构化感知机和CRF的区别是什么？为什么CRF效果更好？</h2>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2020-03-01 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '自然语言处理入门',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2020 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>