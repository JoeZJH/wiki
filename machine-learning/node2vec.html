<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Node2Vec - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;Node2Vec</div>
</div>
<div class="clearfix"></div>
<div id="title">Node2Vec</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#_2">摘要</a></li>
<li><a href="#_3">特征学习框架</a><ul>
<li><a href="#_4">邻居节点的搜索策略</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>论文：node2vec: Scalable Feature Learning for Networks</p>
<h2 id="_2">摘要</h2>
<ul>
<li>将一个网络中的节点变成一个低维的连续向量，作为其他模型的输入特征。</li>
<li>通过最大化网络邻居的似然函数。</li>
<li>基于特征值分解的线性或非线性降维方法在实际的大规模数据应用中，计算太慢？并且性能还不好！</li>
<li>目标函数，保证邻居节点依然相近；保证具有相似结构的节点的嵌入向量也相近！</li>
</ul>
<p><img src="/wiki/static/images/node2vec1.png" style="float:left;width:300px;margin-right:10px;" /></p>
<ul>
<li>目标函数：maximize the likelihood of pre- serving network neighborhoods of nodes in a d-dimensional feature space。</li>
<li>2阶 random walk 方法产生节点的网络上的邻居样本。</li>
<li>node2vec可扩展到边</li>
<li>用学到的向量去做分类任务的特征，结果比其他方法好很多，并且这种方法很鲁棒！即使缺少边也没问题。</li>
<li>可扩展到大规模 node！</li>
<li>基于特征值分解的方法难以扩展到大规模？suffer from both computational and statistical performance drawbacks；不够鲁邦！不能</li>
</ul>
<h2 id="_3">特征学习框架</h2>
<ul>
<li>网络：$(G = (V, E))$</li>
<li>学习目标：$(f : V \rightarrow R^d)$，实际上是一个$(|V| \times d)$参数</li>
<li>采样策略$(S)$生成的节点$(u)$的网络邻居 $(N_S(u) \in V)$</li>
<li>极大似然估计：</li>
</ul>
<p>$$<br />
\max_f \sum_{u \in V} \log Pr(N_S(u)| f(u) )<br />
$$</p>
<ul>
<li>几个假设：<ul>
<li>条件独立：$( Pr(N_S(u)| u ) = \Pi_{n_i \in N_S(u)} Pr(n_i|f(u)) )$</li>
<li>对称性：特征空间中，两个互为邻居的边有对称效应：</li>
</ul>
</li>
</ul>
<p>$$<br />
Pr(n_i|f(u)) = \frac{\exp(f(n_i) \dot f(u))}{\sum_{v\in V} f(v) \dot f(u)}<br />
$$</p>
<p>和 word2vec 一样，可以通过负采样来优化分母的计算量！</p>
<ul>
<li>最大的问题是对领居节点的采样，skip-gram是通过一个固定宽度的滑动窗，网络由于不是线性的，比较麻烦。不一定是直接邻居可以当邻居，这取决于采样策略 $(S)$</li>
</ul>
<h3 id="_4">邻居节点的搜索策略</h3>
<ul>
<li>经典的搜索策略：<ul>
<li>BFS：宽度优先搜索，找直接相连的节点</li>
<li>DFS：深度优先搜索</li>
</ul>
</li>
<li>node2vec 的搜索策略综合了这两种策略</li>
</ul>
<p><img src="/wiki/static/images/node2vec2.png" style="float:left;width:300px;margin-right:10px;" /></p>
<ul>
<li>对于源节点$(u)$，通过 random walk (马尔科夫链) 采样 l 长度的邻居节点 $(c_i, c_0=u)$</li>
<li>条件概率为：</li>
</ul>
<p>$$<br />
P(c_i=x|c_{i-1}=v) = \begin{cases}<br />
    \frac{\pi_{vx}}{Z}, if (v,x) \in E. \\<br />
    0, otherwise<br />
\end{cases}<br />
$$</p>
<p>其中$(\pi_{vx})$是未归一化的概率，Z是归一化常数。对于最简单的情况，可以用边的权重作为未归一化概率<br />
$(\pi_{vx} = w_{vx})$。</p>
<p>对于2阶 random walk，未归一化概率和权重之间的关系为：$(\pi_{vx} = \alpha_{pq}(t,x)w_{vx})$<br />
t是上一个节点，v是当前节点，x是下一个可能的节点，系数</p>
<p>$$<br />
\alpha_{pq}(t, x) = \begin{cases}<br />
            \frac{1}{p}, d_{tx} = 0, \\<br />
            1, d_{tx}=1,\\<br />
            \frac{1}{q}, d_{tx}=2.<br />
\end{cases}<br />
$$</p>
<p>$(d_{tx})$是两个节点的距离，p是return参数，q是in-out参数。</p>
<ul>
<li>random walk 的好处：<ol>
<li>可以减少邻居的存储空间到$(O(a^2|V|))$.本来是$(O(E))$。</li>
<li>每一次产生的链可以复用，因为马尔科夫性。</li>
</ol>
</li>
</ul>
</div>
<div id="content-footer">created in <span class="create-date date"> 2017-03-21 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: 'Node2Vec',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2017 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>

    </body>
</html>