<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Explore Exploit 探索与利用 - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;<a href="/wiki/#-reinforcement-learning">reinforcement-learning</a>&nbsp;»&nbsp;Explore Exploit 探索与利用</div>
</div>
<div class="clearfix"></div>
<div id="title">Explore Exploit 探索与利用</div>
<div id="content">
  <p>Explore 和 Exploit 是做推荐的常用问题,由于推荐的点击率/转化率模型是由历史数据产生的,对新的item点击率估计误差会很大,需要对曝光不足的item进行探索。</p>
<p>考虑多臂老虎机问题,即有N个老虎机,每个臂有概率 Pi 会有奖励,并且这个概率不随时间变化。</p>
<p>显然,最优策略是选择概率 Pi 最大的那个臂不停地摇,每次期望回报最大 $(r^* = \max_i p_i)$,<br />
而由于一开始不知道 Pi ,所以实际第t步的回报期望可能小于最大值。定义 $(r_t)$为第t步的回报,如果有奖励,回报为1,否则回报为0.<br />
其期望值等于摇的那个臂对应的 Pi 的值。</p>
<p>可以定义 regret 为他们的差 $(regret_t = r^* - E r_t)$。</p>
<div class="hlcode"><pre><span></span><span class="c1">## 多臂老虎机仿真</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 10个臂</span>
<span class="n">bandit_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gen_data</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">bandit_p</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="k">return</span> <span class="mf">1.0</span>  <span class="c1"># reward</span>
    <span class="k">return</span> <span class="mf">0.0</span> <span class="c1"># no reward</span>
</pre></div>


<p>在这个问题中,一般有这些经典的探索方法:</p>
<ul>
<li>随机策略:随机选择一个动作,简称瞎猜,每一个动作都有概率。所以 $( regret = \max_i p_i - \bar{p_i} )$ </li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">random_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">])</span>
</pre></div>


<ul>
<li>确定性策略:每次都选择当前最优的动作,如果最优动作很多个,随机选一个</li>
</ul>
<div class="hlcode"><pre><span></span><span class="c1"># 稍作修改,前n次依次选择每一个动作</span>
<span class="k">def</span> <span class="nf">determine_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_reward_action&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_action&#39;</span><span class="p">]))</span>
</pre></div>


<ul>
<li>$(\epsilon)$-贪心策略:即以 $(\epsilon)$ 的概率选择随机策略,以 $(1 - \epsilon)$选择确定性策略, $(\epsilon)$ 随着时间衰减</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span> <span class="c1"># random</span>
        <span class="k">return</span> <span class="n">random_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">determine_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</pre></div>


<ul>
<li>naive 策略: 前N步随机探索, 从第 N+1 步开始,采用确定性策略</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">naive_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">random_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">determine_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</pre></div>


<ul>
<li>softmax 随机策略:即对每一个动作的累积平均回报做softmax归一化,作为每个动作的选择概率</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">softmax_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">n0</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
    <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_reward_action&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_action&#39;</span><span class="p">])</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">avg_reward</span> <span class="o">/</span> <span class="n">n0</span> <span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<ul>
<li>thompson 采样: 认为每个动作的Pi 服从Beta(a, b)先验分布, 而每个动作的历史平均回报后验分布则服从 Beta(a + win, b + loss) 分布,确定性策略相当于按照后验分布的均值/众数 选择最优策略,然而前期由于探索不充分,这种估计方差很大,必然要引入随机性进行探索,Thompson的方法是从后验分布中采样一个值代替对Pi的估计,然后按照确定性策略选择动作。</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">thompson_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="n">win</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_reward_action&#39;</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_action&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">win</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># 期望概率 p_i = (1 + win[i]) / (1 + trials[i]), 现在不是用期望的概率,而是引入一定的随机性,随机从p_i服从的后验beta分布中采样一个结果</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">win</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">loss</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>


<ul>
<li>UCB方法: 因为利用历史平均回报对Pi估计不准,所以可以用估计的置信区间的上界 $(\bar{r_i} + \sqrt{\frac{2 ln T}{n_i}})$ 作为对Pi的乐观估计。<br />
   然后采用确定性策略。其中 T 是总的步数, n_i 是第i个臂被选择的次数。</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">ucb_select</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
    <span class="n">cum_n</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span>
    <span class="n">up_bound</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_reward_action&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_action&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">cum_n</span><span class="p">)</span> <span class="o">/</span> <span class="n">ctx</span><span class="p">[</span><span class="s1">&#39;cum_action&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">up_bound</span><span class="p">)</span>
</pre></div>


<ul>
<li>仿真代码</li>
</ul>
<div class="hlcode"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">simulation</span><span class="p">(</span><span class="n">select_action</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cum_reward_per</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">cum_reward_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">cum_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_</span><span class="p">):</span>
        <span class="n">t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n&#39;</span> <span class="p">:</span> <span class="n">n</span><span class="p">,</span> <span class="s1">&#39;cum_reward_action&#39;</span> <span class="p">:</span> <span class="n">cum_reward_action</span><span class="p">,</span> <span class="s1">&#39;cum_action&#39;</span> <span class="p">:</span> <span class="n">cum_action</span><span class="p">,</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
        <span class="n">cum_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">gen_data</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">action_n</span> <span class="o">=</span> <span class="n">cum_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">cum_reward_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">cum_reward_per</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cum_reward_per</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">select_action</span><span class="p">,</span> <span class="n">rnd_</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rnd_</span><span class="p">):</span>
        <span class="n">t</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">simulation</span><span class="p">(</span><span class="n">select_action</span><span class="p">,</span> <span class="n">n_</span><span class="p">)</span>
        <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">avg_reward</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;simulate </span><span class="si">%d</span><span class="s2"> round.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">bandit_p</span><span class="p">)</span> <span class="o">-</span> <span class="n">avg_reward</span> <span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">n_</span> <span class="o">=</span> <span class="mi">2000</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">random_select</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">epsilon_greedy</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">naive_select</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">thompson_select</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">ucb_select</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">softmax_select</span><span class="p">,</span> <span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>


    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon greedy&#39;</span><span class="p">,</span> <span class="s1">&#39;naive&#39;</span><span class="p">,</span> <span class="s1">&#39;thompson select&#39;</span><span class="p">,</span> <span class="s1">&#39;ucb select&#39;</span><span class="p">,</span> <span class="s1">&#39;softmax select&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;regret&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>


<p><img alt="ee simulation" src="/wiki/static/images/ee-simulation.png" /></p>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2018-12-17 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'Explore Exploit 探索与利用',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2020 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>