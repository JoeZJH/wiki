<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>MaxOut 网络 - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;MaxOut 网络</div>
</div>
<div class="clearfix"></div>
<div id="title">MaxOut 网络</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#maxout-networks">Maxout networks 论文导读</a><ul>
<li><a href="#_2">导言</a></li>
<li><a href="#review-of-dropout">Review of Dropout</a></li>
<li><a href="#maxout">Maxout 模型</a></li>
<li><a href="#_3">效果</a></li>
<li><a href="#keras">Keras 实现</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>2013年， Ian Goodfellow, Yoshua Bengio 提出maxout网络，号称只需要两个隐层，就可以逼近任意分片连续函数。</p>
<h2 id="maxout-networks">Maxout networks 论文导读</h2>
<p>论文地址 <a href="http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf">maxout networks</a></p>
<h3 id="_2">导言</h3>
<p>Dropout 是 Hinton 2012 年提出的一种简单的方法， 模型平均？！</p>
<blockquote>
<p>We argue that rather than using dropout as a slight<br />
performance enhancement applied to arbitrary models,<br />
the best performance may be obtained by directly<br />
designing a model that enhances dropout’s abilities as<br />
a model averaging technique</p>
</blockquote>
<h3 id="review-of-dropout">Review of Dropout</h3>
<p>对于给定的输入向量$(v)$，预测输出$(y)$，一系列的隐层 $(\vec{h} = { h^{(1)}, ..., h^{(L)} })$。<br />
Dropout 用变量$(v, \vec{h})$ 的一部分变量，训练一组模型（这是随机置0的高级理解！！？）。<br />
这些模型采用同一个模型参数 $(\theta \in \mathcal{M})$，和一个二元 mask 变量 $(\mu)$。<br />
对于每一个样本，对不同的随机参数$(\mu)$，我们通过$(log p(y|v; \theta, \mu))$ 的梯度训练一个<br />
子模型。</p>
<p>Dropout 很像 bagging 的方法，在数据集的很多子集上训练一组模型。<br />
不同的是， Dropout 每一个模型只训练一步，并且不同模型共享一个参数。<br />
因此，在训练的时候，每一步需要比较大的影响（把步长调大？）。</p>
<h3 id="maxout">Maxout 模型</h3>
<p>Maxout 模型也是一个多层的前馈结构，它的基本单元是 Maxout unit。<br />
对给定的 $(x \in \mathbb{R}^d)$，一个 maxout 隐层为：</p>
<p>$$<br />
h_i(x) = \max_{j \in [1, k]} z_{i,j} \\<br />
z_{i, j} = x^T W_{.ij} + b_{ij}, W \in \mathbb{R}^{d \times m \times k}, b \in \mathbb{R}^{m\times k}<br />
$$</p>
<p>相当于$(k)$个常规的纺射变换（一个线性变化$(W)$ 加上平移 $(b)$），然后取每一个模型的最大值作为最终的输出。</p>
<p>单个 Maxout 单元可以理解为，对凸的分片连续函数的近似。</p>
<ul>
<li>Stone-Weierstrass 函数逼近理论说，紧集（闭区间的推广）上的连续函数可以用连续的分片线性函数(PWL) <strong>一致逼近</strong>。</li>
<li>Wang 2004 的理论表明，任意连续分片线性函数可以表达为两个凸的分片线性函数之差。</li>
</ul>
<p>个人总结：Maxout 模型很简单，理论也容易理解，就是要逼近效果好，需要$(k)$值足够大，这将会导致参数个数随$(k)$增大，而线性增长。</p>
<h3 id="_3">效果</h3>
<ul>
<li>MNIST 手写数字数据集两层 Conv. maxout + dropout 得到最佳效果 0.45%</li>
</ul>
<h3 id="keras">Keras 实现</h3>
<p><a href="https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L807">https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L807</a></p>
<div class="hlcode"><pre><span></span><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># no activation, this layer is only linear.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-08-04 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'MaxOut 网络',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2017 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>