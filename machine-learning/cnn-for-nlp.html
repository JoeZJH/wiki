<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Convolutional Neural Networks for NLP - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;Convolutional Neural Networks for NLP</div>
</div>
<div class="clearfix"></div>
<div id="title">Convolutional Neural Networks for NLP</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#cnn">字母级别的CNN</a><ul>
<li><a href="#_2">模型</a></li>
<li><a href="#_3">数据</a></li>
<li><a href="#_4">设计细节</a></li>
<li><a href="#_5">结论</a></li>
</ul>
</li>
<li><a href="#very-deep-convolutional-networks-for-natural-language-processing">Very Deep Convolutional Networks for Natural Language Processing</a><ul>
<li><a href="#_6">结论</a></li>
</ul>
</li>
<li><a href="#cnn_1">CNN 句子建模</a></li>
<li><a href="#cnn_2">CNN句子分类</a></li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>长期以来，RNN、LSTM及其变种模型被应用到自然语言处理方面。<br />
近年来，将CNN应用到自然处理方面也有一些工作。</p>
<h2 id="cnn">字母级别的CNN</h2>
<p>主要论文：Character-level Convolutional Networks for Text Classification, <strong>Xiang Zhang</strong>, Junbo Zhao, <strong>Yann LeCun</strong>, 2016</p>
<p>目前文本分类研究已从设计好的特征转为选择好的分类模型。<br />
目前，所有的文本分类技术都是以词为基本单位的，简单统计词和词的n-gram就可以做到最好的效果。</p>
<p>T. Joachims. Text categorization with suport vector machines: Learning with many relevant features. In<br />
Proceedings of the 10th European Conference on Machine Learning, pages 137–142. Springer-Verlag,<br />
1998.</p>
<p>卷积网络很适合从raw signals中提取有用的特征，已在机器视觉和语音识别等任务中得到应用。<br />
而实际上，time-delay networks 早在深度学习出来以前就将卷积网络应用到序列数据之上。</p>
<ol>
<li>L. Bottou, F. Fogelman Soulie, P. Blanchet, and J. Lienard. Experiments with time delay networks and ´<br />
dynamic time warping for speaker independent isolated digit recognition. In Proceedings of EuroSpeech<br />
89, volume 2, pages 537–540, Paris, France, 1989.</li>
<li>R. Johnson and <strong>T. Zhang</strong>. Effective use of word order for text categorization with convolutional neural<br />
networks. CoRR, abs/1412.1058, 2014.</li>
</ol>
<p>在这篇文章中，将文本当做字符为单位的序列数据，然后应用时间卷积网络(temporal (one-dimensional) ConvNets)。</p>
<p>卷积网络应用到文本和自然语言处理已有一些研究了，它既可以应用到连续值的embedding数据，也可以应用到离散值的embedding数据，<br />
并不需要任何语法和语义信息！其结果也和经典的方法具有可比性！</p>
<ol>
<li>C. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In<br />
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical<br />
Papers, pages 69–78, Dublin, Ireland, August 2014. Dublin City University and Association for<br />
Computational Linguistics.</li>
<li>Y. Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference<br />
on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar,<br />
October 2014. Association for Computational Linguistics.</li>
<li>R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional neural<br />
networks. CoRR, abs/1412.1058, 2014</li>
</ol>
<p>使用字母级别特征来做NLP也有一些早起工作，能够在POS tagging和IR方面的提升。This article is the first to apply ConvNets only on characters.<br />
可以简化特征工程，能够学到拼写错误和emoji符号。</p>
<ol>
<li>character-level n-grams with linear classifiers: I. Kanaris, K. Kanaris, I. Houvardas, and E. Stamatatos. Words versus character n-grams for anti-spam filtering. International Journal on Artificial Intelligence Tools, 16(06):1047–1067, 2007</li>
<li>incorporating character-level features to ConvNets: C. D. Santos and B. Zadrozny. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818–1826, 2014</li>
<li>Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101–110. ACM, 2014.</li>
</ol>
<h3 id="_2">模型</h3>
<p>采用一位的卷积和max-pooling！</p>
<ul>
<li>对 Pooling 的分析文章：Y.-L. Boureau, J. Ponce, and <strong>Y. LeCun</strong>. A theoretical analysis of feature pooling in visual recognition.<br />
In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 111–118, 2010</li>
<li>ReLU 最早文章：V. Nair and <strong>G. E. Hinton</strong>. Rectified linear units improve restricted boltzmann machines. In Proceedings<br />
of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010</li>
</ul>
<h3 id="_3">数据</h3>
<p>字母通过one-hot编码为70维向量，包括26个字母，10个数字，33个其他字符和换行符。字符包括：</p>
<div class="hlcode"><pre><span></span>abcdefghijklmnopqrstuvwxyz0123456789
-,;.!?:’’’/\|_@#$%ˆ&amp;*˜‘+-=&lt;&gt;()[]{}
</pre></div>


<p>对中文的处理，将中文转换为拼音 pypingyin。</p>
<h3 id="_4">设计细节</h3>
<p>模型设计</p>
<p>6层卷积层+3层全连接层，kernel维度为7和3，pool维度为3。</p>
<p>数据增强</p>
<p>用近义词进行替换，增加样本！平移不适应于这里！</p>
<p>和传统方法比较：</p>
<ul>
<li>Bag of word with TFIDF</li>
<li>Bag of n-gram with TFIDF</li>
<li>Bag of means with word embedding</li>
</ul>
<p>深度学习方法比较：</p>
<ul>
<li>word-based CNN:</li>
<li>LSTM:</li>
</ul>
<h3 id="_5">结论</h3>
<p>数据集达到百万量级才能观察到这种方法的优势，数据集小的时候还是 n-gram with TFIDF 好</p>
<blockquote>
<p><strong>There is no free lunch</strong>. Our experiments once again verifies that there is not a single machine<br />
learning model that can work for all kinds of datasets. The factors discussed in this section could all<br />
play a role in deciding which method is the best for some specific application.</p>
</blockquote>
<h2 id="very-deep-convolutional-networks-for-natural-language-processing">Very Deep Convolutional Networks for Natural Language Processing</h2>
<p>论文： Very Deep Convolutional Networks for Natural Language Processing, <strong>Le Cun</strong>, 2016</p>
<p>目前RNN,LSTM,CNN应用到NLP中的深度和在CV中相比，还比较浅，这篇文章提出一种方案可以从字母级别开始学习，模型深度达到29层！</p>
<p>CNN：将特征提取和分类进行联合训练！除了自动特征提取之外，还可以根据具体任务调整特征！</p>
<p>目前主流的方法，是利用 word embedding + RNN(LSTM)。</p>
<ol>
<li>Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. LSTM neural networks for language<br />
modeling. In Interspeech, 2012.</li>
<li>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural<br />
networks. In NIPS, pages 3104–3112, 2014.</li>
</ol>
<p>作者argue：</p>
<ul>
<li>作者认为 LSTM 是一种一般的序列学习方法，缺乏领域特性 "lacking task specific structure"</li>
<li>单词按照顺序进入，第一个单词变换了很多次，而最后一个词只变换一次！ =&gt; bidirectional LSTM</li>
<li>深度不够，超过4层就没啥提升了，尽管加入了 dropout 正则化！</li>
</ul>
<p>观点：</p>
<blockquote>
<p>We believe that <strong>the challenge in NLP</strong> is to develop deep architectures which are able to learn hierarchical<br />
representations of whole sentences, jointly with the task.</p>
</blockquote>
<p>recursive neural network : 在RNN上增加了序列融合的顺序结构（树结构），RNN可以看做一个特殊的 recursive NN.</p>
<ol>
<li>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.<br />
Semi-supervised recursive autoencoders for predicting sentiment distributions. In<br />
EMNLP, 2011.</li>
</ol>
<p>模型结构：</p>
<p><img src="/wiki/static/images/deep-cnn-for-text.png" alt="模型结构"  style="float:left; width:300px;" /></p>
<p>s 是时间窗长度，首先将字符embedding到16维的向量！<br />
第一层64个特征，后续是ConvNet Block，采用下述策略（from VGG and ResNet）：</p>
<ol>
<li>如果时间分辨率不变，输入和输出特征维度相同</li>
<li>如果时间分辨率减半，输出特征维度加倍</li>
</ol>
<p>更多的卷积层，意味着能够学习更长的依赖关系！并且，对所有的时间几乎是平等的！而不像RNN，LSTM那样！<br />
其中一个CNN Block 结构如图Fig.2。包含两个维度为3的核的卷积层，每个卷积层后跟一个BN层和一个非线性层！<br />
多个小尺寸的卷积层可以用较少的参数实现一个大尺寸的卷积层相同的功能（视野和非线性度）！</p>
<p><img src="/wiki/static/images/deep-cnn-for-text-cnnblock.png" alt="模型结构"  style="float:right; width:250px;"/></p>
<p>输入字符增加了一个表示未知符号的特殊字符，一共72个token。输入文本padding到长度为1014！字符embedding到16维的向量。<br />
其他参数：</p>
<ul>
<li>mini-batch of size 128</li>
<li>initial learning rate of 0.01</li>
<li>momentum of 0.9</li>
<li>每次验证集错误增加就将学习率减半</li>
<li>初始化采用 何凯明 的方案</li>
<li>采用 BN 而没有dropout</li>
</ul>
<h3 id="_6">结论</h3>
<ol>
<li>在大数据集上有明显提升，即使深度较小</li>
<li>深度可以提升效果！</li>
<li>Max-pooling 最优</li>
<li>degradation：增加深度，性能下降，通过shortcut减少这种效果。</li>
</ol>
<blockquote>
<p>Exploring the impact of the depth of temporal<br />
convolutional models on categorization tasks with hundreds or thousands of classes would be an<br />
interesting challenge and is left for future research.</p>
</blockquote>
<h2 id="cnn_1">CNN 句子建模</h2>
<p>A Convolutional Neural Network for Modelling Sentences，2014.</p>
<p>Dynamic Convolutional Neural Network (DCNN): 采用Dynamic k-Max Pooling，即pooling的时候，选取最大的k个值，而不是一个最大值。<br />
采用多个滤波器，提取多个特征。</p>
<ul>
<li>Neural Bag-of-Words (NBoW) models：<ul>
<li>投影层：将 word, sub-word, n-gram 映射到高维 embedding 向量。</li>
<li>组合：将这些向量组合（求和，均值，加权和等）</li>
<li>将组合后的向量作为句子的特征表达，传入全连接神经网络进行监督学习。</li>
</ul>
</li>
<li>Recursive Neural Network (RecNN）：利用一个额外的 parse tree。<ul>
<li>递归地组合叶子节点</li>
<li>将根节点的向量作为句子的特征表达，传入全连接神经网络进行监督学习。</li>
</ul>
</li>
<li>RNN：最为RecNN 的一个特例，即线性地组合相邻的向量，把最后节点对应的向量作为句子的特征向量。</li>
<li>TDNN：Time delay，利用卷积。</li>
</ul>
<p>一维卷积，即只对时间维度进行卷积。 TDNN对时间维度是卷积，对另一个维度（每个词都对应一个向量）则是全连接，并且采用窄版的卷积。<br />
Max-TDNN解决可变长度问题：每一个滤波器，最终只得到一个特征，即对卷积后的序列只取一个最大值。最终有几个卷积核，特征的维度就是几。<br />
最后得到的特征传入全连接神经网络进行监督学习。整个过程是联合优化的。</p>
<ul>
<li>特点：对词序不敏感；不需要额外的语言特征（dependency tree，parse tree)。</li>
<li>缺点：首先于卷积的长度，相当于只考虑 m-gram 的词特征，长距离的词关联无法学到。一个 max-pooling 导致的问题：抹去了顺序的信息。</li>
</ul>
<p>解决的办法：Dynamic k-max pooling</p>
<p>最后一层的k是固定的，保证最后一层的输出特征数目是定长的，但是中间的k是动态的。</p>
<p>$$<br />
k_l = \max (k_{top}, \ceil{\frac{L - l}{L} s})<br />
$$</p>
<p>L 是所有的卷积层数目，l是当前层的序号，s是句子长度，实际上是为了使得pooling是平滑的线性降低维度。</p>
<h2 id="cnn_2">CNN句子分类</h2>
<p>Convolutional Neural Networks for Sentence Classification，Kim 2014.</p>
<ul>
<li>要点：<ul>
<li>双通道，一个词向量通道可变，用于学习与目标有关的词向量（情感相关），另一个通道不可变，防止过拟合。<br />
这个很关键，可以参看论文，解决 word2vec 反义词的问题。</li>
<li>时间卷积，在另一个维度求和</li>
<li>max-over-time pooling</li>
<li>dropout</li>
</ul>
</li>
</ul>
</div>
<div id="income">
    <img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-08-20 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'Convolutional Neural Networks for NLP',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>