<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Convolutional Neural Networks for NLP - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;Convolutional Neural Networks for NLP</div>
</div>
<div class="clearfix"></div>
<div id="title">Convolutional Neural Networks for NLP</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#cnn">字母级别的CNN</a><ul>
<li><a href="#_2">模型</a></li>
<li><a href="#_3">数据</a></li>
<li><a href="#_4">设计细节</a></li>
<li><a href="#_5">结论</a></li>
</ul>
</li>
<li><a href="#very-deep-convolutional-networks-for-natural-language-processing">Very Deep Convolutional Networks for Natural Language Processing</a><ul>
<li><a href="#_6">结论</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>长期以来，RNN、LSTM及其变种模型被应用到自然语言处理方面。
近年来，将CNN应用到自然处理方面也有一些工作。</p>
<h2 id="cnn">字母级别的CNN</h2>
<p>主要论文：Character-level Convolutional Networks for Text Classification, <strong>Xiang Zhang</strong>, Junbo Zhao, <strong>Yann LeCun</strong>, 2016</p>
<p>目前文本分类研究已从设计好的特征转为选择好的分类模型。
目前，所有的文本分类技术都是以词为基本单位的，简单统计词和词的n-gram就可以做到最好的效果。</p>
<p>T. Joachims. Text categorization with suport vector machines: Learning with many relevant features. In
Proceedings of the 10th European Conference on Machine Learning, pages 137–142. Springer-Verlag,
1998.</p>
<p>卷积网络很适合从raw signals中提取有用的特征，已在机器视觉和语音识别等任务中得到应用。
而实际上，time-delay networks 早在深度学习出来以前就将卷积网络应用到序列数据之上。</p>
<ol>
<li>L. Bottou, F. Fogelman Soulie, P. Blanchet, and J. Lienard. Experiments with time delay networks and ´
dynamic time warping for speaker independent isolated digit recognition. In Proceedings of EuroSpeech
89, volume 2, pages 537–540, Paris, France, 1989.</li>
<li>R. Johnson and <strong>T. Zhang</strong>. Effective use of word order for text categorization with convolutional neural
networks. CoRR, abs/1412.1058, 2014.</li>
</ol>
<p>在这篇文章中，将文本当做字符为单位的序列数据，然后应用时间卷积网络(temporal (one-dimensional) ConvNets)。</p>
<p>卷积网络应用到文本和自然语言处理已有一些研究了，它既可以应用到连续值的embedding数据，也可以应用到离散值的embedding数据，
并不需要任何语法和语义信息！其结果也和经典的方法具有可比性！</p>
<ol>
<li>C. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical
Papers, pages 69–78, Dublin, Ireland, August 2014. Dublin City University and Association for
Computational Linguistics.</li>
<li>Y. Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar,
October 2014. Association for Computational Linguistics.</li>
<li>R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional neural
networks. CoRR, abs/1412.1058, 2014</li>
</ol>
<p>使用字母级别特征来做NLP也有一些早起工作，能够在POS tagging和IR方面的提升。This article is the first to apply ConvNets only on characters.
可以简化特征工程，能够学到拼写错误和emoji符号。</p>
<ol>
<li>character-level n-grams with linear classifiers: I. Kanaris, K. Kanaris, I. Houvardas, and E. Stamatatos. Words versus character n-grams for anti-spam filtering. International Journal on Artificial Intelligence Tools, 16(06):1047–1067, 2007</li>
<li>incorporating character-level features to ConvNets: C. D. Santos and B. Zadrozny. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818–1826, 2014</li>
<li>Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101–110. ACM, 2014.</li>
</ol>
<h3 id="_2">模型</h3>
<p>采用一位的卷积和max-pooling！</p>
<ul>
<li>对 Pooling 的分析文章：Y.-L. Boureau, J. Ponce, and <strong>Y. LeCun</strong>. A theoretical analysis of feature pooling in visual recognition.
In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 111–118, 2010</li>
<li>ReLU 最早文章：V. Nair and <strong>G. E. Hinton</strong>. Rectified linear units improve restricted boltzmann machines. In Proceedings
of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010</li>
</ul>
<h3 id="_3">数据</h3>
<p>字母通过one-hot编码为70维向量，包括26个字母，10个数字，33个其他字符和换行符。字符包括：</p>
<div class="hlcode"><pre><span class="n">abcdefghijklmnopqrstuvwxyz0123456789</span>
<span class="o">-</span><span class="p">,;.</span><span class="o">!?:</span><span class="err">’’’</span><span class="o">/</span><span class="err">\</span><span class="o">|</span><span class="n">_</span><span class="err">@#$</span><span class="o">%</span><span class="err">ˆ</span><span class="o">&amp;*</span><span class="err">˜‘</span><span class="o">+-=&lt;&gt;</span><span class="p">()[]{}</span>
</pre></div>


<p>对中文的处理，将中文转换为拼音 pypingyin。</p>
<h3 id="_4">设计细节</h3>
<p>模型设计</p>
<p>6层卷积层+3层全连接层，kernel维度为7和3，pool维度为3。</p>
<p>数据增强</p>
<p>用近义词进行替换，增加样本！平移不适应于这里！</p>
<p>和传统方法比较：</p>
<ul>
<li>Bag of word with TFIDF</li>
<li>Bag of n-gram with TFIDF</li>
<li>Bag of means with word embedding</li>
</ul>
<p>深度学习方法比较：</p>
<ul>
<li>word-based CNN:</li>
<li>LSTM:</li>
</ul>
<h3 id="_5">结论</h3>
<p>数据集达到百万量级才能观察到这种方法的优势，数据集小的时候还是 n-gram with TFIDF 好</p>
<blockquote>
<p><strong>There is no free lunch</strong>. Our experiments once again verifies that there is not a single machine
learning model that can work for all kinds of datasets. The factors discussed in this section could all
play a role in deciding which method is the best for some specific application.</p>
</blockquote>
<h2 id="very-deep-convolutional-networks-for-natural-language-processing">Very Deep Convolutional Networks for Natural Language Processing</h2>
<p>论文： Very Deep Convolutional Networks for Natural Language Processing, <strong>Le Cun</strong>, 2016</p>
<p>目前RNN,LSTM,CNN应用到NLP中的深度和在CV中相比，还比较浅，这篇文章提出一种方案可以从字母级别开始学习，模型深度达到29层！</p>
<p>CNN：将特征提取和分类进行联合训练！除了自动特征提取之外，还可以根据具体任务调整特征！</p>
<p>目前主流的方法，是利用 word embedding + RNN(LSTM)。</p>
<ol>
<li>Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. LSTM neural networks for language
modeling. In Interspeech, 2012.</li>
<li>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In NIPS, pages 3104–3112, 2014.</li>
</ol>
<p>作者argue：</p>
<ul>
<li>作者认为 LSTM 是一种一般的序列学习方法，缺乏领域特性 "lacking task specific structure"</li>
<li>单词按照顺序进入，第一个单词变换了很多次，而最后一个词只变换一次！ =&gt; bidirectional LSTM</li>
<li>深度不够，超过4层就没啥提升了，尽管加入了 dropout 正则化！</li>
</ul>
<p>观点：</p>
<blockquote>
<p>We believe that <strong>the challenge in NLP</strong> is to develop deep architectures which are able to learn hierarchical
representations of whole sentences, jointly with the task.</p>
</blockquote>
<p>recursive neural network : 在RNN上增加了序列融合的顺序结构（树结构），RNN可以看做一个特殊的 recursive NN.</p>
<ol>
<li>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.
Semi-supervised recursive autoencoders for predicting sentiment distributions. In
EMNLP, 2011.</li>
</ol>
<p>模型结构：</p>
<p><img src="/wiki/static/images/deep-cnn-for-text.png" alt="模型结构"  style="float:left; width:300px;" /></p>
<p>s 是时间窗长度，首先将字符embedding到16维的向量！
第一层64个特征，后续是ConvNet Block，采用下述策略（from VGG and ResNet）：</p>
<ol>
<li>如果时间分辨率不变，输入和输出特征维度相同</li>
<li>如果时间分辨率减半，输出特征维度加倍</li>
</ol>
<p>更多的卷积层，意味着能够学习更长的依赖关系！并且，对所有的时间几乎是平等的！而不像RNN，LSTM那样！
其中一个CNN Block 结构如图Fig.2。包含两个维度为3的核的卷积层，每个卷积层后跟一个BN层和一个非线性层！
多个小尺寸的卷积层可以用较少的参数实现一个大尺寸的卷积层相同的功能（视野和非线性度）！</p>
<p><img src="/wiki/static/images/deep-cnn-for-text-cnnblock.png" alt="模型结构"  style="float:right; width:250px;"/></p>
<p>输入字符增加了一个表示未知符号的特殊字符，一共72个token。输入文本padding到长度为1014！字符embedding到16维的向量。
其他参数：</p>
<ul>
<li>mini-batch of size 128</li>
<li>initial learning rate of 0.01</li>
<li>momentum of 0.9</li>
<li>每次验证集错误增加就将学习率减半</li>
<li>初始化采用 何凯明 的方案</li>
<li>采用 BN 而没有dropout</li>
</ul>
<h3 id="_6">结论</h3>
<ol>
<li>在大数据集上有明显提升，即使深度较小</li>
<li>深度可以提升效果！</li>
<li>Max-pooling 最优</li>
<li>degradation：增加深度，性能下降，通过shortcut减少这种效果。</li>
</ol>
<blockquote>
<p>Exploring the impact of the depth of temporal
convolutional models on categorization tasks with hundreds or thousands of classes would be an
interesting challenge and is left for future research.</p>
</blockquote>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-08-20 </span></div>

        </div>
        <div id="footer">
            <span>
                Copyright © 2016 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        
    </body>
</html>