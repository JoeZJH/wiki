<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Convolutional Neural Networks - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;Convolutional Neural Networks</div>
</div>
<div class="clearfix"></div>
<div id="title">Convolutional Neural Networks</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#alexnet">AlexNet</a><ul>
<li><a href="#_2">结构上的创新</a></li>
<li><a href="#_3">降低过拟合技巧</a></li>
<li><a href="#_4">量化评估</a></li>
</ul>
</li>
<li><a href="#zfnet">ZFNet</a><ul>
<li><a href="#_5">卷积网络可视化</a></li>
<li><a href="#_6">特征泛化能力</a></li>
<li><a href="#_7">问题</a></li>
</ul>
</li>
<li><a href="#vgg-net">VGG net</a></li>
<li><a href="#googlelenet">GoogleLeNet</a></li>
<li><a href="#resnet">ResNet</a></li>
<li><a href="#_8">定位与检测</a></li>
<li><a href="#overfeat">OverFeat</a></li>
<li><a href="#hog">目标检测 HOG</a></li>
<li><a href="#rcnn">RCNN</a><ul>
<li><a href="#region-proposals">Region proposals</a></li>
<li><a href="#feature-extraction">Feature extraction</a></li>
<li><a href="#test-time-detection">Test-time detection</a></li>
<li><a href="#train">Train</a></li>
</ul>
</li>
<li><a href="#fast-r-cnn">Fast R-CNN</a></li>
<li><a href="#faster-r-cnn">Faster R-CNN</a></li>
<li><a href="#cnn">CNN可视化</a></li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>卷积网络经典文章导读，文章列表是参考 CS231N 课程。</p>
<h2 id="alexnet">AlexNet</h2>
<p>论文：Imagenet classification with deep convolutional neural networks<br />
Alex Krizhevsky, Ilya Sutskever, <strong>Geoffrey E Hinton</strong>, 2012</p>
<p>Hinton 带学生打比赛的故事。</p>
<ul>
<li>求解问题： ImageNet LSVRC-2010 比赛，1.2M高精度图片，1000分类！ILSVRC-2012 TOP5 error：15.3%，第二名是 26.2%！</li>
<li>效果： TOP1 error：37.5%， TOP2 error：17.0%。</li>
<li>网络参数：60M 参数，650,000个神经元</li>
<li>重要创新： ReLU激活函数， GPU计算卷积，dropout</li>
<li>5层卷积层+3层全连接层，卷积层的深度是很关键的，移除任何一层都将导致性能的降低！</li>
<li>GTX 580 3GB GPUs 训练 5-6天</li>
</ul>
<p><strong>Amazon’s Mechanical Turk crowd-sourcing tool</strong></p>
<ul>
<li>对图像做下采样到固定大小 256x256，满足固定大小输入；对每个像素减去在整个训练集上的均值</li>
</ul>
<h3 id="_2">结构上的创新</h3>
<ul>
<li>ReLU 非线性：加速训练，CIFAR-10上达到25%错误率，比tanh快6倍！</li>
</ul>
<p>相关论文：V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010</p>
<ul>
<li>多 GPU 训练：2个GPU</li>
<li>Local Response Normalization：将错误减少1-2个点。</li>
<li>Overlapping Pooling：Pooling尺寸=3，步长却是2</li>
<li>结构：前面五层是卷积层，每个卷积层分为两个部分，每个部分放在一个GPU中，在卷积过程中，第2、4、5层的两个GPU互不干扰，第3层和全连接层又相互交错连接的部分。maxpooling层在第1，2，5层卷积层，Local Response Normalization layer 在第1、2层。</li>
</ul>
<p><img src="/wiki/static/images/alexnet.png" /></p>
<ul>
<li>每一层的详细参数：输入 224x224x3<ol>
<li>96个11x11x3的滤波器，分为上下两部分，每部分48个</li>
<li>256个5x5x48的滤波器，两个GPU互不干扰</li>
<li>384个3x3x256的滤波器，两个GPU有交互</li>
<li>384个3x3x192的滤波器，两个GPU互不干扰</li>
<li>256个3x3x192的滤波器</li>
<li>全连接层为4096个神经元</li>
</ol>
</li>
<li>卷积层参数：1.45M，卷积层输出为6x6x256；三个全连接层分别是：37.75M，21.92M，4.10M！！可以看到参数主要集中在卷积层最近的两个全连接层！！</li>
</ul>
<h3 id="_3">降低过拟合技巧</h3>
<p><img src="/wiki/static/images/alexnet-error.png" style="float:left;width:400px;margin-right:20px;"/></p>
<ul>
<li>Data Augmentation: 数据增强：<ul>
<li>平移和水平翻转，从256x256的图片，截取224x224的图片块，加上水平翻转，一张图片就变成了32x32x2=2048个样本！预测的时候；预测的时候，截取四个角+中央以及他们的水平翻转10张图片，结果取平均！</li>
<li>加噪，有点像 denoise 的概念，对每一个像素 $(I_{xy} = [I_{xy}^R, I_{xy}^G, I_{xy}^B]^T)$，不是简单的在每个分量上简单地叠加，而是在三个通道的协方差矩阵的三个主方向上，叠加对应比例的噪声。下式中，p与lambda分别是协方差矩阵的三个特征向量和特征值，$(\alpha_i)$ 是叠加的噪声比例，服从0均值方差为0.1的高斯分布。</li>
</ul>
</li>
</ul>
<p>$$<br />
[\mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3] [\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T<br />
$$</p>
<ul>
<li>
<p>Dropout：可以看做一种大量的神经网络的模型组合。可以解决过拟合问题，学习到鲁邦的特征，预测的时候，则将神经元的值乘以概率即可。 dropout 技术大致使得收敛的迭代次数增加一倍。</p>
</li>
<li>
<p>配置：NVIDIA GTX 580 3GB GPUs，两块</p>
</li>
</ul>
<h3 id="_4">量化评估</h3>
<p>用最后的4096维特征作为图像向量，评估图像的相似度，效果很不错，用 auto-encoder 于这些特征上比在raw data上效果应该会更好。</p>
<p><img src="/wiki/static/images/alexnet-res.png" /></p>
<h2 id="zfnet">ZFNet</h2>
<p>论文：Zeiler M D, Fergus R. Visualizing and Understanding Convolutional Networks[C]. european conference on computer vision, 2013: 818-833.</p>
<ul>
<li>ZFNet 在 AlexNet 上改进的不多，主要贡献在 CNN 的可视化。</li>
<li>解释 AlexNet 为什么效果好（主要是通过可视化分析），以及怎么进一步改进。</li>
<li>数据集：Caltech-101，Caltech-256.</li>
<li>可视化技术：<strong>解卷积</strong>，通过显示激活任意一层的单一的 feature map 的输入图像的方法，可视化某个神经元学到的东西。 <strong>Zeiler, M., Taylor, G., Fergus, R.: Adaptive deconvolutional networks for mid and high level feature learning. In: ICCV (2011)</strong></li>
<li>敏感性分析：通过遮蔽输入图片的一部分，展示图片的哪一部分对分类结果比较重要。</li>
<li>对 AlexNet 改进，并迁移到其他任务，只将最后一层 softmax 重新训练，有监督的 pre-training。</li>
<li>之前的可视化工作一直停留在第一层。</li>
<li>通过梯度下降最大化某个神经元的输出，从而找出最优激励图像（BP to Image）<strong>Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent，Visualizing higher-layer features of a deep network，2009</strong>，没有解释神经元的不变性！？</li>
<li>计算在最优点处的 Hessian 矩阵，理解这种不变性？</li>
<li>解卷积是无监督学习，相当于一个探针，探测一个已经学好的网络</li>
</ul>
<p><img src="/wiki/static/images/zfnet-deconv.png" /></p>
<ul>
<li>解卷积过程：将同一层的其他神经元置0，将该层作为解卷积的输入，依次经历了(i) unpool, (ii) rectify and (iii) filter</li>
<li>Unpooling: Max-pooling 不可逆，为了解决这个问题，在做 Max-pooling 的时候，用一个 switch 变量记录最大值的位置。<em>问题，可视化的时候，没有正向卷积过程，这个 switch 变量从哪来？</em></li>
<li>Rectification：直接将重构信号通过 ReLU？</li>
<li>Filtering：将卷积核做水平、垂直翻转后，再进行卷积。这就可以解卷积了？不应该要做个逆滤波？</li>
</ul>
<blockquote>
<p>解卷积解释：设原始信号为 $(f)$，卷积核为$(k)$，解卷积核为$(k')$，那么经过卷积和解卷积，信号变为<br />
$(f * k * k')$，利用卷积运算的结合律，也可以表达为 $( f * (k * k') )$，如果要使得解卷积后的信号<br />
和原始信号一致，那么需要 $( k * k' = \delta )$，即两个卷积核的卷积为单位冲击函数，也就是<br />
$( \sum_{x',y'} k(x - x', y - y') k'(x', y') = \delta(x, y))$，即只有在$(x=0,y=0)$时为1，<br />
其他情况为0。这里将卷积核水平和垂直翻转后，相当于 $( \sum_{x',y'} k(x - x', y - y') k(-x', -y'))$<br />
可以看到，当x和y都为0时取得最大值（达到匹配），其他情况虽然不为0，但小于匹配的时候的值，所以可以看做逆滤波的一种近似实现. 不过简单试验结果表明，这种近似太粗糙了。</p>
</blockquote>
<ul>
<li>CNN 训练的输入是[-128,128]，居然没有归一化？！初始化是随机取的，幅度为$(10^{-2})$</li>
</ul>
<h3 id="_5">卷积网络可视化</h3>
<p><img alt="zfnet-res" src="/wiki/static/images/zfnet-res.png" /></p>
<ul>
<li>特征可视化：选取TOP9</li>
<li>结构选择：11x11滤波器改为7x7，stride减少到2，从而使得第1，2层滤波器提取到更多有用的信息。？？</li>
<li>遮挡敏感性：测试分类器是否真的检测到了图片中的目标，还是只是用周围的信息。</li>
<li>选取第5层最强的 feature map 的响应值之和，随着遮挡的位置的变化。可视化的结果如图(b)。</li>
</ul>
<p><img alt="zfnet-res2" src="/wiki/static/images/zfnet-res2.png" /></p>
<h3 id="_6">特征泛化能力</h3>
<ul>
<li>利用 ImageNet 学出来的模型，应用到其他任务，例如：Caltech</li>
<li>只改变最后一层，前面的层都固定不变。</li>
</ul>
<h3 id="_7">问题</h3>
<ul>
<li>解卷积可视化具体是怎么样做的？</li>
</ul>
<h2 id="vgg-net">VGG net</h2>
<ul>
<li>论文：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION, Simonyan and Zisserman, 2014 @ICLR 2015</li>
<li>重要贡献：通过非常小的卷积核 3x3，提升模型的深度！</li>
<li>1x1 卷积核的使用！</li>
<li>卷积的 stride 保持为1，保证卷积层后空间分辨率是不变的。</li>
<li>pooling层保持为 2x2 大小的窗，stride=2.</li>
<li>没有使用 AlexNet 的 Local Response Normalisation 层！没有明显收益！</li>
</ul>
<p><img src="/wiki/static/images/vggnet01.png" style="float:left;width:400px;" /></p>
<ul>
<li>用两层3x3的卷积层代替一层5x5卷积层；3层3x3的卷积侧代替一层7x7卷积层；这种方法可以在不减少卷积核的覆盖范围情况下，增加非线性变换次数并减少参数！</li>
<li>1x1卷积层在不影响空间变换情况下，增加非线性变幻的次数！Network in network.</li>
<li>训练参数细节：<ul>
<li>mini-batch sgd, momentum = 0.9, batch size=256</li>
<li>L2 正则参数5e-4</li>
<li>dropout 0.5，最前面两侧全连接层</li>
<li>学习率初始值1e-2，当验证集不降低时除以10</li>
<li>74 epoch</li>
</ul>
</li>
<li>训练图像尺寸：CNN输入块大小是224x224，图像被rescale尺寸为S。S可以是固定大小，也可以是多个分辨率。<ul>
<li>固定尺寸：256，384（用256的权重初始化网络）</li>
<li>可变尺寸，在[256,512]之间随机变动S，scale jittering.</li>
</ul>
</li>
<li>testing:将全连接层变成全卷积层：将最后一层的通道作为class通道，然后在空间上平均得到不同位置的分类概率的平均值！这样就不用切割原始图像为多个块了！只要简答的rescale为固定的Q值即可。</li>
<li>对于固定resclae，取Q=S，对于scale jittering，取Q为S的平均值！</li>
</ul>
<p><img src="/wiki/static/images/vggnet02.png" style="float:left;width:400px;" /><br />
<img src="/wiki/static/images/vggnet03.png" style="float:left;width:400px;" /><br />
<img src="/wiki/static/images/vggnet04.png" style="float:left;width:400px;" /></p>
<ul>
<li>单个scale的评估：Q的取值策略如上；试验表明，scale jittering帮助提升效果！</li>
<li>多个scale的评估：Q取值策略，对固定S值，取S-32,S,S+32;对变动S值，取S_min,S_avg,S_max三个值。可以看到，比单个scale效果要好！</li>
<li>多个crop的评估：略</li>
<li>卷积网络融合：将每一个网络输出的多类概率平均。</li>
<li>D模型参数数目分析：138M参数！大约是Alex的两倍！</li>
<li>参数主要集中在FC层，而内存消耗主要集中在前面几层！</li>
</ul>
<h2 id="googlelenet">GoogleLeNet</h2>
<ul>
<li>论文：Going Deeper with Convolutions，CVPR2015</li>
<li>最大创新：增加网络的深度和宽度，但是保持计算代价不变！22层网络！</li>
<li>参数数目只有Alexnet的1/12.</li>
<li>Inception结构，借鉴自 network in network.</li>
<li>Hebbian principle: neurons that fire together, wire together</li>
</ul>
<blockquote>
<p>if the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the pre- ceding layer activations and clustering neurons with highly correlated outputs</p>
</blockquote>
<ul>
<li>将稀疏矩阵乘法通过聚集后变成 dense matrix 乘法，可以充分利用计算资源。</li>
<li>non-uniform deep-learning architectures</li>
</ul>
<p><img src="/wiki/static/images/inception.png" style="float:left;width:300px;" /></p>
<ul>
<li>通过多个不同的滤波器，实现多尺度的抽取；通过1x1滤波器实现降维，减少大尺寸滤波器计算复杂度，也减少了参数！</li>
<li>22层，为了减少梯度消失效应，增加了中间的输出，以期望中间的特征也有一定的区分度！提供一种正则。<br />
训练的时候，将这些低层分类的损失函数加到最终损失函数中，作为正则项！结果显示，这种效果不明显。</li>
</ul>
<p><img src="/wiki/static/images/googlenet.png" /></p>
<p><img src="/wiki/static/images/googlenet2.png" style="float:right;width:200px;"/></p>
<ul>
<li>GoogLeNet 的网络结构参数如表所示，其中#1x1,#3x3,#5x5分别代表对应的滤波器数目，而 #3×3 reduce 和 #5x5 reduce 分别代表 inception 中3x3滤波器和5x5滤波器前面用作降维的1x1滤波器数目，pool proj代表inception中Pool层后面的1x1滤波器数目！</li>
<li>完全移除了全连接层，取而代之的是 avg pool 层（top-1准确率提高了0.6%）！但是保留了dropout！</li>
<li>为了减少梯度消失的问题，在中间加了两个输出抽头！抽头的结构如下：<ul>
<li>5x5 avg pooling, stride=3，分别将图片降维至 4x4x512, 4x4x528！</li>
<li>1x1滤波器降维至128维</li>
<li>全连接层1024的神经元</li>
<li>70% dropout</li>
</ul>
</li>
<li>
<p>训练方法：用的是 DistBelief 分布式训练！模型并行和数据并行训练。CPU集群</p>
<ul>
<li>异步 SGD， momenton=0.9</li>
<li>固定学习率策略，每8个poch减少4%</li>
<li>Polyak averaging：B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Con- trol Optim., 30(4):838–855, July 1992.</li>
<li>采样不同尺寸不同位置的patch</li>
<li>photometric distortions：A. G. Howard. Some improvements on deep con- volutional neural network based image classification. CoRR, abs/1312.5402, 2013.</li>
</ul>
</li>
<li>
<p>参赛配置：</p>
<ul>
<li>没有额外的训练数据</li>
<li>训练了7个不同的版本，然后做融合：相同的初始权重，学习率，只在采用方法和图片的随机顺序不同</li>
<li>testing阶段每个图片采样了不同尺寸不同位置不同镜像的多个块进行预测。</li>
</ul>
</li>
</ul>
<h2 id="resnet">ResNet</h2>
<p>参考WIKI<a href="residual-network.html">残差网络</a></p>
<h2 id="_8">定位与检测</h2>
<ul>
<li>简单回归问题：<ul>
<li>将定位作为一个回归问题，输出定位的坐标和尺寸4个数字，用L2损失函数，简单！</li>
<li>直接从分类模型最后一层的feature map引出一个回归抽头！</li>
</ul>
</li>
<li>滑动窗：<ul>
<li>在高分辨率图片中的不同尺寸和不同位置运行 分类+回归 网络</li>
<li>融合所有尺寸的分类+回归结果作为最终的输出</li>
</ul>
</li>
</ul>
<h2 id="overfeat">OverFeat</h2>
<p>论文：OverFeat:Integrated Recognition, Localization and Detection<br />
using Convolutional Networks，Pierre Sermanet, David Eigen,<br />
Xiang Zhang, Michael Mathieu, Rob Fergus, <strong>Yann LeCun</strong>，2014.</p>
<ul>
<li>classification, localization and detection 的CNN集成框架</li>
<li>multiscale 和 sliding window 技术</li>
<li>ILSVRC2013 目标识别冠军</li>
</ul>
<blockquote>
<p>combining many localization predictions, detection can be performed without training on background samples and that it is possible to avoid the time-consuming and complicated bootstrapping training passes</p>
</blockquote>
<ul>
<li>在不同位置和不同scale使用CNN：大量的窗只包含目标的一部分，分类效果好，但是定位和检测效果不好。</li>
<li>每一个window不但输出不同类别的预测概率分布，还输出目标相对window的位置和大小！</li>
<li>
<p>累积每一个类别的每一个window的预测结果！</p>
</li>
<li>
<p>文本检测：M.DelakisandC.Garcia.Textdetectionwithconvolutionalneuralnetworks.InInternationalConference on Computer Vision Theory and Applications (VISAPP 2008), 2008.</p>
</li>
<li>人脸识别： C. Garcia and M. Delakis. Convolutional face finder: A neural architecture for fast and robust face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.</li>
<li>人脸检测：M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based models. Journal of Machine Learning Research, 8:1197–1215, May 2007.</li>
<li>
<p>行人检测：P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi- stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition (CVPR’13). IEEE, June 2013.</p>
</li>
<li>
<p>预测的box和groundtruth的box至少相交50%（IOU）才认为是对的。</p>
</li>
</ul>
<blockquote>
<p>IOU的定义：label框为A，groundtruth框为B，$(IOU = \frac{ area(A \bigcap B)}{ area(A \bigcup B)} )$</p>
</blockquote>
<ul>
<li>通过滑动窗，产生多个块，得到多个块预测结果，然后平均。滑动窗可以自底向上计算，不用每个滑动窗计算一个结果，减少计算量！</li>
<li>不同尺寸和位置检测得到的box融合成一个高可信的box，实现定位！</li>
</ul>
<h2 id="hog">目标检测 HOG</h2>
<p>论文：Histograms of Oriented Gradients for Human Detection，Navneet Dalal and Bill Triggs，2005.</p>
<h2 id="rcnn">RCNN</h2>
<p>论文：Rich feature hierarchies for accurate object detection and semantic segmentation</p>
<p><img alt="RCNN" src="/wiki/static/images/rcnn.png" /></p>
<h3 id="region-proposals">Region proposals</h3>
<ul>
<li>objectness: B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. TPAMI, 2012.</li>
<li><strong>selective search</strong>: J.Uijlings,K.vandeSande,T.Gevers,andA.Smeulders.Selective search for object recognition. IJCV, 2013.</li>
<li>category-independent object proposals: I. Endres and D. Hoiem. Category independent object proposals. In ECCV, 2010.</li>
<li>constrained parametric min-cuts (CPMC): J. Carreira and C. Sminchisescu. CPMC: Automatic object segmentation using constrained parametric min-cuts. TPAMI, 2012.</li>
<li>multi-scale combinatorial grouping: P.Arbelaez,J.Pont-Tuset,J.Barron,F.Marques,andJ.Malik.Multiscale combinatorial grouping. In CVPR, 2014.</li>
<li>CNN: D.Cires ̧an,A.Giusti,L.Gambardella,andJ.Schmidhuber.Mitosis detection in breast cancer histology images with deep neural networks. In MICCAI, 2013.</li>
</ul>
<h3 id="feature-extraction">Feature extraction</h3>
<ul>
<li>利用一个训练好的CNN网络（如 AlexNet 网络），对每个区域提取特征。</li>
<li>将每个区域补全和变形到标准的输入尺寸， alex net 要求输入时 227x227</li>
</ul>
<h3 id="test-time-detection">Test-time detection</h3>
<ol>
<li>利用选择性搜索选出近2000个候选区域</li>
<li>用 CNN 提取每一个区域的特征向量，对每一个类别，使用对应的 SVM 分类器对特征打分</li>
<li>
<p>采用贪心的非最大值抑制方法（greedy non-maximum suppression， 每一个类是独立的）：如果一个区域和另一个得分更高的区域 IoU 重叠度高于某个阈值，那么就拒绝这个得分低的区域。阈值是学习到的阈值？</p>
</li>
<li>
<p>性能对比（10K个类被）：DPM+Hashing，5min/image; RCNN, 1min/image. T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijayanarasimhan, and J. Yagnik. Fast, accurate detection of 100,000 object classes on a single machine. In CVPR, 2013.</p>
</li>
</ol>
<h3 id="train">Train</h3>
<ul>
<li>将在ImageNet上训练好的CNN最后一层替换成多个SVM（每一个类别一个，背景一个，SVM参数随机初始化），CNN参数也通过SGD调优</li>
<li>将于ground-truth重叠度IoU超过50%的区域作为该类的正样本，其他的作为负样本</li>
<li>CNN调优的学习率降低10倍</li>
<li>每一个SGD的minbatch中，均匀采样32个正例和96个负例</li>
<li>hard negative mining method：将分值较高的负例放到样本中重新训练</li>
<li>pool5的特征就很好的，全连接层可以不要！</li>
<li>
<p>bbox regression：为每个区域训练一个回归模型，用相同的特征，只改变最后一层，预测目标的相对偏移。</p>
</li>
<li>
<p>最大的问题：慢，需要对每一个区域用CNN提特征！</p>
</li>
</ul>
<h2 id="fast-r-cnn">Fast R-CNN</h2>
<ul>
<li>先用CNN对整个图片进行特征抽取（pool5特征，有空间维度的特征），在选取的RoI区域，用一个RoI Pooling层将特征尺寸变成固定的空间尺寸HxW，（空间尺寸固定了，整个特征的尺寸也固定了），然后为每一个区域中建立分类和回归模型。</li>
<li>RoI还是通过预先的 region proposal 方法得到，这个部分是 Fast R-CNN 计算的瓶颈。</li>
<li>由于ROI将近2000个，计算最后的全连接层是计算瓶颈，可以通过 Truncated SVD 优化，其效果相当于用两层线性网络替换。</li>
<li>优点：只需要计算一次CNN即可！</li>
<li>优点：将回归和分类损失函数加到一起，优化一个目标，multi-task loss，端到端学习！</li>
<li>相比 R-CNN，训练时间加速8.8倍，预测时间加速146倍！每张图片的预测时间降低到0.32s，之前在分钟量级！效果也稍好；但是加上区域搜索时间（大约2s）后，只有25倍速度提升，搜索时间是瓶颈！</li>
<li>关键层： RoI pooling layer</li>
</ul>
<p><img alt="fast rcnn" src="/wiki/static/images/fast-rcnn.png" /></p>
<h2 id="faster-r-cnn">Faster R-CNN</h2>
<p>用CNN做 region proposal，关键技术： Region Proposal Networks</p>
<h2 id="cnn">CNN可视化</h2>
<ul>
<li>R-CNN 计算所有的区域对某个神经元激活值，按照激活值从大到小排序，选取TOP区域可视化。</li>
</ul>
<p><img alt="RCNN可视化" src="/wiki/static/images/rcnn-visual.png" /></p>
<ul>
<li>
<p>直接可视化权重：只能可视化第一层</p>
</li>
<li>
<p>可视化特征表达，例如用 t-SNE 可视化 AlexNet 最后一层的4096维特征</p>
</li>
</ul>
<p><a href="http://cs.stanford.edu/people/karpathy/cnnembed/">http://cs.stanford.edu/people/karpathy/cnnembed/</a></p>
<ul>
<li>
<p>遮挡试验：分类概率与遮挡位置的函数关系！ZFNet</p>
</li>
<li>
<p>解卷积方法</p>
</li>
</ul>
<p>BP to Image 方法：</p>
<p>可视化某个神经元的响应对输入图片的梯度（BP to Image），将该层所有神经元的梯度置0，将要可视化的那个神经元梯度置1！<br />
然后运用BP算法，求出梯度。</p>
<p>$$<br />
\frac{\Delta active}{\Delta I}<br />
$$</p>
<p>由于高级特征具有不变性，不是针对某一个图片的，直接解卷积可视化得到的效果不好。<br />
可以对于特定的图片，用这个图片做引导，通过 guided bp 得到条件梯度。</p>
<p><img alt="Guided BP" src="/wiki/static/images/guided-bp.png" /><br />
<img alt="Guided BP2" src="/wiki/static/images/guided-bp2.png" /></p>
<p>ZF 解卷积方法</p>
<ul>
<li>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</li>
</ul>
<p>寻找图像I使得在类c上的score $(S_c(I))$ 最大！</p>
<p>$$<br />
\arg \max_I S_c(I) - \lambda ||I|| _ 2^2<br />
$$</p>
<p>利用 BP 算法优化，固定权重，优化输入！输入初始化为0值图片！ Sc 是未归一化的score，优化归一化的score（即概率）效果反而不明显。</p>
<p>给定图像$(I_0)$，根据输入像素对某个类的score影响效果排序，影响效果通过梯度刻画</p>
<p>$$<br />
w = \frac{\partial S_c}{\partial I} | _ {I_0}<br />
$$</p>
<ul>
<li>给定一个图片的code，寻找最接近这个code的图片</li>
</ul>
<p>$$<br />
x*  = \arg \min _ x l(\Phi(x) - \Phi(x_0)) + \lambda R(x)<br />
$$</p>
<ul>
<li>DeepDream：从一个初始图片开始，每次梯度沿着正反馈方向下降 dx = x!!这里x是神经网络某一层的响应值。即目标函数是，使得某个已经训练好的模型的某一层，激活函数的幅度最大化!<br />
其结果是，如果最大化的是前面的层，那么图片中会显示出一些低级纹理，如果是后面的层，那么图片中会显示出一些学到的高级目标，如狗、猫的一些局部！</li>
</ul>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2017-03-16 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'Convolutional Neural Networks',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2020 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>