<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>word2vec - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;word2vec</div>
</div>
<div class="clearfix"></div>
<div id="title">word2vec</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#word2vec">word2vec 论文</a><ul>
<li><a href="#_2">导论</a></li>
<li><a href="#the-skip-gram-model">The Skip-gram Model</a><ul>
<li><a href="#hierarchical-softmax">Hierarchical Softmax</a></li>
<li><a href="#negative-sampling">Negative Sampling</a></li>
<li><a href="#_3">高频词的负采样</a></li>
</ul>
</li>
<li><a href="#_4">测试数据</a></li>
<li><a href="#_5">短语学习</a></li>
</ul>
</li>
<li><a href="#cbow">CBOW 论文</a></li>
<li><a href="#glove">Glove</a></li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>词向量相关论文学习。</p>
<h2 id="word2vec">word2vec 论文</h2>
<ul>
<li>
<p><strong>Mikolov, T.</strong>, Sutskever, I., <strong>Chen, K.</strong>, Corrado, G. S., &amp; <strong>Dean, J.</strong> (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</p>
</li>
<li>
<p>问题：</p>
<ul>
<li>相比直接通过VSM学习到的词向量，优势是什么？</li>
</ul>
</li>
</ul>
<h3 id="_2">导论</h3>
<ul>
<li>
<p>创新点：</p>
<ol>
<li>针对连续 skip-gram 模型：采用一些介壳提升词向量质量和训练速度</li>
<li>负采样技术： 对高频词负采样可以显著提升训练时间（2-10倍的提升），同时也能提高低频词的词向量质量？</li>
</ol>
</li>
<li>
<p>recursive autoencoders ：Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, 2011.</p>
</li>
</ul>
<h3 id="the-skip-gram-model">The Skip-gram Model</h3>
<p>用中间词预测周围的词，最大化对数似然函数</p>
<p>$$
\frac{1}{T} \sum_{t=1}^T \sum_{-c \le j \le c, j \neq 0} \log p(w_{t+j}| w_t) \\
p(w_O| w_I) = \frac{\exp(v^'_{w_O}^T v_{w_I})}{\sum_{w=1}^W \exp(v^'_{w}^T v_{w_I}) }
$$</p>
<p>计算代价正比于词典规模 W（10^5-10^7），因此很费时间。</p>
<h4 id="hierarchical-softmax">Hierarchical Softmax</h4>
<p>Frederic Morin and <strong>Yoshua Bengio</strong>. Hierarchical probabilistic neural network language model. In Pro- ceedings of the international workshop on artificial intelligence and statistics, pages 246–252, 2005.</p>
<p>只需要计算$(\log_2W)$个节点！
将条件概率的计算，变成多个分类概率的计算。可以用一个二叉树表示出来，叶子结点对应词，中间节点有一个参数！
每一个内部节点可以看做一个二分类逻辑回归，其参数就是二分类参数。这个参数也要学习！
文中表示用哈夫曼树作为这个二叉树可以简单提升性能。</p>
<p>$$
p\left( w_O | w_I \right) = \prod_{j = 1}^{L(w) - 1} \sigma \left( [n(w,j+1) = ch(n(w,j))] \centerdot v_{n(w,j)}^{\top} v_{w_I} \right)
$$</p>
<h4 id="negative-sampling">Negative Sampling</h4>
<ul>
<li>Noise Contrastive Estimation (NCE)：<ol>
<li>Michael U Gutmann and Aapo Hyva ̈rinen. Noise-contrastive estimation of unnormalized statistical mod- els, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361, 2012.</li>
<li>Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.</li>
</ol>
</li>
</ul>
<p>负采样解释可以看2014年的文章：<a href="http://cn.arxiv.org/pdf/1402.3722v1.pdf">http://cn.arxiv.org/pdf/1402.3722v1.pdf</a></p>
<p>可以将每一项理解为一个二元分类问题，正样本是词在中心词的上下文，而负样本是不在中心词上下文的词。
目标函数相当于让正样本出现以及k各负样本不出现的联合概率最大化！</p>
<p>$$
\log \sigma(v_{w_O}'^T v_{w_I}) + \sum_{i=1}^k \mathbb{E}_{w_i \sim P_n(w)} \left[  \log(-\sigma(v_{w_i}'^T v_{w_I}')) \right]
$$</p>
<p>对于小数据集k取 5-20 即可；对于大数据集k可以取小点2-5.
采样方法 P 取 unigram distribution $(U(w)^{3/4})$最好，即正比于词频的3/4次幂。</p>
<h4 id="_3">高频词的负采样</h4>
<p>以概率P丢弃！</p>
<p>$$
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
$$</p>
<p>t 是阈值，典型值为$(10^{-5})$，f是词频率！
这种方式不但可以加快速度，还能提高低频词的精度！（不是数据越多越好？！）</p>
<h3 id="_4">测试数据</h3>
<ul>
<li>相似推理任务：<ul>
<li>语法相似 syntactic analogies： “quick” : “quickly” :: “slow” : “slowly”</li>
<li>语义相似 semantic analogies：“Germany” : “Berlin” :: “France” : ?
结果如下图所示。</li>
</ul>
</li>
</ul>
<p><img src="/wiki/static/images/word2vec.png" style="width:600px; float:left;" /></p>
<h3 id="_5">短语学习</h3>
<p>将经常出现在一起的，而不经常在其他上下文出现的多个词作为一个token。例如：New York Times；
但是：this is，没有作为一个token！</p>
<h2 id="cbow">CBOW 论文</h2>
<p><strong>Mikolov, T.</strong>, Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>周围词加和预测中心词，Hierarchical Softmax 哈夫曼树！</p>
<h2 id="glove">Glove</h2>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014, October). Glove: Global Vectors for Word Representation. In EMNLP (Vol. 14, pp. 1532-43).</p>
<ul>
<li>square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014)</li>
<li>Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embed- dings based on a PPMI metric.</li>
</ul>
<p>核心思想，直接建模共生矩阵！（skip-gram，CBOW是直接建模上下文！不能利用全局统计信息？）
建模概率比率，而不是建模概率本身！</p>
<p>$$
F(w_i, w_j, \hat{w}_k) = \frac{P_{ik}}{P_{jk}}
$$</p>
<p>希望学习到线性关系？！</p>
<p>$$
F(w_i, w_j, \hat{w}_k) = F(w_i - w_j, \hat{w}_k) \\
 = F( (w_i - w_j)^T \hat{w}_k) = \frac{P_{ik}}{P_{jk}}
$$</p>
<p>考虑对称性，即将共生矩阵行列对换，需要保持不变性！那么要F是群(R, +)到(R+, x)的同态映射</p>
<p>$$
F((w_i - w_j)^T \hat{w}_k) = \frac{F(w_i^T \hat{w}_k)}{F(w_j^T \hat{w}_k)}
$$</p>
<p>因此，F是指数函数！</p>
<p>进而要求具有交换对称性，可以增加bias实现</p>
<p>$$
w_i^T \hat{w}_k + b_i + \hat{b}_k = \log(X_{ik})
$$</p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-10-09 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2017 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>

    </body>
</html>