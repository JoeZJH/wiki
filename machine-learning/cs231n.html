<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>CS231N - Convolutional Neural Networks for Visual Recognition - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;CS231N - Convolutional Neural Networks for Visual Recognition</div>
</div>
<div class="clearfix"></div>
<div id="title">CS231N - Convolutional Neural Networks for Visual Recognition</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a></li>
<li><a href="#bp">BP 算法与计算图</a><ul>
<li><a href="#bp_1">高效的BP算法</a></li>
<li><a href="#_2">自动微分</a></li>
<li><a href="#_3">参考</a></li>
</ul>
</li>
<li><a href="#_4">神经网络历史</a><ul>
<li><a href="#_5">感知器</a></li>
<li><a href="#_6">三层神经网络</a></li>
<li><a href="#rbm">RBM深度网络</a></li>
<li><a href="#_7">第一个强结果</a></li>
<li><a href="#_8">激活函数</a></li>
<li><a href="#cnn">CNN</a></li>
</ul>
</li>
<li><a href="#_9">目标检测</a></li>
</ul>
</div>
<h2 id="_1">关于</h2>
<p>李菲菲在Stanford开的课程，见<a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a></p>
<h2 id="bp">BP 算法与计算图</h2>
<p>一个图节点实现<code>forward</code>计算激活函数和<code>backward</code>计算梯度，图的变对应于变量。</p>
<h3 id="bp_1">高效的BP算法</h3>
<p>LeeCun 1998 的论文中给出了BP算法的一些trick：</p>
<ul>
<li>采用随机梯度下降，更快：考虑对样本的10次复制，随机梯度相当于训练了10次，而批量梯度下降只有1次！
结果通常更好：可以跳出鞍点，也可以更大概率跳出局部最优。可以随时间进一步训练！online learning。
对于SGD，理论上最优学习率随时间线性下降！</li>
<li>每一次epoch重新打散样本！</li>
<li>归一化输入，均值接近0通常收敛更快！同样，输出也尽可能是0均值。去相关，归一化方差，会加快收敛。</li>
<li>基于上一个原则，tanh比sigmoid好。</li>
<li>目标，匹配输出激活函数</li>
<li>初始化权值，权值要使得激活函数工作在线性区，（这样才好学，否则梯度为0）目标是让输入输出的方差相同（都为1）。
当输入方差为1的时候，输出方差为</li>
</ul>
<p>$$
\sigma_{y_i} = (\sum_j w_{ij}^2)^{1/2}
$$</p>
<p>为了保证输出方差也为1，那么</p>
<p>$$
\sigma_w = m^{-1/2}
$$</p>
<p>对于部分连接的网络（如CNN，DTNN），m应该是连接的节点个数。</p>
<ul>
<li>学习率，自动调整衰减。动量机制，减少震荡。</li>
</ul>
<p>$$
\Delta w(t+1) = \yita \frac{\partial E_{t+1}}{\partial w} + \mu \Delta w(t)
$$</p>
<ul>
<li>自适应学习率</li>
</ul>
<h3 id="_2">自动微分</h3>
<p>四种计算梯度的方法：</p>
<ol>
<li>手动推导梯度的公式，然后编码实现：易错，费时</li>
<li>数值微分（有限差分）：简单实现，但是效率低，而且不精确</li>
<li>符号微分（Mathematica, Maple)：表达式通常会比较复杂，存在 expression swell 的问题，而且对表达式形式有要求（close form）</li>
<li>自动微分：可以达到机器精度，和理想的渐进性能只差一个常数因子（性能牛逼！）</li>
</ol>
<h3 id="_3">参考</h3>
<ol>
<li>Automatic differentiation in machine learning: a survey, 2015</li>
<li>Efficient BP, LeeCun 1998</li>
<li><a href="https://cs231n.github.io/optimization-2/">https://cs231n.github.io/optimization-2/</a></li>
<li><a href="https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/">Stochastic Gradient Tricks</a></li>
</ol>
<h2 id="_4">神经网络历史</h2>
<h3 id="_5">感知器</h3>
<p>Frank Rosenblatt 1957</p>
<p>$$
y = f(w x + b) \\
f(z) = 1 when z&gt;0 else 0
$$</p>
<p>更新权值</p>
<p>$$
w_i(t+1) = w_i(t) + \alpha (d_j - y_j(t))x_{j, i}
$$</p>
<p>相当于下述损失函数 + SGD优化（注意这里label是+1，-1和上面有区别，这里只是便于表达）</p>
<p>$$
\max(0, - d_j * y_j)
$$</p>
<h3 id="_6">三层神经网络</h3>
<p>Rumelhart et al. 1986，BP算法</p>
<h3 id="rbm">RBM深度网络</h3>
<p>Hinton 2006</p>
<h3 id="_7">第一个强结果</h3>
<p>Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition George Dahl,
Dong Yu, Li Deng, Alex Acero, 2010 MSR</p>
<p>Imagenet classification with deep convolutional neural networks
Alex Krizhevsky, Ilya Sutskever, <strong>Geoffrey E Hinton</strong>, 2012</p>
<h3 id="_8">激活函数</h3>
<ul>
<li>
<p>sigmoid: 将结果映射到[0,1]之间，有概率解释。问题在于饱和将梯度变为0了，非0均值。而非0均值，导致梯度被限制在各分量全为正或者全为负的区域。
导致收敛变慢。<code>exp</code>函数计算复杂度较大。</p>
</li>
<li>
<p>tanh: 解决了0均值的问题</p>
</li>
<li>ReLU: 在正值区不饱和，计算效率较高，但是还是不是0均值，负向梯度为0</li>
<li>Leaky ReLU = max(0.1x, x),解决负向梯度饱和问题，</li>
<li>Maxout = max(w1 x + b1, w2 x + b2), 基本解决上述问题，但是参数变多了 double</li>
<li>ELU</li>
</ul>
<p>$$
x &gt; 0: x \\
x &lt; 0: \alpha (\exp(x) - 1)
$$</p>
<p>中心化：减去图像均值（AlexNet），减去每个通道的均值（VGGNet）</p>
<p>初始化：
“Xavier initialization” [Glorot et al., 2010]： $(n_{in} Var(w) = 1)$</p>
<p>对于 ReLU，因为一半恒为0，因此有一个0.5因子。$(\frac{1}{2} n_{in} Var(w) = 1)$ He et al., 2015</p>
<p>论文：</p>
<ol>
<li>Understanding the difficulty of training deep feedforward neural networks, Glorot and Bengio, 2010</li>
<li>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015</li>
</ol>
<p>Batch Normalize[Ioffe and Szegedy, 2015]：归一化到标准正态分布，然后让它自己学一个纺射变换。通常插入在全连接层和激活函数之间。</p>
<h3 id="cnn">CNN</h3>
<ol>
<li>1998，LeNet-5, LeCun</li>
<li>LeNet-5: Gradient-based learning applied to document recognition，1998，LeCun, Bottou, Bengio, Haffner</li>
<li>AlexNet: ImageNet Classification with Deep Convolutional Neural Networks，Hinton 2012,</li>
<li>ZFNet:</li>
<li>VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition</li>
<li>GoogLeNet: Going Deeper with Convolutions</li>
<li>ResNet</li>
</ol>
<h2 id="_9">目标检测</h2>
<p>任务：分类 + 定位</p>
<p>Location as Regression: 输入图片，输出4个坐标！（非常简单）</p>
<p>姿势估计：Toshev and Szegedy, “DeepPose: Human Pose Estimation via Deep Neural Networks”, CVPR 2014</p>
<p>sliding window: Overfeat:Integrated Recognition, Localization and Detection using Convolutional Networks,  ICLR 2014</p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-07-01 </span></div>

        </div>
        <div id="footer">
            <span>
                Copyright © 2016 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        
    </body>
</html>