<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>CS224d: Deep Learning for Natural Language Processing - Tracholar的个人wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            <div id="google-search" style="width:200px; float:right; margin: 20px 0;">
                <form action="//cse.google.com/cse" method="get" id="search-form">
                    <input type="hidden" name="cx" value="015970462532790426975:gqlen38ywus"/>
                    <input type="text" name="q"  style="line-height:20px; padding:4px;" placeholder="站内搜索"/>
                    <svg width="13" height="13" viewBox="0 0 13 13" style="position:relative; left: -20px;" onclick="document.getElementById('search-form').submit()">
                        <title>搜索</title>
                        <path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z">
                        </path>
                    </svg>
                </form>

            </div>
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;CS224d: Deep Learning for Natural Language Processing</div>
</div>
<div class="clearfix"></div>
<div id="title">CS224d: Deep Learning for Natural Language Processing</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a><ul>
<li><a href="#_2">一些疑惑</a></li>
<li><a href="#word2vec">word2vec</a><ul>
<li><a href="#_3">负采样近似</a></li>
<li><a href="#skip-gram">skip-gram模型</a></li>
<li><a href="#cbow">CBOW模型</a></li>
</ul>
</li>
<li><a href="#_4">问题</a></li>
<li><a href="#multitask-learning">Multitask learning</a></li>
<li><a href="#tips">神经网络TIPS</a></li>
<li><a href="#language-models">Language Models</a><ul>
<li><a href="#bengio-2003">神经网络语言模型 Bengio 2003</a></li>
<li><a href="#_5">递归神经网络</a></li>
<li><a href="#_6">实现细节</a></li>
<li><a href="#bidirectionl-rnns-rnn">bidirectionl RNNS 双向RNN</a></li>
</ul>
</li>
<li><a href="#deep-learning-package-zoom">Deep-learning package zoom</a></li>
<li><a href="#rnn">RNN 机器翻译</a><ul>
<li><a href="#_7">传统统计机器翻译：</a></li>
<li><a href="#rnn_1">RNN 模型</a></li>
<li><a href="#gru">GRU</a></li>
<li><a href="#lstm">LSTM</a></li>
<li><a href="#grus">更多的门 GRUs</a></li>
</ul>
</li>
<li><a href="#project">Project</a></li>
<li><a href="#nlp-benchmark-tasks">NLP benchmark tasks</a><ul>
<li><a href="#tasks">tasks</a></li>
<li><a href="#models">models</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
<li><a href="#nlp">NLP 综述</a><ul>
<li><a href="#pos-taggingwall-street-journal-wsj-data">POS tagging：数据集：Wall Street Journal (WSJ) data</a></li>
<li><a href="#chunking">Chunking</a></li>
<li><a href="#named-entity-recognition">Named Entity Recognition</a></li>
<li><a href="#semantic-role-labeling">Semantic Role Labeling</a></li>
<li><a href="#_8">神经网络方法</a><ul>
<li><a href="#transforming-words-into-feature-vectors">Transforming Words into Feature Vectors</a></li>
<li><a href="#extracting-higher-level-features-from-word-feature-vectors">Extracting Higher Level Features from Word Feature Vectors</a></li>
</ul>
</li>
<li><a href="#_9">训练</a><ul>
<li><a href="#word-level-log-likelihood">Word-Level Log-Likelihood</a></li>
<li><a href="#sentence-level-log-likelihood">Sentence-Level Log-Likelihood</a></li>
<li><a href="#stochastic-gradient">Stochastic Gradient</a></li>
</ul>
</li>
<li><a href="#_10">结果</a></li>
<li><a href="#_11">更多的未标注数据</a></li>
<li><a href="#training-language-models">Training Language Models</a></li>
</ul>
</li>
<li><a href="#sequence-to-sequence-learning-with-neural-networks">Sequence to Sequence Learning with Neural Networks</a><ul>
<li><a href="#_12">模型</a></li>
<li><a href="#_13">实验</a></li>
</ul>
</li>
<li><a href="#rnn-for-qa">RNN for QA</a><ul>
<li><a href="#matching-text-to-entities-quiz-bowl">Matching Text to Entities: Quiz Bowl</a></li>
</ul>
</li>
<li><a href="#image-retrieval-by-sentences">Image retrieval by Sentences</a></li>
<li><a href="#deep-visual-semantic-alignments-for-generating-image-descriptions">Deep Visual-Semantic Alignments for Generating Image Descriptions</a></li>
<li><a href="#rnn_2">RNN语言模型</a><ul>
<li><a href="#_14">优化技巧</a></li>
</ul>
</li>
<li><a href="#a-neural-probabilistic-language-model">A Neural Probabilistic Language Model</a></li>
<li><a href="#extensions-of-recurrent-neural-network-language-model">EXTENSIONS OF RECURRENT NEURAL NETWORK LANGUAGE MODEL</a></li>
<li><a href="#opinion-mining-with-deep-recurrent-neural-networks">Opinion Mining with Deep Recurrent Neural Networks</a></li>
<li><a href="#gated-feedback-recurrent-neural-networks">Gated Feedback Recurrent Neural Networks</a></li>
<li><a href="#recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">关于</h1>
<p>cs224d这门课是将深度学习应用到自然语言处理上面的课程，十分推荐。</p>
<h2 id="_2">一些疑惑</h2>
<p><a href="https://www.quora.com/How-is-GloVe-different-from-word2vec">https://www.quora.com/How-is-GloVe-different-from-word2vec</a><br />
对于word2vec与GloVe的比较的见解。</p>
<h2 id="word2vec">word2vec</h2>
<ul>
<li>单词的表达： Word-Net， ONE-HOT</li>
<li>文档-单词 共生矩阵， SVD提取， LSA<ul>
<li>潜在问题：SVD计算复杂度高当词典或者文档数目很大时，对新词和新的文档难以处理，与其他DL不同的学习体制。</li>
</ul>
</li>
<li>直接学习低维词向量：word2vect<ul>
<li>Learning  representa4ons  by  back-propaga4ng errors.  Rumelhart  et  al.,    1986</li>
<li>A neural  probabilis4c    language    model   (Bengio et  al.,    2003)</li>
<li>NLP   (almost)    from    Scratch (Collobert  &amp;   Weston, 2008)</li>
<li>A recent, even    simpler and faster  model:  word2vec    (Mikolov    et  al. 2013)   à   intro   now</li>
</ul>
</li>
<li>不是直接统计共同发生的次数，而是预测每一个单词周围的单词；速度快，易于应用到新词和新文档</li>
<li>目标函数<br />
$$<br />
J(\theta) = \frac{1}{T} \sum_{t=1}^T  \sum_{-m \le j \le m, j \neq 0} \log p(w_{t+j} | w_t)<br />
$$<br />
其中条件概率采用如下指数形式<br />
$$<br />
p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^W \exp(u_w^T v_c)}<br />
$$</li>
<li>每一个单词有两个向量$(u, v)$. 最终的词向量是 $(u+v)$?</li>
<li>词向量的线性关系<ul>
<li>$( X_{apple} - X_{apples} \approx X_{car} - X_{cars} \approx X_{family} - X_{families})$</li>
</ul>
</li>
</ul>
<h3 id="_3">负采样近似</h3>
<p>单个输入词向量与单个输出词向量的损失函数<br />
$$<br />
J(u_o, v_c, U) = - \log(\sigma(u_o^T v_c)) - \sum_{k \sim P} \log(\sigma(- u_k^T v_c)).<br />
$$<br />
其中求和是对总体的一个采样。实际上，负采样方法相当于将极大似然估计的问题，转化为多个二分类问题。<br />
正例是两个词出现在同一个上下文，负例是两个词没有出现在同一个上下文！<br />
且正例的概率为：</p>
<p>$$<br />
\sigma(u_o^T v_c)<br />
$$</p>
<h3 id="skip-gram">skip-gram模型</h3>
<p>设由$(w_c)$预测$(w_o)$的单个损失函数为$(F(w_o, w_c))$，那么skip-gram模型可以表示为<br />
由中心单词预测周围的单词，损失函数为<br />
$$<br />
J = \sum_{-m \le j \le, j \neq 0} F(w_{c+j}, v_c).<br />
$$</p>
<h3 id="cbow">CBOW模型</h3>
<p>CBOW模型使用周围单词的词向量之和来预测中心单词$w_c$。<br />
$$<br />
\hat{v} = \sum_{-m \le j \le, j \neq 0} v_{c+j}<br />
$$<br />
他的损失函数为<br />
$$<br />
J = F(w_c, \hat{v})<br />
$$</p>
<blockquote>
<p>WHY？</p>
<p>一般而言，这种方式上的区别使得CBOW模型更适合应用在小规模的数据集上，能够对很多的分布式信息进行平滑处理；而Skip-Gram模型则比较适合用于大规模的数据集上。<br />
</p>
</blockquote>
<h2 id="_4">问题</h2>
<ul>
<li>为什么每一次SGD后需要对参数向量进行标准化？</li>
<li>一般的交叉熵能够理解为最大似然估计么？</li>
</ul>
<h2 id="multitask-learning">Multitask learning</h2>
<p>共享网络前几层的权值，只针对不同任务改变最后一层的权值。<br />
总的代价函数是各代价函数（如交叉熵）之和。</p>
<h2 id="tips">神经网络TIPS</h2>
<ul>
<li>对词向量的监督训练的重新调整，对任务也有提升。 C&amp;W 2011</li>
<li>
<p>非线性函数</p>
<ul>
<li>sigmoid</li>
<li>tanh ： 对很多任务，比sigmoid好，初始值接近0，更快的收敛，与sigmoid一样容易求导</li>
<li>hard tanh : -1, if &lt; -1; x, if -1 &lt;= x &lt;= 1; 1, if x &gt; 1.</li>
<li>softsign(z) = z/(1 + |z|)</li>
<li>rect(z) = max(0, z)<br />
ref: Glorot and Bengio, AISTATS 2011</li>
</ul>
</li>
<li>
<p>MaxOut network (Goodfellow et al. 2013)</p>
</li>
<li>梯度下降优化建议，大数据集采用SGD和mini-batch SGD，小数据集采用L-BFGS或者CG。<br />
  大数据集L-BFGS Le et  al. ICML    2011。</li>
<li>SGD的提升，动量<br />
$$<br />
v = \mu v - \alpha \nabla_{\theta} J_t(\theta)   \\<br />
\theta^{new} = \theta^{old} + v<br />
$$</li>
<li>学习率：adagrad， adam</li>
<li>防止过拟合：<ul>
<li>减少模型大小，隐藏节点数目等</li>
<li>L1 or L2正则化</li>
<li>提前停止，选择在验证集合上最好的结果</li>
<li>隐藏节点的稀疏约束，参考UFLDL教程<br />
$$<br />
KL(1/N \sum_{n=1}^N a_i^{(n)|0.001})<br />
$$</li>
<li>dropout，输入以一定概率随机置0</li>
<li>denoise</li>
</ul>
</li>
<li>超参数的搜索：随即搜索。<br />
Y.  Bengio  (2012), “Practical  Recommendations for GradientBased<br />
Training    of  Deep    Architectures”      </li>
<li>Xavier initialization 初始化策略</li>
</ul>
<h2 id="language-models">Language Models</h2>
<p>所谓语言模型就是建立单词的联合概率模型$(P(w_1,...,w_T))$.</p>
<h3 id="bengio-2003">神经网络语言模型 Bengio 2003</h3>
<p>一个直接连接部分和一个非线性变换部分。输入为前n个词的词向量<br />
$$<br />
y = b + Wx + U tanh(d + Hx) .  \\<br />
P(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}.<br />
$$<br />
缺点是窗口是固定的。记忆能力有限？</p>
<h3 id="_5">递归神经网络</h3>
<p>基于之前见到的所有单词（理论上有无限长的时间窗）</p>
<blockquote>
<p>Condition the neural  network on  all previous<br />
words and tie the weights at  each    time    step</p>
</blockquote>
<p>设词向量列表为 $(x_1, x_2, ..., x_t, ..., x_T)$。L矩阵中的列向量。<br />
$$<br />
h_t = \sigma(W^{(hh) h_{t-1}} + W^{hx} x_{t}). \\<br />
\hat{y}_t = softmax(W^{(S)} h_t). \\<br />
P(x_{t+1}=v_j|x_t, ..., x_1) = \hat{y}_{t, j}.<br />
$$<br />
所有时刻的权值都是相同的。损失函数为所有时刻交叉熵的平均值<br />
$$<br />
J^{(t)}(\theta) = -\sum_{j=1}^{|V|} y_{t,j} \log \hat{y}_{t,j}. \\<br />
J = - \frac{1}{T} \sum_t J^{(t)}<br />
$$<br />
Perplexity ???</p>
<ul>
<li>训练困难，梯度容易衰减或者很大。Bengio et al  1994</li>
<li>
<p>初始化策略</p>
<ul>
<li>$(W^{(hh)})$ 初始化为单位阵</li>
<li>非线性函数用rect函数替换<blockquote>
<p>Parsing   with    Compositional<br />
Vector    Grammars,   Socher  et  al. 2013</p>
<p>A Simple  Way to  Initialize  Recurrent   Networks    of  Rectified   Linear<br />
Units,    Le  et  al. 2015</p>
<p>On    the difficulty   of training    Recurrent   Neural  Networks,   Pascanu et  al. 2013<br />
</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>梯度消减 Mikolov，如果梯度的范数超过阈值，就将梯度归一化到范数等于该阈值的向量或矩阵。</p>
</li>
<li>
<p>补充对RNN求梯度的理论推导</p>
</li>
<li>
<p>利用RNN学到的语言模型，生成词</p>
</li>
</ul>
<h3 id="_6">实现细节</h3>
<ul>
<li>dropout正则化，在TensorFlow里面，可以使用<code>tf.nn.dropout</code>来实现。</li>
</ul>
<h3 id="bidirectionl-rnns-rnn">bidirectionl RNNS 双向RNN</h3>
<p>每一个隐层存在两个变量，$(h^L, h^R)$。</p>
<p>$$<br />
h_t^R = f(W^R x_t + V^R h_{t-1}^R + b^R) \\<br />
h_t^L = f(W^L x_t + V^L h_{t+1}^L + b^L) \\<br />
y_t = g(U [h_t^R; h_t^L] + c)<br />
$$</p>
<p>问题：$(h_{t+1}^L)$的值怎么来？</p>
<p>数据集 MPQA    1.2 corpus</p>
<h2 id="deep-learning-package-zoom">Deep-learning package zoom</h2>
<ul>
<li>Torch</li>
<li>Caffe</li>
<li>Theano(Keras, Lasagne)</li>
<li>CuDNN</li>
<li>Tensorflow</li>
<li>Mxnet</li>
</ul>
<h2 id="rnn">RNN 机器翻译</h2>
<h3 id="_7">传统统计机器翻译：</h3>
<p>参考CS224n</p>
<ul>
<li>翻译模型 p(f|e) 和 语言模型 f(e)，然后得到目标语 Decoder : $(argmax_e p(f|e) p(e))$</li>
<li>alignment</li>
</ul>
<h3 id="rnn_1">RNN 模型</h3>
<ul>
<li>
<p>最简单的encoder + decoder 模型：</p>
<ul>
<li>Encoder：利用RNN将句子变成一个向量  $(h_t = f(W^{hh} h_{t-1} + W^{hx} x_t))$</li>
<li>Decoder：将句子向量变成一句话  $(h_t = W^{hh} h_{t-1}, y_t = softmax(W^S h_t))$</li>
<li>损失函数：最小化所有输出结果的交叉熵， $(\max_{\theta} \frac{1}{N} \sum_{n=1}^N \log p_{\theta} (y^{(n)}| x^{n}))$<br />
用句子向量作为中间桥梁。</li>
</ul>
</li>
<li>
<p>对上述模型的改进措施</p>
<ul>
<li>对Encoder和Decoder训练不同的权值</li>
<li>对Decoder，采用三个变量计算隐层，上一个时间的隐层$(h_{t-1})$，Encoder最后的状态c，上一个输出结果$(y_{t-1})$：$(h_t = \phi(h_{t-1}, c, y_{t-1}))$</li>
<li>多层网络训练</li>
<li>双向RNN Encoder</li>
<li>反过来训练？</li>
<li>门限递归单元GRU</li>
</ul>
</li>
</ul>
<p>论文：2014年，Kyunghyun Cho, Yoshua Bengio, Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</p>
<h3 id="gru">GRU</h3>
<p>门限RNN单元</p>
<ul>
<li>更新门$(z_t)$，基于当前输入和上一时刻隐层状态</li>
<li>重置门$(r_t)$，如果重置门接近于0，那么当前隐层状态将忘记之前的隐层状态，只依赖当前输入</li>
<li>新的隐层$(\widetilde{h}_t)$，另外一个的隐层状态，最终的隐层状态是基于更新门组合这个新隐层和上一时刻的隐层</li>
</ul>
<p>$$<br />
z_t = \sigma(W^{(z)} x_t + U^{(z)} h_{t-1})  \\<br />
r_t = \sigma(W^{(r)} x_t + U^{(r)} h_{t-1})  \\<br />
\widetilde{h}_t = tanh(W x_t + r_t \circ U h_{t-1})    \\<br />
h_t = z_t \circ h_{t-1} + (1 - z_t) \circ \widetilde{h}_t<br />
$$</p>
<p>当重置门接近0，允许模型忘记历史，实现短期依赖。<br />
当更新门接近1，简单复制上一时刻的隐层，导致更少的vanishing gradients，实现长期依赖。</p>
<h3 id="lstm">LSTM</h3>
<p>跟多的门，每一个门都是当前输入和上一时刻的隐层的函数，只是权值不同。</p>
<ul>
<li>输入门 $(i_t)$</li>
<li>忘记门 $(f_t)$</li>
<li>
<p>输出门 $(o_t)$</p>
</li>
<li>
<p>新的存储单元：$(\widetilde{c}_t = tanh(w^c x_t + U^c h_{t-1}))$</p>
</li>
<li>最终的存储单元：$(c_t = f_t \circ c_{t-1} + i_t \circ \widetilde{c}_t)$</li>
<li>新的隐层：$(h_t = tanh(c_t))$</li>
</ul>
<p>存储单元可以保存输入信息，除非输入让它忘记或者重写它；它可以决定是否输出信息或者只是简单地保存信息。</p>
<p>论文：2014, Sutskever, Sequence to Sequence Learning with Neural Networks, Google inc</p>
<p>比赛：WMT  2016 competition</p>
<h3 id="grus">更多的门 GRUs</h3>
<p>Gated Feedback Recurrent Neural Networks, Chung et  al, Bengio. 2015</p>
<p>更多的门来控制多个隐层之间互相连接。</p>
<h2 id="project">Project</h2>
<ul>
<li>利用deeplearning去解决kaggle上的NLP问题。</li>
</ul>
<h2 id="nlp-benchmark-tasks">NLP benchmark tasks</h2>
<h3 id="tasks">tasks</h3>
<ul>
<li>Part-Of-Speech tagging</li>
<li>chunking</li>
<li>Named Entity Recognition (NER)</li>
<li>Semantic Role Labeling (SRL)</li>
</ul>
<h3 id="models">models</h3>
<ul>
<li>
<p>CRF conditional random field</p>
</li>
<li>
<p>词语分布式假说：词的上下文相似，那么这两个词也相似</p>
</li>
</ul>
<h2 id="reference">Reference</h2>
<ol>
<li><a href="http://cs224d.stanford.edu/syllabus.html">http://cs224d.stanford.edu/syllabus.html</a></li>
<li>An   Improved    Model   of  Seman4c Similarity  Based   on  Lexical Co-Occurrence Rohde et  al. 2005</li>
</ol>
<h2 id="nlp">NLP 综述</h2>
<p>论文： Natural Language Processing (almost) from Scratch</p>
<p>文章提出一种统一的神经网络结构，可以用在很多自然语言处理任务当中：POS tagging，chunking，NER，semantic role labeling。<br />
这种方案可以不用针对特定任务进行特征工程和先验知识。</p>
<h3 id="pos-taggingwall-street-journal-wsj-data">POS tagging：数据集：Wall Street Journal (WSJ) data</h3>
<ul>
<li>Toutanova et al. (2003） 最大熵 + bidirectional dependency network =&gt; 97.24%</li>
<li>Gim ́enez and Marquez (2004) SVM + 双向维特比译码 =&gt; 97.16%</li>
<li>Shen et al. (2007) 双向序列分类 =&gt; 97.33%</li>
</ul>
<h3 id="chunking">Chunking</h3>
<p>句法成分标记</p>
<ul>
<li>Kudoh and Matsumoto (2000)： 93.48%</li>
<li>(Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008)：random fields</li>
</ul>
<h3 id="named-entity-recognition">Named Entity Recognition</h3>
<ul>
<li>Florian et al. (2003)： F1 =&gt; 88.76%</li>
<li>Ando and Zhang (2005): 89.31%</li>
</ul>
<h3 id="semantic-role-labeling">Semantic Role Labeling</h3>
<p>[John]ARG0 [ate]REL [the apple]ARG1</p>
<p>预测关系词？</p>
<h3 id="_8">神经网络方法</h3>
<ol>
<li>将词映射到特征向量</li>
<li>在一个窗口中，将词对应的特征向量拼接成一个大的向量，作为下一层的输入</li>
<li>一个正常的神经网络，线性层+非线性层 的多次堆叠！</li>
</ol>
<h4 id="transforming-words-into-feature-vectors">Transforming Words into Feature Vectors</h4>
<p>将词 embedding 到一个低维的词向量，也可以理解为一个查找表 W，输入词的索引 idx，输出W(:, idx)，<br />
而参数 W 通过BP算法学习！</p>
<p>如果加入其它离散特征，把每一个特征做同样的 embedding 操作，每一个特征都有一个查找表 $(W_k)$，这些参数都要通过后续学习。<br />
最终输出的特征向量是这些离散特征 embedding 后的特征拼接而成！</p>
<p>经过这一层后，每一个词输出为一个 $(d_{wrd})$ 维的向量。</p>
<h4 id="extracting-higher-level-features-from-word-feature-vectors">Extracting Higher Level Features from Word Feature Vectors</h4>
<ul>
<li>Window Approach： 加窗，只使用词的邻居词，将窗内的词对应的向量拼接起来，成为一个大的向量，设窗口为 $(k_{sz})$，<br />
那么，加窗后的向量长度为 $(d_{wrd} \times k_{sz})$。将这个固定长度的向量输入到一个正常的多层全连接神经网络。</li>
</ul>
<p>边界效应：对于句子起始和结束的词，在前后补充半个窗口的特殊词“PADDING”，这个词对应的词向量也是通过学习得到的。等价于学习序列的开始和结束！</p>
<ul>
<li>Sentence Approach：利用时间一维卷积层 Waibel et al. (1989) ，Time Delay Neural Networks (TDNNs)。<br />
卷积层可以学习局部特征</li>
</ul>
<p>卷积层，Max pooling层。最后得到的固定长度的特征向量进入一个标准的全连接神经网络。<br />
边界通过相同的 PADDING 方法解决不同长度的问题！</p>
<h3 id="_9">训练</h3>
<h4 id="word-level-log-likelihood">Word-Level Log-Likelihood</h4>
<p>每一个词是独立的，最大化极大似然函数等价于最小化交叉熵损失函数。</p>
<h4 id="sentence-level-log-likelihood">Sentence-Level Log-Likelihood</h4>
<p>词的标注之间是不独立的！从一个TAG到另一个TAG可能是不允许的。<br />
设TAG之间转移用score矩阵 A 表示，最终对一个句子$([x]_1^T)$ 标注序列$([i]_1^T)$ 的score是<br />
标注序列转移score和神经网络输出的score之和！</p>
<p>$$<br />
s([x]_1^T, [i]_1^T, \hat{\theta}) = \sum_{t=1}^T ([A]_{[i]_{t-1}, [i]_t} + [f_{\theta}]_{[i]_t,t}) \\<br />
\hat{\theta} = \theta \union {A_{ij}, \forall i,j }<br />
$$</p>
<p>最后输出的是score对所有路径的softmax归一化后的值，解释为路径的条件概率。<br />
因为路径数目随句子长度字数增长，所以分母上的求和项也有指数个。<br />
幸运的是，可以在线性时间复杂度内求得。</p>
<p>优化算法：动态规划，Viterbi algorithm ！</p>
<ul>
<li>其他方法：<ul>
<li>Graph Transformer Networks</li>
<li>Conditional Random Fields</li>
</ul>
</li>
</ul>
<h4 id="stochastic-gradient">Stochastic Gradient</h4>
<p>stochastic gradient (Bottou, 1991)</p>
<p>目标函数的可微性，因为 max 层的引入，导致在某些点不可微，但是随机梯度下降仍然可以找到极小值点！</p>
<h3 id="_10">结果</h3>
<p>很不幸，神经网络的结果都不如 baseline！无监督学习加入可能有用？！</p>
<h3 id="_11">更多的未标注数据</h3>
<p>利用更多未标注数据，学习词向量，然后初始化 embedding 权重！</p>
<ul>
<li>
<p>数据集：</p>
<ul>
<li>English Wikipedia</li>
<li>Reuters RCV1 (Lewis et al., 2004) dataset</li>
</ul>
</li>
<li>
<p>排序准则和熵准则</p>
</li>
</ul>
<h3 id="training-language-models">Training Language Models</h3>
<p>训练一系列词典增大的神经网络，每一个网络用之前的网络初始化 embedding 层！(Bengio et al., 2009)</p>
<h2 id="sequence-to-sequence-learning-with-neural-networks">Sequence to Sequence Learning with Neural Networks</h2>
<p>序列到序列学习！经典文献。来自 Google。</p>
<p>DNN 只能对固定长度的输入，进行建模，但是很多时候需要实现序列到序列的学习：语音识别，机器翻译！</p>
<p>序列学习的方法：用一个RNN（通常是LSTM）将一个序列编码成一个大的固定长度的向量（Encoder），<br />
然后再用一个RNN将该向量解码成一个新的序列（Decoder）。<br />
译码方案：Beam-search decoder。</p>
<p>评估指标：BLEU？</p>
<p>一个 trick： 将序列（句子）反序后，加入训练集。？</p>
<h3 id="_12">模型</h3>
<p>标准的RNN需要将输入和输出对齐才能用。<br />
利用标准的 RNN 做编码和译码的方案： Cho et al. [5]<br />
这种方案的问题在于标准的RNN难以学到长期依赖。</p>
<p>LSTM可以学到长期依赖，编码器将输入序列编码到一个固定长度的向量，解码器是一个标准的 LSTM-LM 形式的解码结构，<br />
用上述向量初始化该结构的初始隐层状态！解码序列直到输出 <strong>结束标记</strong> 才停止。</p>
<ul>
<li>这篇文章的创新点：<ul>
<li>编码器和解码器采用不同的 LSTM，参数不同，同时学习两种序列的结构</li>
<li>深层 LSTM 比浅层好，用了4层</li>
<li>将输入反序而输出不反序，再进行训练， LSTM学得更好！</li>
</ul>
</li>
</ul>
<h3 id="_13">实验</h3>
<p>WMT’14 English to French MT task</p>
<p>解码通过一个从左到右的简单 beam search 方案，每次保存 B 个最可能的前缀！<br />
每次将这B个前缀扩展，然后再保存最可能的 B 个新的前缀，直到都碰到结束符！<br />
B=1 就很好了！增加B的值，收益不是很大！？</p>
<p>将输入反序进行训练带来的收益很大！</p>
<ul>
<li>正序：1,2,3 =&gt; a,b,c</li>
<li>反序：3,2,1 =&gt; a,b,c</li>
</ul>
<blockquote>
<p>the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6</p>
</blockquote>
<p>作者给了一个简单的解释：不反序，第一个词..... 没看懂</p>
<p>比 baseline 好，但比最好的结果还是稍微差点。<br />
baseline：phrase-based SMT system？</p>
<p>H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/,<br />
2014. [Online; accessed 03-September-2014]</p>
<h2 id="rnn-for-qa">RNN for QA</h2>
<ul>
<li>一般的 RNN 应用: 词序列（句子） =&gt; 连续值或者有限的离散值</li>
<li>QA： 词序列（句子） =&gt;  富文本</li>
</ul>
<h3 id="matching-text-to-entities-quiz-bowl">Matching Text to Entities: Quiz Bowl</h3>
<p>对同一个结果，需要有冗余的样本进行学习。</p>
<p>模型：dependency-tree rnn (dt-rnn)，对语法变化具有鲁棒性，同时训练问题和答案，映射到同一个向量空间。</p>
<p>问题：词之间的这个树结构怎么得到的？！De Marneffe et al., 2006</p>
<ul>
<li>每一个关系的终止节点（词）通过矩阵 $(W_v \in \mathbb{R}^{d \times d})$ 映射到隐层。</li>
<li>中间节点也关联一个词，通过下式将该词和子节点映射到隐层。</li>
</ul>
<p>$$<br />
h_n = (W_v w_n +b + \sum_{k \in K(n)} W_{R(n, k)} h_k)<br />
$$</p>
<p>权重 $(W_{R(n, k)})$ 描述当前词n与子节点隐层 $(h_k)$ 之间的组合关系。</p>
<p>设S是所有的节点，给定一个句子，设词c是正确结果，Z是所有不正确的结果集合。那么对这一个样本，损失函数为</p>
<p>$$<br />
C(S, \theta) = \sum_{s \in S} \sum_{z \in Z} L(rank(c, s, Z)) \max(0, 1- x_c h_s + x_z h_s) \\<br />
L(r) = \sum_{i=1}^r 1/i<br />
$$</p>
<h2 id="image-retrieval-by-sentences">Image retrieval by Sentences</h2>
<p>论文：Grounded Compositional Semantics for Finding and Describing Images with Sentences, 2013, Richard Socher, <strong>Andrej Karpathy</strong>, Quoc V. Le*, Christopher D. Manning, <strong>Andrew Y. Ng</strong></p>
<p>DT—RNN：CT-RNN, Recurrent NN</p>
<p>将图像和句子映射到同一个空间，这样就可以用一个来查另外一个了。</p>
<p>zero shot learning</p>
<p>最简单的将词向量变成句子或短语的方式是，简单地线性平均这些词向量，（词向量中的 bag of word）。<br />
RNN 的方法就没有这些问题。</p>
<p>句子 parsed by the dependency parser of de Marneffe et al. (2006)</p>
<p>每一个句子被表达为一个词，词向量序列， $(s = ( (w_1, x_{w_1}), (w_2, x_{w_2}), ..., (w_n, x_{w_n}) ))$。<br />
parse后得到树状结构，可以用(孩子，父亲)对来表示 $(d(s) = { (i, j) })$。<br />
最后输入 DT-RNN 的样本是两者组成的 (s, d)</p>
<p>图像特征提取是用 DNN(Le et al., 2012)，利用未标注的web图片和标注的 ImageNet 训练学出来的，dim=4096。<br />
输入：200x200，使用了三个层：滤波（CNN），pooling(L2)，local contrast normalization.</p>
<p>local contrast normalization: 将输入的子图块（文章中5x5）减去均值，除以方差进行归一化。 有点像 layer nomalize.</p>
<ul>
<li>Multimodal Mappings<br />
    - 固定图片特征4096维<ul>
<li>联合训练图片特征向量映射到联合空间矩阵和DT-RNN参数。</li>
</ul>
</li>
</ul>
<p>损失函数：大间隔损失函数，略。</p>
<h2 id="deep-visual-semantic-alignments-for-generating-image-descriptions">Deep Visual-Semantic Alignments for Generating Image Descriptions</h2>
<p>论文：Deep Visual-Semantic Alignments for Generating Image Descriptions, <strong>Andrej Karpathy</strong>, <strong>Li Fei-Fei</strong></p>
<ul>
<li>图像特征： RCNN</li>
<li>文本特征： 双向RNN</li>
</ul>
<p>将文本特征和图像特征映射到同一个空间，并学习图像块和文本的对齐向量。</p>
<h2 id="rnn_2">RNN语言模型</h2>
<p>Recurrent neural network based language model</p>
<p>动态模型：在训练的时候，每个样本在多个epoch出现，测试的时候，也更新模型，不过一个样本只出现在一个epoch中</p>
<p>cache techiques。</p>
<h3 id="_14">优化技巧</h3>
<ol>
<li>将出现频率低于某个阈值的词映射为同一个词，称作rare token。条件概率变为：</li>
</ol>
<p>$$<br />
p(w_i(t+1)| w(t), s(t-1)) = \begin{cases}<br />
                            y_{rare}(t)/C_{rare}, w_i(t+1) is rare. \\<br />
                            y_i(t), otherwise<br />
                            \end{cases}<br />
$$</p>
<p>s 是隐层，即上下文向量。因为 rare 是多个词概率之和，所以对某个词来说，它的概率就要把 rare 的概率除以rare词的数目。<br />
对这些词来说，概率都是一样的。</p>
<p>RNN LM： 6小时； Bengio：几天，24小时 sampling</p>
<h2 id="a-neural-probabilistic-language-model">A Neural Probabilistic Language Model</h2>
<p>Yoshua Bengio，2003.</p>
<p>传统的 n-gram 模型的问题，维数灾难。随着n增大，测试集中的 n-gram 是训练集中没有的概率越来越大。<br />
解决之道：神经网络模型，词的分布式表达。</p>
<ol>
<li>学习到了词的分布是表达</li>
<li>基于这种词的表达的条件概率模型，语言模型</li>
</ol>
<p>维数灾难：建模离散随机变量的联合分布时，10个变量就有 |V|^10 个可能的状态（参数）。<br />
而建模连续变量就容易一些，可以用神经网络，等等。（连续函数一般具有局部光滑，即局部可微）。</p>
<p>传统n-gram的问题：考虑的近邻词数目太少，最好的结果也就是trigram；<br />
没有考虑到词之间的相似性。</p>
<p>维数灾难解决办法：词的分布式表达。</p>
<p>n-gram 语言模型，也会通过所有的gram进行平滑。</p>
<p>perplexity， 条件概率的倒数的集合平均值！</p>
<p>impotance sampling: Quick training of probabilistic neural nets by importance sampling， Bengio, 2003.</p>
<h2 id="extensions-of-recurrent-neural-network-language-model">EXTENSIONS OF RECURRENT NEURAL NETWORK LANGUAGE MODEL</h2>
<p>Toma ́sˇ Mikolov。问题，计算复杂度太高。要计算 softmax</p>
<p>BPTT， 4-5步就可以达到不错的精度了。</p>
<p>将输出层分解，减少计算量？？？</p>
<h2 id="opinion-mining-with-deep-recurrent-neural-networks">Opinion Mining with Deep Recurrent Neural Networks</h2>
<p>将观点挖掘作为序列标注的问题。</p>
<ul>
<li>BIO tagging scheme：<ul>
<li>B，表达观点的开始位置</li>
<li>I，表达观点的词中</li>
<li>O，词外</li>
</ul>
</li>
</ul>
<p>3-4层后，性能也上不去了！</p>
<h2 id="gated-feedback-recurrent-neural-networks">Gated Feedback Recurrent Neural Networks</h2>
<ol>
<li>多层 RNN（简单RNN， LSTM， GRU都可以）</li>
<li>不同层RNN互相连接，通过全局 rest 门控制不同层之间的交互</li>
</ol>
<p>其中，第i层到第j层的全局 reset 门，由当前时刻第 j-1 层的输入（对于第一层，是x），<br />
以及上一时刻所有的隐层共同决定，如下式：</p>
<p>$$<br />
g^{i \rightarrow j} = \sigma(w_g^{i \rightarrow j} h_t^{j-1} + u_g^{i \rightarrow j} h_{t-1}^{*})<br />
$$</p>
<p>隐层的更新，将单层隐态更新方程中上一时刻隐层项，变成对所有隐层通过全局 reset 门的组合。<br />
所有类型的 RNN如LSTM，GRU都适用，详细见论文。</p>
<ul>
<li>试验任务：<ul>
<li>character-level 语言模型，评估指标 bits-per-character</li>
<li>python 程序结果预测：输入一段python程序，要求预测输出结果。every input script ends with a print statement。 属于 sequence to sequence 的问题。通过调节程序的难度，可以在不同难度上评估不同模型的优劣。</li>
</ul>
</li>
</ul>
<h2 id="recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</h2>
<p>作者：Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, <strong>Andrew Y. Ng</strong> and Christopher Potts</p>
<p>Semantic vector spaces：？</p>
<p>在合并子节点的时候，除了传统RNN的二阶张量线性项，还增加了三阶张量线性项，对短句子建模更有效？！</p>
</div>
<div id="income">
    <!--img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" /-->

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-06-28 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'CS224d: Deep Learning for Natural Language Processing',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>