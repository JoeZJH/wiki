<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>CS224d: Deep Learning for Natural Language Processing - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />
        
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
        
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;CS224d: Deep Learning for Natural Language Processing</div>
</div>
<div class="clearfix"></div>
<div id="title">CS224d: Deep Learning for Natural Language Processing</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">关于</a><ul>
<li><a href="#_2">一些疑惑</a></li>
<li><a href="#word2vec">word2vec</a><ul>
<li><a href="#_3">负采样近似</a></li>
<li><a href="#skip-gram">skip-gram模型</a></li>
<li><a href="#cbow">CBOW模型</a></li>
</ul>
</li>
<li><a href="#_4">问题</a></li>
<li><a href="#multitask-learning">Multitask learning</a></li>
<li><a href="#tips">神经网络TIPS</a></li>
<li><a href="#language-models">Language Models</a><ul>
<li><a href="#bengio-2003">神经网络语言模型 Bengio 2003</a></li>
<li><a href="#_5">递归神经网络</a></li>
</ul>
</li>
<li><a href="#deep-learning-package-zoom">Deep-learning package zoom</a></li>
<li><a href="#project">Project</a></li>
<li><a href="#nlp-benchmark-tasks">NLP benchmark tasks</a><ul>
<li><a href="#tasks">tasks</a></li>
<li><a href="#models">models</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">关于</h1>
<p>cs224d这门课是将深度学习应用到自然语言处理上面的课程，十分推荐。</p>
<h2 id="_2">一些疑惑</h2>
<p><a href="https://www.quora.com/How-is-GloVe-different-from-word2vec">https://www.quora.com/How-is-GloVe-different-from-word2vec</a>
对于word2vec与GloVe的比较的见解。</p>
<h2 id="word2vec">word2vec</h2>
<ul>
<li>单词的表达： Word-Net， ONE-HOT</li>
<li>文档-单词 共生矩阵， SVD提取， LSA<ul>
<li>潜在问题：SVD计算复杂度高当词典或者文档数目很大时，对新词和新的文档难以处理，与其他DL不同的学习体制。</li>
</ul>
</li>
<li>直接学习低维词向量：word2vect<ul>
<li>Learning  representa4ons  by  back-propaga4ng errors.  Rumelhart  et  al.,    1986</li>
<li>A neural  probabilis4c    language    model   (Bengio et  al.,    2003)</li>
<li>NLP   (almost)    from    Scratch (Collobert  &amp;   Weston, 2008)</li>
<li>A recent, even    simpler and faster  model:  word2vec    (Mikolov    et  al. 2013)   à   intro   now</li>
</ul>
</li>
<li>不是直接统计共同发生的次数，而是预测每一个单词周围的单词；速度快，易于应用到新词和新文档</li>
<li>目标函数
$$
J(\theta) = \frac{1}{T} \sum_{t=1}^T  \sum_{-m \le j \le m, j \neq 0} \log p(w_{t+j} | w_t)
$$
其中条件概率采用如下指数形式
$$
p(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^W \exp(u_w^T v_c)}
$$</li>
<li>每一个单词有两个向量$(u, v)$. 最终的词向量是 $(u+v)$?</li>
<li>词向量的线性关系<ul>
<li>$( X_{apple} - X_{apples} \approxy X_{car} - X_{cars} \approxy X_{family} - X_{families})$</li>
</ul>
</li>
</ul>
<h3 id="_3">负采样近似</h3>
<p>单个输入词向量与单个输出词向量的损失函数
$$
J(u_o, v_c, U) = - \log(\sigma(u_o^T v_c)) - \sum_{k \sim P} \log(\sigma(- u_k^T v_c)).
$$
其中求和是对总体的一个采样?</p>
<h3 id="skip-gram">skip-gram模型</h3>
<p>设由$(w_c)$预测$(w_o)$的单个损失函数为$(F(w_o, w_c))$，那么skip-gram模型可以表示为
由中心单词预测周围的单词，损失函数为
$$
J = \sum_{-m \le j \le, j \neq 0} F(w_{c+j}, v_c).
$$</p>
<h3 id="cbow">CBOW模型</h3>
<p>CBOW模型使用周围单词的词向量之和来预测中心单词$w_c$。
$$
\hat{v} = \sum_{-m \le j \le, j \neq 0} v_{c+j}
$$
他的损失函数为
$$
J = F(w_c, \hat{v})
$$</p>
<h2 id="_4">问题</h2>
<ul>
<li>为什么每一次SGD后需要对参数向量进行标准化？</li>
<li>一般的交叉熵能够理解为最大似然估计么？</li>
</ul>
<h2 id="multitask-learning">Multitask learning</h2>
<p>共享网络前几层的权值，只针对不同任务改变最后一层的权值。
总的代价函数是各代价函数（如交叉熵）之和。</p>
<h2 id="tips">神经网络TIPS</h2>
<ul>
<li>对词向量的监督训练的重新调整，对任务也有提升。 C&amp;W 2011</li>
<li>
<p>非线性函数</p>
<ul>
<li>sigmoid</li>
<li>tanh ： 对很多任务，比sigmoid好，初始值接近0，更快的收敛，与sigmoid一样容易求导</li>
<li>hard tanh : -1, if &lt; -1; x, if -1 &lt;= x &lt;= 1; 1, if x &gt; 1.</li>
<li>softsign(z) = z/(1 + |z|)</li>
<li>rect(z) = max(0, z)
ref: Glorot and Bengio, AISTATS 2011</li>
</ul>
</li>
<li>
<p>MaxOut network (Goodfellow et al. 2013)</p>
</li>
<li>梯度下降优化建议，大数据集采用SGD和mini-batch SGD，小数据集采用L-BFGS或者CG。
  大数据集L-BFGS Le et  al. ICML    2011。</li>
<li>SGD的提升，动量
$$
v = \mu v - \alpha \nabla_{\theta} J_t(\theta)   \
\theta^{new} = \theta^{old} + v
$$</li>
<li>学习率：adagrad， adam</li>
<li>防止过拟合：<ul>
<li>减少模型大小，隐藏节点数目等</li>
<li>L1 or L2正则化</li>
<li>提前停止，选择在验证集合上最好的结果</li>
<li>隐藏节点的稀疏约束，参考UFLDL教程
$$
KL(1/N \sum_{n=1}^N a_i^{(n)|0.001})
$$</li>
<li>dropout，输入以一定概率随机置0</li>
<li>denoise</li>
</ul>
</li>
<li>超参数的搜索：随即搜索。
Y.  Bengio  (2012), “Practical  Recommendations for GradientBased
Training    of  Deep    Architectures”      </li>
</ul>
<h2 id="language-models">Language Models</h2>
<p>所谓语言模型就是建立单词的联合概率模型$(P(w_1,...,w_T))$.</p>
<h3 id="bengio-2003">神经网络语言模型 Bengio 2003</h3>
<p>一个直接连接部分和一个非线性变换部分。输入为前n个词的词向量
$$
y = b + Wx + U tanh(d + Hx) .  \
P(w_t|w_{t-1},...,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}.
$$
缺点是窗口是固定的。记忆能力有限？</p>
<h3 id="_5">递归神经网络</h3>
<p>基于之前见到的所有单词（理论上有无限长的时间窗）</p>
<blockquote>
<p>Condition the neural  network on  all previous
words and tie the weights at  each    time    step</p>
</blockquote>
<p>设词向量列表为 $(x_1, x_2, ..., x_t, ..., x_T)$。L矩阵中的列向量。
$$
h_t = \sigma(W^{(hh) h_{t-1}} + W^{hx} x_{t}). \
\hat{y}_t = softmax(W^{(S)} h_t). \
P(x_{t+1}=v_j|x_t, ..., x_1) = \hat{y}_{t, j}.
$$
所有时刻的权值都是相同的。损失函数为交叉熵
$$
J^{(t)}(\theta) = -\sum_{j=1}^{|V|} y_{t,j} \log \hat{y}_{t,j}. \
J = - \frac{1}{T} \sum_t J^{(t)}
$$
Perplexity ???</p>
<ul>
<li>训练困难，梯度容易衰减或者很大。Bengio et al  1994</li>
<li>
<p>初始化策略</p>
<ul>
<li>$(W^{(hh)})$ 初始化为单位阵</li>
<li>非线性函数用rect函数替换<blockquote>
<p>Parsing   with    Compositional
Vector    Grammars,   Socher  et  al. 2013</p>
<p>A Simple  Way to  Initialize  Recurrent   Networks    of  Rectified   Linear
Units,    Le  et  al. 2015</p>
<p>On    the difficulty   of training    Recurrent   Neural  Networks,   Pascanu et  al. 2013
</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>梯度消减 Mikolov，如果梯度的范数超过阈值，就将梯度归一化到范数等于该阈值的向量或矩阵。</p>
</li>
</ul>
<h2 id="deep-learning-package-zoom">Deep-learning package zoom</h2>
<ul>
<li>Torch</li>
<li>Caffe</li>
<li>Theano(Keras, Lasagne)</li>
<li>CuDNN</li>
<li>Tensorflow</li>
<li>Mxnet</li>
</ul>
<h2 id="project">Project</h2>
<ul>
<li>利用deeplearning去解决kaggle上的NLP问题。</li>
</ul>
<h2 id="nlp-benchmark-tasks">NLP benchmark tasks</h2>
<h3 id="tasks">tasks</h3>
<ul>
<li>Part-Of-Speech tagging</li>
<li>chunking</li>
<li>Named Entity Recognition (NER)</li>
<li>Semantic Role Labeling (SRL)</li>
</ul>
<h3 id="models">models</h3>
<ul>
<li>CRF conditional random field</li>
</ul>
<h2 id="reference">Reference</h2>
<ol>
<li><a href="http://cs224d.stanford.edu/syllabus.html">http://cs224d.stanford.edu/syllabus.html</a></li>
<li>An   Improved    Model   of  Seman4c Similarity  Based   on  Lexical Co-Occurrence Rohde et  al. 2005</li>
</ol>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-06-28 </span></div>

        </div>
        <div id="footer">
            <span>
                Copyright © 2016 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        
    </body>
</html>