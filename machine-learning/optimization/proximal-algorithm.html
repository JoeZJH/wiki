<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>近似算法 - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning">machine-learning</a>&nbsp;»&nbsp;<a href="/wiki/#machine-learning-optimization">optimization</a>&nbsp;»&nbsp;近似算法</div>
</div>
<div class="clearfix"></div>
<div id="title">近似算法</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">投影算子</a></li>
<li><a href="#moreau">Moreau 分解</a></li>
<li><a href="#_2">近似梯度法</a><ul>
<li><a href="#forward-backward-splitting">forward-backward splitting</a></li>
<li><a href="#_3">投影梯度算法</a></li>
<li><a href="#_4">交替投影算法</a></li>
</ul>
</li>
<li><a href="#_5">投影算子计算</a><ul>
<li><a href="#_6">二次函数</a></li>
<li><a href="#_7">标量函数</a></li>
<li><a href="#_8">纺射集投影</a></li>
<li><a href="#_9">半空间</a></li>
<li><a href="#_10">集合的支持函数</a></li>
<li><a href="#_11">范数</a><ul>
<li><a href="#l2">L2范数</a></li>
<li><a href="#l1">L1范数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_12">软阈值算法</a><ul>
<li><a href="#l1_1">L1范数正则</a></li>
<li><a href="#l2_1">L2范数</a></li>
<li><a href="#l21">L21范数</a></li>
</ul>
</li>
<li><a href="#_13">软阈值算法实现</a></li>
<li><a href="#admm">交替方向乘子ADMM</a></li>
<li><a href="#_14">多个函数</a></li>
<li><a href="#_15">相关资料</a></li>
</ul>
</div>
<h2 id="_1">投影算子</h2>
<p>点到闭凸集合的投影</p>
<p>$$<br />
prox_C(x) = \arg\min_y I_C(y) + \frac{1}{2}||x - y||^2<br />
$$</p>
<p>其中$(I_C)$是集合C的示性函数</p>
<p>$$<br />
I_C(x) = \begin{cases}<br />
        0, \quad x \in C \\<br />
        \infty, \quad x \not\in C<br />
        \end{cases}<br />
$$</p>
<p>几何解释就是点x到集合C的投影就是C中到x最近的点！</p>
<p>把示性函数替换成一般的凸函数f，可以得到一般的投影算子</p>
<p>$$<br />
prox_f(x) = \arg\min_y f(y) + \frac{1}{2}||x - y||^2<br />
$$</p>
<p>不动点方程：如果一个点x*在f的投影下是他自己，那么根据上式，第二项为0，所以</p>
<p>$$<br />
x^ * = prox_f(x^* ) = \arg\min_y f(x)<br />
$$</p>
<p>几何解释就是在f的最小值等高面上的点的投影是它自己！</p>
<p>由于点x到投影点$(prox_f(x))$的方向向量$(x - prox_f(x))$与等高面垂直（投影的几何解释），因此，投影操作可以看做梯度下降的推广！<br />
投影是往函数f的较小值等高面上进行投影！</p>
<p>令$(p = prox_{\lambda f}(x))$，</p>
<p>$$<br />
p = \arg\min_y \frac{1}{2} ||y - x||^2 + \lambda f(y) \\<br />
0 \in p-x + \lambda \partial f(p) \\<br />
x \in (I + \lambda \partial f) (p) \\<br />
p = (I + \lambda \partial f)^{-1} (x)<br />
$$</p>
<p>注意，次梯度的逆有唯一的像！所以投影算子与次梯度关系为</p>
<p>$$<br />
prox_{\lambda f}  = (I + \lambda \partial f)^{-1}<br />
$$</p>
<p>如果$(\lambda)$很小且f存在常规梯度，那么可以近似为</p>
<p>$$<br />
prox_{\lambda f}(v) \approx v -  \lambda \nabla f(v)<br />
$$</p>
<p>也就是说 <strong>投影操作是梯度下降的一种推广</strong>！实际上，可以看做一种前向梯度下降，即下降的梯度不是在当前点v计算得到的，而是在下降的目标点p计算得到的！</p>
<p>如果投影操作计算方便（有简单的解析解），那么用投影操作做优化可以取代梯度下降，并且可以应用到梯度下降没法用的场景——梯度不存在的函数优化！</p>
<p>近似点算法：求函数f的最小值，利用投影算子是压缩算子，且投影算子的不动点是f的最小值点性质可得迭代近似点算法</p>
<p>$$<br />
x_{n+1} = prox_{\lambda f}(x_n)<br />
$$</p>
<p>前面说到投影算子就相当于梯度下降的推广，那么近似点算法可以看做梯度下降求最小值的推广！</p>
<h2 id="moreau">Moreau 分解</h2>
<p>点v可以分解为投影和共轭投影之和</p>
<p>$$<br />
v = prox_f(v) + prox_{f^ * }(v) \\<br />
f^ * (y) = \sup_x \left(y^T x - f(x)\right)<br />
$$</p>
<p>这个分解可以看做几何中的正交分解！</p>
<h2 id="_2">近似梯度法</h2>
<h3 id="forward-backward-splitting">forward-backward splitting</h3>
<p>如果目标函数存在不可微分部分，可以将目标函数分解为两部分，一部分可以微分，另一部分不可微分</p>
<p>$$<br />
\min_x f(x) + g(x)<br />
$$</p>
<p>假设f可微，g是不可微部分。若$(p)$是最优解，那么根据最优条件可知</p>
<p>$$<br />
0 \in \lambda \nabla f(p ) + \lambda \partial g(p ) \\<br />
0 \in \lambda \nabla f(p) - p + p+ \lambda \partial g(p ) \\<br />
(I - \lambda \nabla f) (p) \in (I + \lambda \partial g) (p ) \\<br />
p = (I + \lambda \partial g)^{-1} (I - \lambda \nabla f) (p) \\<br />
p = prox_{\lambda g}\left( p - \lambda \nabla f(p)  \right)<br />
$$</p>
<p>上述不动点方程给出了优化迭代步骤，<strong>先按着可微函数梯度下降，然后对不可微函数做投影下降</strong>！</p>
<p>如果两步都采用投影来做</p>
<p>$$<br />
p = prox_{ g}\left( prox_f (p) \right)<br />
$$</p>
<p>就是 backward-backward splitting.</p>
<h3 id="_3">投影梯度算法</h3>
<p>求解约束优化问题</p>
<p>$$<br />
\min_{x \in C} f_2(x)<br />
$$</p>
<p>迭代算法</p>
<p>$$<br />
x_{k+1} = P_C\left(x_k - \gamma \nabla f_2(x_k)\right)<br />
$$</p>
<p>$(P_C)$是到凸集的投影，该算法的集合意义是先沿着f2的梯度方向下降，让后将结果点投影到集合C中，以保证解不会离开约束区域！</p>
<h3 id="_4">交替投影算法</h3>
<p>求解约束优化问题</p>
<p>$$<br />
\min_{x\in C} \frac{1}{2} d_D^2(x)<br />
$$</p>
<p>$(d_D(x))$是点x到凸集D的距离，采 backward-backward 分割，可得迭代算法</p>
<p>$$<br />
x_{k+1} = P_C(P_D(x_k))<br />
$$</p>
<p>该算法的几何图像是，交替像两个凸集C和D进行投影，直到收敛！</p>
<h2 id="_5">投影算子计算</h2>
<p>常规的计算方法是直接根据定义，求解优化问题：</p>
<p>$$<br />
\min_y f(y) + \frac{1}{2\lambda}||y - x||^2<br />
$$</p>
<table>
<thead>
<tr>
<th></th>
<th>$(dom f)$有限</th>
<th>$(dom f = R^n)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>f光滑</td>
<td>投影梯度法、内点法</td>
<td>梯度下降</td>
</tr>
<tr>
<td>f不光滑</td>
<td>投影次梯度法</td>
<td>次梯度法</td>
</tr>
</tbody>
</table>
<h3 id="_6">二次函数</h3>
<p>$(f(x) = 1/2x^T A x + b^T x +c )$，</p>
<p>$$<br />
prox_{\lambda f} (v)= (I + \lambda A)^{-1}(y - \lambda b)<br />
$$</p>
<p>如果 A = I, 且只有二次项，即$(f = || \cdot || _ 2^2)$，那么投影算子表现为shrinkage operator，直观来看就是把做了一个衰减！</p>
<p>$$<br />
prox_{\lambda f}(v) = \frac{v}{1 + \lambda}<br />
$$</p>
<h3 id="_7">标量函数</h3>
<p>如果f是标量函数，自变量是单变量，那么很容易求得</p>
<p>$$<br />
v \in \lambda \partial f(x) + x \\<br />
x =prox_{\lambda f}(v) = (1 + \partial f)^{-1} v<br />
$$</p>
<p>当$(f(x) = |x|)$时，有</p>
<p>$$<br />
prox_{\lambda f}(v) = \max(|v| - \lambda , 0) sgn(v)<br />
$$</p>
<p>即软阈值（soft thresholding）操作！</p>
<h3 id="_8">纺射集投影</h3>
<p>集合$(C = \{x| Ax = b \})$,投影操作为</p>
<p>$$<br />
\Pi_C(v) = v - A^\dagger(Ax - b)<br />
$$</p>
<p>$(\dagger)$是<a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">伪逆</a>。</p>
<p>当A是一个向量时，相当于投影到超平面，</p>
<p>$$<br />
\Pi_C(v) = v - \frac{a^Tx-b}{||a||_ 2^2} a<br />
$$</p>
<h3 id="_9">半空间</h3>
<p>集合$(C = \{x| a^Tx \le b \})$,投影操作为</p>
<p>$$<br />
\Pi_C(v) = v - \frac{(a^Tx-b) _ + }{||a|| _ 2^2} a<br />
$$</p>
<p>即要减掉向量在另外一边的分量！</p>
<h3 id="_10">集合的支持函数</h3>
<p>集合C的支持函数定义为</p>
<p>$$<br />
S_c(x) = \sup_{y\in C}y^T x<br />
$$</p>
<p>结合解释是，以x为外法向量的点就是支持点！支持函数与示性函数共轭$(S_C^ * = I_C )$。<br />
根据Moreau分解，知</p>
<p>$$<br />
prox_{\lambda S_c}(v) = v - \lambda \Pi_C(v/\lambda)<br />
$$</p>
<p>注意最后一个式子的几何理解！</p>
<h3 id="_11">范数</h3>
<p>如果函数$(f = || \cdot ||)$，那么对偶函数$(f^ * = I_B)$， B是对偶范数的单位球！</p>
<p>$$<br />
prox_{\lambda f}(v) = v - \lambda \Pi_B(v/\lambda)<br />
$$</p>
<h4 id="l2">L2范数</h4>
<p>L2范数的对偶范数还是L2范数，L2单位球就是欧式空间的单位球！所以</p>
<p>$$<br />
\Pi_B(v) = \begin{cases}<br />
            v/||v|| _ 2, \quad ||v|| _ 2 &gt; 1, \\<br />
            v, \quad ||v|| _ 2 \le 1<br />
            \end{cases}<br />
$$</p>
<p>所以L2范数的投影为</p>
<p>$$<br />
prox_{\lambda f}(v) = \begin{cases}<br />
            (1 - \lambda/||v|| _ 2) v, \quad ||v|| _ 2 &gt; \lambda, \\<br />
            0, \quad ||v|| _ 2 \le \lambda<br />
            \end{cases}<br />
$$</p>
<p>也就是将原始向量v沿着v方向减少$(\lambda)$长度，除非减到0向量！也叫做 block soft thresholding操作！</p>
<h4 id="l1">L1范数</h4>
<p>L1范数的对偶范数是$(L_{\infty})$，对应的单位球是单位立方体，投影为</p>
<p>$$<br />
\Pi_B(v) = \begin{cases}<br />
            sgn(v_i), \quad |v _ i| &gt; 1, \\<br />
            v_i, \quad |v_i| \le 1<br />
            \end{cases}<br />
$$</p>
<p>所以</p>
<p>$$<br />
v_{i+1} = \begin{cases}<br />
    v_i - \lambda, \quad v_i&gt;\lambda \\<br />
    -v_i + \lambda, \quad v_i&lt;-\lambda \\<br />
    0, \quad other<br />
    \end{cases}<br />
$$</p>
<p><img alt="软阈值" src="/wiki/static/images/soft-threshold.png" style="width:400px;float:left;" /></p>
<h2 id="_12">软阈值算法</h2>
<h3 id="l1_1">L1范数正则</h3>
<p>L1正则问题</p>
<p>$$<br />
\min_x f(x) + \lambda |x|_ 1<br />
$$</p>
<p>利用近似梯度法，令$(g=|\cdot| _ 1)$有迭代算法</p>
<p>$$<br />
z_k = x_k - \eta  \nabla f(x_k) \\<br />
x_{k+1} = prox_{g} z_k<br />
$$</p>
<p>最后一步是一个投影，根据定义</p>
<p>$$<br />
prox_{\lambda g} (v) = \arg \min_x \lambda |x| _ 1 + \frac{1}{2}||x - v||^2<br />
$$</p>
<p>对上式微分得</p>
<p>$$<br />
v_i - x_i \in \lambda \partial |x_i|, i=1,...<br />
$$</p>
<p>上式表明下降量不超过$(\lambda)$，如果$(v_i)$绝对值大于$(\lambda)$那么下降不会越过不可微分点，可以按照正常的梯度下降，<br />
但是如果小于，那么只能下降到0，才能保证上式成立！</p>
<p>$$<br />
x_i = \begin{cases}<br />
    v_i - \lambda, \quad v_i&gt;\lambda \\<br />
    -v_i + \lambda, \quad v_i&lt;-\lambda \\<br />
    0, \quad other<br />
    \end{cases}<br />
$$</p>
<p>这表明，加了L1正则项，相当于将阈值为$(\lambda)$以内的分量都置0，以上的都减小$(\lambda)$。</p>
<h3 id="l2_1">L2范数</h3>
<h3 id="l21">L21范数</h3>
<h2 id="_13">软阈值算法实现</h2>
<p>求解问题</p>
<p>$$<br />
\min_x \frac{1}{2} ||Ax - b||^2 + \lambda_2 ||x|| _ 2^2 + \lambda_1 ||x|| _ 1<br />
$$</p>
<p>可以看到，算法很快就收敛了，并且L1正则很容易得到稀疏解！</p>
<div class="hlcode"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1">## 问题100维</span>
<span class="c1">#</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># 超定方程</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">0.8</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">xx</span> <span class="o">*</span> <span class="n">mask</span>
<span class="k">print</span> <span class="s1">&#39;ground truth:&#39;</span><span class="p">,</span> <span class="n">xx</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">xx</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="c1"># 加入一些噪声</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Output</span><span class="p">:</span>
    <span class="n">ground</span> <span class="n">truth</span><span class="p">:</span> <span class="p">[</span> <span class="mf">0.</span>          <span class="mf">0.42171383</span> <span class="o">-</span><span class="mf">0.94716965</span>  <span class="mf">0.54640995</span>  <span class="mf">0.67948742</span>  <span class="mf">0.88857648</span>
    <span class="mf">0.14209921</span> <span class="o">-</span><span class="mf">0.70685128</span>  <span class="mf">0.43310285</span> <span class="o">-</span><span class="mf">1.2423563</span>   <span class="mf">0.27323468</span> <span class="o">-</span><span class="mf">0.23007387</span>
    <span class="mf">1.53049499</span> <span class="o">-</span><span class="mf">0.03076339</span> <span class="o">-</span><span class="mf">0.</span>          <span class="mf">0.53795911</span>  <span class="mf">1.1853585</span>   <span class="mf">0.66830622</span>
    <span class="mf">0.0813316</span>  <span class="o">-</span><span class="mf">0.39508658</span>  <span class="mf">0.75451939</span>  <span class="mf">0.53967945</span> <span class="o">-</span><span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.79440595</span>
    <span class="mf">0.10416921</span> <span class="o">-</span><span class="mf">0.76577241</span>  <span class="mf">2.21847433</span> <span class="o">-</span><span class="mf">1.38163076</span>  <span class="mf">0.6089114</span>   <span class="mf">1.18767332</span>
    <span class="o">-</span><span class="mf">1.28999937</span>  <span class="mf">0.65445551</span> <span class="o">-</span><span class="mf">0.3248272</span>  <span class="o">-</span><span class="mf">0.88002173</span> <span class="o">-</span><span class="mf">0.82729771</span>  <span class="mf">0.47309462</span>
    <span class="o">-</span><span class="mf">0.8384278</span>  <span class="o">-</span><span class="mf">1.66928355</span>  <span class="mf">0.85613791</span>  <span class="mf">0.31921217</span>  <span class="mf">2.51727067</span>  <span class="mf">1.11885762</span>
    <span class="mf">0.38646877</span>  <span class="mf">0.32068998</span>  <span class="mf">0.</span>          <span class="mf">1.02912399</span> <span class="o">-</span><span class="mf">0.4607417</span>  <span class="o">-</span><span class="mf">0.84519518</span>
    <span class="o">-</span><span class="mf">0.</span>          <span class="mf">0.34949314</span>  <span class="mf">0.56150765</span>  <span class="mf">0.08035849</span>  <span class="mf">1.812666</span>   <span class="o">-</span><span class="mf">1.23004836</span>
    <span class="mf">1.65564242</span>  <span class="mf">0.23581581</span> <span class="o">-</span><span class="mf">0.03529459</span> <span class="o">-</span><span class="mf">0.33258733</span> <span class="o">-</span><span class="mf">0.65909872</span> <span class="o">-</span><span class="mf">1.1317373</span>
    <span class="o">-</span><span class="mf">0.46223132</span>  <span class="mf">0.97113475</span>  <span class="mf">0.</span>          <span class="mf">0.17753836</span> <span class="o">-</span><span class="mf">0.</span>          <span class="mf">0.</span>
    <span class="mf">0.08929848</span>  <span class="mf">0.02685682</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.</span>          <span class="mf">1.86521051</span> <span class="o">-</span><span class="mf">1.02918525</span>
    <span class="mf">1.39816556</span> <span class="o">-</span><span class="mf">0.32507115</span>  <span class="mf">0.20111102</span> <span class="o">-</span><span class="mf">0.</span>         <span class="o">-</span><span class="mf">1.81123986</span>  <span class="mf">0.18043876</span>
    <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.84625861</span>  <span class="mf">0.8709556</span>   <span class="mf">0.</span>          <span class="mf">0.65961205</span>  <span class="mf">2.35225572</span>
    <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.04910171</span> <span class="o">-</span><span class="mf">1.35667457</span> <span class="o">-</span><span class="mf">1.45385942</span>  <span class="mf">0.15419398</span>  <span class="mf">1.1789595</span>
    <span class="mf">0.7340732</span>  <span class="o">-</span><span class="mf">0.</span>          <span class="mf">0.85819805</span>  <span class="mf">0.57832173</span>  <span class="mf">0.49845621</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.6047393</span>
    <span class="mf">0.99361454</span>  <span class="mf">0.45679531</span>  <span class="mf">0.28392374</span><span class="p">]</span>


<span class="c1"># 目标函数</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lab</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lab2</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span><span class="o">+</span> <span class="n">lab</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">lab2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 除了L1正则项外的梯度</span>
<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lab2</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">lab2</span><span class="o">*</span><span class="n">x</span>

<span class="c1"># 软阈值迭代算法</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">lamb</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">lab2</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># STEP1：梯度下降</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lab2</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span>

    <span class="c1"># STEP2：投影操作，也就是软阈值操作</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span>  <span class="n">lamb</span><span class="p">:</span>
            <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">lamb</span>
        <span class="k">elif</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span> <span class="n">lamb</span><span class="p">:</span>
            <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lamb</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># 打印中间迭代结果</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">i</span><span class="p">,</span> <span class="s1">&#39;loss=&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lamb</span><span class="p">,</span> <span class="n">lab2</span><span class="p">)</span>

<span class="k">print</span> <span class="n">x</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Output</span><span class="p">:</span>
    <span class="mi">0</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">7532.16879981</span>
    <span class="mi">10</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">11.0285759018</span>
    <span class="mi">20</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9840030328</span>
    <span class="mi">30</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842744582</span>
    <span class="mi">40</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842770859</span>
    <span class="mi">50</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842771092</span>
    <span class="mi">60</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842771094</span>
    <span class="mi">70</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842771094</span>
    <span class="mi">80</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842771094</span>
    <span class="mi">90</span> <span class="n">loss</span><span class="o">=</span> <span class="mf">10.9842771094</span>
    <span class="p">[</span> <span class="mf">0.</span>          <span class="mf">0.41005474</span> <span class="o">-</span><span class="mf">0.93416517</span>  <span class="mf">0.52872225</span>  <span class="mf">0.66901803</span>  <span class="mf">0.87710187</span>
    <span class="mf">0.1254697</span>  <span class="o">-</span><span class="mf">0.69410934</span>  <span class="mf">0.42094427</span> <span class="o">-</span><span class="mf">1.23128115</span>  <span class="mf">0.25644887</span> <span class="o">-</span><span class="mf">0.22262353</span>
    <span class="mf">1.51692245</span> <span class="o">-</span><span class="mf">0.02018758</span>  <span class="mf">0.</span>          <span class="mf">0.53116499</span>  <span class="mf">1.17967392</span>  <span class="mf">0.64459272</span>
    <span class="mf">0.06787308</span> <span class="o">-</span><span class="mf">0.38043066</span>  <span class="mf">0.74631218</span>  <span class="mf">0.52152388</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.79768153</span>
    <span class="mf">0.09135038</span> <span class="o">-</span><span class="mf">0.75555311</span>  <span class="mf">2.20245087</span> <span class="o">-</span><span class="mf">1.36570672</span>  <span class="mf">0.59778203</span>  <span class="mf">1.17212485</span>
    <span class="o">-</span><span class="mf">1.27509079</span>  <span class="mf">0.63889237</span> <span class="o">-</span><span class="mf">0.31797818</span> <span class="o">-</span><span class="mf">0.87161519</span> <span class="o">-</span><span class="mf">0.81344888</span>  <span class="mf">0.464237</span>
    <span class="o">-</span><span class="mf">0.82969714</span> <span class="o">-</span><span class="mf">1.66337076</span>  <span class="mf">0.83671452</span>  <span class="mf">0.30766759</span>  <span class="mf">2.50494207</span>  <span class="mf">1.1034511</span>
    <span class="mf">0.38148637</span>  <span class="mf">0.31093457</span>  <span class="mf">0.</span>          <span class="mf">1.02118932</span> <span class="o">-</span><span class="mf">0.45228788</span> <span class="o">-</span><span class="mf">0.8302418</span>   <span class="mf">0.</span>
    <span class="mf">0.32929066</span>  <span class="mf">0.54006833</span>  <span class="mf">0.06742762</span>  <span class="mf">1.80640673</span> <span class="o">-</span><span class="mf">1.2229943</span>   <span class="mf">1.64774712</span>
    <span class="mf">0.21735476</span> <span class="o">-</span><span class="mf">0.02296309</span> <span class="o">-</span><span class="mf">0.3222897</span>  <span class="o">-</span><span class="mf">0.65161403</span> <span class="o">-</span><span class="mf">1.10748401</span> <span class="o">-</span><span class="mf">0.44978335</span>
    <span class="mf">0.95982422</span>  <span class="mf">0.</span>          <span class="mf">0.16099326</span> <span class="o">-</span><span class="mf">0.00337374</span>  <span class="mf">0.</span>          <span class="mf">0.0845288</span>
    <span class="mf">0.01798603</span>  <span class="mf">0.</span>          <span class="mf">0.</span>          <span class="mf">1.84882179</span> <span class="o">-</span><span class="mf">1.01734222</span>  <span class="mf">1.39139325</span>
    <span class="o">-</span><span class="mf">0.31544124</span>  <span class="mf">0.18957113</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">1.80153297</span>  <span class="mf">0.17600738</span>  <span class="mf">0.</span>
    <span class="o">-</span><span class="mf">0.83960423</span>  <span class="mf">0.86347204</span>  <span class="mf">0.</span>          <span class="mf">0.64546066</span>  <span class="mf">2.35318802</span>  <span class="mf">0.</span>
    <span class="o">-</span><span class="mf">0.03208243</span> <span class="o">-</span><span class="mf">1.34212428</span> <span class="o">-</span><span class="mf">1.44773413</span>  <span class="mf">0.14815159</span>  <span class="mf">1.17458127</span>  <span class="mf">0.72367534</span>
    <span class="mf">0.</span>          <span class="mf">0.84649559</span>  <span class="mf">0.56543654</span>  <span class="mf">0.47495661</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.59702874</span>
    <span class="mf">0.98713984</span>  <span class="mf">0.4507846</span>   <span class="mf">0.27214572</span><span class="p">]</span>
</pre></div>


<h2 id="admm">交替方向乘子ADMM</h2>
<p>求目标可分解优化问题</p>
<p>$$<br />
\min_{x,z} f(x) + g(z) \\<br />
s.t. \quad Ax + Bz = c<br />
$$</p>
<p>利用增广拉格朗日乘子</p>
<p>$$<br />
L_{\rho}(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - c) + \rho/2 ||Ax+Bz-c||^2<br />
$$</p>
<p>利用拉格朗日对偶问题的性质可知，在强对偶条件下，最优解为</p>
<p>$$<br />
x * , y * , z * = \arg\max_y (\arg\min_{x,z} L(x,z, y) )<br />
$$</p>
<p>于是有交替迭代算法(ADMM):</p>
<p>$$<br />
x_{k+1} = \arg\min_x L_{\rho}(x, z_k, y_k) \\<br />
z_{k+1} = \arg\min_z L_{\rho}(x_{k+1}, z, y_k) \\<br />
y_{k+1} = y_k + \rho (Ax_{k+1} +Bz_{k+1} - c)<br />
$$</p>
<p>其中前两个式子实际上是两个投影算子，最后一个式子是用梯度上升求对偶函数的最大值！</p>
<p>当A=-B=I时，可得到无约束最优化问题$(\min_x f(x) + g(x))$的求解算法</p>
<p>$$<br />
x_{k+1} = prox_{\lambda_k f}(z_k - u_k) \\<br />
z_{k+1} = prox_{\lambda_k f}(x_{k+1} + u_k) \\<br />
u_{k+1} = u_k + x_{k+1} - z_{k+1}<br />
$$</p>
<h2 id="_14">多个函数</h2>
<p>优化如下问题，其中这些函数都有可能是不光滑的</p>
<p>$$<br />
\min f_1(x) + ... + f_m(x)<br />
$$</p>
<p>将问题转化为约束优化问题</p>
<p>$$<br />
\min_{x \in D}  f(x) \\<br />
f(x) = f_1(x_1) + ... + f_m(x_m) \\<br />
x = (x_1, ..., x_m), D= {(x_1,...,x_m) | x_1=...=x_m}<br />
$$</p>
<h2 id="_15">相关资料</h2>
<ol>
<li>比较全面介绍近似算法的书；Parikh N, Boyd S. Proximal algorithms[J]. Foundations and Trends® in Optimization, 2014, 1(3): 127-239.</li>
<li>近似算法综述论文：Combettes P L, Pesquet J C. Proximal splitting methods in signal processing[M]//Fixed-point algorithms for inverse problems in science and engineering. Springer, New York, NY, 2011: 185-212.</li>
<li>分布式ADMM经典论文：Boyd S, Parikh N, Chu E, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers[J]. Foundations and Trends® in Machine Learning, 2011, 3(1): 1-122.</li>
<li>压缩感知书籍：Foucart S, Rauhut H. A mathematical introduction to compressive sensing[M]. Basel: Birkhäuser, 2013.</li>
</ol>
</div>
<div id="content-footer">created in <span class="create-date date"> 2018-01-30 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '近似算法',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>