/*DEBUG
setencoding(utf-8)
*/


var tipuesearch = {"pages": [
    
      
      
      
        
      
      {"title": "关于这个WIKI", "text": "  关于       @ tracholar   过去 学 物理 ， 后来 搞 通信 ， 现在 搞 数据挖掘 。       硕士论文 :   光通信 中 量子 接收机 的 理论 与 初步 实验 研究   是 做 量子 接收机 ， 现在 做 数据挖掘 。       这 是 我 的 个人 知识 WIKI ， 采用   @ simiki       制作 而成 ， 用于 保存 自己 在 学习 和 工作 中 的 琐碎 知识 ， 如果 希望 阅读 我 的 观点 和     深入分析 ， 请 移步 到   我 的 个人 博客   。       如果 想 和 我 一样 使用 simiki 制作 一个 自己 的 wiki ， 除了 可以 参考 官方 文档 外 ， 也     可以 参考   在 github   pages 中 使用 simiki 指南   。  ", "tags": "about", "url": "/wiki/about/intro.html"},
      
      
      {"title": "短中长期规划", "text": "  短期           熟悉 spark 开发 流程 ， 学会 使用 mllib 在线 上 建模   ✅       cs224d     ✅       cs231n     ✅       cs294 ,   cs234   RL       GAN           中期           deep   learning   overview   ：   Bengio   的 书 及 相关 论文       machine   learning   overview   ：   周志华 的 书 ， prml ,   element   of   ml       分布式 工具 和 环境 ，   Spark ，   Hbase ，   Hadoop ，   Hive       分布式 算法 ， 优化 算法 等           长期           ML   技术 栈 ， 从 工程 到 算法 到 系统       金融学 基础知识       从 财务 和 技术 两 方面 分析 主流 IT 企业           2018 年           6 篇 深入 思考 博客       10 篇   关键 论文 结果 的 复现      ", "tags": "about", "url": "/wiki/about/plan.html"},
      
      
      
      
      
        
      
      {"title": "Faiss", "text": "    Table   of   Contents          ", "tags": "algo", "url": "/wiki/algo/faiss.html"},
      
      
      {"title": "最近邻搜索", "text": "    Table   of   Contents           关于           近似 方法                 关于         最近 邻 搜索   算法 在 文本 、 图像 检索 中 应用 广泛 ， 在 机器 学习 中 也 有 很多 应用 场景 ， 例如   knn   分类 。       近似 方法             LSH ( Locality - sensitive   hashing )         随机 投影 方法 ：   SimHash              ", "tags": "algo", "url": "/wiki/algo/nearest-search.html"},
      
      
      
      
      
        
      
      {"title": "以太坊网络的命令行工具", "text": "    Table   of   Contents           客户端           Geth           MAC   OS           WINDOWS           LINUX                   ETH           MAC           在 LINUX 上 安装 ：           在 WINDOWS 上 安装                   Python           其他 选项                   运行           连接 到 一个 私人 测试 网           日志           了解 有关 运行 节点 的 更 多 信息                   用法 示例           创建 帐户           获取 帐户 的 余额           一次 检查 所有 余额                         这些 是 区块 链 开发者 的 工具 。   命令行 工具 将 允许 您 连接 服务器 或 在 以太 坊 区块 链上 运行 应用程序   -   或者 您 自己 的 私人 区块 链 。       客户端       为了 安全 起 见   ， 为 以太 坊 创建 了 三个 独立 的 实现 。   客户端 具有 几乎 相同 的 功能 ， 因此 您 选择 的 客户端 可以 在 平台 、 语言 以及 您 计划 的 网络 使用 方面 进行 个人 选择 。       如果 您 正在 构建 一个 需要 对 以太 坊 网络 提供 最大 正常 运行 时间 保证 的 业务 ， 那么 我们 建议您 至少 运行 两个 客户端 的 一个 实例 以 确保 可靠性 。       Geth               Go 语言 的 实现 被 称为 Geth   。   Geth 已经 过 安全 审计 ， 将 成为 面向 最终用户 的     Mist   Browser     的 未来 基础 ， 所以 如果 你 有 网站 开发 经验 ， 并且 有 兴趣 为 dapps 构建 前端 ， 你 应该 试用 Geth 。       MAC   OS       安装 Homebrew   ， 并 确保 它 是 最新 的 ：             brew   update   brew   upgrade               然后 使用 这些 命令 来 安装 以太 坊 ：             brew   tap   ethereum / ethereum   brew   install   ethereum               有关 更 多 信息 ， 请参阅   Mac   OSX   Geth 的 完整 文档         WINDOWS       下载 最新 的 稳定   二进制 文件     ， 解压缩 ， 下载 压缩文件 ， 从 压缩包 中 提取 geth . exe ， 打开 一个 命令 终端 并 输入 ：             chdir   & lt ; path   to   extracted   binary & gt ;   open   geth . exe               有关 更 多 信息 ， 请参阅   Windows   Geth 上 的 完整 文档         LINUX       在 Ubuntu 上 ， 执行 这些 命令 ：             sudo   apt - get   install   software - properties - common   sudo   add - apt - repository   - y   ppa : ethereum / ethereum   sudo   apt - get   update   sudo   apt - get   install   ethereum               有关 其他 环境 和 更 多 说明 ， 请参阅   Geth   linux 的 完整 文档         ETH               C++ 实现 称为 Eth   。   如果 您 希望 通过 并行 运行 两种 不同 的 实现 来 增加 安全性 ， 或者 认真对待 GPU 挖掘 ， 那么 C++ “ Eth ” 客户端 就是 为您服务 的 。       MAC       安装 Homebrew   ， 然后 确保 它 是 最新 的 ：             brew   update   brew   upgrade               然后 使用 这些 命令 来 安装 cpp - ethereum ：             brew   tap   ethereum / ethereum   brew   install   cpp - ethereum   brew   linkapps   cpp - ethereum                 cpp - ethereum 文档   提供 了 有关 OS   X   Homebrew 软件包 和 从 Source 构建 OS   X 的 详细信息 。       在 LINUX 上 安装 ：       如果 您 使用 Apt - get ， 请 将 其 粘贴 到 终端 中 ：             apt - get   install   cpp - ethereum               cpp - ethereum 文档 详细 介绍 了 关于 Ubuntu 的 PPA 以及 从 Source 构建 Linux 的 信息   。       在 WINDOWS 上 安装       cpp - ethereum 文档 包含 有关 从 源代码 构建 Windows 的 详细信息 。       Python       Python 的 实现 被 称为 Pyethapp 。   如果 您 有 兴趣 了解 以太 坊 如何 工作 以及 如何 扩展 它 ， 那么 这个 客户端 的 代码 基础 可能 是 最具 可读性 的 ， 并且 拥有 一个 具有 快速 开发周期 的 合同 测试 程序库 。   这 并 不 意味着 高端 的 使用 ， 因为 这个 客户端 的 性能 没有 那么 高 的 清晰度 和 可靠性 。   如果 你 是 一个 Python 开发者 ， 希望 构建 分布式 的 应用程序 ， 或者 对 以太 坊 有 兴趣 进行 研究 或 学术研究 ， 那么 这是 一个 很 好 的 客户端 ： 我们 邀请 你 来 看看 并 为 之 作出贡献   。       其他 选项           奇偶校验 技术   （   Parity   Technologies ） 实施 了 一个 Rust 实施       由 Blockapps 开发 的 一个 Haskell 实现       如果 您 有 兴趣 开发 一个 完全 在 Web 浏览器 中 运行 的 轻型 应用程序 ， 那么 我们 推荐 使用 EthereumJS 作为 基础 。       如果 您 想 创建 一个 小型 硬件 项目 ， 请 查看 Raspberry   Pi 的 实现       如果 你 想 为 非 Ubuntu   Linux 安装 geth ， 那么 我们 建议 你 从 源代码 开始 构建       如果 你 想 在 Mac 上 有 更 多 的 灵活性 ， 试试 Homebrew           运行       Geth 和 Eth 是 运行 完整 的 以太 坊 节点 的 多用途 命令行 工具 。   它们 提供 了 多个 接口 ：   命令行 子 命令 和 选项 ，   JSON - RPC 服务器 和 交互式 控制台   。       对于 本 指南 ， 我们 将 重点 介绍 控制台 ， 这是 一个 包含 您 可能 需要 的 所有 主要 功能 的 JavaScript 环境 。   根据 您 的 客户端 ， 粘贴 以下 任一 命令 ：       Geth             geth   console               Eth       Eth 仍然 有 一个 内置 的 控制台 ， 但 它 很快 就 会 被 移除 。   开始 使用             eth               然后 使用   geth   attach     （ 如果 你 也 有 geth ） 或者 下面 的 npm 控制台 。   Eth 可能 需要 一些 时间 才能 启动 。             npm   install   - g   ethereum - console   ethconsole               第一次 启动 命令 行时 ， 您 将 获得 许可证 。   在 您 使用 之前 ， 您 必须 先 接受 此 许可证 ， 请 仔细阅读 。         注意 ： 如果 您 只 想 测试 技术 并 玩耍 ， 请勿 使用 主 网络 。   进一步 阅读 以 了解 如何 部署 私有 测试 网络 ， 而 不用 花费 太 多 时间 。         连接 到 一个 私人 测试 网       有时 你 可能 不想 连接 到 现场 公共 网络 ;   相反 ， 您 可以 选择 创建 自己 的 私人 测试 网 。   如果 您 不 需要 测试 公共 合同 ， 只想 尝试 或 开发技术 ， 这 非常 有用 。   由于 您 是 您 专用 网络 的 唯一 成员 ， 因此 您 有 责任 查找 所有 块 ， 验证 所有 交易 并 执行 所有 智能 合约 。   这 使得 开发 更 容易 ， 因为 您 可以 灵活 控制 个人 区块 链中 的 交易 。       Geth             geth   - - datadir   ~ / . ethereum _ private   init   ~ / dev / genesis . json   geth   - - fast   - - cache   512   - - ipcpath   ~ / Library / Ethereum / geth . ipc   - - networkid   1234   - - datadir   ~ / . ethereum _ private   console               Eth             eth   - - private   12345   - - genesis - json   ~ / test / genesis . json   - - db - path   ~ / . ethereum _ experiment               将 12345 替换 为 您 想要 用作 网络 ID 的 任意 数字 。   最好 改变 生成 块 的 内容 ， 因为 如果 有人 不 小心 使用 真正 的 链 连接 到 你 的 测试 网络 ， 你 的 本地 副本 将 被 认为 是 一个 旧 的 分支 ， 并 更新 为 “ 真实 ” 的 分支   。   改变 数据 地址 也 会 改变 您 本地 的 区块 链 副本 ， 否则 ， 为了 成功 地 挖掘 一个 区块 ， 您 需要 对付 区块 链 本地 副本 中 存在 的 最后 一个 区块   -   这 可能 需要 几个 小时 。       如果 你 想 创建 一个 私人 网络 ， 你 应该 出于 安全 原因 使用 不同 的 起源 块 （ genesis   block :   一个 包含 Ether 所有 交易 的 数据库 ） 。   你 可以 阅读 我们 的 博客 文章 如何 生成 您 的 文件   。   在 不久 的 将来 ， 我们 将 提供 更好 的 方法 来 获得 其他 的 起源 块 。       这些 命令 可以 防止 任何人 不 知道 您 选择 的 密码 ， 网络 ID 和 起源 文件 连接 到 您 或 为 您 提供 不 需要 的 数据 。   如果 你 想 连接 到 其他 节点 ， 并 创建 一个 多台计算机 的 小型 私人 网络 ， 他们 都 将 需要 使用 相同 的 网络 ID 和 一个 相同 的 起源 块 。   您 还 必须 帮助 每个 节点 找到 其他 节点 。   要 做到 这 一点 ， 首先 你 需要 自己 的 节点 URL ：             admin . nodeInfo . NodeUrl               该 命令 将会 返回 你 的 节点 url   -   记下 它 ， 然后 在 其他 客户端 上 ， 告诉 他们 通过 执行 这个 命令 来 添加 你 的 节点 ：             admin . addPeer ( & quot ; YOURNODEURL & quot ; )               您 不 需要 将 每个 客户端 添加 到 彼此 ， 因为 一旦 连接 ， 他们 将 共享 关于 他们 连接 到 的 任何 其他 同伴 的 信息 。       如果 您 使用 的 是 Eth ， 那么 只 需 找出 您 的 IP 并 执行 以下 命令 ：             web3 . admin . net . connect ( & quot ; YOURIP : 30303 & quot ; )               日志       如果 你 正在 运行 Geth ， 你 会 注意 到 在 你 的 控制 台上 弹出 了 很多 日志 条目   -   有时 在 你 输入 的 时候 。   这 是因为 所有 警告 和 进度 信息 都 由 客户端 实时 记录 到 您 的 终端 中 。   如果 要 将 日志 保存 到 可以 稍后 查看 的 文件 ， 请 使用 以下 命令 ：             geth   console   2 & gt ; & gt ; geth . log               Geth 支持 多个 终端 窗口 ， 你 可以 用 一个 日志 和 另 一个 控制台 启动 一个 新 的 日志 。   这 将 给 你 与 原来 的 控制台 完全相同 的 功能 ， 但 没有 混乱 。   要 做到 这 一点 ， 打开 一个 新 的 终端 窗口 并 输入 ：             geth   attach               控制台 具有 自动 完成 和 历史 命令 支持 。   您 可以 通过 按 Tab 键 完成 一个 命令 ， 然后 geth 会 自动 补全 当前 的 语句 ， 当有 多个 补全 选择 时 显示 可用 补全 列表 。   您 可以 使用 向上 和 向下 箭头键 导航 您 的 命令 历史记录 。       了解 有关 运行 节点 的 更 多 信息             备份 还原           连接 到 网络             用法 示例       创建 帐户       为了 使用 以太 坊 网络 ， 你 需要 生成 一个 帐户 。   有 很多 方法 可以 解决 这个 问题   ， 但 最 简单 的 方法 是 通过 控制台 。       注意 ： 如果 您 是 在 奥运 期间 或 开发 早期 运行 以太网 ，   请 不要 重复使用   Frontier 客户端 软件 1.0 发布 之前 生成 的 密钥   ， 否则 可能 会 受到 重播 攻击 （ replay   attacks ）   。   备份 这些 密钥 ， 并 使用 Frontier 发布 客户端 创建 新 密钥 。             personal . newAccount ( & quot ; Write   here   a   good ,   randomly   generated ,   passphrase ! & quot ; )               注意 ： 拿 起 一个 很 好 的 密码 并 写 下来 。   如果 您 丢失 了 用于 加密 帐户 的 密码 ， 则 无法访问 该 帐户 。   重复 ： 没有 安全网 。   如果 没有 有效 的 密码 ， 无法访问 您 的 帐户 ， 并且 此处 没有 “ 忘记 我 的 密码 ” 选项 。   有关 详细信息 ， 请参阅 此 XKCD   。         不要 忘记 你 的   PASSPHRASE ！         您 可以 创建 尽可能 多 或 尽可能少 的 帐户 。   按照 惯例 我们 称 您 创建 您 的 主要 帐户 的 第一个 帐户 。   您 可以 使用 以下 命令 查看 所有 帐户 ：             web3 . eth . accounts               账户 的 排序 反映 了 他们 创建 的 时间 。   密钥 文件 存储 在   DATADIR / keystore   下 ， 可以 通过 复制 其中 包含 的 文件 在 客户端 之间 传输 。   这些 文件 使用 密码 进行 加密 ， 如果 包含 任何 数量 的 ether ， 则 应 备份 。   但是 ， 请 注意 ， 如果 您 传输 个别 密钥 文件 ， 则 提交 的 帐户 顺序 可能 会 发生变化 ， 您 可能 无法 在 同一个 位置 结束 同一个 帐户 。   所以 请 注意 ， 只要 您 不 将 外部 密钥 文件 复制到 您 的 密钥 库 ， 依靠 帐户 索引 就是 合理 的 。       获取 帐户 的 余额       控制 台上 的 所有 命令 实际上 都 是 在 JavaScript 中 运行 ， 因此 您 可以 创建 变量 和 函数 。   您 也 可以 将 任何 “ eth ” 函数 编写 为 “ web3 . eth ” ， 因为 它 实际上 是 主 “ web3 ” 对象 的 一部分 。       试试 这个 例子 ：               var       primaryAccount       =       web3     .     eth     .     accounts     [     0     ]                 您 现在 有 一个 名为   primaryAccount   的 变量 ， 您 可以 在 其他 调用 中 使用 该 变量 。   要 获得 任何 帐户 的 余额 ， 请 使用   eth . getBalance   函数 ， 如下 所示 ：               web3     .     eth     .     getBalance     (     primaryAccount     )                 你 的 余额 应该 返回 0 ， 因为 你 刚 创建 它 。   为了 进行 下 一步 操作 ， 您 需要 在 帐户 中 拥有 一些 以太网 帐户 ， 以便 支付 gas 费用 。   在 下 一节 中 ， 您 将 了解 什么 是 gas ， 以及 如何 与 网络 进行 交互 。       一次 检查 所有 余额       命令行 工具 是 JavaScript 环境 ， 这 意味着 您 可以 像 创建 JavaScript 一样 创建 函数 。   例如 ， 如果 您 想要 一次 查看 所有 帐户 的 余额 ， 请 使用 此 JavaScript 代码段 。       它 将 遍历 每个 帐户 ， 并 在 以太网 中 打印 其 余额 ， 可以 使用 以下 代码 ：               function       checkAllBalances     ( )       {                 web3     .     eth     .     getAccounts     (     function     (     err     ,       accounts     )       {                           accounts     .     forEach     (     function     (     id     )       {                                     web3     .     eth     .     getBalance     (     id     ,       function     (     err     ,       balance     )       {                                               console     .     log     (     & quot ; & quot ;       +       id       +       & quot ; : \ \ tbalance :   & quot ;       +       web3     .     fromWei     (     balance     ,       & quot ; ether & quot ;     )       +       & quot ;   ether & quot ;     ) ;                                       } ) ;                       } ) ;               } ) ;       } ;                 一旦 你 执行 上面 的 行 ， 你 需要 检查 所有 的 余额 是 调用 下面 的 函数 ：               checkAllBalances     ( )                 提示 ： 如果 您 有 许多 这样 的 小巧 方便 的 脚本 ， 您 可以 将 它们 保存 到 一个 文件 中 ， 然后 使用 loadScript 一次 加载 它们 ：               loadScript     (     &# 39 ; / some / script / here . js &# 39 ;     )        ", "tags": "blockchain", "url": "/wiki/blockchain/getch-eth.html"},
      
      
      {"title": "使用命令行构建智能合约", "text": "    Table   of   Contents           使用 命令行 构建 智能 合约           你 的 第一个 公民 ： Greeter           Solc 编译器           编译 你 的 合同           使用 在线 编译器                   运行 Greeter           让 其他人 与 您 的 代码 进行 交互           清理 后 自己 ：                                 使用 命令行 构建 智能 合约       这个 页面 将 帮助 你 在 以太 坊 命令行 建立 一个 Hello ， World 合约 。 如果 您 不 知道 如何 使用 命令行 ， 我们 建议您 跳过 本 教程 ， 而是 使用 图形用户界面 构建 自定义 标记 。       智能 合约 是 以太 坊 区块 链上 的 账户 持有 对象 。 它们 包含 代码 功能 ， 可以 与 其他 合同 进行 交互 ， 做出 决定 ， 存储 数据 ， 并 将 ether 发送给 其他人 。 契约 是 由 他们 的 创造者 定义 的 ， 但 他们 的 执行 ， 以及 他们 所 提供 的 服务 ， 都 是 由 以太网 本身 提供 的 。 只要 整个 网络 存在 ， 它们 就 会 存在 并且 可 执行 ， 只有 被 编程 为 自毁 ， 它们 才 会 消失 。       你 可以 用 合同 做 什么 ？ 事实上 ， 你 几乎 可以 做 任何 事情 ， 但是 对于 我们 的 入门 指南 ， 我们 来 做 一些 简单 的 事情 ： 首先 你 将 创建 一个 经典 的 “ Hello     World ” 合约 ， 然后 你 可以 建立 你 自己 的 密码 令牌 发送给 任何 你 喜欢 的 人 。 一旦 你 掌握 了 这 一点 ， 那么 你 将 通过 众筹 筹集资金 ， 如果 成功 ， 将 提供 一个 完全 透明 和 民主 的 组织 ， 只会 服从 自己 的 公民 ， 永远 不会 摆脱 它 的 宪法 ， 不能 被 检查 或 关闭 。 而 所有 这些 都 在 不到 300 行 的 代码 中 。       在 你 开始 之前 ：             安装 以太 坊 CLI           详细 了解 合同             进入   geth   控制台 之前 ， 请 确认 GUI 已 关闭 。   运行   geth   开始 同步 过程 （ 第一次 运行 可能 需要 一段时间 ） 。       那么 现在 就 开始 吧 。       你 的 第一个 公民 ： Greeter       现在 你 已经 掌握 了 以太 坊 的 基本知识 ， 让 我们 进入 你 的 第一个 严肃 的 合同 。 Frontier 是 一个 很大 的 开放 领域 ， 有时 你 可能 会 感到 孤独 ， 所以 我们 的 第一步 就是 创造 一个 自动 的 伴侣 ， 当 你 感到 孤独 的 时候 迎接 你 。 我们 会称 他 为 “ Greeter ” 。       Greeter 是 一个 智能 的 数字 实体 ， 它 存在 于 区块 链中 ， 并 能够 根据 其 输入 与 任何 与 之 交互 的 人 进行 交谈 。 它 可能 不是 一个 演讲者 ， 但 它 是 一个 很 好 的 倾听 者 。 这 是 它 的 代码 ：                       contract       mortal       {                       / *   Define   variable   owner   of   the   type   address   * /                       address       owner     ;                         / *   This   function   is   executed   at   initialization   and   sets   the   owner   of   the   contract   * /                       function       mortal     ( )       {       owner       =       msg     .     sender     ;       }                         / *   Function   to   recover   the   funds   on   the   contract   * /                       function       kill     ( )       {       if       (     msg     .     sender       = =       owner     )       selfdestruct     (     owner     ) ;       }               }                 contract       greeter       is       mortal       {                       / *   Define   variable   greeting   of   the   type   string   * /                       string       greeting     ;                         / *   This   runs   when   the   contract   is   executed   * /                       function       greeter     (     string       _ greeting     )       public       {                               greeting       =       _ greeting     ;                       }                         / *   Main   function   * /                       function       greet     ( )       constant       returns       (     string     )       {                               return       greeting     ;                       }               }                 你 会 注意 到 这个 代码 中有 两个 不同 的 合约 ： “ mortal ” 和 “ greeter ” 。 这 是因为 Solidity （ 我们 使用 的 高级 合同 语言 ） 具有 继承性 ， 这 意味着 一个 契约 可以 继承 另 一个 契约 的 特征 。 这 对于 简化 编码 非常 有用 ， 因为 合同 的 常见 特征 不 需要 每次 重写 ， 所有 合同 都 可以 用 更 小 ， 更 易读 的 块 来 编写 。 所以 只要 声明 迎宾 者 是 凡人 ， 你 就 继承 了 “ 凡人 ” 契约 的 所有 特征 ， 并 使 迎宾 者 的 代码 简单 易读 。       继承 特征 “ 凡人 ” 仅仅 意味着 迎宾 合同 可以 被 其 所有者 杀死 ， 清理 区块 链 ， 并 在 不再 需要 合同 时 收回 锁定 的 资金 。 以太 坊 的 契约 默认 为 不 死 的 ， 没有 所有者 ， 这 意味着 一旦 被 部署 ， 作者 就 没有 特殊 的 特权 了 。 在 部署 之前 考虑 这 一点 。       Solc 编译器       在 你 能够 部署 它 之前 ， 你 需要 两件 事情 ： 编译 代码 和 应用程序 二进制 接口 ， 它 是 一个 JavaScript 对象 ， 它 定义 了 如何 与 合约 进行 交互 。       这 两个 事情 你 都 可以 通过 使用 编译器 完成 。 你 可以 使用 solidity 编译器 。       如果 你 还 没有 安装 编译器 ， 那么 你 需要 安装 一个 。 你 可以 在 这里 找到   安装 Solidity 的 说明   。             译者 注       solc 编译器 安装 方法 ： 直接 利用 npm 包 管理器 安装       npm   install   - g   solc             编译 你 的 合同       如果 你 没有 得到 以上 的 固体 ， 那么 你 需要 安装 它 。 你 可以 在 这里 找到 安装 Solidity 的 说明 。       现在 你 已经 安装 了 编译器 ， 你 需要 编译 合约 来 获取 编译 后 的 代码 和 应用程序 二进制 接口 。             solc   - o   target   - - bin   - - abi   Greeter . sol               这 将 创建 两个 文件 ， 一个 文件 包含 已 编译 的 代码 ， 另 一个 文件 在 名为 target 的 目录 中 创建 应用程序 二进制 接口 。             $ tree   .   ├ ─ ─   Greeter . sol   └ ─ ─   target         ├ ─ ─   Greeter . abi         ├ ─ ─   Greeter . bin         ├ ─ ─   Mortal . abi         └ ─ ─   Mortal . bin               你 会 看到 有 为 两个 合同 创建 的 文件 ; 但是 因为 Greeter 包括 Mortal ， 所以 你 不 需要 部署 Mortal 来 部署 Greeter 。       您 可以 使用 这 两个 文件 来 创建 和 部署 合同 。             var   greeterFactory   =   eth . contract ( & lt ; contents   of   the   file   Greeter . abi & gt ; )     var   greeterCompiled   =   & quot ; 0x & quot ;   +   & quot ; & lt ; contents   of   the   file   Greeter . bin & quot ;               你 现在 已经 编译 了 你 的 代码 ， 并 把 它 提供 给 Geth 。 现在 您 需要 准备 好 进行 部署 ， 包括 设置 一些 变量 ， 例如 您 希望 使用 的 问候语 。 将 下面 的 第一行 编辑 为 比 “ Hello     World ！ ” 更 有趣 的 内容 并 执行 这些 命令 ：                       var       _ greeting       =       & quot ; Hello   World ! & quot ;                 var       greeter       =       greeterFactory     .     new     (     _ greeting     , {     from     :     eth     .     accounts     [     0     ] ,     data     :     greeterCompiled     ,     gas     :     47000000     } ,       function     (     e     ,       contract     ) {                       if     (     !     e     )       {                             if     (     !     contract     .     address     )       {                               console     .     log     (     & quot ; Contract   transaction   send :   TransactionHash :   & quot ;       +       contract     .     transactionHash       +       & quot ;   waiting   to   be   mined ...& quot ;     ) ;                             }       else       {                               console     .     log     (     & quot ; Contract   mined !   Address :   & quot ;       +       contract     .     address     ) ;                               console     .     log     (     contract     ) ;                           }                         }               } )                 使用 在线 编译器       如果 你 没有 安装 solC ， 你 可以 简单 地 使用 在线 编译器 。 将 上面 的 源代码 复制到 在线 solidity 编译器 ， 然后 您 的 编译 代码 应该 出现 在 左侧 窗格 中 。 将   greeter   合同 和   mortal   合同 中标 有 Web3     deploy 的 框中 的 代码 复制到 单个 文本文件 中 。 现在 ， 在 该 文件 中 ， 将 第一行 更 改为 您 的 问候语 ：                       var       _ greeting       =       & quot ; Hello   World ! & quot ;                 现在 您 可以 将 结果 文本 粘贴 到 您 的 geth 窗口 中 ， 或者 使用   loadScript ( \" yourFilename . js \" )   导入 文件 。 等待 30 秒 ， 你 会 看到 这样 的 消息 ：             Contract   mined !   address :   0xdaa24d02bad7e9d6a80106db164bad9399a0423e               您 可能 必须 使用 您 在 开始 时 选择 的 密码 来 “ 解锁 ” 发送 交易 的 帐户 ， 因为 您 需要 支付 部署 合同 的 天然气 费用 。       personal . unlockAccount ( web3 . eth . accounts [ 0 ] ,   \" yourPassword \" )   。       该 合同 估计 需要 约 18 万个 气体 部署 （ 根据 在线 固体 编译器 ） ， 在 撰写 本文 时 ， 测试 网上 的 气体 价格 为 20   gwei （ 等于 （ 20000000000     wei ， 或 0.00000002   ether ） 每 单位 有 很多 有用 的 统计数据 ， 包括 网络 统计 页面 的 最新 天然气 价格 。       请 注意 ， 这些 费用 并 不 支付 给 以太 坊 开发者 ， 而是 交给 了 矿工 们 ， 那些 计算机 正在 努力 寻找 新 的 区块 并 保证 网络安全 的 同行 。 天然气 价格 是 由 当前 的 计算 供求 市场 决定 的 。 如果 天然气 价格 太高 ， 你 可以 成为 一名 矿工 ， 降低 你 的 要价 。       在 不到 一分钟 的 时间 内 ， 你 应该 有 一个 合同 地址 的 日志 ， 这 意味着 你 已经 成功 地 部署 了 你 的 合同 。 您 可以 使用 以下 命令 验证 部署 的 代码 （ 将 被 编译 ） ：             eth . getCode ( greeter . address )               如果 它 返回 “ 0x ” 以外 的 任何 内容 ， 那么 恭喜 ！ 你 的 小家伙 活着 ！ 如果 再次 创建 合同 （ 通过 执行 另 一个 eth . sendTransaction ） ， 它 将 被 发布 到 一个 新 的 地址 。       运行 Greeter       为了 打电话 给 你 的 机器人 ， 只 需 在 你 的 终端 上 输入 以下 命令 ：             greeter . greet ( ) ;               由于 这个 调用 在 区块 链上 没有 任何 变化 ， 所以 它 立即 返回 并且 没有 任何 天然气 成本 。 你 应该 看到 它 返回 你 的 问候语 ：             &# 39 ; Hello   World ! &# 39 ;               让 其他人 与 您 的 代码 进行 交互       为了 让 其他人 运行 你 的 合同 ， 他们 需要 两件 事情 ： 合同 所在 的 地址 和 ABI （ 应用程序 二进制 接口 ） ， 这是 一种 用户手册 ， 描述 了 它 的 功能 名称 以及 如何 给 你 的 JavaScript 控制台 。 为了 让 他们 每个 人 都 运行 这些 命令 ：             greeterCompiled . greeter . info . abiDefinition ;   greeter . address ;               如果 使用 基于 浏览器 的 工具 进行 编译 ， 则 可以 从 标有 “ 接口 ” 的   greeter   和   mortal   合同 的 字 段 中 获取 ABI 。       然后 ， 您 可以 实例 化 一个 JavaScript 对象 ， 该 对象 可 用于 在 连接 到 网络 的 任何 计算机 上 调用 合约 。 替换 ' ABI ' （ 一个 数组 ） 和 ' Address ' （ 一个 字符串 ） 在 JavaScript 中 创建 一个 契约 对象 ：             var   greeter   =   eth . contract ( ABI ) . at ( Address ) ;               这个 特殊 的 例子 可以 通过 简单 的 调用 来 实例 化 ：                       var       greeter2       =       eth     .     contract     ( [ {     constant     :     false     ,     inputs     :     [ ] ,     name     :     &# 39 ; kill &# 39 ;     ,     outputs     :     [ ] ,     type     :     &# 39 ; function &# 39 ;     } , {     constant     :     true     ,     inputs     :     [ ] ,     name     :     &# 39 ; greet &# 39 ;     ,     outputs     :     [ {     name     :     &# 39 ; &# 39 ;     ,     type     :     &# 39 ; string &# 39 ;     } ] ,     type     :     &# 39 ; function &# 39 ;     } , {     inputs     :     [ {     name     :     &# 39 ; _ greeting &# 39 ;     ,     type     :     &# 39 ; string &# 39 ;     } ] ,     type     :     &# 39 ; constructor &# 39 ;     } ] ) .     at     (     &# 39 ; greeterAddress &# 39 ;     ) ;                 将 greeterAddress 替换 为 合同 的 地址 。       提示 ： 如果 你 的 机器 上 没有 正确 的 安装 编译器 ， 你 可以 从 在线 编译器 获取 ABI 。 为此 ， 请 使用 下面 的 代码 ， 仔细 地用 编译器 中 的 abi 替换 greeterCompiled . greeter . info . abiDefinition 。       清理 后 自己 ：       你 必须 非常高兴 才能 签下 第一份 合同 ， 但是 当 业主 继续 写下 更 多 的 合同 时 ， 这种 激动 有时 会 消失 ， 导致 在 区块 链上 看到 放弃 的 合同 。 未来 ， 区块 链 租金 可能 会 实施 ， 以 增加 区块 链 的 可扩展性 ， 但 现在 ， 成为 一个 好 公民 ， 并 人 道 地 放弃 你 的 废弃 机器人 。       交易 将 需要 发送到 网络 ， 并 在 下面 的 代码运行 后 支付 块 链 更改 的 费用 。 自毁 是 由 网络 补贴 的 ， 所以 它 的 成本 要 比 平常 的 交易 少得 多 。             greeter . kill . sendTransaction ( { from : eth . accounts [ 0 ] } )               这 只能 由 合同 所有者 发送 的 交易 触发 。 您 可以 验证 该 行为 是否 完成 ， 只要 看看 是否 返回 0 ：             eth . getCode ( greeter . address )               请 注意 ， 每个 合约 都 必须 执行 自己 的 kill 子句 。 在 这种 特殊 情况 下 ， 只有 创建 合同 的 帐户 才能 杀死 它 。       如果 你 不 添加 任何 杀人 条款 ， 它 可能 永远 独立 于 你 和 任何 地球 上 的 边界 永远 活着 ， 所以 在 你 实际 使用 之前 ， 请 检查 你 的 当地 法律 对此 的 看法 ， 包括 对 技术 出口 的 任何 可能 的 限制 ， 任何 有关 数字 众生 公民权利 的 立法 。 对待 你 的 机器人 人 道 。  ", "tags": "blockchain", "url": "/wiki/blockchain/greeter.html"},
      
      
      {"title": "合约", "text": "    Table   of   Contents           什么 是 合同 ？           以太 坊 高级 语言 ¶           Solidity ¶           蛇 ¶           LLL ¶           Mutan （ 已弃 用 ） ¶                   写 一份 合同 ¶           编译 合同 ¶           在 geth 中 设置 solidity 编译器           编译 一个 简单 的 合同 ¶                   创建 和 部署 一个 合同 ¶           与 合同 交互 ¶           合约 元 数据           测试 合同 和 交易 ¶                 什么 是 合同 ？       契约 是 代码 （ 其 功能 ） 和 数据 （ 其 状态 ） 的 集合   位于 以太 坊 区块 链上 的 特定 地址 。 合同 账户 是   能够 在 他们 自己 之间 传递 消息 以及 实际上 图灵     完成 计算 。 合约 在 以太 坊 特有 的 区块 链上 生存   二进制 格式 称为 以太 坊 虚拟机 （ EVM ） 字节 码 。       合同 通常 用 一些 高级 语言 编写 ， 例如 Solidity ， 然后 编译成 字节 码   上 传到 区块 链上 。       也 可以 看看       还有 其他 的 语言 ， 特别 是 Serpent 和 LLL ， 在 本 文档 的 以太 坊 高级 语言 部分 中有 进一步 描述 。       Dapp 开发资源 列出 了 集成 开发 环境 ， 帮助 您 使用 这些 语言 开发 的 开发工具 ， 提供 测试 和 部署 支持 等 功能 。       以太 坊 高级 语言 ¶       以 Ethereum 虚拟机 （ EVM ） 执行 的 特定 于 以太 坊 的 二进制 格式 （ EVM 字节 码 ） 形式 签署 合同 。 然而 ， 契约 通常 是 用 更 高级 别的 语言 编写 的 ， 然后 使用 EVM 编译器 将 其 编译成 字节 代码 以 部署 到 区块 链 。       以下 是 开发人员 可 用于 为 以太 坊 编写 智能 合约 的 不同 高级 语言 。       Solidity ¶       Solidity 是 一种 类似 于 JavaScript 的 语言 ， 它 允许 您 开发 合同 并 编译 为 EVM 字节 码 。 它 目前 是 以太 坊 的 旗舰 语言 ， 也 是 最 受欢迎 的 。           Solidity 文档   -   Solidity 是 用于 编写 合同 的 旗舰 Ethereum 高级 语言 。       Solidity 在线 实时 编译器       标准化 合约 API       有用 的 应用程序 模式   -   对于 应用 程序开发 非常 有用 的 代码 片段 。           蛇 ¶       Serpent 是 一种 类似 于 Python 的 语言 ， 可 用于 开发 契约 并 编译 为 EVM 字节 码 。 它 旨在 最大 限度 地 简化 和 清理 ， 将 低级语言 的 许多 效率 优势 与 易于 使用 的 编程 风格 相结合 ， 同时 为 合约 编程 添加 特定 领域 特定 功能 。 蛇 是 使用 LLL 编译 的 。           蛇 在 ethereum 维基 上       蛇 EVM 编译器           LLL ¶       Lisp   Like   Language （ LLL ） 是 一种 类似 于 Assembly 的 低级语言 。 它 意味着 非常简单 和 简约 ; 基本上 就是 直接 在 EVM 中 进行 编码 的 一个 小包装 。           LIBLLL 在 GitHub       LLL 的 例子           Mutan （ 已弃 用 ） ¶       Mutan 是 由 Jeffrey   Wilcke 设计 和 开发 的 静态 类型 C语言 。 它 不再 被 维护 。       写 一份 合同 ¶       如果 没有 Hello   World 程序 ， 没有 语言 是 完整 的 。 在 内部 操作   以太 坊 环境 下 ， Solidity 没有 明显 的 “ 输出 ” 字符串 的 方式 。     我们 可以 做 的 最 接近 的 是 使用 日志 事件 来 放置 一个 字符串   blockchain ：             contract   HelloWorld   {                   event   Print ( string   out ) ;                   function ( )   {   Print ( & quot ; Hello ,   World ! & quot ; ) ;   }   }               此 合约 将 在 Print 类型 的 区块 链上 创建 一个 日志 条目   参数 “ Hello ， World ！ ” 每次 执行 。       也 可以 看看       Solidity 文档 有 更 多 的 例子 和 指导 编写 Solidity 代码 。       编译 合同 ¶       可靠性 合同 的 汇编 可以 通过 一些   机制 。           通过 命令行 使用 solc 编译器 。       在 JavaScript 控制台 提供 的 web3 . eth . compile . solidity 中 使用   geth 或 eth （ 这 仍然 需要 solc 编译器   安装 ） 。       在线 Solidity 实时 编译器 。       流星 戴斯 科斯 莫 建设 稳固 的 合同 。       混合 IDE 。       以太 坊 钱包 。           注意       关于 solc 和 编译 Solidity 合同 代码 的 更 多 信息 可以 在 这里 找到 。       在 geth 中 设置 solidity 编译器       如果 你 启动 你 的 geth 节点 ， 你 可以 检查 哪些 编译器   可用 。             & gt ;   web3 . eth . getCompilers ( ) ;   [ & quot ; lll & quot ; ,   & quot ; solidity & quot ; ,   & quot ; serpent & quot ; ]               该 命令 返回 一个 字符串 数组 ， 指示 哪些 编译器 是   目前 可用 。       注意       solc 编译器 与 cpp - ethereum 一起 安装 。 或者 ，   你 可以 自己 构建 它 。       如果 您 的 solc 可执行文件 位于 非标准 位置 ， 则 可以 指定 一个   使用 th   -   solc 标志 的 solc 可执行文件 的 自定义 路径 。             $   geth   - - solc   / usr / local / bin / solc               或者 ， 您 可以 通过 控制台 在 运行 时 设置 此 选项 ：             & gt ;   admin . setSolc ( & quot ; / usr / local / bin / solc & quot ; )   solc ,   the   solidity   compiler   commandline   interface   Version :   0.2 . 2 - 02bb315d / . - Darwin / appleclang / JIT   linked   to   libethereum - 1.2 . 0 - 8007cef0 / . - Darwin / appleclang / JIT   path :   / usr / local / bin / solc               编译 一个 简单 的 合同 ¶       我们 来编 一个 简单 的 合同 来源 ：             & gt ;   source   =   & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ;               这个 合同 提供 了 一个 单一 的 方法 乘以 a   正整数 a 并 返回 *   7 。       您 已经 准备 好 使用 geth   JS 控制台 来 编译 solidity 代码   eth . compile . solidity （ ） ：               & gt ;       contract       =       eth     .     compile     .     solidity     (     source     )     .     test       {           code     :       &# 39 ; 605280600c6000396000f3006000357c010000000000000000000000000000000000000000000000000000000090048063c6888fa114602e57005b60376004356041565b8060005260206000f35b6000600782029050604d565b91905056 &# 39 ;     ,           info     :       {               language     :       &# 39 ; Solidity &# 39 ;     ,               languageVersion     :       &# 39 ; 0 &# 39 ;     ,               compilerVersion     :       &# 39 ; 0.9 . 13 &# 39 ;     ,               abiDefinition     :       [     {                   constant     :       false     ,                   inputs     :       [     {                       name     :       &# 39 ; a &# 39 ;     ,                       type     :       &# 39 ; uint256 &# 39 ;                   }       ]     ,                   name     :       &# 39 ; multiply &# 39 ;     ,                   outputs     :       [     {                       name     :       &# 39 ; d &# 39 ;     ,                       type     :       &# 39 ; uint256 &# 39 ;                   }       ]     ,                   type     :       &# 39 ; function &# 39 ;               }       ] ,               userDoc     :       {                   methods     :       {                   }               }     ,               developerDoc     :       {                   methods     :       {                   }               }     ,               source     :       &# 39 ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } &# 39 ;           }       }                 注意       编译器 也 可以 通过 RPC ， 因此 通过   web3 . js 到 任何 浏览器 中 的 Ð app 连接   通过 RPC   /   IPC 进行 通信 。       以下 示例 显示 了 如何 通过 JSON - RPC 将 geth 连接 到   使用 编译器 。                     $   geth   - - datadir   ~ / eth /   - - loglevel   6   - - logtostderr = true   - - rpc   - - rpcport   8100   - - rpccorsdomain   &# 39 ; * &# 39 ;   - - mine   console     2 & gt ; & gt ;   ~ / eth / eth . log           $   curl   - X   POST   - - data   &# 39 ; { & quot ; jsonrpc & quot ; : & quot ; 2.0 & quot ; , & quot ; method & quot ; : & quot ; eth _ compileSolidity & quot ; , & quot ; params & quot ; : [ & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ; ] , & quot ; id & quot ; : 1 } &# 39 ;   http : / / 127.0 . 0.1 : 8100               一个 源 的 编译器 输出 将 为 您 提供 每个 合同 对象   代表 一份 合同 。 实际 的 回报 值   eth . compile . solidity 是 契约 对象 对 的 契约 名称 映射 。     由于 我们 的 合同 名称 是 test ， eth . compile . solidity （ source ） . test   将会 给 你 包含 合同 对象 的 测试 合同   以下 领域 ：       码       信息       资源       语言       languageVersion       compilerVersion       abiDefinition             [ Application   Binary   Interface   Definition ] ( https : / / github . com / ethereum / wiki / wiki / Ethereum - Contract - ABI )               userDoc               [     NatSpec       Doc     ] (     https     :     / / github . com / ethereum / wiki / wiki / Ethereum - Natural - Specification - Format )   for   users .                 developerDoc               [     NatSpec       Doc     ] (     https     :     / / github . com / ethereum / wiki / wiki / Ethereum - Natural - Specification - Format )   for   developers .                 编译器 输出 的 直接 结构 （ 编码 和 信息 ）   反映 了 两种 截然不同 的 部署 路径 。 编译 的 EVM 代码   通过 合同 创建 交易 发送到 区块 链     休息 （ 信息 ） 将 理想 地 生活 在 可 公开 验证 的 分散式 云上   元 数据 补充 区块 链上 的 代码 。       如果 您 的 来源 包含 多个 合同 ， 则 输出 将 包含 一个 条目   对于 每个 合同 ， 相应 的 合同 信息 对象 可以 被 检索   合同 的 名称 作为 属性 名称 。 你 可以 尝试 通过 检查     最新 的 GlobalRegistrar 代码 ：             contracts   =   eth . compile . solidity ( globalRegistrarSrc )               创建 和 部署 一个 合同 ¶       在 开始 本 节 之前 ， 请 确保您 同时 拥有 一个 未 锁定 的 帐户   以及 一些 资金 。       现在 ， 您 将 通过 将 事务处理 以前 一节 中 的 EVM 代码 作为 数据 发送到 空白 地址 来 在 区块 链上 创建 合同 。       注意       使用 在线 Solidity 实时 可以 轻松 完成 此 操作   编译器 或 Mix   IDE 程序 。             var   primaryAddress   =   eth . accounts [ 0 ]   var   abi   =   [ {   constant :   false ,   inputs :   {   name :   &# 39 ; a &# 39 ; ,   type :   &# 39 ; uint256 &# 39 ;   }   } ]   var   MyContract   =   eth . contract ( abi )   var   contract   =   MyContract . new ( arg1 ,   arg2 ,   ... ,   { from :   primaryAddress ,   data :   evmByteCodeFromPreviousSection } )               所有 二进制 数据 都 以 十六进制 格式 进行 序列化 。 十六进制 字符串 总是 有 一个   十六进制 前缀 0x 。       注意       请 注意 ， arg1 ， arg2 ， ... 是 合约 的 参数   构造函数 ， 以防 它 接受 任何 。 如果 合同 不 需要 任何   构造 函数参数 ， 那么 这些 参数 可以 省略 。       值得 指出 的 是 ， 这 一步 需要 您 付费 执行 。 你 的   账户 余额 （ 您 作为 发件人 在 发件人 字段 中 ） 将会 是   一旦 交易 完成 ， 根据 EVM 的 气体 规则 减少     成 块 。 过 了 一段时间 ， 你 的 交易 应该 出现 在 一个   确认 它 所 带来 的 状态 是 一个 共识 。 你 的 合同   现在 居住 在 区块 链上 。       异步 的 做法 是 这样 的 ：             MyContract . new ( [ arg1 ,   arg2 ,   ... , ] { from :   primaryAccount ,   data :   evmCode } ,   function ( err ,   contract )   {       if   ( ! err   & amp ; & amp ;   contract . address )           console . log ( contract . address ) ;   } ) ;               与 合同 交互 ¶       与 合约 的 交互 通常 使用 抽象 层来 完成   作为 eth . contract （ ）   函数 ， 它 返回 一个 javascript 对象 与 所有 的 合约 功能     在 JavaScript 中 可 用作 可 调用函数 。       描述 合同 可用 功能 的 标准 方式 是 ABI   定义 。   这个 对象 是 一个 描述 呼叫 签名 和 返回值 的 数组   为 每个 可用 的 合约 功能 。             var   Multiply7   =   eth . contract ( contract . info . abiDefinition ) ;   var   myMultiply7   =   Multiply7 . at ( address ) ;               现在 在 ABI 中 指定 的 所有 函数调用 都 可用   合同 实例 。 您 可以 在 合同 实例 上 调用 这些 方法   以 两种 方式 之一 。             & gt ;   myMultiply7 . multiply . sendTransaction ( 3 ,   { from :   address } )   & quot ; 0x12345 & quot ;   & gt ;   myMultiply7 . multiply . call ( 3 )   21               当 使用 sendTransaction 调用 时 ， 函数调用 通过 发送 来 执行   交易 。 这 将 花费 乙醚 发送 和 通话 将 被 记录   永远 在 区块 链上 。 以 这种 方式 进行 的 呼叫 的 返回值 是     交易 的 散列 。       当 使用 调用 进行 调用 时 ， 该 功能 在 EVM 和 本地 执行   函数 的 返回值 与 函数 一起 返回 。 打电话 给 在 这   方式 不 记录 在 区块 链上 ， 因此 不能 修改 内部     合同 的 状态 。 这种 呼叫 方式 被 称为 常数   函数调用 。 以 这种 方式 进行 的 呼叫 不会 花费 任何 代价 。       如果 您 只 对 返回值 和 使用 感兴趣 ， 您 应该 使用 呼叫   sendTransaction 如果 你 只 关心 对 状态 的 副作用   合同 。       在 上面 的 例子 中 ， 没有 副作用 ， 因此 sendTransaction   只 燃烧 气体 ， 增加 宇宙 的 熵 。       合约 元 数据       在 前面 的 章节 中 ， 我们 解释 了 你 如何 创建 合同   blockchain 。 现在 我们 将 处理 其余 的 编译器 输出 结果   合同 元 数据 或 合同 信息 。       与 合同 交互 时 ， 您 并未 创建 您 可能 需要 的 合同   文档 或 查看 源代码 。 合同 作者 受到 鼓励   通过 在 区块 链上 注册 来 提供 这些 信息     通过 第三方 服务 ， 如 EtherChain 。 管理 API 提供   方便 的 方法 来 获取 这个 包 的 任何 合同 ， 选择   寄存器 。             / /   get   the   contract   info   for   contract   address   to   do   manual   verification   var   info   =   admin . getContractInfo ( address )   / /   lookup ,   fetch ,   decode   var   source   =   info . source ;   var   abiDef   =   info . abiDefinition               使 这项 工作 的 基本 机制 是 ：           合同 信息 被 上 传到 可以 通过 URI 识别 的 地方   是 可 公开 访问 的       任何人 都 可以 找出 什么 是 只 知道 合约 的 URI   地址           这些 要求 是 通过 使用 2 步 区块 链 注册表 来 实现 的 。 首先   步骤 在 合同 中将 合同 代码 （ 哈希 ） 与 内容 哈希 注册   称为 HashReg 。 第二步 注册 一个 带有 内容 哈希 的 url     UrlHint 合同 。 这些 注册 合同   是 边疆 释放 的 一部分 ， 并 进入 了 宅基地 。       通过 使用 这种 方案 ， 知道 合同 的 地址 来 查找 URL 并 获取 实际 合同 元 数据 信息 包 就 足够 了 。       所以 如果 你 是 一个 认真 的 合同 创造者 ， 其 步骤 如下 ：           将 合约 本身 部署 到 区块 链       获取 合同 信息 json 文件 。       将 合同 信息 json 文件 部署 到 您 选择 的 任何 网址       注册 codehash   -   & gt ; 内容 哈希   -   & gt ; 网址           JS   API 通过 提供 帮助 程序 使 这个 过程 非常简单 。 呼叫   admin . register 从 合约 中 提取 信息 ， 写出 它 的 json     序列化 在 给定 的 文件 中 ， 计算 文件 的 内容 哈希 值   最后 把 这个 内容 哈希 注册 到 合同 的 代码 哈希 。 一旦 您     部署 该 文件 到 任何 网址 ， 您 可以 使用 admin . registerUrl 进行 注册   在 区块 链上 的 内容 哈希 链接 也 是 如此 。   （ 注意 ， 如果 是     固定 内容 寻址 模型 被 用作 文档 存储 ， url - hint 是 no   需要 更长 的 时间 ）                     source   =   & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ;   / /   compile   with   solc   contract   =   eth . compile . solidity ( source ) . test   / /   create   contract   object   var   MyContract   =   eth . contract ( contract . info . abiDefinition )   / /   extracts   info   from   contract ,   save   the   json   serialisation   in   the   given   file ,   contenthash   =   admin . saveInfo ( contract . info ,   & quot ; ~ / dapps / shared / contracts / test / info . json & quot ; )   / /   send   off   the   contract   to   the   blockchain   MyContract . new ( { from :   primaryAccount ,   data :   contract . code } ,   function ( error ,   contract ) {       if ( ! error   & amp ; & amp ;   contract . address )   {           / /   calculates   the   content   hash   and   registers   it   with   the   code   hash   in   ` HashReg `           / /   it   uses   address   to   send   the   transaction .           / /   returns   the   content   hash   that   we   use   to   register   a   url           admin . register ( primaryAccount ,   contract . address ,   contenthash )           / /   here   you   deploy   ~ / dapps / shared / contracts / test / info . json   to   a   url           admin . registerUrl ( primaryAccount ,   hash ,   url )       }   } ) ;               测试 合同 和 交易 ¶       通常 你 需要 采取 低级 别的 测试 和 调试 策略   合同 和 交易 。 本 节 介绍 一些 调试 工具 和   您 可以 使用 的 做法 。 为了 测试 合同 和 交易 没有     真正 的 结果 ， 你 最好 在 私人 区块 链上 测试 它 。 这 可以   通过 配置 替代 网络 ID （ 选择 唯一 的 整数 ） 来 实现   和 / 或 禁用 对等体 。 建议 练习 ， 为了 测试 你 使用 一个     替代 数据 目录 和 端口 ， 以便 您 甚至 不会 意外 冲突   与 您 的 实时 运行 的 节点 （ 假设 使用 默认 运行   你 的 虚拟机 调试模式 下 的 配置文件 和 最高 的 日志 记录   详细 级别 建议 ：             geth   - - datadir   ~ / dapps / testing / 00 /   - - port   30310   - - rpcport   8110   - - networkid   4567890   - - nodiscover   - - maxpeers   0   - - vmdebug   - - verbosity   6   - - pprof   - - pprofport   6110   console   2 & gt ; & gt ;   ~ / dapp / testint / 00 / 00 . log               在 您 提交 任何 交易 之前 ， 您 需要 设置 您 的 私人 测试   链 。 请参阅 测试 网络 。             / /     create       account   .     will       prompt       for       password       personal   .   newAccount   ( ) ;   / /     name       your       primary       account   ,     will       often       use       it       primary     =     eth   .   accounts   [   0   ] ;   / /     check       your       balance     (   denominated       in       ether   )     balance     =     web3   .   fromWei   (   eth   .   getBalance   (   primary   ) ,     & quot ; ether & quot ;   ) ;         / /     assume       an       existing       unlocked       primary       account       primary     =     eth   .   accounts   [   0   ] ;     / /     mine       10       blocks       to       generate       ether       / /     starting       miner       miner   .   start   (   4   ) ;   / /     sleep       for       10       blocks     (   this       can       take       quite       some       time   ) .     admin   .   sleepBlocks   (   10   ) ;   / /     then       stop       mining     (   just       not       to       burn       heat       in       vain   )     miner   .   stop   ( ) ;     balance     =     web3   .   fromWei   (   eth   .   getBalance   (   primary   ) ,     & quot ; ether & quot ;   ) ;               创建 交易 后 ， 您 可以 使用 以下 几行 强制 处理 它们 ：             miner . start ( 1 ) ;   admin . sleepBlocks ( 1 ) ;   miner . stop ( ) ;               您 可以 使用             / /   shows   transaction   pool   txpool . status   / /   number   of   pending   txs   eth . getBlockTransactionCount ( & quot ; pending & quot ; ) ;   / /   print   all   pending   txs   eth . getBlock ( & quot ; pending & quot ; ,   true ) . transactions               如果 您 提交 了 合同 创建 交易 ， 您 可以 检查 所 需 的 代码 是否 实际 插入 到 当前 区块 链中 ：             txhash   =   eth . sendTansaction ( { from : primary ,   data :   code } )   / / ...   mining   contractaddress   =   eth . getTransactionReceipt ( txhash ) ;   eth . getCode ( contractaddress )      ", "tags": "blockchain", "url": "/wiki/blockchain/contracts.html"},
      
      
      {"title": "帐户类型，gas和交易", "text": "    Table   of   Contents           EOA 与 合约 账户           外部 拥有 账户 （ EOAs ）           合同 帐户                   什么 是 交易 ？           什么 是 信息 ？           什么 是 天然气 ？           估算 交易成本           gasUsed           天然气 价格           交易成本 示例                   帐户 互动 示例   -   博彩 合同           离线 签署 交易                 EOA 与 合约 账户       以太 坊 有 两种 类型 的 账户           外部 拥有 的 帐户       合同 账户           在 Serenity 中 可以 消除 这种 区别 。       外部 拥有 账户 （ EOAs ）       一个 外部 控制 的 账户           有 一个   ether   balance ，       可以 发送 交易 （ 以太 汇款 或 触发 合约 代码 ） ，       由 私钥 控制 ，       没有 相关 的 代码 。           合同 帐户       合同           有 一个 ether   balance ，       有 相关 的 代码 ，       代码执行 由 从 其他 合同 收到 的 事务 或 消息 （ 调用 ） 触发 。       当 执行   -   执行 任意 复杂 的 操作 （ 图灵 完备 性 ）   -   操纵 自己 的 永久 存储 ， 即 可以 拥有 自己 的 永久 状态   -   可以 调用 其他 合同           以太 坊 区块 链上 的 所有 操作 都 是 由 外部 拥有 账户 发起 的 交易 启动 的 。 每当 合同 账户 收到 一笔 交易 时 ， 其 代码 就 会 按照 作为 交易 一部分 发送 的 输入 参数 的 指示 执行 。 合约 代码 由 参与 网络 的 每个 节点 上 的 以太 坊 虚拟机 执行 ， 作为 对 新块 进行 验证 的 一部分 。       这种 执行 需要 完全 确定性 ， 唯一 的 上下文 是 区块 链上 的 块 位置 和 所有 可用 的 数据 。     区块 链上 的 区块 表示 时间 单位 ， 区块 链 本身 是 一个 时间 维度 ， 并 表示 由链 上 的 区块 指定 的 离散 时间 点处 的 状态 的 整个 历史 。       所有 乙醚 的 余额 和 价值 都 以 魏为 单位 ： 1 乙醚 是 1e18   wei 。       注意       以太 坊 中 的 “ 契约 ” 不应 被 视为 应该 被 “ 履行 ” 或 “ 被 遵守 ” 的 事物 。 相反 ， 它们 更 像是 以太 坊 执行 环境 中 的 “ 自主 代理 ” ， 当 被 消息 或 事务 “ 戳穿 ” 时 总是 执行 特定 的 代码段 ， 并 直接 控制 自己 的 以太 平衡 和 自己 的 密钥 / 价值 商店 来 存储 他们 的 永久 状态 。       什么 是 交易 ？       在 以太 坊 使用 术语 “ 交易 ” 来 指代 存储 从 外部 拥有 账户 发送到 区块 链上 另 一个 账户 的 消息 的 签名 数据包 。       交易 包含 ：   消息 的 接收者 ，   识别 发送者 的 签名 并 证明 他们 通过 区块 链向 接收者 发送 消息 的 意图 ，   VALUE 字段   -   从 发件人 传送 给 收件人 的 金额 ，     一个 可选 的 数据字 段 ， 可以 包含 发送给 合同 的 消息 ，   表示 允许 执行 交易 执行 的 最大 计算 步骤 数量 的 STARTGAS 值 ，     一个 GASPRICE 值 ， 代表 发送者 愿意 为 天然气 支付 的 费用 。 一个 单位 的 气体 对应 于 一个 原子 指令 的 执行 ， 即 计算 步骤 。       什么 是 信息 ？       合同 能够 将 “ 消息 ” 发送给 其他 合同 。 消息 是从 不 序列化 的 虚拟 对象 ， 只 存在 于 以太 坊 执行 环境 中 。   他们 可以 被 认为 是 函数调用 。       消息 包含 ：   消息 的 发送者 （ 隐含 的 ） 。   消息 的 接收者   VALUE 字段   -   将 消息 一起 传送 到 合同 地址 的 wei 的 数量 ，   一个 可选 的 数据字 段 ， 即 合约 的 实际 输入 数据     一个 STARTGAS 值 ， 它 限制 了 消息 触发 的 代码执行 的 最大 气体 量 。       从 本质 上 讲 ， 消息 就 像 一个 交易 ， 除了 它 是 由 合同 而 不是 由 外部 参与者 产生 的 。 当 一个 正在 执行 代码 的 协议 执行 CALL 或 DELEGATECALL 操作码 时 ， 产生 一条 消息 ， 该 操作码 产生 并 执行 一条 消息 。 就 像 一个 交易 ， 一条 消息 导致 收件人 帐户 运行 其 代码 。 因此 ， 契约 可以 和 其他 契约 的 关系 完全 一样 ， 外部 行为 者 可以 。       什么 是 天然气 ？       以太 坊 在 称为 以太 坊 虚拟机 （ EVM ） 的 区块 链上 实施 了 一个 执行 环境 。 参与 网络 的 每个 节点 都 运行 EVM 作为 块 验证 协议 的 一部分 。 他们 检查 正在 验证 的 块 中 列出 的 交易 ， 并 运行 EVM 内 交易 触发 的 代码 。 网络 中 的 每个 完整 节点 都 执行 相同 的 计算 并 存储 相同 的 值 。 显然 以太 坊 并 不是 要 优化 计算 效率 。 其 并行处理 冗余 并行 。 这是 提供 一个 有效 的 方式 来 达成 一致 的 系统 状态 ， 而 不 需要 信任 的 第三方 ， 神谕 或 暴力 垄断 。 但 重要 的 是 ， 他们 不 在 那里 进行 最佳 计算 。 事实上 ， 合约 执行 是 冗余 复制 跨 节点 ， 自然 使得 它们 昂贵 ， 这 通常 会 产生 一个 激励 ， 不要 使用 区块 链 进行 离线 计算 。       当 您 运行 分散 的 应用程序 （ dapp ） 时 ， 它会 与 区块 链 交互 以 读取 和 修改 其 状态 ， 但 dapps 通常 只会 放置 对于 区块 链上 的 共识 至关重要 的 业务 逻辑 和 状态 。       当 由于 被 消息 或 事务 触发 而 执行 合同 时 ， 每个 指令 都 在 网络 的 每个 节点 上 执行 。 这有 一个 成本 ： 对于 每个 执行 的 操作 都 有 一个 指定 的 成本 ， 用 多个 气体 单位 表示 。       天然气 是 交易 发件人 在 以太 坊 区块 链上 进行 的 每一项 操作 都 需要 支付 的 执行 费用 的 名称 。 这个 名字 的 灵感 来源于 这个 费用 充当 了 加密 燃料 ， 推动 了 智能 合约 的 运动 。 天然气 是从 执行 代码 的 矿工 那里 购买 的 。 由于 天然气 的 单位 与 自然 成本 的 计算 单位 相一致 ， 因此 天然气 和 乙醚 的 故意 脱钩 ， 而 乙醚 的 价格 一般 会因 市场 力量 而 波动 。 二者 是 由 自由市场 调节 的 ： 天然气 的 价格 实际上 是 由 矿工 决定 的 ， 他们 可以 拒绝 以 低于 最低 限度 的 天然气 价格 进行 交易 。 为了 得到 天然气 ， 你 只 需要 添加 乙醚 到 你 的 帐户 。 以太 坊 客户 自动 为 您 的 乙醚 购买 天然气 ， 金额 为 您 指定 的 金额 ， 作为 交易 的 最大 支出 。       以太 坊 协议 收取 合同 或 交易 中 执行 的 每个 计算 步骤 的 费用 ， 以 防止 以太 坊 网络 上 的 故意 攻击 和 滥用 。 每笔 交易 都 需要 包含 一个 天然气 限制 和 一个 愿意 为 每个 天然气 支付 的 费用 。 矿工 可以 选择 包括 交易 和 收取 费用 。 如果 由 交易 产生 的 计算 步骤 （ 包括 原始 消息 和 可能 触发 的 任何 子 消息 ） 使用 的 气体 总量 小于 或 等于 气体 限制 ， 则 处理 交易 。 如果 天然气 总量 超过 天然气 限制 ， 则 所有 的 变化 都 将 被 恢复 ， 除非 交易 仍然 有效 ， 矿工 仍然 可以 收取 费用 。 交易 执行 中未 使用 的 所有 多余 的 天然气 将 作为 Ether 退还给 发货人 。 你 不必 担心 超支 ， 因为 你 只 收取 你 消耗 的 气体 。 这 意味着 发送 高于 估计值 的 气体 限制 的 交易 是 有用 的 ， 也 是 安全 的 。       估算 交易成本       一笔 交易 的 总成本 是 基于 两个 因素 ：       gasUsed 是 交易 消耗 的 总 天然气       天然气 在 交易 中 指定 的 一个 单位 天然气 的 价格 （ 以太 ）       总成本 =   gasUsed   *   gasPrice       gasUsed       EVM 中 的 每个 操作 都 分配 了 一定 数量 的 消耗 气体 。   gasUsed 是 执行 所有 操作 的 所有 气体 的 总和 。 有 一个 电子表格 提供 了 一些 背后 的 分析 一瞥 。       对于 估算 gasUsed ， 有 一个 可以 使用 的 估计 天然气 API ， 但 有 一些 注意事项 。       天然气 价格       用户 构建 并 签署 交易 ， 并且 每个 用户 可以 指定 他们 想要 的 任何 价格 ， 可以 是 零 。 然而 ， 在 边境 发起 的 以太 坊 客户 有 一个 0.05 e12 的 默认 煤气 价格 。 随着 矿工 们 的 收入 得到 优化 ， 如果 大多数 交易 都 是 以 0.05 e12 的 价格 提交 的话 ， 要 说服 矿工 接受 指定 较 低 或 零 的 煤气 价格 的 交易 是 困难 的 。       交易成本 示例       我们 拿 一个 只 增加 2 个 数字 的 合约 。   EVM   OPCODE   ADD 消耗 3 种 气体 。       使用 默认 天然气 价格 （ 截至 2016 年 1 月 ） 的 大致 成本 将 为 ：       3   *   0.05 e12   =   1.5 e11   wei       由于 1 个 乙醚 为 1 个 乙烯 ， 所以 总成本 为 0.00000015 乙醚 。       这是 一种 简化 ， 因为 它 忽略 了 一些 成本 ， 比如 将 两个 数字 合同 在 一起 的 成本 ， 甚至 在 它们 被 添加 之前 。           题       燃气 费       燃气 成本 计算器       以太 坊 天然气 价格                       Operation   Name       Gas   Cost       Remark                       步       1       每个 执行 周期 的 默认 金额               停止       0       自由               自杀       0       自由               沙 三段       20                     SLOAD       20       从 永久 存储 中 获得               sstore       100       永久 保存               平衡       20                     创建       100       合同 创建               呼叫       20       启动 一个 只读 的 调用               记忆       1       扩展 内存 时 每 增加 一个 单词               TXDATA       五       交易 的 每个 字节 的 数据 或 代码               交易       500       基本 费用 交易               合同 创建       53000       从 21000 改为 宅基地                   帐户 互动 示例   -   博彩 合同       如前所述 ， 有 两种 类型 的 帐户 ：           外部 拥有 账户 （ EOAs ） ： 由 私钥 控制 的 账户 ， 如果 您 拥有 与 EOA 关联 的 私钥 ， 则 可以 从 其 发送 以 太和 消息 。       合同 ： 一个 拥有 自己 代码 的 账户 ， 由 代码 控制 。           默认 情况 下 ， 以太 坊 执行 环境 无生气 ， 没有 任何 事情 发生 ， 每个 帐户 的 状态 保持 不变 。 但是 ， 任何 用户 都 可以 通过 从 外部 拥有 的 账户 发送 交易 来 触发 行动 ， 从而 使 以太 坊 的 轮子 运动 。 如果 交易 的 目的地 是 另 一个 EOA ， 那么 该 交易 可能 转移 一些 以太 ， 但 否则 什么 都 不 做 。 但是 ， 如果 目的地 是 合同 ， 合同 又 会 激活 ， 并 自动 运行 其 代码 。       代码 能够 读 / 写 自己 的 内部 存储器 （ 数据库 将 32 字节 的 密钥 映射 为 32 字节 的 值 ） ， 读取 接收 到 的 消息 的 存储 ， 并 将 消息 发送到 其他 契约 ， 并 依次 触发 它们 的 执行 。 一旦 执行 停止 ， 并且 由 合同 发送 的 消息 触发 的 所有 子 执行 都 会 停止 （ 这 一切 都 以 确定性 和 同步 的 顺序 进行 ， 即子 调用 在 父 调用 进一步 之前 完全 完成 ） ， 那么 执行 环境 将 停止 再 一次 ， 直到 下 一次 交易 被 唤醒 。       合同 通常 有 四个 目的 ：           维护 一个 数据 存储 ， 代表 对 其他 合同 或 外部 世界 有用 的 东西 ; 其中 一个 例子 是 模拟 货币 的 合同 ， 另 一个 例子 是 记录 特定 组织 的 成员 资格 的 合同 。       作为 一种 更 复杂 的 访问 策略 的 外部 账户 ; 这 被 称为 “ 转发 合同 ” ， 并且 通常 涉及 只有 在 满足 某些 条件 时才 简单 地 将 传入 消息 重新 发送到 某个 期望 的 目的地 ; 例如 ， 可以 有 一个 转发 合同 ， 等待 直到 给定 的 三个 私钥 中 的 两个 在 重新 发送 它 之前 确认 了 一个 特定 的 消息 （ 即 multisig ） 。 更 复杂 的 转发 合同 根据 发送 的 消息 的 性质 具有 不同 的 条件 。 这个 功能 的 最 简单 的 用例 是 一个 可以 通过 一些 更 复杂 的 访问 过程 覆盖 的 提取 限制 。 钱包 合同 就是 一个 很 好 的 例子 。       管理 多个 用户 之间 正在 进行 的 合同 或 关系 。 这方面 的 例子 包括 一个 金融 合同 ， 一些 特定 的 调解人 的 代管 ， 或者 某种 保险 。 也 可以 有 一方 开放 合同 ， 让 任何一方 随时 参与 ; 其中 一个 例子 就是 一个 合同 ， 它 自动 支付 给 谁 提出 一个 有效 的 解决方案 ， 以 解决 某个 数学 问题 ， 或者 证明 它 提供 了 一些 计算资源 。       为 其他 合同 提供 功能 ， 本质 上 充当 软件 库 。           合同 通过 交替 地 称为 “ 呼叫 ” 或 “ 发送 消息 ” 的 活动 彼此 交互 。     “ 消息 ” 是 一个 包含 一定 数量 的 以太 数据 的 对象 ， 任何 大小 的 数据 的 字节 数组 ， 发送者 和 接收者 的 地址 。 当 合同 收到 一条 消息 时 ， 它 可以 选择 返回 一些 数据 ， 消息 的 原始 发送者 可以 立即 使用 这些 数据 。 这样 ， 发送 消息 就 像 调用 一个 函数 一样 。       因为 合同 可以 扮演 不同 的 角色 ， 所以 我们 期望 合约 会 互相 影响 。 例如 ， 考虑一下 Alice 和 Bob 赌 100     GavCoin 的 情况 ， 即 旧金山 的 温度 在 明年 的 任何 时候 都 不会 超过 35 º C 。 然而 ， Alice 非常 注重 安全性 ， 而且 她 的 主 账户 使用 转发 合同 ， 这个 转发 合同 只能 通过 三个 私人 密钥 中 的 两个 批准 发送 消息 。 鲍勃 对 量子 密码学 是 偏执 的 ， 所以 他 使用 转发 契约 ， 只 传递 与 传统 的 ECDSA 一起 签名 Lamport 签名 的 消息 （ 但 由于 他 是 老式 的 ， 他 倾向 于 使用 基于 SHA256 的 Lamport     sigs 版本 ， 这是 以太 坊 直接 不 支持 ） 。       投注 合约 本身 需要 从 某个 合同 中 提取 有关 旧金山 天气 的 数据 ， 并且 在 需要 将 GavCoin 实际 发送给 Alice 或 Bob （ 或者 更 确切 地说 ， Alice 或 Bob 的 转发 ） 时 还 需要 与 GavCoin 合同 进行 交谈 合同 ） 。 我们 可以 显示 账户 之间 的 关系 ：               当 鲍勃想 完成 下注 时 ， 会 发生 以下 步骤 ：           发送 交易 ， 触发 Bob 的 EOA 到 他 的 转发 合同 的 消息 。       Bob 的 转发 合同 将 消息 的 散列 和 Lamport 签名 发送到 用作 Lamport 签名 验证 库 的 合同 。       Lamport 签名 验证 库 看到 Bob 想要 一个 基于 SHA256 的 Lamport   sig ， 所以 根据 需要 多次 调用 SHA256 库来 验证 签名 。       一旦 Lamport 签名 验证 库 返回 1 ， 表示 签名 已经 过 验证 ， 它会 向 代表 该 合同 的 合同 发送 一条 消息 。       投注 合约 检查 提供 旧金山 温度 的 合约 以 查看 温度 。       投注 合同 认为 对 消息 的 回应 显示 温度 高于 35 º C ， 因此 它 向 GavCoin 合同 发送 一条 消息 ， 将 GavCoin 从 其 账户 转移 到 Bob 的 转发 合同 。           请 注意 ， GavCoin 全部 “ 存储 ” 为 GavCoin 合同 数据库 中 的 条目 ; 在 步骤 6 的 上下文 中 的 “ 帐户 ” 一词 仅仅 意味着 在 GavCoin 合同 存储器 中 存在 具有 用于 投注 合同 地址 的 密钥 和 其 余额 的 值 的 数据 条目 。 收到 此 消息 后 ， GavCoin 合同 将 此值 减少 一定 数量 ， 并 增加 与 Bob 的 转发 合同 地址 对应 的 条目 中 的 值 。 我们 可以 在 下图 中 看到 这些 步骤 ：               离线 签署 交易       [ 也许 把 这个 添加 到 常见问题 中 ， 并 指向 turboethereum 指南 的 ethkey 部分 ？   ]           韧性 原始 交易 广播者      ", "tags": "blockchain", "url": "/wiki/blockchain/account-types-gas-and-transactions.html"},
      
      
      
      
      
        
      
      {"title": "alibaba", "text": "    Table   of   Contents          ", "tags": "company-data", "url": "/wiki/company-data/alibaba.html"},
      
      
      {"title": "jd.com", "text": "    Table   of   Contents           2016 年                 2016 年  ", "tags": "company-data", "url": "/wiki/company-data/jd.html"},
      
      
      {"title": "华为", "text": "    Table   of   Contents          ", "tags": "company-data", "url": "/wiki/company-data/huawei.html"},
      
      
      
      
      
        
      
      {"title": "认识商业", "text": "    Table   of   Contents           关于                 关于       师兄 推荐 了 《 认识 商业 》 这 本书 ， 据说 很 不错 ， 还 没 来得及 看 ， 先占个 坑 ， 看 了 之后 来 写 。  ", "tags": "economics", "url": "/wiki/economics/intro-business.html"},
      
      
      {"title": "财务会计教程", "text": "    Table   of   Contents           关于           会计 ： 一门 商业 语言           一些 基本概念           会计 恒等式           经济实体 所有制 类型                   计量 收益 评价 业绩           基本概念           一些 基本 假设 和 原则           权责 发生 制 和 收付 实现 制           配比 原则           股利 ( dividend ) 和 存留 收益 的 会计 处理           四个 财务 比率                   记录 交易           权责 发生 制 与 财务报表           权责 发生 制及 会计 调整           财务报表           分类 资产 负债表 ( classified   balance   sheet )           利润表 ( income   sheet )                           现金流量 表           销售 的 会计 处理           存货 与 销货成本           存货 计价 方法                   长期 资产           负债 与 利息           债务 比率 与 利息 保障 倍数                   股东权益           公司 间 投资 与 合并 财务报表           合并 财务报表           收购                   财务报表 分析           TIPS                         关于       空余 时间 ， 看 了 一下 查尔斯   T .   亨格瑞 的 《 财务会计 教程 》 一书 ，     不得不 说 老外 的 经管类 书写 得 确实 好 ， 通俗易懂 ， 举例 丰富 。     下面 是 记录下来 的 一些 重点 信息 。       会计 ： 一门 商业 语言       一些 基本概念               会计 ( accounting ) 分为 财务会计 ( financial   accounting ) 和 管理         会计 。 财务会计 关注 外部 决策者 的 需要 ， 比如 股东 、 供应商 、 银         行 、 投资者 或者 政府 机构 。 而 管理 会计 侧重于 管理者 对 公司 管理         的 需要 。               道琼斯 工业 平均 指数 ( Dow   Jones   Industrial   Average ,   DJIA ) 从         12 家 公司 增加 到 30 .   其他 指数 有 伦敦 经融 时报 指数 ( FTSE ) 、 日经指数 ( Nikkei ) 等 。         当然 还有 我们 的 上证指数 。               财务数据 来源 有 公司 年报 ( annual   report ) 、 公司 向 美国 证券交易 委员会 ( SEC )         报送 的 10 - K 表 ( form   10 - K ) 。               资产 负载 表 ( balance   sheet ) 也 称 财务报表 ( statement   of   financial   position )         反应 公司 在   某个 特定 时间 点上   的 资产 、 负载 和 有所 者 权益 的 状况 。         联系 两个 不同 时间 点 （ 通常 是 一个 财务 周期 ） 的 表示 现金流 表 、 利润表 和 所有者         权益 变动表 这 三个 表 。               会计 主体 ( entity ) 是 指 作为 一个 独立 的 经济 单位 。 公司 或者 一个 车间 都 可以 ， 不同于         法律 主体 。               交易 ( transaction ) 是 指 任何 影响 一个 会计 主体 的 财务状况 且 会计师 能以 货币 可靠 计量 、         记录 的 经济 活动 。               复式 记账法 是 指 对 每 一次 交易 至少 会 影响 两个 账户 的 一种 记账 方法 ， 这种 方法 的 理论依据         是 会计 恒等式 。               账户 ( account ) 是 指 核算 一项 资产 、 负债 或 所有者 权益 变动 的 汇总 记录 。               存货 ( inventory ) 指 公司 持有 的 以备 出售 的 商品 和 材料 。 商品 未 完成 的 也 算 存货 。               记账 交易会 产生 应付账款 ( account   payable ) 。               公认 会计准则 ( GAAP ) 包括 国际 财务报告 准则 ( IFRS ) 和 美国 公认 会计准则 ( U . S . GAAP ) 。               审计师 ( auditor ) 通过 审计 对 公司 的 财务 信息 的 可信性 进行 监督 ， 通常 需要 独立 的 公共 会计师         或 注册 会计师 进行 设计 署名 ， 公司 的 财报 才能 被 监管 机构 接受 。               会计师 职业 ： 注册 会计师 事务所 、 CFO 、 CEO 等 。               会计 恒等式           资产   =   负债   +   所有者 权益                   资产 ( assets )             公司 目前 具有 所有权 且 能 帮助 创造 未来 现金 流入 或者 减少 未来 现金 流出 的         经济 资源 。 如 现金 、 存货 和 设备 等 。               负债 ( liabilities )         定义 ： 一个 组织 对外 所 承担 的 经济 义务 或 外部 主体 对 其 资产 的 要求 权 。 例如 银行         债务 常 以 应付 票据 存在 。               所有者 权益 ( owner ' s   equity )         定义 ： 指 所有者 对 组织 的 资产 的 要求 权 ， 它 的 优先级 低于 负债 。 确切的说 是 对 净值 产         的 要求 权 。               实缴 资本 = 面值 + 溢缴 资本 。               经济实体 所有制 类型               独资企业 ( sole   proprietorship ) 是 指 只有 一位 所有者 的 企业 。               合伙 企业 ( partnership ) 指有 两个 或 两个 以上 个人 联合 作为 共同 所有者 的 一种         组织 形式 。 例如 会计 事务所 一般 都 是 合伙人 的 合伙 企业 。               公司 ( corporation ) 依 法律 成立 的 承担 有限责任 ( limited   liability ) 的 一种 经济         组织 。 公司 债权人 只 对 公司 资产 有 追索权 ， 而 对 所有 这个 人 的 财产 没有 追索权 。         公众 公司 ( publicly   owned ) 是 指 公司 向 公众 出售 所有权 股票 的 企业 。         与 之 相对 的 事 私营 公司 ( privately   owned ) 。 \" Co . \" 、 \" Corp . \" 或 \" Inc . \" 都 代表 公司 。         其他 国家 也 有 其他 标识 。         只有 20 % 的 美国 企业 采用 公司制 ， 却 从事 着 90 % 的 经济 活动 。 其中 72 % 独资企业 只有 5 %         的 经济 活动 。               董事会 ( board   of   directors ) 是 由 股东 选举 出来 代表 股东 执行 任命 和 监督 权利 的 人 。               首席 执行官 ( chief   executive   officer ,   CEO ) 有时 也 担任 董事长 。               计量 收益 评价 业绩       基本概念               经营 周期 ( operating   cycle ) 是 指 公司 用 现金 采购 - & gt ; 商品 存货 - & gt ; 客户 欠款 - & gt ; 回收 现金 整个 过程 。               会计 期间 通常 以年 为 单位 ， 也 有 中期 、 季度 等 会计 周期 。 会计 期间 导致 了 不同 确认 制度 — —         全责 发生 制 ( 会计 要求 ) 、 收付 实现 制 ( 纳税 要求 ) 。               收入 ( revenue ) 是 指 销售 产品 或 提供 劳务 而 形成 的 净资产 的 增加 （ 不用 扣除 成本 ） ，         收入 增 所有者 权益 ， 也 称 销售收入 ( sales ,   sales   revenue ) 。 费用 ( expenses ) 是 指 由于         在 向 客户 销售 产品 或 提供 劳务 的 过程 中 消耗 或者 让渡 资源 而 形成 的 净资产 的 减少 。         收入 不 包括 非 日常行为 产生 的 收益 ， 例如 变卖 资产 、 投资收益 等 。         另外 请 注意 后面 所说 的 权责 发生 制对 收入 时间 上 的 约束 。               收益 ( income ,   earnings ) 指 收入 超出 费用 的 部分 ， 也 称 利润 ( profits ) 。 如果 费用 超过         收入 ， 就 成 超过 部分 为 损失 。 由 利润 累计 形成 的 所有者 权益 总额 成为 存留 收益 ( retained   earnings ，         retained   income ) 。 收益 常 影响 股价 ， 如果 收益 出乎意料 之外 ， 股价 一般 会 做出 反应 ，         正向 意外 上涨 ， 负 向 意外 下跌 。               记账 交易 同时 产生 应收 账款 ( accounts   receivable ,   trade   receivables ) ， 对于 应收 账款 ， 后面         会 提到 由于 坏账 的 影响 形成 的 坏账 准备 。               销货成本 ( cost   of   goods   sold ,   cost   of   sales ,   cost   of   revenue ) 是 指 报告 期间 公司 向 客户 销售 的         存货 的 原始 采购 成本 。 对于 提供 劳务 服务 的 企业 ， 这个 成本 应该 怎么 算 ？ 比如 腾讯               利润表 汇集 了 一个 会计 周期 内 所有者 权益 的 所有 变动 ， 反应 一定 期间 的 所有 收入 和 费用 。               净 收益 ( net   income ,   net   earnings ) 是 扣除 所有 费用 后 的 剩余 部分 ， 如果 为 负 的 就是 净 损失 ( net   loss ) 。               一些 基本 假设 和 原则               会计 主体 假设 ： 一个 独立 的 经济 单位 。               可靠性 ( reliability ) 要求 会计师 只 确认 能 可靠 记录 的 特定 类型 的 事项 。 比如 高管 遇害 等 无法 可靠         计量 的 事件 就 不 记录 。               持续 经营 假设 ( going   concern ,   continuity )   假定 会计 主体 将会 无限期 地 持续 经营 。 长期 资产 采用         历史 成本 计价 的 合理性 来自 于 这里 。               重要性 原则 使得 许多 应 被 记作 资产 的 采购 项目 因为 无足轻重 而 被 计入 了 费用 ， 比如 衣架 等 。         通常 有个 最低 限额 。 应以 不 影响 财务报表 使用者 的 决策 为 前提 。               成本 效益 原则 对 会计 系统 变革 提供 了 知道 ， 要求 变革 的 收益 要 大于 成本 。               货币 计量 假设 ， 所有 交易 都 能 货币 计量 ， 通常 以 本币 计量 ， 美国 的 美元 ， 中国 的 人民币 。               相关性 、 可比性 、 可 理解 性 、 可 核实 性 、 可 理解 性 等 。 。 。 。         这些 貌似 是 国内 会计师 证要 考 的 ， 但 感觉 不怎么 太 重要 。               权责 发生 制 和 收付 实现 制               全责 发生 制 ( accrual   basis ) 要求 收入 在 获得 时 予以 确认 、 费用 在 发生 时 予以 确认 ， 这 导致 收入 和 费用 确认 的 时间     未必 与 现金 收付 的 时间 一致 。 而 收付 实现 制 ( cash   baasis ) 要求 公司 在 收到 现金 时 确认 收入 、 在 支付现金 时 确认 费用 。               公司 安装 全责 发生 制 制备 财务报表 ， 但是 税收 按照 收付 实现 制 ， 此外 公司 还 编制 现金流量 表附 在 年报 中 。               配比 原则               配比 原则 ( mathing ) 要求 在 确认 相关 收入 的 同一 会计 周期 确认 和 记录 费用 。               租赁费 管理费 等 费用 会用 在 一定 期间 的 经营 活动 ， 这些 成本 被 称为 期间 成本 ( period   costs ) ，         而 与 收入 本身 相关联 的 费用 被 称为 产品成本 ( product   costs ) ， 如 销货成本 、 销售 佣金 。               按照 配比 原则 ， 需要 对 长期 资产 进行 折旧 ( depreciation ) ， 而 预付 租金 需要 平摊 到 每 一个 会计 周期 。               股利 ( dividend ) 和 存留 收益 的 会计 处理               股利 在 宣布 日 成为 公司 负债 ， 在 支付 日 负债 和 现金 同时 减少 。               存留 收益 是 剩余 要求 权 ， 而 不是 真 金白银 。 破产 时 所有者 清偿 负债 后 的 剩余 数量 ， 可能 比         现金 多 也 可能 少 。 这是 由于 市场 和 物价水平 变动 等 因素 的 作用 。               股东权益 变动表 ( statement   of   stockholder ' s   equity ) 变动 原因 ：           净 收益 或净 损失       与 股东 之间 的 交易 ， 比如 宣布 股利       其他 全面 收益 ( other   comprehensive   income ) 上述 两者 之外 的 其他 收益 。                   四个 财务 比率               每股 收益 ( earning   per   share ) 是 要求 在 财务报表 中 列报 的 唯一 一个 财务 比率 。           每股 收益   =   净 收益   /   平均 发行 在外 的 普通股 股数                   稀释 每股 收益 是 指 因为 期权 行权 或 债权人 等 有权 以 低于 市场价 购买 普通股 ， 在 其行 权后 ，         每股 收益 将 下降 。               市盈率 ( price - earnings   radio ,   P - E   raio ) 也 称 收益 乘数 ， 是 当前 收益 回收 当前 股价 的 年数 。           市盈率   =   普通股 每股 市场 价值 / 普通股 每股 收益                   股息 率 ( dividend - yield   ratio ) 。 因为 股利 和 每股 收益 同场 不同 ，         这个 指标 体现 了 公司 支付现金 股利 占 每股 收益 的 比例 ， 可能 小于 1 也 可能 大于 1 ， 甚至 可能 为 0 ，         因为 有些 公司 根本 就 不 支付 股利 。           股利 支付 率   =   普通股 每股 股利   /   每股 收益                   记录 交易           记账 方式 ： 复式 记账法 ( double - entry   system )       借 ( debit ) 表示 左边 ， 贷 ( credit ) 表示 右边       会计 恒等式 也 等价 于   借方 = 贷方       数据处理 ： ERP 系统 取代 人工 处理       计算机 处理 记账 语言 — —   XBRL             这章 涉及 具体 细节 ， 我 不大 关心 ， 所以 如 有 需要 请 参看 原文 或者 相关 书籍 。       权责 发生 制 与 财务报表       权责 发生 制及 会计 调整           显性 交易 和 隐性 交易 ： 是否是 实际 发生 。 火灾 导致 的 损失 是 显性 交易 而 折旧 算 隐性 交易       会计 调整 也 称 调整 分录 ， 是 指 在 期末 将 隐性 交易 的 财务 影响 分配 给 正确 的 会计 期间 。 它         从不 影响 现金 。       应计 ( accrue ) 事项 是 指 在 一定 会计 期间 ， 及时 未 发生 显性 交易 ， 也 归集 应收 项目 （ 资产 ）         或 应付 项目 （ 负债 ） 。       应付 员工工资       借款 到期 之前 的 应付 利息               预付 成本 耗用 ， 过去 的 显性 交易 形成 一项 资产 ， 日后 的 隐性 交易 确认 这项 资产 的 耗费 。       折旧 ， 累计 折旧       预付 租金               加盟 的 会计 处理 ： 加盟 属于 特许 经营 ， 公司制 将 特许 经营 费 确认 为 收入 ， 而 加盟商 的 产品         销售收入 不 计入 收入 总额 。       快递 行业 的 加盟 方式 以及 诸多 连锁店 加盟店 ， 主 公司 的 收入 不 应 包括 这些 加盟店 的 收入               应计 未计 费用       应付 工资       应计 利息       应计 所得税               应计 未计 收入       应收 利息       尚未 出具 账单 的 服务费 — — 律师 、 会计 等               确认 原则 ： 稳健性 原则           财务报表       分类 资产 负债表 ( classified   balance   sheet )               流动资产 ( current   assets ) ： 一年 或 一个 正常 经营 周期 内能 变现 的 资产           预付 租金       应收 账款       应收 票据       应收 利息       商品 存货                   长期 资产           设备       房产       土地                   流动 负债 ( current   liabilities ) ： 一年 内 或 一个 正常 经营 周期 到期 的 债务           应付账款       未 获得 租赁 收入       应付 工资       应付 利息       应交 所得 说       应付 票据                   营运 资本 ( net   working   capital ,   net   current   assets )   =   流动资产   -   流动 负债               流动比率 ( current   ratio ) 衡量 流动性           流动比率   =   流动资产   /   流动 负债         太高 ： 持有 过多 流动资产 没有 使用 ， 相当于 浪费           太低 ： 可能 难以 履行 短期 债务                   速动 比率 ( quick   ratio ) ， 也 称 酸性 测试 比率 ， 去除 流动性 差 的 资产           速动 比率   =   ( 流动资产   -   流动性 差 的 资产 如 存货 )   /   流动 负债                       公司 知应 持有 满足 支出 需要 所 必须 的 现金 ， 而 将 任何 临时性 冗余 现金 都 进行 投资 ，     以 创造 额外 收益 。           利润表 ( income   sheet )           分类 ： 单步 式 ( 没有 小计 ) 和 多步式 ( 有小计 ， 大多 公司 都 采用 这种 )       毛利 ( gross   profit ) 或 毛 边际 ( gross   margin ) ， 是 销售收入 超出 销售 成本 的 部分 。       我 的 理解 ： 可以 理解 为 没多 卖出 单位 产品 而 增加 的 利润 ， 因此 可以 忽略 固有 成本     运营 成本 等 间接成本 。               经营 费用 ( operating   expenses )       经营 费用 低 不 一定 是 好事 ， 比如 研发 费用 低 了 会 影响 未来 的 竞争力               经营 收益 ( operating   profit )       盈利 能力 指标       毛利率   =   毛利   ÷   销售收入       销售 利润率   =   净 收益   ÷   销售收入       净资产 收益率 ( return   on   common   stockholder ' s   equity   ratio ,   ROE )         =   净 收益   ÷   普通股 股东权益 平均 余额       资产 收益率   =   净 收益   ÷   总资产 平均 余额                   现金流量 表       现金流量 表 ( statement   of   cash   flow ,   cash   flow   statement ) 按 经营 活动 现 就 流量 、     投资 活动 现金流量 和 筹资 活动 现金流量 分类 报告 一个 会计 主体 在 一定 期间 的 现金 收入     和 现金 支出 。           如果 损失 的 现金 太 多 ， 公司 就 必须 宣布 破产 。 依照 联邦 法律 ， 破产 意味着 公司 需要     寻求 法院 的 破产 保护 。                   现金 等价物 ( cash   equivalents ) 是 指 公司 能够 快捷 地 转化 为 现金 的 流动性 极高 的 短         期 投资 。 例如 货币 市场 基金 和 国库券 。 一般 把 现金 和 现金 等价物 统称 为 现金 。               经营 活动           日常 产品 采购 、 生产 和 销售       利息       股利 ( ? 见 后面 的 说明 ， 存在 分歧 )                   筹资 活动           从 银行借款       从 其他 贷款 机构 借款       偿还 借款       回购 股票       发布 股利                   投资 活动           购置 和 处置 厂房 、 设备 和 其他 生产性 长期 资产       作为 有价证券 的 债权人 或 所有者 提供 或 回收 资金 ： 例如 以 投资 的 方式 持股 另一家         公司 的 股票 （ 注意 区别 子公司 要求 51 % 以上 的 股权 ）                   汇率 对 现金 的 影响 也 会 放到 现金流量 表 下面 ， 作为 会计 调整               分歧           对于 股利 ， 美国 公认 会计准则 规定 股利 既 可以 作为 经营 活动 现金流量 也 可以 作为         筹资 活动 现金流量 。                   非现金 投资 和 筹资 活动 不用 列 在 现金流量 表中 ， 单 必须 单独 列报 。               经营 活动 现金流量 的 计算           计算方法 分为 直接 法 和 间接 法       间接 法 需要 提供 净 收益 与 经营 活动 现金流量 调节 表                   现金流量 表与 会计 恒等式 的 关系           $ ( \ \ Delta ) $ 现金   =   $ ( \ \ Delta ) $ 负债   +   $ ( \ \ Delta ) $ 股东权益   -   $ ( \ \ Delta ) $ 非现金 类 资产       直接 发是 列报 现金 的 所有 变动 （ 左边 ） ， 而 间接 法 列报 变动 的 原因 （ 右边 ）                   现金流量 表 分析           同一 期间 投资 活动 现金流量 为 负 的 公司 ， 即 使用 资金 扩大 固定资产 投资 的 公司 ，         往往 都 是 健康 的 、 成长型 的 公司       经营 活动 现金流量 为 负而 投资 活动 现金流量 为 正 的 公司 都 是 要 跑 路 了                   自由 现金流量 ( free   cash   flow ) 定义 为 经营 活动 现金流量 减去 资本 支出 。               销售 的 会计 处理               双重 测试 确认 收入 ： 公司 必须 以向 客户 交付 商品 或 提供 劳务 ， 即 收入 “ 已 获得 ” ；         他 必须 受到 了 现金 或者 一项 实际上 能 转换 为 现金 的 资产 ， 及 收入 必须 “ 以 实现 ” 。               完工 百分比 法 ： 在 生产 发生 之 时 确认 长期 合同 收入 ， 按照 配比 原则 ， 还 确认 相关         费用 。               销售收入 总额 ( gross   sales ) 是 指 扣除 销售 退回 及 折让 、 销售 折扣 之前 的 销售收入 总额 。         销售收入 净额 ( net   sales ) 是 扣除 这些 折扣 之后 的 净额 。               折扣           商业 折扣       现金 折扣                   补偿性 余额 ( compensating   balance ) ， 银行 常 要求 公司 维持 一个 最低 现金余额 。 它         增加 了 借款人 所 支付 的 实际 利率 。 年 报上 必须 披露 显著 的 补偿性 余额 。               现金管理           分权       收入支出 分 不同 人 负责       收入 和 记账 也 分 不同 人 负责       ....                   坏账 ( doubleful   accounts )           直接 冲销 法 ： 对于 坏账 极少 的 公司       坏账 准备金 ： 采用 历史数据 估计       销售收入 百分比 法       应收 账款 余额 百分比 法       应收 账款 账龄 分析法 ： 不同 时间 赋予 不同 的 百分比                           应收 账款 周转 次数 ( accounts   receivable   turnover ) ： 本期 赊销 销售收入   ÷   本期 应收 账款 平均 余额               应收 账款 天数 ： 365   ÷   应收 账款 周转 次数               内部 控制 ： 保证 记录 的 真实性           管理控制       会计 控制                       案例 ： 某 程序员 将 会计 系统 数值 的 分 后面 一位 存 到 自己 账户 ， 如果 交易量 很大 ， 收入 非常 可观 。               内部 控制 方法       人员 可靠 、 责任 明确       分工       适当 审批       账证 齐全       程序 完善       物理 安防       休假 与 轮岗       独立 检查       成本 效益 分析 ： 是否 投资 财务 系统                   存货 与 销货成本           永续 盘存 制       定期 盘存 制 ： 商品 被盗 也 算入 销货成本               存货 盘亏 控制 专家 一般 认为 ， 最佳 威慑 盗窃 行为 是 在 销售点 安排     一名 警戒 员工 。               采购 成本 ， 一般 计入 期间 成本 ， 但 也 有 计入 销货成本       运输成本 ， 由于 太 困难 分配 成本 ， 所以 一般 也 是 直接 计入 费用       折让 和 折扣               研发 成本 被 视作 期间 成本 ， 而 不是 产品成本               存货 周转 次数 ( inventory   turnover )   =   销货成本   ÷   平均 存货 余额       存货 周转 天数   =   365   ÷   存货 周转 次数               降低 毛利率 提高 毛利 的 方法 长期 来看 是 有利 的 吗 ？ 如果 旺销 不是 因为     基本 需求 的 增加 ， 不过 是 将 未来 的 采购 转为 现在 的 采购 ， 那么 就 不是 。     他 有 可能 造成 未来 产品 的 滞销 。           存货 计价 方法           个别 认定 法 ： 一个 一个 跟踪 ， 以前 这种 方法 成本 很 高 ， 没人用 ， 但是 随着         计算机 发展 ， 这个 方法 成本 已经 降低 很多 了 。 主要 针对 单个 价值 高 的 商品 。       先进先出 法 ： 使 前期 成本 低于 个别 认定 法 ， 电子产品 可以 采用 这个 方法 拖延 所得税       后进先出 法 ： 使 前期 成本 高于 个别 认定 法 ， 可以 合理 拖延 所得税 ， 记住 是 拖延 ，         不是 避税 。 《 国际 财务报告 准则 》 禁止 这种 方法 。 缺点 是 可能 减少 管理者 的 奖金 ，         股价 、 贷款 等 。       加权 平均法       存货 持 有利 得 ， 因为 存货 成本 的 上升 导致 的 利得       原材料 、 在 产品 和 产成品 都 是 一种 存货 形式 ， 在 资产 负债表 中 列报 为 流动资产 。       公司 选择 计价 方法 必须 保持 一致性 ， 但 也 有 例外       存货 价值 采用 成本 与 市价 孰 低法       国际 准则 的 市场 价格 指可 实现 净值 。 而 美国 准则 指 现行 重置成本 ， 即 公司 今天 买 一件 存货     的 成本 。               截止期 差错 ， 将 当前 成本 记录 到 下 一个 会计 期间           长期 资产               定义 ： 公司 必须 在 日常 经营 中 使用 这些 资产 ， 而 不是 为了 在 销售 或者 投资 而 持有 的 。 例如         公司 应该 将 为了 投资 目的 闲置 的 土地 作为 投资 ， 而 不是 作为 厂房 及 设备 。               有形资产 是 看得见 摸得着 的 资产 ， 如 土地 、 建筑 和 设备 等 。               无形资产 也 算是 长期 资产 ， 包括 合同 权利 、 法律 权利 或者 经济 利益 ， 如 专利权 、 商标权 和 版权 。               折旧 ： 有形资产 在 收益 期 的 成本 分配               摊销 ： 对于 无形资产 的 摊销               折耗 ( depletion ) 指对 自然资源 的 摊销               对于 一些 短期 经济 利益 ， 如 广告费 、 预付 租金 等 ， 在 计入 费用 之前 被 当做 流动资产 ；         而 对于 不 只 在 一个 款及 年度 收益 的 资产 ， 将 其 资本化 ， 然后 在 每个 会计 期间 确认 一部分         费用 。               土地 的 计价 ： 美国 会计准则 要求 公司 将 土地 按照 预案 是 历史 成本 计价 ， 除非 土地 公允 价值         跌倒 原始 成本 以下 。               建筑物 和 设备 ： 一项 资产 在 第一次 使用 之前 的 维修 成本 ， 计入 购置 成本 ； 而 之后 的 维修 成本         计入 日常 维护费用 。               采用 非现金 支付 购买 的 资产 ， 按照 公允 价格 计入 成本 。               在 特定 情况 下 ， 公司 可以 减计 资产 价值 ， 但是 永远 不能 增加 其 价值 。               总成本 = 应计 折旧 额   +   残值               使用寿命 取 物理 寿命 和 经济 寿命 最短 的               折旧 方法 ：           直线 法 ， 均摊 到 使用寿命       工作量 法 ： 例如 卡车 按照 行驶 路程       加速 折旧 法 ： 余额 递减 法 ， 双倍 余额 递减 法 。 如果 残值 较大 ， 双倍 余额 递减 法会 使得             价值 减少 的 残值 以下 ， 为了 防止 这种 情况 ， 会 提前 修改 折旧 方法 。             加速 折旧 会 使得 纳税 时间 推迟 ， 从而 使 公司 从中 获益 。                   公司 在 纳税 申报 一般 采用 加速 法 ， 而 给 股东 的 报告 采用 直线 法               估计 使用寿命 或 残值 的 变更 ： 一个 原则 ， 不 追溯到 以前 ， 只 修改 以后               折旧 不 影响 税前 现金余额 ， 但 影响 所得税               资本 改良 ： 以 增加 现有 固定资产 所 提供 的 未来 收益 而 发生 的 支出 。 例如 改良 设备 延长 使用寿命 ，         这个 改良 成本 应 在 剩余时间 进行 分摊               有形资产 出售 利得 和 损失 常 计入 其他 收益 ( 损失 ) ， 但是 不 计入 经营 活动 现金流               有形资产 的 重新 计价 ： 只有 国际 财务报告 准则 才 有 可能 上调 资产 价值 ， 但 都 要求 在 厂房设备 发生         减值 的 时候 重新 计价 ， 下调 资产 价值 。               国际 财务报告 准则 要求 不论 账面 价值 增加 或者 减少 ， 都 要 按照 公允 价值 反映 固定资产 。               公司 对 任何 资产 进行 重新 计价 ， 也 必须 对 同类 资产 中 的 其他 资产 进行 重新 计价 。         采用 重新 计价法 的 资产 不 计提 折旧 。 由于 一旦 公司 采用 重新 计价法 调整 ， 就 必须         一直 进行 调整 ， 实际 中 很多 公司 很少 应用 。 （ 国际 财务报告 准则 ）               公允 价值 常 采用 评估师 基于 市场 的 证据 来 确定 。               重新 计价 利得 和 损失 作为 其他 全面 收益 直接 加 在 股东权益 中 。               资产 减值 记录 条件 ： 资产 市场 价格 显著 下降 ； 资产 使用 方式 显著 变化 ； 法律 或 经济 环境 发生 不利 变化 ；         有 迹象 表明 资产 报废 或者 发生 了 物理 损毁 ； 预测 表明 公司 将 因 使用 资产 而 持续 发生 损失 。               美国 公认 会计准则 要求 一旦 减值 就 不能 恢复 。               减值 损失   =   账面 价值   -   可回收 额 ； 可回收 额是 未来 金 现金流 的 现值 和 公允 价值 减 出售 成本 后 的 净值         中 最少 的 。               无形资产 ： 只有 公司 从 外部 购买 了 资产 的 所有权 ， 公司 才能 在 资产 负债表 中 列报 无形资产 ， 而 公司         自己 创造 的 则 不算 。 公司 内部 的 研发 活动 计入 费用 ， 经管 它 能 为 未来 创造 经济效益 。               美国 公认 会计准则 规定 了 一个 例外 ， 允许 计算机软件 公司 不 自动 地 将 内部 研发 成本费用 化 。 这些 公司         可以 将 旨在 销售 或 租赁 的 软件产品 研发 成本 资本化 。               对于 认定 具有 无限 使用寿命 的 无形资产 ， 公司 一般 不 予以 摊销 ， 二是 定期 评估 这些 资产 是否 减值 。         而 有限 寿命 的 需要 摊销 到 寿命 期内 。               收购 的 未 完工 研发 支出 也 要 资本化 ！ ？               无形资产           专利权       版权       商标       特许权 （ 许可证 ） ： 政府 颁发 的 许可证 ，       租赁 权 ： 土地 租赁                   对于 有限 寿命 的 无形资产 减值 与 有形 长期 资产 一样               对于 无限 使用寿命 的 无形资产 ， 如 商标 ， 要求 比较 账面 价值 与 公允 价值 ， 确认 减值 。               商誉 ： 是 一项 无形资产 ， 不能 出售 或 转让 （ ？ 商标 可以 ） ， 公司 只能 在 收购 另一家 公司         是 确认 商誉 。 商誉 是 向 被 收购 公司 支付 的 收购价格 超出 其 可辨认 净资产 公允 价值 的 部分 。               公司 不 分摊 商誉 ， 但是 要求 每年 审核 一次 商誉 是否 减值 。               自然资源 折耗 ： 未来 恢复 生态 的 费用 需要 计入 计提 折耗 的 总成本               负债 与 利息               负债 分为 流动 负债 和 长期负债 ， 一般 以 1 年期 为限 。               流动 负债           应付账款       应付 票据 ： 期票 ， 大多 是 向 银行 签发 的 定期 贷款 ； 还有 一些 短期 商业 票据 也 算       应计 员工工资       应交 所得税       一年 内 到期 的 长期负债 ： 如果 长期负债 一年 内要 到期 了 ， 就 变成 了 流动 负债       销售税       可 退还 保证金       尚未 获得 收入 ： 预付款 收入 ； 有些 公司 称为 递延 收入       产品 保修 ： 售后服务 也 是 一项 负债 ， 需 定期 提供 保修 “ 准备金 ”                   长期负债           公司债券 ( bonds )       各种 债券                   债券 特点 ： 清算 优先级 为   抵押 债券   & gt ;   信用 债券 （ 应付账款 ）   & gt ;   次级 信用 债券   & gt ;   优先股   & gt ;   股票           债券 保护性 条款 ： 限制 支付 股利 ， 限制 再 借款 ， 保证 一定 的 财务 比率       债权 会计 处理 ： 过于 琐碎 ， 跳过           债券 对 现金流量 的 影响 ： 影响 筹资 活动 现金流量 。 US . GAAP 要求 每年 一次 支付 的 利息 计入 经营 活动 现金 流出量 ；         而 国际 准则 都 可以               债券 的 折价 溢价 不 影响 现金 ， 对 其 的 摊销 就 像 折旧 一样 ， 是 一项 非现金 费用               租赁 ： 资本 租赁 ( 列报 为 资产 和 负债 ) 和 经营 租赁 ( 列报 为 费用 )               资本 租赁 列报 一项 资产 和 一项 负债 ， 金额 为 未来 支付 的 租赁费 的   现值                 其他 长期负债 ：           养老金 ： 退休 后 福利 基金       递延 所得税 ： 公认 会计准则 的 财务报告 要求 与 税法 之间 的 差异 ， 使得 公司 记录 的 税费 ( 直线 法 )         比 实际 缴纳 ( 加速 折旧 ) 的 时间 早 ， 就 形成   递延 所得税 负债         重组 产生 的 负债       或 有 负债 ： 诉讼 导致 的 赔偿金                   债务 比率 与 利息 保障 倍数           净 资产负债率   =   负债 总额   ÷   股东权益 总额       长期负债 与 资本 总额 比   =   负债 总额   ÷   股东权益 总额       资产负债率   =   负债 总额   ÷   资产 总额       利息 保障 倍数   =   ( 税前 收益   +   利息费用 )   ÷   利息费用           股东权益           股东权益 背景       A 类股票 （ 不 公开 交易 ， 员工 持股 ， 表决权 大 ） 和 B 类股票 （ 普通股 ）       库藏 股 ： 公司 回购 用于 未来 分配 的 股票       溢缴 资本       存留 收益               普通股       公司 授权 委托书 ： 股东 委托 代表 执行 投票权               2002 联邦政府 通过 《 奥 半死 - 奥克斯 利 法案 》 加强 监管 ， 董事长 不 兼任 管理 职务 以 保持 独立性               现金 股利 ， 宣布 日 成为 负债               支付 股利 一方面 可能 表明 公司 在 经营 之外 还有 多余 现金支付 股利 ， 表明 公司 运作 良好 ，     另一方面 也 可能 表明 公司 没有 其他 投资 项目 可以 吸收 其 创造 的 全部 现金 。               优先股 不具 表决权 ， 但是 拥有 对 资产 的 优先 要求 权           累计 股利 ： 对 优先股 未付 的 股利 ， 以后 还是 要 付               股票 增发           股票 期权 ： 在 一定 期间 ， 按照 一定 价格 购买 一定 数量 公司股票 的 权利       限制性 股票 ： 作为 溢缴 资本                   股票 分割 ， 降低 每股 价格           股票 股利 ： 是 公司 向 股东 无偿 增发 的 新股 * ， 为什么 可以 算作 股利           不足 一股 ， 发现 金 补齐               股票 回购           原因 ： 管理层 认为 自己 股票 被 低估 ， 相当于 投资 自己 ； 需要 作为 股票 期权 分配 给 员工       会 导致 现有 股票 的 稀释 ， 因为 回购 价格 高 ？                   普通股 其他 发行 方式 ：           非现金 交换 土地 等 资产       债转股                   存留 收益 ： 也 叫 “ 资本 公积 ”               净资产 收益率   =   ( 净 收益   -   优先股 股利 )   ÷   普通股 股东权益 平均 余额           普通股 每股 账面 价值   =   ( 股东权益 总额   -   优先股 账面 价值 )   ÷   发行 在外 普通股 股数           公司 间 投资 与 合并 财务报表           合并 财务报表 是 指 两个 或 两个 以上 的 独立 法律 实体 的 财务 记录 合并 在 一起 列报       在 非 控制 但 能 显著 影响 其 经营 和 财务 政策 的 经济实体 的 投 在 采用 权益法                       控股 比例       试用 法则                       & lt ;   20 %       市场 法               20 %   ~   50 %       权益法               & gt ;   50 %       子公司 ， 合并 财务报表                   合并 财务报表           公司 间 交易 抵消       非 控股 股东权益 ： 反应 费 多数 股东 对 合并 入 多数 股东 报告 中 的 公司 资产 和 收益 的 要求 权 。         合并 时 ， 所有 收益 均 予以 合并 ， 然后 减去 非 控股 股东 持有 的 收益 份额 。           收购           收购价格 高于 收购 净资产 的 公允 价值 的 部分 ， 被 称为 商誉 （ goodwill ） 。       商誉 减值 处理           财务报表 分析           目标 ： 用 财务报表 了解 公司 过去 的 业绩 。       年报 的 附注 非常 重要       重要 表 ： 10 - K ， 10 - Q ， 8 - K 表       银行 或 其他 提供 大额 贷款 的 债权人 可能 要求 提供 一份 预测 财务报表 或 对 预测 结果 的 其他 估计 。           TIPS           制造业 企业 对 折旧 方法 的 选择 会 影响 所 报告 的 毛 边际 ， 因为 他 影响 销货成本      ", "tags": "economics", "url": "/wiki/economics/financial-accounting.html"},
      
      
      {"title": "货币金融学", "text": "    Table   of   Contents           关于           为什么 要 研究 货币 、 银行 和 金融市场           基本概念                   金融体系 概览           金融市场 的 结构           金融市场 工具           金融市场 的 国际化           金融 中介机构 的 功能 ： 间接 融资           金融 中介机构 的 类型           金融体系 监管           网络 练习题                   什么 是 货币           网络 练习                   利率                 关于       在 看 完 了 财务会计 教程 之后 ， 继续 看 一些 货币 金融学 方面 的 内容 。     之前 看过 昆曼 的 经济学 原理 ， 收获 很多 ， 对 一些 经济 现象 也 能 在     一定 程度 上 进行 推演 ， 理解 其 背后 的 经济学 原理 。 但是 当 设计 到     金融 方面 的 内容 的 时候 ， 就 一脸 懵 逼 了 。 而且 由于 每天 接触 的 新闻     和 其他 媒体 内容 ， 使得 我 对 金融 方面 的 疑惑 越来越 多 ， 所以 是 时候     看看 这方面 的 东西 。 这 一次 我 选择 了 弗雷德里克   S .   米 什金 的     《 货币 金融学 》 一书 ， 多 看 上 花 了 50RMB 多 买 的 商学院 版本 中译本 。     至于 为什么 要学 这门 课 ， 可以 用 这 本书 中 的 一句 话 来 回答           对 货币 、 银行 和 金融市场 的 学习 和 研究 ， 可以 帮助 你 了解 那些 在 政治 领域     中 激烈 争辩 的 货币政策 执行 议题 的 含义 ， 帮助 你 更加 清晰 的 理解 媒体报道 中     涉及 的 各种 经济 现象 。 由此 获得 的 知识 可以 使 你 获益 终身 。           为什么 要 研究 货币 、 银行 和 金融市场       目标 ： 通过考察 金融市场 （ 诸如 债券市场 、 股票市场 和 外汇市场 等 ）     和 金融机构 （ 诸如 银行 、 保险公司 、 共同 基金 以及 其他 金融机构 等 ）     的 运行 过程 ， 探求 货币 在 经济 循环 中 所 发挥 的 作用 。           观点 ：   健全 的 金融市场 是 实现 高速 经济 增长 的 关键 要素 。           基本概念               证券 ( security ) 又称 金融工具 ， 是 对 发行者 未来 收益 或者 资产 的 一种 求偿 权 。               债券 ( bond ) 是 神诺 在 约定 期限内 定期 偿付 的 债务 证券 。               普通股 ( common   stock ) 代表 了 其 持有人 对于 公司 具有 一定 份额 的 所有权 ， 代表         对 公司 的 资产 和 收益 具有 的 求偿 权 。               不同 利率 具有 一致 的 变动 趋势 ， 经济学家 通常 将 不同 的 利率 集合 在 一起 ，         统称 为 利率 。           高利率 可能 会 使 公司 推迟 新 厂房 的 建设 计划 ， 这 原本 可以 提供 更 多 的 就业机会 。                   金融体系 有 各种各样 的 私人 金融机构 组成 ， 具体 包括 ： 银行 、 保险公司 、 共同 基金 、         财务 公司 以及 投资银行 等 。 自然人 一般 通过 金融 中介机构 ( financial   intermediaries )         间接 向 公司 提供 贷款 。               金融危机               经济周期               通货膨胀               外汇市场 ( foreign   exchange   market ) 是 完成 货币 兑换 的 场所 。               研究 方法 ：           资产 需求 分析 的 简化 方法       均衡 的 概念       。 。 。                   GDP 平减 指数               金融体系 概览           金融市场 的 重要 意义 在于 他 能 有效 地 配置 资本 ( captial ) ， 从而 提高 整体 经济 的 产出 水平 和 效率 。           金融市场 的 结构               债务 市场 ： 债券 或 抵押 票据 等 抵押 债务 工具 。 通常 远大于 股票市场           短期 债务 工具   短于 1 年       中期 债务 工具   1 - 10 年       长期债务 工具   10 年 或 10 年 以上                   股权 市场 ： 发行 普通股 股票 ， 特点 是 剩余 求偿 权 ， 优先级 比 债务 低 。               一级 市场 ( primary   market ) 是 筹集资金 的 公司 发行 的 债券 或 股票 等 出售 给 最初 购买者 的 经融 市场 。           通常 不是 公开 的       投资银行 ( investment   bank ) 进行 证券 承销                   二级 市场 是 将 投资人 交易 证券 的 市场 *           纽交所 、 纳斯达克 、 外汇市场 、 期货市场 和 期权 市场 等       使 金融工具 的 变现 活动 更加 迅速 和 便捷 ， 提高 流动性 ( liquid ) ， 流动性 越高 ， 越 受欢迎       圈定 一级 市场 发行 价格       组织 形式       交易所   ( exchanges )       场外 交易市场 ( OTC   market )   如 美国政府 债券市场 、 可 转让 定期存单 、 联邦 基金 、 外汇市场                           货币 市场 ( money   market )   是 短期 债务 工具 进行 交易 的 金融市场 ， 资本 市场 ( capital   market ) 是 长期         债务 工具 和 股权 工具 交易 的 市场 。 短期 证券 流动性 好 ， 价格 波动 小 ， 更为 安全 。               金融市场 工具               货币 市场 工具 ： 短期 债务 工具 ， 其 原始 期限 通常 短于 1 年               美国 国库券 ， 通过 折价 发行 ( 贴现 ) 来 支付 利息 ， 流动性 最强 ， 几乎 不 存在 违约 风险 ( default ) 。         因为 联邦政府 可以 通过 增加 税收 或者 增发 货币 来 偿还债务 。 银行 是 国库券 的 主要 持有人 。               可 转让 银行 定期存单 ： 定期存单 是 银行 向 存款人 发行 的 ， 定期 支付 利息 到期 后以 初始 购置 价格 还本 。         可以 在 二级 市场 出售 的 定期存单 就是 可 转让 银行 定期存单 。               商业 票据 ： 有 大型 银行 和 诸如 微软 等 民企 发行 的 短期 债务 工具 。               回购 协议 ( repurchase   agreement ,   repos ) ： 是 一种 有效 地 短期贷款 （ 低于 2 年 ） ， 以 国库券 作 抵押 ，         如果 借款者 无法 偿还 就 可以 将 国库券 收归 己 有 。 回购 协议 通常 借款者 是 银行 ， 它 成为 银行 的 主要 资金来源 ，         而 大 公司 是 主要 贷款 者 。               联邦 基金 ： 美联储 中 拥有 存款 的 银行 间 的 隔夜 贷款 。 对应 的 利率 叫做 联邦 基金 利率 。 国内 当然 是         商业银行 间 的 隔夜 拆借 啦 。 银行 在 央行 要 有 足够 的 准备 经 。 该 利率 高 说明 银行 资金紧张 。                       资本 市场 工具 ： 长期债务 工具 ， 其 原始 期限 通常 为 1 年 或者 大于 1 年               股票 ， 股票 市值 占 比大 ， 但是 每年 新 发 股票价值 小 。 大约 一半 股票 由 个人 持有 ， 其余 由 养老金 、         共同 基金 和 保险公司 等 持有 。               抵押 贷款 ， 向 购买 房屋 、 土地 等 不动产 的 个人 或者 企业 提供 的 ， 以 不动产 作 抵押 的 贷款 。 它 是 美国 最大 的         债务 市场 ， 而 住房 抵押 贷款 是 所有 其他 抵押 贷款 的 4 倍 。           住房 抵押 贷款 主要 贷款 者 是 储贷 协会 和 互助 储蓄银行 ， 还有 商业银行 。       商业 农业 抵押 贷款 主要 是 有 商业银行 和 人寿保险 公司 发放 的 。       联邦政府 通过 房利美 ( FNMA ) 、 房地 美 ( FHLMC ) 、 政府 国民 抵押 协会 ( GNMA ) 发行 债券 筹集资金 ， 使用 这些 资金 购买 抵押 贷款 ， 从而 向 市场         提供 融资 。                   公司债券 ， 信用 评级 优异 的 公司 发行 的 长期 债券 。           某些 债券 可 转换成 规定 数量 的 股票 ， 成为 可 转换 债券 。 这种 债券 流动性 更好 。 虽然 市场 规模 比 股票市场 小 ， 但是 每年 发行 的 新 债券 比新 股票 多得多 。       主要 买方 是 人寿保险 公司 、 养老 基金 和 居民 。                   美国政府 证券 ， 美国财政部 发行 的 为 联邦政府 财政赤字 提供 融资 的 长期债务 工具 。           交易量 最大 、 流动性 最强       主要 持有者 包括 联邦 储备 体系 、 银行 、 居民 和 外国人                   美国政府 机构 证券 ， 就是 其他 一些 政府 机构 发行 的 为 政府 项目 提供 融资 的 证券 。 得到 联邦政府 担保 ， 与 政府 债券 类似 。               州 和 地方 政府 债券 ， 市政府 债券 ， 为 当地政府 建 学校 、 修 公路 等 大型项目 提供 融资 的 长期债务 工具 。           免税       商业银行 是 最大 购买者 ， 其他 还有 富人 和 保险公司                   消费 贷款 和 银行 商业贷款 ， 就是 想 消费者 提供 的 普通 商业贷款 啦                       金融市场 的 国际化               外国 债券 ( foreign   bonds ) 是 在 境外 发行 、 以 发行 国 货币 计价 的 债券 。           德国 汽车 公司 在 美国 发行 的 美元 债券       19 世纪 美国 建设 铁路 在 英国 发售 的 外国 债券                   欧洲 债券 ， 是 以 发行 国 以外 的 货币 计价 的 债券 。           伦敦 发行 的 以 美元 计价 的 债券       占 国际 债券 比例 80 %       大多数 欧洲 债券 不是 以 欧元 计价 的 ， 而是 以 美元 计价 的                   欧洲 货币 ， 存放 在 货币 发行 国 境外 银行 的 外汇 存款 。           欧洲 美元 ： 存放 在 美国 境外 的 外国 银行 或者 美国银行 境外 分行 的 美元 存款       不 一定 跟 欧洲 有 关系                   股票市场               金融 中介机构 的 功能 ： 间接 融资           向 贷款 者 借入 资金 ， 向 借款者 发放贷款 来 实现 资金 转移       是 公司 主要 的 融资 来源       存在 的 价值 ： 可 降低 金融交易 中 的 交易成本 、 风险 分担 和 信息 不对称性           交易成本 ( transaction   cost ) 是 指 在 金融交易 过程 中小 号 的 时间 和 金钱 。           是 计划 借出 富余 资金 的 人 所 面临 的 主要 问题 。       金融机构 可以 通过 规模 效应 摊薄 交易成本 ： 例如 一份 法律合同 可以 用 在 上万份 贷款 业务 中       低廉 的 交易成本 可以 使 中介机构 提供 流动性 服务 ： 支票 、 银联 支付 等                   风险 分担 ： 中介机构 低廉 的 交易成本 分散 风险 ， 通过 多样化 的 投资 分散 风险               信息 不对称性           逆向 选择 ( adverse   selection ) ： 交易 前 ， 因为 高风险 的 借款者 对 借款 更 有 积极性 ， 由于 信息 部队 称 导致 他们 更能 借到 钱 ，         但是 偿还 能力差 使得 整个 市场 风险 提高 ， 最终 导致 借款者 不愿 借款       道德风险 ( moral   hazard ) ： 交易 后 ， 借款者 借到 钱 后 可能 会 挪用 到 更 高风险 业务 当中 ， 导致 整个 市场 风险 提高 ，         由于 信息 不 对称 导致 借款者 不愿 借款       金融机构 具有 专业 的 能力 解决 信息 不 对称 问题 ， 比如 可以 对 贷款 企业 和 个人 做 详尽 的 调查                   金融 中介机构 的 类型               存款 机构 ( 银行 ) ： 货币 供给 的 主要 组成部分           商业银行       储贷 协会 和 互助 银行 ， 越来越 像 商业银行       信用社 ： 通过 股份 的 存款 来 募集 资金 ， 主要 用于 发放 消费 贷款                   契约型 储蓄机构 ： 能够 精确 预测 未来 年度 需要 向 受益人 支付 的 金额 ， 主要 投资 公司债券 、 股票 和 抵押 贷款 等 长期 证券           人寿保险 公司 ： 出售 年 金 募集 资金 ， 购买 长期 证券 获取 投资收益 ， 股票投资 数目 受限       火灾 意外 伤害 保险公司 ： 就是 各种 保险公司 啦       养老 基金 和 政府 退休 基金                   投资 中介机构           财务 公司 ： 出售 商业 票据 、 发行股票 和 债券 来 融资 ， 资金 借给 消费者 和 小型企业 ， 有些 财务 公司 是 母公司 建立 的 ，         帮助 出售 母公司 商品 ， 例如 汽车 公司 下 的 信贷 公司       共同 基金 ： 通过 股份 汇集 资金 ， 然后 批量 购买 股票 和 债券 获取 更 低 的 交易成本 ， 更 多样 的 投资 组合       货币 市场 共同 基金 ： 简称 货币基金 ， 和 共同 基金 融资 方式 类似 ， 但是 投资 的 是 安全性 和 流动性 都 很 高 的 货币 市场 工具 。       投资银行 ： 不 吸收 存款 也 不 发放贷款 ， 而是 协助 公司 发行 证券 ， 靠收 佣金                   金融体系 监管           证券法 ， 1933       金融 恐慌       准入 限制       信息 披露       对于 资产 和 业务 活动 的 限制 ： 比如 限制 参与 高风险 投资 活动       存款 保险       中国 从 2015 年 开始 实施 存款 保险 ， 消息来源   http : / / finance . people . com . cn / money / GB / 217428 / 391026 /   ，     也 有 知乎 网友 表示   这种 制度 短期内 是 负面 的   ，     因为 中国 本来 存款 是 由 政府 兜底 的 ， 这个 政策 一出 表明 政府 不想 兜底 了 ，     也 就是 暗示 银行 可能 破产 ， 会 导致 挤兑 ， 对 小型 银行 不利 。 历史 上 第一家 破产 的 银行 是 海南 发展 银行 ，     后来 债务 并入 工行 才 得以 解决 。               对 竞争 的 限制 ： 限制 竞争 来 降低 盲目 竞争 带来 的 风险       利率 管制 ： 放置 利率 恶性竞争 带来 的 风险           网络 练习题           联邦储备委员会 发布 的 美国 资金 流动 报告     https : / / www . federalreserve . gov / releases / z1 /         证券 交易所     https : / / www . nyse . com             什么 是 货币               银行券 ： 早期 由 私立 银行 发行 的 纸币               货币 的 含义           任何 一种 被 普遍 接受 的 、 可 用于 购买 商品 和 服务 支付 行为 的 或者 偿付 债务 的 物品 。       通货 ( currency ) ： 纸币 和 硬币       如果 很 容易 将 它们 迅速 地 转换成 通货 或者 支票 账户 存款 ， 那么 它们 就 具备 了 货币 的 功能 ， 比如 储蓄存款       财富 ： 指 储藏 的 各种 财产 综合 ， 不仅 包括 货币 ， 还有 各种 长期 资产 ， 如 不动产 、 债券 、 股票 、 艺术品 、 汽车 等                   货币 的 功能 ： 1 .   交易 媒介 ，   2 .   记账 单位 ，   3 .   价值 储藏               作为 交易 媒介 ， 提高 经济效益 ， 因为 可以 提高 交易 的 成功率 ， 降低 交易成本 。 比如 物物 交换 需要 交易 双方 都         需要 对方 的 物品 时 ， 才能 进行 交易 。 提高 社会分工 ， 使得 程序员 不用 去 种菜 也 能 买 到 便宜 的 菜 吃 。               记账 单位 ， 如果 没有 货币 ， 需要 知道 N 个 商品 的 价格 ( 相对 价格 ) ， 需要 知道 N ^ 2 个 数据 ， 而 有 了 货币 计量 的话 ，         就 只要 N 个 数据 。               价值 储藏 ： 货币 作为 价值 储藏 的 效果 取决于 物价水平 ， 物价上涨 的话 就 相当于 贬值 。 但是 由于 货币 具有 最大 的         流动性 ！ 它 是 流动性 最高 的 资产 。               支付 体系 ：           商品 货币 ： 如 贵金属 ， 但是 过于 沉重 。       不 兑现 纸币 ： 轻 ， 但是 需要 提高 伪造 成本 才能 实施 ， 易 被盗 。       支票 ：       电子 支付 ： 借记卡 、 储值卡 、 预付卡 、 智能卡 、 电子 现金 等                   货币 的 计量 ： M1 和 M2                   犯罪分子 是 拥有 现金 最多 的 人           网络 练习       利率           预期 利率      ", "tags": "economics", "url": "/wiki/economics/money-banking-and-financial-markets.html"},
      
      
      
      
      
        
      
      {"title": "Hive full join 的优化", "text": "    Table   of   Contents           问题 背景           优化 方案                 问题 背景       有 多个 表 A , B , ... , Z ， 主 key 是 用户 ID ， 需要 full   join   到 一个 表中 。               select       coalesce     (     A     .     ID     ,       B     .     ID     ,       C     .     ID     )       as       ID     ,       A     .     col1     ,       B     .     col2     ,       C     .     col3       from       A       full       outer       join       B       on       A     .     ID     =     B     .     ID       full       outer       join       C       on       coalesce     (     A     .     ID     ,       B     .     ID     )     =     C     .     ID                 如果 有 很多 个表 ， 由于 上述 多个 连接 操作 的 key 中 并 没有 一个 固定 的 key ， 所以 HIVE 无法 优化 到 一个 MR ， 只能 顺序 的 join ， 导致 速度 较慢 。       优化 方案       可以 通过   UNION   ALL   优化               select       ID     ,       sum     (     col1     )       as       col1     ,       sum     (     col2     )       as       col2     ,       sum     (     col3     )       as       col3       from       (               select       ID     ,       col1     ,       NULL       as       col2     ,       NULL       as       col3       from       A                 UNION       ALL                 select       ID     ,       NULL       as       col1     ,       col2     ,       NULL       as       col3       from       B                 UNION       ALL                 select       ID     ,       NULL       as       col1     ,       NULL       as       col2     ,       col3       from       C       )     T       group       by       ID                 对于 数值 类型 ， 用   sum   聚合 ，   如果 是 字符串 类型 ， 可以 用   concat _ ws ( ' ' ,   collect _ list ( col ) )   聚合 。       在 一个 任务 上 实测 优化 前 需要 20min ， 优化 后 减少 到 10min ！  ", "tags": "hive", "url": "/wiki/hive/hive-full-join-optimization.html"},
      
      
      {"title": "Hive join 数据倾斜的一个case优化", "text": "    Table   of   Contents           问题           解决方案                 问题       设有 一个 关于 用户 信息 表 A ， 用户 ID 字段 为 uid ， 存在 少量 ( & lt ; 1000 ) uid 有 大量 记录 ( & gt ; 100000 ) ， 使得 表 A 和 其他 表以 uid 为 key 做 JOIN 的 时候 ， 存在 数据 倾斜 。             uid   |     f1     |   f2   1       |     xx     |   yy   1       |     xx     |   yy   2       |     xx     |   yy               解决方案       思路 一 ： 在 WHERE 条件 中 过滤 掉 那些 导致 数据 倾斜 的 uid 。       可以 解决 ， 但是 如果 这种 uid 太多 ， 或者 每天 会 随 时间 变化 ， 那么 这种 硬 编码 的 方法 并 不 通用 。       思路 二 ： 利用 skewjoin 的 优化 选项               set       hive     .     optimize     .     skewjoin     =     true     ;       set       hive     .     skewjoin     .     key     =     1000     ;                       将 一个 join 操作 变为 两个 ， 第一个 会 将 同一个 key 分散 到 不同 的 reduce ， 从而 解决 JOIN 时 数据 倾斜 问题 。   hive . skewjoin . key   要 设置 为 需要 分散 的 key 最少 条数 ， 默认 10W 条才 会 做 这个 操作 。       思路 三 ： 利用   JOIN   过滤 掉 那些 导致 倾斜 的 uid       根据上述 思路 ， 写下 如下 代码               select       A     .     *       from       A       JOIN       (               select       uid       as       guid       from       A               group       by       uid               having       sum     (     1     )       & lt ;       1000       )       B       ON       A     .     uid     =     B     .     guid                 B 表 用于 选择 满足条件 的 uid ， 然后 通过 JOIN 过滤 ！ 问题 来 了 ， 这个 过滤 也 是 JOIN ， 所以 仍然 会 有 数据 倾斜 的 问题 ！ ！       由于 这样 的 uid 总体 上 来说 比较 少 ， 而 问题 的 关键 出 在 不能 有 JOIN 的 REDUCE 操作 ， 所以 可以 利用 MAPJOIN 来 解决               select       A     .     *       from       A       LEFT       OUTER       JOIN       (               select       uid       as       guid       from       A               group       by       uid               having       sum     (     1     )       & gt ;       1000       )       B       ON       A     .     uid     =     B     .     guid       WHERE       B     .     guid       is       null                 注意 这里 的 B 表是 一个 小表 ， 可以 通过 MAPJOIN 优化 使得 这个 LEFT   OUTER   JOIN 没有 reduce 操作 ， 实现 在 map 端 过滤 ， 完美 解决 ！  ", "tags": "hive", "url": "/wiki/hive/hive-skew-join-optimize-case.html"},
      
      
      {"title": "Hive 提取最大值所对应行的操作优化", "text": "    Table   of   Contents           问题 背景           解决方案                 问题 背景       设有 表 A 包含 3 列 , 分别 是 c1 ,   c2 ,   c3 ; 其中 c1 到 c2 和 c3 是 1 对 多 的 关系 , 需要 在 每 一个 相同 c1 中 , 提取 c3 最大 ( 或 最小 ) 的 那 一行 的 c2 值 。     即 实现 如下 操作               select       c1     ,       c2       from     (               select       c1     ,       c2     ,       row _ number     ( )       over       (     partition       by       c1       order       by       c3       desc     )       as       r               from       A       )     T       where       r     =     1                 如果 不 存在 某些 c1 对应 大量 的 c2 , 即 不 存在 数据 倾斜 , 上述 代码 非常容易 实现 上述 逻辑 。 但是 当 存在 数据 倾斜 , 那么 这种 写法 将 导致 程序 卡死 在 reduce 操作 上面 。     窗 函数 在 实现 的 时候 会 将 同一个 partition 放在 一个 reduce 操作 , 貌似 没有 什么 优化 参数 可以 设置 这种 情况 的 数据 倾斜 。       解决方案       方案 1 : 实现 一个 通用 的 UDAF ,     select _ max ( c3 ,   c1 )   , 选出 c3 最大 的 c1 的 值 。       方案 2 : 如果 c3 是 日期 或者 其他 满足 字典 序 关系 的 字 段 ,   比如 日期   yyyy - MM - dd 。 那么 可以 用 一个 简单 的 trick 搞定 。               select       c1     ,       split     (     max     (     concat     (     c3     ,     &# 39 ; __&# 39 ;     ,       c2     ) ) ) [     1     ]       as       c2       from       A       group       by       c1                 即先 将 c3 作为 前缀 与 c2 通过 一个 特殊 分隔符 拼接 成 一个 字符串 , 然后 里面 字符串 求 最大值 的 方法 求 出 !     然后 设置   set   hive . groupby . skewindata = true   可以 有效 的 解决 数据 倾斜 的 问题 。       通过 这种 优化 之后 , 能够 有效 的 解决 数据 倾斜 问题 , 速度 有 了 很大 的 提升 !  ", "tags": "hive", "url": "/wiki/hive/hive-first-row-optimization.html"},
      
      
      {"title": "Hive 用户自定义函数开发", "text": "    Table   of   Contents           几个 重要 概念           UDF           UDAF           UDTF                 几个 重要 概念             ObjectInspector     用于 对象 描述 , 实际上 应该 理解 为 对 SQL 一个 字 段 属性 的 抽象 ?         ConstantObjectInspector     用于 描述 常 数字 段 的 描述 , 可以 通过 方法   getWritableConstantValue   获取 到 常数 的 值         WritableConstantIntObjectInspector     整数                   UDF       UDAF       UDTF  ", "tags": "hive", "url": "/wiki/hive/hive-udf.html"},
      
      
      {"title": "Hive 行列互转", "text": "    Table   of   Contents           行 转列           列 转行                 行 转列       比如 表 A 有 三个 字 段 如下       ID ,   key ,   value     1 ,     A ,     1     1 ,     B ,     0.4     2 ,     A ,     0.3       需要 将 同一个 ID 的 多条 记录 转为 一列 ， 显然 需要 通过 聚合 操作 实现 。     对于 value 是 数值 类型 的 ， 聚合 函数 可以 选 为   sum   ， 对于 字符串 类型 ， 需要 使用 聚合 函数   collect _ list   。               select       ID     ,       sum     (     if     (     key     =     &# 39 ; A &# 39 ;     ,       value     ,       NULL     ) )       as       A _ value     ,       sum     (     if     (     key     =     &# 39 ; B &# 39 ;     ,       value     ,       NULL     ) )       as       B _ value       group       by       ID                 字符串 类型               select       ID     ,       concat _ ws     (     &# 39 ; &# 39 ;     ,       collect _ list     (     if     (     key     =     &# 39 ; A &# 39 ;     ,       value     ,       NULL     ) ) )       as       A _ value     ,       concat _ ws     (     collect _ list     (     if     (     key     =     &# 39 ; B &# 39 ;     ,       value     ,       NULL     ) ) )       as       B _ value       group       by       ID                 列 转行       这个 需要 利用   UDTF   实现 ！  ", "tags": "hive", "url": "/wiki/hive/hive-col-row-convert.html"},
      
      
      {"title": "Hive中窗函数数据倾斜的问题", "text": "    Table   of   Contents           问题 背景           现象           解释           解决办法                 问题 背景       现有 源表 A ， 包含 两列 ， id 和 score                   id       score                       1       0.3               2       0.5               8       0.9                   A 表 大约 有 3 亿条 记录 ， 现在 需要 根据 score 的 排序 将 记录 分为 10 个 等级 。     将 score 按照 从小到大 排序 ， score 在 前 10 % 的 记录 ， 等级 为 0 ； 10 % - 20 % 之间 的 ， 等级 为 1 ； 以此类推 。                   id       score       level                       1       0.3       1               2       0.5       5               8       0.9       8                   如果 用 全局 排序 实现 ， 需要 对 3 亿条 记录 排序 ， 非常 慢 。 可以 先 利用 分 位点 近似算法 ， 近似 寻找 10 % , 20 % , ... , 90 % 分 位点 来 优化 。     于是 ， 很 容易 用 聚合 函数     percentile _ approx ( )     通过 转换 为 窗 函数 实现               select       id     ,       array _ search     (     score     ,       splits     )       from       (               select       id     ,       score     ,                       percentile _ approx     (     score     ,       array     (     0     .     1     ,     0     .     2     ,     0     .     3     ,     0     .     4     ,     0     .     5     ,     0     .     6     ,     0     .     7     ,     0     .     8     ,     0     .     9     ) )       over       ( )       as       splits               from       A       )     t                   array _ search     是 自己 写 的 一个 UDF ， 寻找 score 所在 的 分段 ！       现象       运行 的 时候 发现 ， 多个 reduce 最后 一个 运行 特别 慢 ， 而且 当 运行 到 100 % 后 会 卡死 ， 直到 大约 20 分钟 结束 ，     在 输出 的 数据 分片 中 ， 发现 所有 的 数据 都 集中 到 一个 分片 了 。       解释       问题 在     over   ( )     将 聚合 函数 转换 为 窗 函数 上 ， 窗 函数 会 将 同一个 partition 的 数据 分 到 一个 reduce 上 ，     如果 是     over   ( partition   by   xx )   ， 那么 xx 字段 相同 的 值会 在 同一个 reduce 上 ， 然而 这里 是 将 所有 的 记录 当做 同一个 分片 ，     因而 出现 了 所有 数据 集中 到 一个 reduce 上面 的 现象 ， 进而 导致 写入 数据 的 时候 ， 只有 一个 reduce 在 写入 3 亿条 记录 ，     使得 看起来 就 像 卡死 在 100 % 这个 状态 了 ！       解决办法       不要 用窗 函数 ， 而用   ( map )   join   ， HIVE 会 自动 将 这个 join 操作 转为   map   join 。               select       id     ,       array _ search     (     score     ,       splits     )       from       A       join       (               select       percentile _ approx     (     score     ,       array     (     0     .     1     ,     0     .     2     ,     0     .     3     ,     0     .     4     ,     0     .     5     ,     0     .     6     ,     0     .     7     ,     0     .     8     ,     0     .     9     ) )       as       splits               from       A       )     t                 经过 改造 后 ， 运行 时间 从 之前 接近 1 小时 减少 到 5 分钟 ！  ", "tags": "hive", "url": "/wiki/hive/hive-window-function-question.html"},
      
      
      {"title": "Hive中窗函数的妙用", "text": "    Table   of   Contents           问题 背景                 问题 背景       先有表 如下                       A       B       C                       1       5       abc               7       4       abc               5       6       abc               - - - - - - - - - - - - - - - - - - - -                               现在 需要 对 同一个 C ， 按照 A 列 排序 （ 倒序 ） ， 计算 B 的 累积 分布 。     例如 ， 在 这个 例子 中 ， C = abc ， A = 7 ， 累积 值   CB = 4 ;   A = 5 ,   累计 值   CB = 4 + 6 = 10 ；   A = 1 ,   累积 值 CB = 4 + 6 + 5 = 15 ；    ", "tags": "hive", "url": "/wiki/hive/hive-window-function-usage.html"},
      
      
      
      
      
        
      
      {"title": "Beyes optimization", "text": "    Table   of   Contents           关于           导论           Bayesian   Optimization   with   Gaussian   Process   Priors           高斯 过程           Acquisition   Functions                   In   Action           Covariance   Functions   and   Treatment   of   Covariance   Hyperparameters                         关于       贝叶斯 优化 用来 调参 ！     论文 ： Practical   Bayesian   Optimization   of   Machine   Learning   Algorithms ,   Jasper   Snoek ,   Hugo   Larochelle ,   Ryan   P .   Adams .       导论           关键词 ： 高斯 过程 （ Gaussian   Process ）       关键 要素 ： type   of   kernel ， treatment   of   its   hyperparameters       贝叶斯 优化 原始 文献 ：       Jonas   Mockus ,   Vytautas   Tiesis ,   and   Antanas   Zilinskas .   The   application   of   Bayesian   methods   for   seeking   the   extremum .   Towards   Global   Optimization ,   2 : 117 – 129 ,   1978 .       D . R .   Jones .   A   taxonomy   of   global   optimization   methods   based   on   response   surfaces .   Journal   of   Global   Optimization ,   21 ( 4 ) : 345 – 383 ,   2001 .       Niranjan   Srinivas ,   Andreas   Krause ,   Sham   Kakade ,   and   Matthias   Seeger .   Gaussian   process   optimization   in   the   bandit   setting :   No   regret   and   experimental   design .   In   Proceedings   of   the   27th   International   Conference   on   Machine   Learning ,   2010 .       Adam   D .   Bull .   Convergence   rates   of   efficient   global   optimization   algorithms .   Journal   of   Machine   Learning   Research ,   ( 3 - 4 ) : 2879 – 2904 ,   2011 .               假定 连续函数 采样 自 高斯 过程 ， 并 保存 一个 后验 概率 ， 根据 观测 值来 调整 。           expected   improvement   ( EI ) ；   Gaussian   process   upper   confidence   bound   ( UCB )               random   search   is   beter   than   grid   search :   James   Bergstra   and   Yoshua     Bengio   .   Randomsearch   for   hyper - parameter   optimization .   Journal   of   Machine   Learning   Research ,   13 : 281 – 305 ,   2012 .               Bayesian   Optimization   with   Gaussian   Process   Priors           优化 函数   $ ( f ( x ) ,   x   \ \ in   \ \ mathcal { X } ) $ ， $ ( \ \ mathcal { X } ) $   是   $ ( \ \ mathbb { R } ^ n ) $   上 的 闭集 。       贝叶斯 优化 基本 思想 ： 为 函数   $ ( f ( x ) ) $   建立 概率模型 ， 然后 根据 贝叶斯 法则 决定 下 一个 搜索 点 ！       贝叶斯 优化     综述   ： Eric   Brochu ,   Vlad   M .   Cora ,   and   Nando   de   Freitas .   A   tutorial   on   Bayesian   optimization   of   ex -   pensive   cost   functions ,   with   application   to   active   user   modeling   and   hierarchical   reinforcement   learning .   pre - print ,   2010 .   arXiv : 1012.2599 .           高斯 过程           在 有限 集   $ ( \ \ {   x _ n   \ \ in   \ \ mathcal { X }     \ \ } _ { n = 1 } ^ N ) $ ， 第 n 个点 的 值 为   $ ( f ( x _ n ) ) $ 。       均值 函数   $ ( m :   \ \ mathcal { X }   \ \ rightarrow   \ \ mathbb { R } ) $       正定 协方差 函数 （ covariance   function ）   $ ( K :   \ \ mathcal { X }   \ \ times   \ \ mathcal { X }   \ \ rightarrow   \ \ mathbb { R } ) $       高斯 过程 综述 ： Carl   E .   Rasmussenand   Christopher   Williams .   Gaussian   Processes   for   Machine   Learning .   MIT   Press ,   2006 .           Acquisition   Functions           假定 函数   $ ( f ( x ) ) $   来自 高斯 先验 ， 观测 量   $ ( \ \ { x _ n ,   y _ n   \ \ } _ { n = 1 } ^ N ,   y _ n   \ \ sim   \ \ mathcal { N } ( f ( x _ n ) ,   v ) ) $ ， v 是 观测 噪声 。       Acquisition   function :   $ ( a :   \ \ mathcal { X }   \ \ right   \ \ mathbb { R } ^ + ) $ ， 下 一个 最优 搜索 点 通过 该 函数 得到 ： $ ( x _ { next }   =   \ \ arg \ \ max _ x   a ( x ) ) $ 。 跟 之前 的 观测 值 有关 ， 也 写作   $ ( a ( x ;   \ \ { x _ n ,   y _ n   \ \ } ,   \ \ theta ) ) $ ，   $ ( \ \ theta ) $ 是 超 参数 。           In   Action       Covariance   Functions   and   Treatment   of   Covariance   Hyperparameters  ", "tags": "machine-learning", "url": "/wiki/machine-learning/beyes-opt.html"},
      
      
      {"title": "Collaborative Filtering with Recurrent Neural Networks", "text": "    Table   of   Contents           论文 导读           协同 过滤 作为 一个 序列 预测 问题                         论文 导读       Collaborative   Filtering   with   Recurrent   Neural   Networks ，   IRIDIA ， 2016       协同 过滤 作为 一个 序列 预测 问题       传统 方法 ， 用 t 时刻 之前 ， 用户 消费 的 item 集合 预测 t 时刻 之后 用户 消费 的 集合 。     没有 考虑 顺序 ！       新 方法 ： 用 t 时刻 之前 的 消费 序列 $ ( x _ 1 ,   x _ 2 ,   x _ 3 ) $ ， 预测 t 之后 消费 $ ( x _ 4 , x _ 5 ) $ 。     考虑 顺序 ！ 预测 结果 也 考虑 顺序 ！           短期 预测 ： 预测 下 一个 消费 的 item       长期 预测 ： 预测 最终 消费 的 item           序列 预测 方法 适合 短期 预测 ！ 短期 预测 可以 提高 推荐 的 多样性 ！     而 不管 顺序 的 静态方法 适合 长期 预测 ！       作者 任务 隐藏 在 序列 里面 的 信息 很 重要 ！ 静态方法 没有 充分考虑 。     可以 找到 用户 兴趣 变化 ， 帮助 辨别 哪些 item 与 当前 用户 兴趣 是 无关 的 ， 或者 哪些 商品 导致用户 兴趣 的 消失 。     也 能 了解 哪些 item 对 用户 变化 的 兴趣 更 有 影响力 ？ ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cf-rnn.html"},
      
      
      {"title": "Convolutional Neural Networks", "text": "    Table   of   Contents           关于           AlexNet           结构 上 的 创新           降低 过 拟合 技巧           量化 评估                   ZFNet           卷积 网络 可视化           特征 泛化 能力           问题                   VGG   net           GoogleLeNet           ResNet           定位 与 检测           OverFeat           目标 检测   HOG           RCNN           Region   proposals           Feature   extraction           Test - time   detection           Train                   Fast   R - CNN           Faster   R - CNN           CNN 可视化                 关于       卷积 网络 经典 文章 导读 ， 文章 列表 是 参考   CS231N   课程 。       AlexNet       论文 ： Imagenet   classification   with   deep   convolutional   neural   networks     Alex   Krizhevsky ,   Ilya   Sutskever ,     Geoffrey   E   Hinton   ,   2012       Hinton   带 学生 打 比赛 的 故事 。           求解 问题 ：   ImageNet   LSVRC - 2010   比赛 ， 1.2 M 高精度 图片 ， 1000 分类 ！ ILSVRC - 2012   TOP5   error ： 15.3 % ， 第二名 是   26.2 % ！       效果 ：   TOP1   error ： 37.5 % ，   TOP2   error ： 17.0 % 。       网络 参数 ： 60M   参数 ， 650 , 000 个 神经元       重要 创新 ：   ReLU 激活 函数 ，   GPU 计算 卷积 ， dropout       5 层 卷积 层 + 3 层全 连接 层 ， 卷积 层 的 深度 是 很 关键 的 ， 移除 任何 一层 都 将 导致 性能 的 降低 ！       GTX   580   3GB   GPUs   训练   5 - 6 天             Amazon ’ s   Mechanical   Turk   crowd - sourcing   tool             对 图像 做下 采样 到 固定 大小   256x256 ， 满足 固定 大小 输入 ； 对 每个 像素 减去 在 整个 训练 集上 的 均值           结构 上 的 创新           ReLU   非线性 ： 加速 训练 ， CIFAR - 10 上 达到 25 % 错误率 ， 比 tanh 快 6 倍 ！           相关 论文 ： V .   Nair   and   G .   E .   Hinton .   Rectified   linear   units   improve   restricted   boltzmann   machines .   In   Proc .   27th   International   Conference   on   Machine   Learning ,   2010           多   GPU   训练 ： 2 个 GPU       Local   Response   Normalization ： 将 错误 减少 1 - 2 个点 。       Overlapping   Pooling ： Pooling 尺寸 = 3 ， 步长 却是 2       结构 ： 前面 五层 是 卷积 层 ， 每个 卷积 层 分为 两个 部分 ， 每个 部分 放在 一个 GPU 中 ， 在 卷积 过程 中 ， 第 2 、 4 、 5 层 的 两个 GPU 互不 干扰 ， 第 3 层 和 全 连接 层 又 相互 交错 连接 的 部分 。 maxpooling 层 在 第 1 ， 2 ， 5 层 卷积 层 ， Local   Response   Normalization   layer   在 第 1 、 2 层 。                       每 一层 的 详细 参数 ： 输入   224x224x3       96 个 11x11x3 的 滤波器 ， 分为 上下 两 部分 ， 每 部分 48 个       256 个 5x5x48 的 滤波器 ， 两个 GPU 互不 干扰       384 个 3x3x256 的 滤波器 ， 两个 GPU 有 交互       384 个 3x3x192 的 滤波器 ， 两个 GPU 互不 干扰       256 个 3x3x192 的 滤波器       全 连接 层为 4096 个 神经元               卷积 层 参数 ： 1.45 M ， 卷积 层 输出 为 6x6x256 ； 三个 全 连接 层 分别 是 ： 37.75 M ， 21.92 M ， 4.10 M ！ ！ 可以 看到 参数 主要 集中 在 卷积 层 最近 的 两个 全 连接 层 ！ ！           降低 过 拟合 技巧                   Data   Augmentation :   数据 增强 ：       平移 和 水平 翻转 ， 从 256x256 的 图片 ， 截取 224x224 的 图片 块 ， 加上 水平 翻转 ， 一张 图片 就 变成 了 32x32x2 = 2048 个 样本 ！ 预测 的 时候 ； 预测 的 时候 ， 截取 四个 角 + 中央 以及 他们 的 水平 翻转 10 张 图片 ， 结果 取 平均 ！       加噪 ， 有点像   denoise   的 概念 ， 对 每 一个 像素   $ ( I _ { xy }   =   [ I _ { xy } ^ R ,   I _ { xy } ^ G ,   I _ { xy } ^ B ] ^ T ) $ ， 不是 简单 的 在 每个 分量 上 简单 地 叠加 ， 而是 在 三个 通道 的 协方差 矩阵 的 三个 主 方向 上 ， 叠加 对应 比例 的 噪声 。 下式 中 ， p 与 lambda 分别 是 协方差 矩阵 的 三个 特征向量 和 特征值 ， $ ( \ \ alpha _ i ) $   是 叠加 的 噪声 比例 ， 服从 0 均值 方差 为 0.1 的 高斯分布 。                   $ $     [ \ \ mathbf { p } _ 1 ,   \ \ mathbf { p } _ 2 ,   \ \ mathbf { p } _ 3 ]   [ \ \ alpha _ 1   \ \ lambda _ 1 ,   \ \ alpha _ 2   \ \ lambda _ 2 ,   \ \ alpha _ 3   \ \ lambda _ 3 ] ^ T     $ $               Dropout ： 可以 看做 一种 大量 的 神经网络 的 模型 组合 。 可以 解决 过 拟合 问题 ， 学习 到鲁邦 的 特征 ， 预测 的 时候 ， 则 将 神经元 的 值 乘以 概率 即可 。   dropout   技术 大致 使得 收敛 的 迭代 次数 增加一倍 。               配置 ： NVIDIA   GTX   580   3GB   GPUs ， 两块               量化 评估       用 最后 的 4096 维 特征 作为 图像 向量 ， 评估 图像 的 相似 度 ， 效果 很 不错 ， 用   auto - encoder   于 这些 特征 上 比 在 raw   data 上 效果 应该 会 更好 。               ZFNet       论文 ： Zeiler   M   D ,   Fergus   R .   Visualizing   and   Understanding   Convolutional   Networks [ C ] .   european   conference   on   computer   vision ,   2013 :   818 - 833 .           ZFNet   在   AlexNet   上 改进 的 不 多 ， 主要 贡献 在   CNN   的 可视化 。       解释   AlexNet   为什么 效果 好 （ 主要 是 通过 可视化 分析 ） ， 以及 怎么 进一步 改进 。       数据 集 ： Caltech - 101 ， Caltech - 256 .       可视化 技术 ：   解 卷积   ， 通过 显示 激活 任意 一层 的 单一 的   feature   map   的 输入 图像 的 方法 ， 可视化 某个 神经元 学到 的 东西 。     Zeiler ,   M . ,   Taylor ,   G . ,   Fergus ,   R . :   Adaptive   deconvolutional   networks   for   mid   and   high   level   feature   learning .   In :   ICCV   ( 2011 )         敏感性 分析 ： 通过 遮蔽 输入 图片 的 一部分 ， 展示 图片 的 哪 一部分 对 分类 结果 比较 重要 。       对   AlexNet   改进 ， 并 迁移 到 其他 任务 ， 只 将 最后 一层   softmax   重新 训练 ， 有 监督 的   pre - training 。       之前 的 可视化 工作 一直 停留 在 第一层 。       通过 梯度 下降 最大化 某个 神经元 的 输出 ， 从而 找出 最优 激励 图像 （ BP   to   Image ）   Dumitru   Erhan ,   Yoshua   Bengio ,   Aaron   Courville ,   and   Pascal   Vincent ， Visualizing   higher - layer   features   of   a   deep   network ， 2009   ， 没有 解释 神经元 的 不变性 ！ ？       计算 在 最 优点 处 的   Hessian   矩阵 ， 理解 这种 不变性 ？       解 卷积 是 无 监督 学习 ， 相当于 一个 探针 ， 探测 一个 已经 学好 的 网络                       解 卷积 过程 ： 将 同 一层 的 其他 神经元 置 0 ， 将 该层 作为 解 卷积 的 输入 ， 依次 经历 了 ( i )   unpool ,   ( ii )   rectify   and   ( iii )   filter       Unpooling :   Max - pooling   不 可逆 ， 为了 解决 这个 问题 ， 在 做   Max - pooling   的 时候 ， 用 一个   switch   变量 记录 最大值 的 位置 。   问题 ， 可视化 的 时候 ， 没有 正向 卷积 过程 ， 这个   switch   变量 从 哪来 ？         Rectification ： 直接 将 重构 信号 通过   ReLU ？       Filtering ： 将 卷积 核做 水平 、 垂直 翻转 后 ， 再 进行 卷积 。 这 就 可以 解 卷积 了 ？ 不 应该 要 做 个 逆 滤波 ？               解 卷积 解释 ： 设 原始 信号 为   $ ( f ) $ ， 卷积 核为 $ ( k ) $ ， 解 卷积 核为 $ ( k ' ) $ ， 那么 经过 卷积 和解 卷积 ， 信号 变为     $ ( f   *   k   *   k ' ) $ ， 利用 卷积 运算 的 结合律 ， 也 可以 表达 为   $ (   f   *   ( k   *   k ' )   ) $ ， 如果 要 使得 解 卷积 后 的 信号     和 原始 信号 一致 ， 那么 需要   $ (   k   *   k '   =   \ \ delta   ) $ ， 即 两个 卷积 核 的 卷积 为 单位 冲击 函数 ， 也 就是     $ (   \ \ sum _ { x ' , y ' }   k ( x   -   x ' ,   y   -   y ' )   k ' ( x ' ,   y ' )   =   \ \ delta ( x ,   y ) ) $ ， 即 只有 在 $ ( x = 0 , y = 0 ) $ 时为 1 ，     其他 情况 为 0 。 这里 将 卷积 核 水平 和 垂直 翻转 后 ， 相当于   $ (   \ \ sum _ { x ' , y ' }   k ( x   -   x ' ,   y   -   y ' )   k ( - x ' ,   - y ' ) ) $     可以 看到 ， 当 x 和 y 都 为 0 时 取得 最大值 （ 达到 匹配 ） ， 其他 情况 虽然 不为 0 ， 但 小于 匹配 的 时候 的 值 ， 所以 可以 看做 逆 滤波 的 一种 近似 实现 .   不过 简单 试验 结果表明 ， 这种 近似 太 粗糙 了 。               CNN   训练 的 输入 是 [ - 128 , 128 ] ， 居然 没有 归一化 ？ ！ 初始化 是 随机 取 的 ， 幅度 为 $ ( 10 ^ { - 2 } ) $           卷积 网络 可视化                   特征 可视化 ： 选取 TOP9       结构 选择 ： 11x11 滤波器 改为 7x7 ， stride 减少 到 2 ， 从而 使得 第 1 ， 2 层 滤波器 提取 到 更 多 有用 的 信息 。 ？ ？       遮挡 敏感性 ： 测试 分类器 是否 真的 检测 到 了 图片 中 的 目标 ， 还是 只是 用 周围 的 信息 。       选取 第 5 层 最强 的   feature   map   的 响应值 之 和 ， 随着 遮挡 的 位置 的 变化 。 可视化 的 结果 如图 ( b ) 。                   特征 泛化 能力           利用   ImageNet   学 出来 的 模型 ， 应用 到 其他 任务 ， 例如 ： Caltech       只 改变 最后 一层 ， 前面 的 层 都 固定 不变 。           问题           解 卷积 可视化 具体 是 怎么样 做 的 ？           VGG   net           论文 ： VERY   DEEP   CONVOLUTIONAL   NETWORKS   FOR   LARGE - SCALE   IMAGE   RECOGNITION ,   Simonyan   and   Zisserman ,   2014   @ ICLR   2015       重要 贡献 ： 通过 非常 小 的 卷积 核   3x3 ， 提升 模型 的 深度 ！       1x1   卷积 核 的 使用 ！       卷积 的   stride   保持 为 1 ， 保证 卷积 层后 空间 分辨率 是 不变 的 。       pooling 层 保持 为   2x2   大小 的 窗 ， stride = 2 .       没有 使用   AlexNet   的   Local   Response   Normalisation   层 ！ 没有 明显 收益 ！                       用 两层 3x3 的 卷积 层 代替 一层 5x5 卷积 层 ； 3 层 3x3 的 卷积 侧 代替 一层 7x7 卷积 层 ； 这种 方法 可以 在 不 减少 卷积 核 的 覆盖范围 情况 下 ， 增加 非线性 变换 次数 并 减少 参数 ！       1x1 卷积 层 在 不 影响 空间 变换 情况 下 ， 增加 非线性 变幻 的 次数 ！ Network   in   network .       训练 参数 细节 ：       mini - batch   sgd ,   momentum   =   0.9 ,   batch   size = 256       L2   正则 参数 5e - 4       dropout   0.5 ， 最 前面 两侧 全 连接 层       学习 率 初始值 1e - 2 ， 当 验证 集 不 降低 时 除以 10       74   epoch               训练 图像 尺寸 ： CNN 输入 块 大小 是 224x224 ， 图像 被 rescale 尺寸 为 S 。 S 可以 是 固定 大小 ， 也 可以 是 多个 分辨率 。       固定 尺寸 ： 256 ， 384 （ 用 256 的 权重 初始化 网络 ）       可变 尺寸 ， 在 [ 256 , 512 ] 之间 随机 变动 S ， scale   jittering .               testing : 将 全 连接 层 变成 全 卷积 层 ： 将 最后 一层 的 通道 作为 class 通道 ， 然后 在 空间 上 平均 得到 不同 位置 的 分类 概率 的 平均值 ！ 这样 就 不用 切割 原始 图像 为 多个 块 了 ！ 只要 简答 的 rescale 为 固定 的 Q 值 即可 。       对于 固定 resclae ， 取 Q = S ， 对于 scale   jittering ， 取 Q 为 S 的 平均值 ！                                   单个 scale 的 评估 ： Q 的 取值 策略 如 上 ； 试验 表明 ， scale   jittering 帮助 提升 效果 ！       多个 scale 的 评估 ： Q 取值 策略 ， 对 固定 S 值 ， 取 S - 32 , S , S + 32 ; 对 变动 S 值 ， 取 S _ min , S _ avg , S _ max 三个 值 。 可以 看到 ， 比 单个 scale 效果 要 好 ！       多个 crop 的 评估 ： 略       卷积 网络 融合 ： 将 每 一个 网络 输出 的 多类 概率 平均 。       D 模型 参数 数目 分析 ： 138M 参数 ！ 大约 是 Alex 的 两倍 ！       参数 主要 集中 在 FC 层 ， 而 内存 消耗 主要 集中 在 前面 几层 ！           GoogleLeNet           论文 ： Going   Deeper   with   Convolutions ， CVPR2015       最大 创新 ： 增加 网络 的 深度 和 宽度 ， 但是 保持 计算 代价 不变 ！ 22 层 网络 ！       参数 数目 只有 Alexnet 的 1 / 12 .       Inception 结构 ， 借鉴 自   network   in   network .       Hebbian   principle :   neurons   that   fire   together ,   wire   together               if   the   probability   distribution   of   the   dataset   is   representable   by   a   large ,   very   sparse   deep   neural   network ,   then   the   optimal   network   topology   can   be   constructed   layer   after   layer   by   analyzing   the   correlation   statistics   of   the   pre -   ceding   layer   activations   and   clustering   neurons   with   highly   correlated   outputs               将 稀疏 矩阵 乘法 通过 聚集 后 变成   dense   matrix   乘法 ， 可以 充分利用 计算资源 。       non - uniform   deep - learning   architectures                       通过 多个 不同 的 滤波器 ， 实现 多 尺度 的 抽取 ； 通过 1x1 滤波器 实现 降维 ， 减少 大 尺寸 滤波器 计算 复杂度 ， 也 减少 了 参数 ！       22 层 ， 为了 减少 梯度 消失 效应 ， 增加 了 中间 的 输出 ， 以 期望 中间 的 特征 也 有 一定 的 区分度 ！ 提供 一种 正则 。     训练 的 时候 ， 将 这些 低层 分类 的 损失 函数 加 到 最终 损失 函数 中 ， 作为 正则 项 ！ 结果显示 ， 这种 效果 不 明显 。                               GoogLeNet   的 网络结构 参数 如表 所示 ， 其中 # 1x1 , # 3x3 , # 5x5 分别 代表 对应 的 滤波器 数目 ， 而   # 3 × 3   reduce   和   # 5x5   reduce   分别 代表   inception   中 3x3 滤波器 和 5x5 滤波器 前面 用作 降维 的 1x1 滤波器 数目 ， pool   proj 代表 inception 中 Pool 层 后面 的 1x1 滤波器 数目 ！       完全 移 除了 全 连接 层 ， 取而代之 的 是   avg   pool   层 （ top - 1 准确率 提高 了 0.6 % ） ！ 但是 保留 了 dropout ！       为了 减少 梯度 消失 的 问题 ， 在 中间 加 了 两个 输出 抽头 ！ 抽头 的 结构 如下 ：       5x5   avg   pooling ,   stride = 3 ， 分别 将 图片 降维至   4x4x512 ,   4x4x528 ！       1x1 滤波器 降维至 128 维       全 连接 层 1024 的 神经元       70 %   dropout                   训练方法 ： 用 的 是   DistBelief   分布式 训练 ！ 模型 并行 和 数据 并行 训练 。 CPU 集群           异步   SGD ，   momenton = 0.9       固定 学习 率 策略 ， 每 8 个 poch 减少 4 %       Polyak   averaging ： B .   T .   Polyak   and   A .   B .   Juditsky .   Acceleration   of   stochastic   approximation   by   averaging .   SIAM   J .   Con -   trol   Optim . ,   30 ( 4 ) : 838 – 855 ,   July   1992 .       采样 不同 尺寸 不同 位置 的 patch       photometric   distortions ： A .   G .   Howard .   Some   improvements   on   deep   con -   volutional   neural   network   based   image   classification .   CoRR ,   abs / 1312.5402 ,   2013 .                   参赛 配置 ：           没有 额外 的 训练 数据       训练 了 7 个 不同 的 版本 ， 然后 做 融合 ： 相同 的 初始 权重 ， 学习 率 ， 只 在 采用 方法 和 图片 的 随机 顺序 不同       testing 阶段 每个 图片 采样 了 不同 尺寸 不同 位置 不同 镜像 的 多个 块 进行 预测 。                   ResNet       参考 WIKI   残差 网络         定位 与 检测           简单 回归 问题 ：       将 定位 作为 一个 回归 问题 ， 输出 定位 的 坐标 和 尺寸 4 个 数字 ， 用 L2 损失 函数 ， 简单 ！       直接 从 分类 模型 最后 一层 的 feature   map 引出 一个 回归 抽头 ！               滑动 窗 ：       在 高分辨率 图片 中 的 不同 尺寸 和 不同 位置 运行   分类 + 回归   网络       融合 所有 尺寸 的 分类 + 回归 结果 作为 最终 的 输出                   OverFeat       论文 ： OverFeat : Integrated   Recognition ,   Localization   and   Detection     using   Convolutional   Networks ， Pierre   Sermanet ,   David   Eigen ,     Xiang   Zhang ,   Michael   Mathieu ,   Rob   Fergus ,     Yann   LeCun   ， 2014 .           classification ,   localization   and   detection   的 CNN 集成 框架       multiscale   和   sliding   window   技术       ILSVRC2013   目标 识别 冠军               combining   many   localization   predictions ,   detection   can   be   performed   without   training   on   background   samples   and   that   it   is   possible   to   avoid   the   time - consuming   and   complicated   bootstrapping   training   passes               在 不同 位置 和 不同 scale 使用 CNN ： 大量 的 窗 只 包含 目标 的 一部分 ， 分类 效果 好 ， 但是 定位 和 检测 效果 不好 。       每 一个 window 不但 输出 不同 类别 的 预测 概率分布 ， 还 输出 目标 相对 window 的 位置 和 大小 ！           累积 每 一个 类别 的 每 一个 window 的 预测 结果 ！               文本 检测 ： M . DelakisandC . Garcia . Textdetectionwithconvolutionalneuralnetworks . InInternationalConference   on   Computer   Vision   Theory   and   Applications   ( VISAPP   2008 ) ,   2008 .           人脸识别 ：   C .   Garcia   and   M .   Delakis .   Convolutional   face   finder :   A   neural   architecture   for   fast   and   robust   face   detection .   IEEE   Transactions   on   Pattern   Analysis   and   Machine   Intelligence ,   2004 .       人脸 检测 ： M .   Osadchy ,   Y .   LeCun ,   and   M .   Miller .   Synergistic   face   detection   and   pose   estimation   with   energy - based   models .   Journal   of   Machine   Learning   Research ,   8 : 1197 – 1215 ,   May   2007 .           行人 检测 ： P .   Sermanet ,   K .   Kavukcuoglu ,   S .   Chintala ,   and   Y .   LeCun .   Pedestrian   detection   with   unsupervised   multi -   stage   feature   learning .   In   Proc .   International   Conference   on   Computer   Vision   and   Pattern   Recognition   ( CVPR ’ 13 ) .   IEEE ,   June   2013 .               预测 的 box 和 groundtruth 的 box 至少 相交 50 % （ IOU ） 才 认为 是 对 的 。                   IOU 的 定义 ： label 框为 A ， groundtruth 框为 B ， $ ( IOU   =   \ \ frac {   area ( A   \ \ bigcap   B ) } {   area ( A   \ \ bigcup   B ) }   ) $               通过 滑动 窗 ， 产生 多个 块 ， 得到 多个 块 预测 结果 ， 然后 平均 。 滑动 窗 可以 自 底向上 计算 ， 不用 每个 滑动 窗 计算 一个 结果 ， 减少 计算 量 ！       不同 尺寸 和 位置 检测 得到 的 box 融合 成 一个 高 可信 的 box ， 实现 定位 ！           目标 检测   HOG       论文 ： Histograms   of   Oriented   Gradients   for   Human   Detection ， Navneet   Dalal   and   Bill   Triggs ， 2005 .       RCNN       论文 ： Rich   feature   hierarchies   for   accurate   object   detection   and   semantic   segmentation               Region   proposals           objectness :   B .   Alexe ,   T .   Deselaers ,   and   V .   Ferrari .   Measuring   the   objectness   of   image   windows .   TPAMI ,   2012 .         selective   search   :   J . Uijlings , K . vandeSande , T . Gevers , andA . Smeulders . Selective   search   for   object   recognition .   IJCV ,   2013 .       category - independent   object   proposals :   I .   Endres   and   D .   Hoiem .   Category   independent   object   proposals .   In   ECCV ,   2010 .       constrained   parametric   min - cuts   ( CPMC ) :   J .   Carreira   and   C .   Sminchisescu .   CPMC :   Automatic   object   segmentation   using   constrained   parametric   min - cuts .   TPAMI ,   2012 .       multi - scale   combinatorial   grouping :   P . Arbelaez , J . Pont - Tuset , J . Barron , F . Marques , andJ . Malik . Multiscale   combinatorial   grouping .   In   CVPR ,   2014 .       CNN :   D . Cires   ̧ an , A . Giusti , L . Gambardella , andJ . Schmidhuber . Mitosis   detection   in   breast   cancer   histology   images   with   deep   neural   networks .   In   MICCAI ,   2013 .           Feature   extraction           利用 一个 训练 好 的 CNN 网络 （ 如   AlexNet   网络 ） ， 对 每个 区域 提取 特征 。       将 每个 区域 补全 和 变形 到 标准 的 输入 尺寸 ，   alex   net   要求 输入 时   227x227           Test - time   detection           利用 选择性 搜索 选出 近 2000 个 候选 区域       用   CNN   提取 每 一个 区域 的 特征向量 ， 对 每 一个 类别 ， 使用 对应 的   SVM   分类器 对 特征 打分           采用 贪心 的 非 最大值 抑制 方法 （ greedy   non - maximum   suppression ，   每 一个 类 是 独立 的 ） ： 如果 一个 区域 和 另 一个 得分 更 高 的 区域   IoU   重叠 度 高于 某个 阈值 ， 那么 就 拒绝 这个 得分 低 的 区域 。 阈值 是 学习 到 的 阈值 ？               性能 对比 （ 10K 个类 被 ） ： DPM + Hashing ， 5min / image ;   RCNN ,   1min / image .   T .   Dean ,   M .   A .   Ruzon ,   M .   Segal ,   J .   Shlens ,   S .   Vijayanarasimhan ,   and   J .   Yagnik .   Fast ,   accurate   detection   of   100 , 000   object   classes   on   a   single   machine .   In   CVPR ,   2013 .               Train           将 在 ImageNet 上 训练 好 的 CNN 最后 一层 替换成 多个 SVM （ 每 一个 类别 一个 ， 背景 一个 ， SVM 参数 随机 初始化 ） ， CNN 参数 也 通过 SGD 调优       将 于 ground - truth 重叠 度 IoU 超过 50 % 的 区域 作为 该类 的 正 样本 ， 其他 的 作为 负 样本       CNN 调优 的 学习 率 降低 10 倍       每 一个 SGD 的 minbatch 中 ， 均匀 采样 32 个 正例 和 96 个 负例       hard   negative   mining   method ： 将 分值 较 高 的 负例 放到 样本 中 重新 训练       pool5 的 特征 就 很 好 的 ， 全 连接 层 可以 不要 ！           bbox   regression ： 为 每个 区域 训练 一个 回归 模型 ， 用 相同 的 特征 ， 只 改变 最后 一层 ， 预测 目标 的 相对 偏移 。               最大 的 问题 ： 慢 ， 需要 对 每 一个 区域 用 CNN 提 特征 ！               Fast   R - CNN           先用 CNN 对 整个 图片 进行 特征 抽取 （ pool5 特征 ， 有 空间 维度 的 特征 ） ， 在 选取 的 RoI 区域 ， 用 一个 RoI   Pooling 层 将 特征 尺寸 变成 固定 的 空间 尺寸 HxW ， （ 空间 尺寸 固定 了 ， 整个 特征 的 尺寸 也 固定 了 ） ， 然后 为 每 一个 区域 中 建立 分类 和 回归 模型 。       RoI 还是 通过 预先 的   region   proposal   方法 得到 ， 这个 部分 是   Fast   R - CNN   计算 的 瓶颈 。       由于 ROI 将近 2000 个 ， 计算 最后 的 全 连接 层 是 计算 瓶颈 ， 可以 通过   Truncated   SVD   优化 ， 其 效果 相当于 用 两层 线性网络 替换 。       优点 ： 只 需要 计算 一次 CNN 即可 ！       优点 ： 将 回归 和 分类 损失 函数 加 到 一起 ， 优化 一个 目标 ， multi - task   loss ， 端到 端 学习 ！       相比   R - CNN ， 训练 时间 加速 8.8 倍 ， 预测 时间 加速 146 倍 ！ 每张 图片 的 预测 时间 降低 到 0.32 s ， 之前 在 分钟 量级 ！ 效果 也 稍 好 ； 但是 加上 区域 搜索 时间 （ 大约 2s ） 后 ， 只有 25 倍 速度 提升 ， 搜索 时间 是 瓶颈 ！       关键 层 ：   RoI   pooling   layer                   Faster   R - CNN       用 CNN 做   region   proposal ， 关键技术 ：   Region   Proposal   Networks       CNN 可视化           R - CNN   计算 所有 的 区域 对 某个 神经元 激活 值 ， 按照 激活 值 从 大到 小 排序 ， 选取 TOP 区域 可视化 。                           直接 可视化 权重 ： 只能 可视化 第一层               可视化 特征 表达 ， 例如 用   t - SNE   可视化   AlexNet   最后 一层 的 4096 维 特征                 http : / / cs . stanford . edu / people / karpathy / cnnembed /                 遮挡 试验 ： 分类 概率 与 遮挡 位置 的 函数 关系 ！ ZFNet               解 卷积 方法               BP   to   Image   方法 ：       可视化 某个 神经元 的 响应 对 输入 图片 的 梯度 （ BP   to   Image ） ， 将 该层 所有 神经元 的 梯度 置 0 ， 将要 可视化 的 那个 神经元 梯度 置 1 ！     然后 运用 BP 算法 ， 求 出 梯度 。       $ $     \ \ frac { \ \ Delta   active } { \ \ Delta   I }     $ $       由于 高级 特征 具有 不变性 ， 不是 针对 某 一个 图片 的 ， 直接 解 卷积 可视化 得到 的 效果 不好 。     可以 对于 特定 的 图片 ， 用 这个 图片 做 引导 ， 通过   guided   bp   得到 条件 梯度 。                     ZF   解 卷积 方法           Deep   Inside   Convolutional   Networks :   Visualising   Image   Classification   Models   and   Saliency   Maps           寻找 图像 I 使得 在 类 c 上 的 score   $ ( S _ c ( I ) ) $   最大 ！       $ $     \ \ arg   \ \ max _ I   S _ c ( I )   -   \ \ lambda   | | I | |   _   2 ^ 2     $ $       利用   BP   算法 优化 ， 固定 权重 ， 优化 输入 ！ 输入 初始化 为 0 值 图片 ！   Sc   是 未 归一化 的 score ， 优化 归一化 的 score （ 即 概率 ） 效果 反而 不 明显 。       给定 图像 $ ( I _ 0 ) $ ， 根据 输入 像素 对 某个 类 的 score 影响 效果 排序 ， 影响 效果 通过 梯度 刻画       $ $     w   =   \ \ frac { \ \ partial   S _ c } { \ \ partial   I }   |   _   { I _ 0 }     $ $           给定 一个 图片 的 code ， 寻找 最 接近 这个 code 的 图片           $ $     x *     =   \ \ arg   \ \ min   _   x   l ( \ \ Phi ( x )   -   \ \ Phi ( x _ 0 ) )   +   \ \ lambda   R ( x )     $ $           DeepDream ： 从 一个 初始 图片 开始 ， 每次 梯度 沿着 正反馈 方向 下降   dx   =   x ! ! 这里 x 是 神经网络 某 一层 的 响应值 。 即 目标 函数 是 ， 使得 某个 已经 训练 好 的 模型 的 某 一层 ， 激活 函数 的 幅度 最大化 !     其 结果 是 ， 如果 最大化 的 是 前面 的 层 ， 那么 图片 中 会 显示 出 一些 低级 纹理 ， 如果 是 后面 的 层 ， 那么 图片 中 会 显示 出 一些 学到 的 高级 目标 ， 如狗 、 猫 的 一些 局部 ！      ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn.html"},
      
      
      {"title": "Convolutional Neural Networks for NLP", "text": "    Table   of   Contents           关于           字母 级别 的 CNN           模型           数据           设计 细节           结论                   Very   Deep   Convolutional   Networks   for   Natural   Language   Processing           结论                   CNN   句子 建模           CNN 句子 分类                 关于       长期以来 ， RNN 、 LSTM 及其 变种 模型 被 应用 到 自然语言 处理 方面 。     近年来 ， 将 CNN 应用 到 自然 处理 方面 也 有 一些 工作 。       字母 级别 的 CNN       主要 论文 ： Character - level   Convolutional   Networks   for   Text   Classification ,     Xiang   Zhang   ,   Junbo   Zhao ,     Yann   LeCun   ,   2016       目前 文本 分类 研究 已 从 设计 好 的 特征 转为 选择 好 的 分类 模型 。     目前 ， 所有 的 文本 分类 技术 都 是 以 词为 基本 单位 的 ， 简单 统计 词和词 的 n - gram 就 可以 做到 最好 的 效果 。       T .   Joachims .   Text   categorization   with   suport   vector   machines :   Learning   with   many   relevant   features .   In     Proceedings   of   the   10th   European   Conference   on   Machine   Learning ,   pages   137 – 142 .   Springer - Verlag ,     1998 .       卷积 网络 很 适合 从 raw   signals 中 提取 有用 的 特征 ， 已 在 机器 视觉 和 语音 识别 等 任务 中 得到 应用 。     而 实际上 ， time - delay   networks   早 在 深度 学习 出来 以前 就 将 卷积 网络应用 到 序列 数据 之上 。           L .   Bottou ,   F .   Fogelman   Soulie ,   P .   Blanchet ,   and   J .   Lienard .   Experiments   with   time   delay   networks   and   ´     dynamic   time   warping   for   speaker   independent   isolated   digit   recognition .   In   Proceedings   of   EuroSpeech     89 ,   volume   2 ,   pages   537 – 540 ,   Paris ,   France ,   1989 .       R .   Johnson   and     T .   Zhang   .   Effective   use   of   word   order   for   text   categorization   with   convolutional   neural     networks .   CoRR ,   abs / 1412.1058 ,   2014 .           在 这 篇文章 中 ， 将 文本 当做 字符 为 单位 的 序列 数据 ， 然后 应用 时间 卷积 网络 ( temporal   ( one - dimensional )   ConvNets ) 。       卷积 网络应用 到 文本 和 自然语言 处理 已有 一些 研究 了 ， 它 既 可以 应用 到 连续 值 的 embedding 数据 ， 也 可以 应用 到 离散 值 的 embedding 数据 ，     并不需要 任何 语法 和 语义 信息 ！ 其 结果 也 和 经典 的 方法 具有 可比性 ！           C .   dos   Santos   and   M .   Gatti .   Deep   convolutional   neural   networks   for   sentiment   analysis   of   short   texts .   In     Proceedings   of   COLING   2014 ,   the   25th   International   Conference   on   Computational   Linguistics :   Technical     Papers ,   pages   69 – 78 ,   Dublin ,   Ireland ,   August   2014 .   Dublin   City   University   and   Association   for     Computational   Linguistics .       Y .   Kim .   Convolutional   neural   networks   for   sentence   classification .   In   Proceedings   of   the   2014   Conference     on   Empirical   Methods   in   Natural   Language   Processing   ( EMNLP ) ,   pages   1746 – 1751 ,   Doha ,   Qatar ,     October   2014 .   Association   for   Computational   Linguistics .       R .   Johnson   and   T .   Zhang .   Effective   use   of   word   order   for   text   categorization   with   convolutional   neural     networks .   CoRR ,   abs / 1412.1058 ,   2014           使用 字母 级别 特征 来 做 NLP 也 有 一些 早起 工作 ， 能够 在 POS   tagging 和 IR 方面 的 提升 。 This   article   is   the   first   to   apply   ConvNets   only   on   characters .     可以 简化 特征 工程 ， 能够 学到 拼写错误 和 emoji 符号 。           character - level   n - grams   with   linear   classifiers :   I .   Kanaris ,   K .   Kanaris ,   I .   Houvardas ,   and   E .   Stamatatos .   Words   versus   character   n - grams   for   anti - spam   filtering .   International   Journal   on   Artificial   Intelligence   Tools ,   16 ( 06 ) : 1047 – 1067 ,   2007       incorporating   character - level   features   to   ConvNets :   C .   D .   Santos   and   B .   Zadrozny .   Learning   character - level   representations   for   part - of - speech   tagging .   In   Proceedings   of   the   31st   International   Conference   on   Machine   Learning   ( ICML - 14 ) ,   pages   1818 – 1826 ,   2014       Y .   Shen ,   X .   He ,   J .   Gao ,   L .   Deng ,   and   G .   Mesnil .   A   latent   semantic   model   with   convolutional - pooling   structure   for   information   retrieval .   In   Proceedings   of   the   23rd   ACM   International   Conference   on   Conference   on   Information   and   Knowledge   Management ,   pages   101 – 110 .   ACM ,   2014 .           模型       采用 一位 的 卷积 和 max - pooling ！           对   Pooling   的 分析 文章 ： Y . - L .   Boureau ,   J .   Ponce ,   and     Y .   LeCun   .   A   theoretical   analysis   of   feature   pooling   in   visual   recognition .     In   Proceedings   of   the   27th   International   Conference   on   Machine   Learning   ( ICML - 10 ) ,   pages   111 – 118 ,   2010       ReLU   最早 文章 ： V .   Nair   and     G .   E .   Hinton   .   Rectified   linear   units   improve   restricted   boltzmann   machines .   In   Proceedings     of   the   27th   International   Conference   on   Machine   Learning   ( ICML - 10 ) ,   pages   807 – 814 ,   2010           数据       字母 通过 one - hot 编码 为 70 维 向量 ， 包括 26 个 字母 ， 10 个 数字 ， 33 个 其他 字符 和 换行符 。 字符 包括 ：             abcdefghijklmnopqrstuvwxyz0123456789   - , ; . ! ? : ’ ’ ’ / \ \ | _ @ # $ % ˆ & amp ; * ˜ ‘ + - = & lt ; & gt ; ( ) [ ] { }               对 中文 的 处理 ， 将 中文 转换 为 拼音   pypingyin 。       设计 细节       模型 设计       6 层 卷积 层 + 3 层全 连接 层 ， kernel 维度 为 7 和 3 ， pool 维度 为 3 。       数据 增强       用 近义词 进行 替换 ， 增加 样本 ！ 平移 不 适应 于 这里 ！       和 传统 方法 比较 ：           Bag   of   word   with   TFIDF       Bag   of   n - gram   with   TFIDF       Bag   of   means   with   word   embedding           深度 学习 方法 比较 ：           word - based   CNN :       LSTM :           结论       数据 集 达到 百万 量级 才能 观察 到 这种 方法 的 优势 ， 数据 集小 的 时候 还是   n - gram   with   TFIDF   好             There   is   no   free   lunch   .   Our   experiments   once   again   verifies   that   there   is   not   a   single   machine     learning   model   that   can   work   for   all   kinds   of   datasets .   The   factors   discussed   in   this   section   could   all     play   a   role   in   deciding   which   method   is   the   best   for   some   specific   application .           Very   Deep   Convolutional   Networks   for   Natural   Language   Processing       论文 ：   Very   Deep   Convolutional   Networks   for   Natural   Language   Processing ,     Le   Cun   ,   2016       目前 RNN , LSTM , CNN 应用 到 NLP 中 的 深度 和 在 CV 中 相比 ， 还 比较 浅 ， 这 篇文章 提出 一种 方案 可以 从 字母 级别 开始 学习 ， 模型 深度 达到 29 层 ！       CNN ： 将 特征提取 和 分类 进行 联合 训练 ！ 除了 自动 特征提取 之外 ， 还 可以 根据 具体任务 调整 特征 ！       目前 主流 的 方法 ， 是 利用   word   embedding   +   RNN ( LSTM ) 。           Martin   Sundermeyer ,   Ralf   Schl ü ter ,   and   Hermann   Ney .   LSTM   neural   networks   for   language     modeling .   In   Interspeech ,   2012 .       Ilya   Sutskever ,   Oriol   Vinyals ,   and   Quoc   V .   Le .   Sequence   to   sequence   learning   with   neural     networks .   In   NIPS ,   pages   3104 – 3112 ,   2014 .           作者 argue ：           作者 认为   LSTM   是 一种 一般 的 序列 学习 方法 ， 缺乏 领域 特性   \" lacking   task   specific   structure \"       单词 按照 顺序 进入 ， 第一个 单词 变换 了 很 多次 ， 而 最后 一个 词 只 变换 一次 ！   = & gt ;   bidirectional   LSTM       深度 不够 ， 超过 4 层 就 没 啥 提升 了 ， 尽管 加入 了   dropout   正则 化 ！           观点 ：           We   believe   that     the   challenge   in   NLP     is   to   develop   deep   architectures   which   are   able   to   learn   hierarchical     representations   of   whole   sentences ,   jointly   with   the   task .           recursive   neural   network   :   在 RNN 上 增加 了 序列 融合 的 顺序 结构 （ 树结构 ） ， RNN 可以 看做 一个 特殊 的   recursive   NN .           Richard   Socher ,   Jeffrey   Pennington ,   Eric   H .   Huang ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .     Semi - supervised   recursive   autoencoders   for   predicting   sentiment   distributions .   In     EMNLP ,   2011 .           模型 结构 ：               s   是 时间 窗 长度 ， 首先 将 字符 embedding 到 16 维 的 向量 ！     第一层 64 个 特征 ， 后续 是 ConvNet   Block ， 采用 下述 策略 （ from   VGG   and   ResNet ） ：           如果 时间 分辨率 不变 ， 输入 和 输出 特征 维度 相同       如果 时间 分辨率 减半 ， 输出 特征 维度 加倍           更 多 的 卷积 层 ， 意味着 能够 学习 更长 的 依赖 关系 ！ 并且 ， 对 所有 的 时间 几乎 是 平等 的 ！ 而 不 像 RNN ， LSTM 那样 ！     其中 一个 CNN   Block   结构 如图 Fig . 2 。 包含 两个 维度 为 3 的 核 的 卷积 层 ， 每个 卷积 层后 跟 一个 BN 层 和 一个 非线性 层 ！     多个 小 尺寸 的 卷积 层 可以 用 较 少 的 参数 实现 一个 大 尺寸 的 卷积 层 相同 的 功能 （ 视野 和 非线性 度 ） ！               输入 字符 增加 了 一个 表示 未知 符号 的 特殊字符 ， 一共 72 个 token 。 输入 文本 padding 到 长度 为 1014 ！ 字符 embedding 到 16 维 的 向量 。     其他 参数 ：           mini - batch   of   size   128       initial   learning   rate   of   0.01       momentum   of   0.9       每次 验证 集 错误 增加 就 将 学习 率 减半       初始化 采用   何凯明   的 方案       采用   BN   而 没有 dropout           结论           在 大 数据 集上 有 明显 提升 ， 即使 深度 较 小       深度 可以 提升 效果 ！       Max - pooling   最优       degradation ： 增加 深度 ， 性能 下降 ， 通过 shortcut 减少 这种 效果 。               Exploring   the   impact   of   the   depth   of   temporal     convolutional   models   on   categorization   tasks   with   hundreds   or   thousands   of   classes   would   be   an     interesting   challenge   and   is   left   for   future   research .           CNN   句子 建模       A   Convolutional   Neural   Network   for   Modelling   Sentences ， 2014 .       Dynamic   Convolutional   Neural   Network   ( DCNN ) :   采用 Dynamic   k - Max   Pooling ， 即 pooling 的 时候 ， 选取 最大 的 k 个值 ， 而 不是 一个 最大值 。     采用 多个 滤波器 ， 提取 多个 特征 。           Neural   Bag - of - Words   ( NBoW )   models ：       投影 层 ： 将   word ,   sub - word ,   n - gram   映射 到 高维   embedding   向量 。       组合 ： 将 这些 向量 组合 （ 求和 ， 均值 ， 加权 和 等 ）       将 组合 后 的 向量 作为 句子 的 特征 表达 ， 传入 全 连接 神经网络 进行 监督 学习 。               Recursive   Neural   Network   ( RecNN ） ： 利用 一个 额外 的   parse   tree 。       递归 地 组合 叶子 节点       将 根 节点 的 向量 作为 句子 的 特征 表达 ， 传入 全 连接 神经网络 进行 监督 学习 。               RNN ： 最为 RecNN   的 一个 特例 ， 即 线性 地 组合 相邻 的 向量 ， 把 最后 节点 对应 的 向量 作为 句子 的 特征向量 。       TDNN ： Time   delay ， 利用 卷积 。           一维 卷积 ， 即 只 对 时间 维度 进行 卷积 。   TDNN 对 时间 维度 是 卷积 ， 对 另 一个 维度 （ 每个 词 都 对应 一个 向量 ） 则 是 全 连接 ， 并且 采用 窄 版 的 卷积 。     Max - TDNN 解决 可变 长度 问题 ： 每 一个 滤波器 ， 最终 只 得到 一个 特征 ， 即 对 卷积 后 的 序列 只取 一个 最大值 。 最终 有 几个 卷积 核 ， 特征 的 维度 就是 几 。     最后 得到 的 特征 传入 全 连接 神经网络 进行 监督 学习 。 整个 过程 是 联合 优化 的 。           特点 ： 对 词序 不 敏感 ； 不 需要 额外 的 语言 特征 （ dependency   tree ， parse   tree ) 。       缺点 ： 首先 于 卷积 的 长度 ， 相当于 只 考虑   m - gram   的 词 特征 ， 长距离 的 词 关联 无法 学到 。 一个   max - pooling   导致 的 问题 ： 抹 去 了 顺序 的 信息 。           解决 的 办法 ： Dynamic   k - max   pooling       最后 一层 的 k 是 固定 的 ， 保证 最后 一层 的 输出 特征 数目 是 定长 的 ， 但是 中间 的 k 是 动态 的 。       $ $     k _ l   =   \ \ max   ( k _ { top } ,   \ \ ceil { \ \ frac { L   -   l } { L }   s } )     $ $       L   是 所有 的 卷积 层 数目 ， l 是 当前 层 的 序号 ， s 是 句子 长度 ， 实际上 是 为了 使得 pooling 是 平滑 的 线性 降低 维度 。       CNN 句子 分类       Convolutional   Neural   Networks   for   Sentence   Classification ， Kim   2014 .           要点 ：       双通道 ， 一个 词 向量 通道 可变 ， 用于 学习 与 目标 有关 的 词 向量 （ 情感 相关 ） ， 另 一个 通道 不可 变 ， 防止 过 拟合 。     这个 很 关键 ， 可以 参看 论文 ， 解决   word2vec   反义词 的 问题 。       时间 卷积 ， 在 另 一个 维度 求和       max - over - time   pooling       dropout              ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn-for-nlp.html"},
      
      
      {"title": "CS224d: Deep Learning for Natural Language Processing", "text": "    Table   of   Contents           关于           一些 疑惑           word2vec           负 采样 近似           skip - gram 模型           CBOW 模型                   问题           Multitask   learning           神经网络 TIPS           Language   Models           神经网络 语言 模型   Bengio   2003           递归 神经网络           实现 细节           bidirectionl   RNNS   双向 RNN                   Deep - learning   package   zoom           RNN   机器翻译           传统 统计 机器翻译 ：           RNN   模型           GRU           LSTM           更 多 的 门   GRUs                   Project           NLP   benchmark   tasks           tasks           models                   Reference           NLP   综述           POS   tagging ： 数据 集 ： Wall   Street   Journal   ( WSJ )   data           Chunking           Named   Entity   Recognition           Semantic   Role   Labeling           神经网络 方法           Transforming   Words   into   Feature   Vectors           Extracting   Higher   Level   Features   from   Word   Feature   Vectors                   训练           Word - Level   Log - Likelihood           Sentence - Level   Log - Likelihood           Stochastic   Gradient                   结果           更 多 的 未 标注 数据           Training   Language   Models                   Sequence   to   Sequence   Learning   with   Neural   Networks           模型           实验                   RNN   for   QA           Matching   Text   to   Entities :   Quiz   Bowl                   Image   retrieval   by   Sentences           Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions           RNN 语言 模型           优化 技巧                   A   Neural   Probabilistic   Language   Model           EXTENSIONS   OF   RECURRENT   NEURAL   NETWORK   LANGUAGE   MODEL           Opinion   Mining   with   Deep   Recurrent   Neural   Networks           Gated   Feedback   Recurrent   Neural   Networks           Recursive   Deep   Models   for   Semantic   Compositionality   Over   a   Sentiment   Treebank                         关于       cs224d 这门 课 是 将 深度 学习 应用 到 自然语言 处理 上面 的 课程 ， 十分 推荐 。       一些 疑惑         https : / / www . quora . com / How - is - GloVe - different - from - word2vec       对于 word2vec 与 GloVe 的 比较 的 见解 。       word2vec           单词 的 表达 ：   Word - Net ，   ONE - HOT       文档 - 单词   共生 矩阵 ，   SVD 提取 ，   LSA       潜在 问题 ： SVD 计算 复杂度 高当 词典 或者 文档 数目 很大 时 ， 对 新词 和 新 的 文档 难以 处理 ， 与 其他 DL 不同 的 学习 体制 。               直接 学习 低维词 向量 ： word2vect       Learning     representa4ons     by     back - propaga4ng   errors .     Rumelhart     et     al . ,         1986       A   neural     probabilis4c         language         model       ( Bengio   et     al . ,         2003 )       NLP       ( almost )         from         Scratch   ( Collobert     & amp ;       Weston ,   2008 )       A   recent ,   even         simpler   and   faster     model :     word2vec         ( Mikolov         et     al .   2013 )       à       intro       now               不是 直接 统计 共同 发生 的 次数 ， 而是 预测 每 一个 单词 周围 的 单词 ； 速度 快 ， 易于 应用 到 新词 和 新 文档       目标 函数     $ $     J ( \ \ theta )   =   \ \ frac { 1 } { T }   \ \ sum _ { t = 1 } ^ T     \ \ sum _ { - m   \ \ le   j   \ \ le   m ,   j   \ \ neq   0 }   \ \ log   p ( w _ { t + j }   |   w _ t )     $ $     其中 条件 概率 采用 如下 指数 形式     $ $     p ( o | c )   =   \ \ frac { \ \ exp ( u _ o ^ T   v _ c ) } { \ \ sum _ { w = 1 } ^ W   \ \ exp ( u _ w ^ T   v _ c ) }     $ $       每 一个 单词 有 两个 向量 $ ( u ,   v ) $ .   最终 的 词 向量 是   $ ( u + v ) $ ?       词 向量 的 线性关系       $ (   X _ { apple }   -   X _ { apples }   \ \ approx   X _ { car }   -   X _ { cars }   \ \ approx   X _ { family }   -   X _ { families } ) $                   负 采样 近似       单个 输入 词 向量 与 单个 输出 词 向量 的 损失 函数     $ $     J ( u _ o ,   v _ c ,   U )   =   -   \ \ log ( \ \ sigma ( u _ o ^ T   v _ c ) )   -   \ \ sum _ { k   \ \ sim   P }   \ \ log ( \ \ sigma ( -   u _ k ^ T   v _ c ) ) .     $ $     其中 求和 是 对 总体 的 一个 采样 。 实际上 ， 负 采样 方法 相当于 将 极大 似然 估计 的 问题 ， 转化 为 多个 二 分类 问题 。     正例 是 两个 词 出现 在 同一个 上下文 ， 负例 是 两个 词 没有 出现 在 同一个 上下文 ！     且 正例 的 概率 为 ：       $ $     \ \ sigma ( u _ o ^ T   v _ c )     $ $       skip - gram 模型       设由 $ ( w _ c ) $ 预测 $ ( w _ o ) $ 的 单个 损失 函数 为 $ ( F ( w _ o ,   w _ c ) ) $ ， 那么 skip - gram 模型 可以 表示 为     由 中心 单词 预测 周围 的 单词 ， 损失 函数 为     $ $     J   =   \ \ sum _ { - m   \ \ le   j   \ \ le ,   j   \ \ neq   0 }   F ( w _ { c + j } ,   v _ c ) .     $ $       CBOW 模型       CBOW 模型 使用 周围 单词 的 词 向量 之 和 来 预测 中心 单词 $ w _ c $ 。     $ $     \ \ hat { v }   =   \ \ sum _ { - m   \ \ le   j   \ \ le ,   j   \ \ neq   0 }   v _ { c + j }     $ $     他 的 损失 函数 为     $ $     J   =   F ( w _ c ,   \ \ hat { v } )     $ $           WHY ？       一般而言 ， 这种 方式 上 的 区别 使得 CBOW 模型 更 适合 应用 在 小规模 的 数据 集上 ， 能够 对 很多 的 分布式 信息 进行 平滑 处理 ； 而 Skip - Gram 模型 则 比较 适合 用于 大规模 的 数据 集上 。               问题           为什么 每 一次 SGD 后 需要 对 参数 向量 进行 标准化 ？       一般 的 交叉 熵 能够 理解 为 最大 似然 估计 么 ？           Multitask   learning       共享 网络 前 几层 的 权值 ， 只 针对 不同 任务 改变 最后 一层 的 权值 。     总 的 代价 函数 是 各 代价 函数 （ 如 交叉 熵 ） 之 和 。       神经网络 TIPS           对词 向量 的 监督 训练 的 重新 调整 ， 对 任务 也 有 提升 。   C & amp ; W   2011           非线性 函数           sigmoid       tanh   ：   对 很多 任务 ， 比 sigmoid 好 ， 初始值 接近 0 ， 更快 的 收敛 ， 与 sigmoid 一样 容易 求导       hard   tanh   :   - 1 ,   if   & lt ;   - 1 ;   x ,   if   - 1   & lt ; =   x   & lt ; =   1 ;   1 ,   if   x   & gt ;   1 .       softsign ( z )   =   z / ( 1   +   | z | )       rect ( z )   =   max ( 0 ,   z )     ref :   Glorot   and   Bengio ,   AISTATS   2011                   MaxOut   network   ( Goodfellow   et   al .   2013 )           梯度 下降 优化 建议 ， 大 数据 集 采用 SGD 和 mini - batch   SGD ， 小 数据 集 采用 L - BFGS 或者 CG 。         大 数据 集 L - BFGS   Le   et     al .   ICML         2011 。       SGD 的 提升 ， 动量     $ $     v   =   \ \ mu   v   -   \ \ alpha   \ \ nabla _ { \ \ theta }   J _ t ( \ \ theta )       \ \ \ \     \ \ theta ^ { new }   =   \ \ theta ^ { old }   +   v     $ $       学习 率 ： adagrad ，   adam       防止 过 拟合 ：       减少 模型 大小 ， 隐藏 节点 数目 等       L1   or   L2 正则 化       提前 停止 ， 选择 在 验证 集合 上 最好 的 结果       隐藏 节点 的 稀疏 约束 ， 参考 UFLDL 教程     $ $     KL ( 1 / N   \ \ sum _ { n = 1 } ^ N   a _ i ^ { ( n ) | 0.001 } )     $ $       dropout ， 输入 以 一定 概率 随机 置 0       denoise               超 参数 的 搜索 ： 随即 搜索 。     Y .     Bengio     ( 2012 ) ,   “ Practical     Recommendations   for   GradientBased     Training         of     Deep         Architectures ”                   Xavier   initialization   初始化 策略           Language   Models       所谓 语言 模型 就是 建立 单词 的 联合 概率模型 $ ( P ( w _ 1 , ... , w _ T ) ) $ .       神经网络 语言 模型   Bengio   2003       一个 直接 连接 部分 和 一个 非线性 变换 部分 。 输入 为 前 n 个 词 的 词 向量     $ $     y   =   b   +   Wx   +   U   tanh ( d   +   Hx )   .     \ \ \ \     P ( w _ t | w _ { t - 1 } , ... , w _ { t - n + 1 } )   =   \ \ frac { e ^ { y _ { w _ t } } } { \ \ sum _ i   e ^ { y _ i } } .     $ $     缺点 是 窗口 是 固定 的 。 记忆 能力 有限 ？       递归 神经网络       基于 之前 见到 的 所有 单词 （ 理论 上 有 无限 长 的 时间 窗 ）           Condition   the   neural     network   on     all   previous     words   and   tie   the   weights   at     each         time         step           设词 向量 列表 为   $ ( x _ 1 ,   x _ 2 ,   ... ,   x _ t ,   ... ,   x _ T ) $ 。 L 矩阵 中 的 列 向量 。     $ $     h _ t   =   \ \ sigma ( W ^ { ( hh )   h _ { t - 1 } }   +   W ^ { hx }   x _ { t } ) .   \ \ \ \     \ \ hat { y } _ t   =   softmax ( W ^ { ( S ) }   h _ t ) .   \ \ \ \     P ( x _ { t + 1 } = v _ j | x _ t ,   ... ,   x _ 1 )   =   \ \ hat { y } _ { t ,   j } .     $ $     所有 时刻 的 权值 都 是 相同 的 。 损失 函数 为 所有 时刻 交叉 熵 的 平均值     $ $     J ^ { ( t ) } ( \ \ theta )   =   - \ \ sum _ { j = 1 } ^ { | V | }   y _ { t , j }   \ \ log   \ \ hat { y } _ { t , j } .   \ \ \ \     J   =   -   \ \ frac { 1 } { T }   \ \ sum _ t   J ^ { ( t ) }     $ $     Perplexity   ? ? ?           训练 困难 ， 梯度 容易 衰减 或者 很大 。 Bengio   et   al     1994           初始化 策略           $ ( W ^ { ( hh ) } ) $   初始化 为 单位 阵       非线性 函数 用 rect 函数 替换       Parsing       with         Compositional     Vector         Grammars ,       Socher     et     al .   2013       A   Simple     Way   to     Initialize     Recurrent       Networks         of     Rectified       Linear     Units ,         Le     et     al .   2015       On         the   difficulty       of   training         Recurrent       Neural     Networks ,       Pascanu   et     al .   2013                               梯度 消减   Mikolov ， 如果 梯度 的 范数 超过 阈值 ， 就 将 梯度 归一化 到 范数 等于 该 阈值 的 向量 或 矩阵 。               补充 对 RNN 求 梯度 的 理论 推导               利用 RNN 学到 的 语言 模型 ， 生成 词               实现 细节           dropout 正则 化 ， 在 TensorFlow 里面 ， 可以 使用   tf . nn . dropout   来 实现 。           bidirectionl   RNNS   双向 RNN       每 一个 隐层 存在 两个 变量 ， $ ( h ^ L ,   h ^ R ) $ 。       $ $     h _ t ^ R   =   f ( W ^ R   x _ t   +   V ^ R   h _ { t - 1 } ^ R   +   b ^ R )   \ \ \ \     h _ t ^ L   =   f ( W ^ L   x _ t   +   V ^ L   h _ { t + 1 } ^ L   +   b ^ L )   \ \ \ \     y _ t   =   g ( U   [ h _ t ^ R ;   h _ t ^ L ]   +   c )     $ $       问题 ： $ ( h _ { t + 1 } ^ L ) $ 的 值 怎么 来 ？       数据 集   MPQA         1.2   corpus       Deep - learning   package   zoom           Torch       Caffe       Theano ( Keras ,   Lasagne )       CuDNN       Tensorflow       Mxnet           RNN   机器翻译       传统 统计 机器翻译 ：       参考 CS224n           翻译 模型   p ( f | e )   和   语言 模型   f ( e ) ， 然后 得到 目标 语   Decoder   :   $ ( argmax _ e   p ( f | e )   p ( e ) ) $       alignment           RNN   模型               最 简单 的 encoder   +   decoder   模型 ：           Encoder ： 利用 RNN 将 句子 变成 一个 向量     $ ( h _ t   =   f ( W ^ { hh }   h _ { t - 1 }   +   W ^ { hx }   x _ t ) ) $       Decoder ： 将 句子 向量 变成 一句 话     $ ( h _ t   =   W ^ { hh }   h _ { t - 1 } ,   y _ t   =   softmax ( W ^ S   h _ t ) ) $       损失 函数 ： 最小化 所有 输出 结果 的 交叉 熵 ，   $ ( \ \ max _ { \ \ theta }   \ \ frac { 1 } { N }   \ \ sum _ { n = 1 } ^ N   \ \ log   p _ { \ \ theta }   ( y ^ { ( n ) } |   x ^ { n } ) ) $     用 句子 向量 作为 中间 桥梁 。                   对 上述 模型 的 改进 措施           对 Encoder 和 Decoder 训练 不同 的 权值       对 Decoder ， 采用 三个 变量 计算 隐层 ， 上 一个 时间 的 隐层 $ ( h _ { t - 1 } ) $ ， Encoder 最后 的 状态 c ， 上 一个 输出 结果 $ ( y _ { t - 1 } ) $ ： $ ( h _ t   =   \ \ phi ( h _ { t - 1 } ,   c ,   y _ { t - 1 } ) ) $       多层 网络 训练       双向 RNN   Encoder       反过来 训练 ？       门限 递归 单元 GRU                   论文 ： 2014 年 ， Kyunghyun   Cho ,   Yoshua   Bengio ,   Learning   Phrase   Representations   using   RNN   Encoder - Decoder   for   Statistical   Machine   Translation       GRU       门限 RNN 单元           更新 门 $ ( z _ t ) $ ， 基于 当前 输入 和 上 一 时刻 隐层 状态       重置 门 $ ( r _ t ) $ ， 如果 重置 门 接近 于 0 ， 那么 当前 隐层 状态 将 忘记 之前 的 隐层 状态 ， 只 依赖 当前 输入       新 的 隐层 $ ( \ \ widetilde { h } _ t ) $ ， 另外 一个 的 隐层 状态 ， 最终 的 隐层 状态 是 基于 更新 门 组合 这个 新隐层 和 上 一 时刻 的 隐层           $ $     z _ t   =   \ \ sigma ( W ^ { ( z ) }   x _ t   +   U ^ { ( z ) }   h _ { t - 1 } )     \ \ \ \     r _ t   =   \ \ sigma ( W ^ { ( r ) }   x _ t   +   U ^ { ( r ) }   h _ { t - 1 } )     \ \ \ \     \ \ widetilde { h } _ t   =   tanh ( W   x _ t   +   r _ t   \ \ circ   U   h _ { t - 1 } )         \ \ \ \     h _ t   =   z _ t   \ \ circ   h _ { t - 1 }   +   ( 1   -   z _ t )   \ \ circ   \ \ widetilde { h } _ t     $ $       当 重置 门 接近 0 ， 允许 模型 忘记 历史 ， 实现 短期 依赖 。     当 更新 门 接近 1 ， 简单 复制 上 一 时刻 的 隐层 ， 导致 更少 的 vanishing   gradients ， 实现 长期 依赖 。       LSTM       跟 多 的 门 ， 每 一个 门 都 是 当前 输入 和 上 一 时刻 的 隐层 的 函数 ， 只是 权值 不同 。           输入 门   $ ( i _ t ) $       忘记 门   $ ( f _ t ) $           输出 门   $ ( o _ t ) $               新 的 存储单元 ： $ ( \ \ widetilde { c } _ t   =   tanh ( w ^ c   x _ t   +   U ^ c   h _ { t - 1 } ) ) $           最终 的 存储单元 ： $ ( c _ t   =   f _ t   \ \ circ   c _ { t - 1 }   +   i _ t   \ \ circ   \ \ widetilde { c } _ t ) $       新 的 隐层 ： $ ( h _ t   =   tanh ( c _ t ) ) $           存储单元 可以 保存 输入 信息 ， 除非 输入 让 它 忘记 或者 重写 它 ； 它 可以 决定 是否 输出 信息 或者 只是 简单 地 保存信息 。       论文 ： 2014 ,   Sutskever ,   Sequence   to   Sequence   Learning   with   Neural   Networks ,   Google   inc       比赛 ： WMT     2016   competition       更 多 的 门   GRUs       Gated   Feedback   Recurrent   Neural   Networks ,   Chung   et     al ,   Bengio .   2015       更 多 的 门 来 控制 多个 隐层 之间 互相 连接 。       Project           利用 deeplearning 去 解决 kaggle 上 的 NLP 问题 。           NLP   benchmark   tasks       tasks           Part - Of - Speech   tagging       chunking       Named   Entity   Recognition   ( NER )       Semantic   Role   Labeling   ( SRL )           models               CRF   conditional   random   field               词语 分布式 假说 ： 词 的 上下文 相似 ， 那么 这 两个 词 也 相似               Reference             http : / / cs224d . stanford . edu / syllabus . html         An       Improved         Model       of     Seman4c   Similarity     Based       on     Lexical   Co - Occurrence   Rohde   et     al .   2005           NLP   综述       论文 ：   Natural   Language   Processing   ( almost )   from   Scratch       文章 提出 一种 统一 的 神经网络 结构 ， 可以 用 在 很多 自然语言 处理 任务 当中 ： POS   tagging ， chunking ， NER ， semantic   role   labeling 。     这种 方案 可以 不用 针对 特定 任务 进行 特征 工程 和 先验 知识 。       POS   tagging ： 数据 集 ： Wall   Street   Journal   ( WSJ )   data           Toutanova   et   al .   ( 2003 ）   最大 熵   +   bidirectional   dependency   network   = & gt ;   97.24 %       Gim   ́ enez   and   Marquez   ( 2004 )   SVM   +   双向 维特 比 译码   = & gt ;   97.16 %       Shen   et   al .   ( 2007 )   双向 序列 分类   = & gt ;   97.33 %           Chunking       句法 成分 标记           Kudoh   and   Matsumoto   ( 2000 ) ：   93.48 %       ( Sha   and   Pereira ,   2003 ;   McDonald   et   al . ,   2005 ;   Sun   et   al . ,   2008 ) ： random   fields           Named   Entity   Recognition           Florian   et   al .   ( 2003 ) ：   F1   = & gt ;   88.76 %       Ando   and   Zhang   ( 2005 ) :   89.31 %           Semantic   Role   Labeling       [ John ] ARG0   [ ate ] REL   [ the   apple ] ARG1       预测 关系 词 ？       神经网络 方法           将 词 映射 到 特征向量       在 一个 窗口 中 ， 将 词 对应 的 特征向量 拼接 成 一个 大 的 向量 ， 作为 下 一层 的 输入       一个 正常 的 神经网络 ， 线性 层 + 非线性 层   的 多次 堆叠 ！           Transforming   Words   into   Feature   Vectors       将 词   embedding   到 一个 低维 的 词 向量 ， 也 可以 理解 为 一个 查找 表   W ， 输入 词 的 索引   idx ， 输出 W ( : ,   idx ) ，     而 参数   W   通过 BP 算法 学习 ！       如果 加入 其它 离散 特征 ， 把 每 一个 特征 做 同样 的   embedding   操作 ， 每 一个 特征 都 有 一个 查找 表   $ ( W _ k ) $ ， 这些 参数 都 要 通过 后续 学习 。     最终 输出 的 特征向量 是 这些 离散 特征   embedding   后 的 特征 拼接 而成 ！       经过 这 一层 后 ， 每 一个 词 输出 为 一个   $ ( d _ { wrd } ) $   维 的 向量 。       Extracting   Higher   Level   Features   from   Word   Feature   Vectors           Window   Approach ：   加窗 ， 只 使用 词 的 邻居 词 ， 将 窗内 的 词 对应 的 向量 拼接 起来 ， 成为 一个 大 的 向量 ， 设 窗口 为   $ ( k _ { sz } ) $ ，     那么 ， 加窗 后 的 向量 长度 为   $ ( d _ { wrd }   \ \ times   k _ { sz } ) $ 。 将 这个 固定 长度 的 向量 输入 到 一个 正常 的 多层 全 连接 神经网络 。           边界 效应 ： 对于 句子 起始 和 结束 的 词 ， 在 前后 补充 半个 窗口 的 特殊 词 “ PADDING ” ， 这个 词 对应 的 词 向量 也 是 通过 学习 得到 的 。 等价 于 学习 序列 的 开始 和 结束 ！           Sentence   Approach ： 利用 时间 一维 卷积 层   Waibel   et   al .   ( 1989 )   ， Time   Delay   Neural   Networks   ( TDNNs ) 。     卷积 层 可以 学习 局部 特征           卷积 层 ， Max   pooling 层 。 最后 得到 的 固定 长度 的 特征向量 进入 一个 标准 的 全 连接 神经网络 。     边界 通过 相同 的   PADDING   方法 解决 不同 长度 的 问题 ！       训练       Word - Level   Log - Likelihood       每 一个 词是 独立 的 ， 最大化 极大 似然 函数 等价 于 最小化 交叉 熵 损失 函数 。       Sentence - Level   Log - Likelihood       词 的 标注 之间 是 不 独立 的 ！ 从 一个 TAG 到 另 一个 TAG 可能 是 不 允许 的 。     设 TAG 之间 转移 用 score 矩阵   A   表示 ， 最终 对 一个 句子 $ ( [ x ] _ 1 ^ T ) $   标注 序列 $ ( [ i ] _ 1 ^ T ) $   的 score 是     标注 序列 转移 score 和 神经网络 输出 的 score 之 和 ！       $ $     s ( [ x ] _ 1 ^ T ,   [ i ] _ 1 ^ T ,   \ \ hat { \ \ theta } )   =   \ \ sum _ { t = 1 } ^ T   ( [ A ] _ { [ i ] _ { t - 1 } ,   [ i ] _ t }   +   [ f _ { \ \ theta } ] _ { [ i ] _ t , t } )   \ \ \ \     \ \ hat { \ \ theta }   =   \ \ theta   \ \ union   { A _ { ij } ,   \ \ forall   i , j   }     $ $       最后 输出 的 是 score 对 所有 路径 的 softmax 归一化 后 的 值 ， 解释 为 路径 的 条件 概率 。     因为 路径 数目 随 句子 长度 字数 增长 ， 所以 分母 上 的 求和 项 也 有 指数 个 。     幸运 的 是 ， 可以 在 线性 时间 复杂度 内 求得 。       优化 算法 ： 动态 规划 ， Viterbi   algorithm   ！           其他 方法 ：       Graph   Transformer   Networks       Conditional   Random   Fields                   Stochastic   Gradient       stochastic   gradient   ( Bottou ,   1991 )       目标 函数 的 可 微性 ， 因为   max   层 的 引入 ， 导致 在 某些 点 不可 微 ， 但是 随机 梯度 下降 仍然 可以 找到 极小值 点 ！       结果       很 不幸 ， 神经网络 的 结果 都 不如   baseline ！ 无 监督 学习 加入 可能 有用 ？ ！       更 多 的 未 标注 数据       利用 更 多 未 标注 数据 ， 学习 词 向量 ， 然后 初始化   embedding   权重 ！               数据 集 ：           English   Wikipedia       Reuters   RCV1   ( Lewis   et   al . ,   2004 )   dataset                   排序 准则 和 熵 准则               Training   Language   Models       训练 一系列 词典 增大 的 神经网络 ， 每 一个 网络 用 之前 的 网络 初始化   embedding   层 ！ ( Bengio   et   al . ,   2009 )       Sequence   to   Sequence   Learning   with   Neural   Networks       序列 到 序列 学习 ！ 经典 文献 。 来自   Google 。       DNN   只能 对 固定 长度 的 输入 ， 进行 建模 ， 但是 很多 时候 需要 实现 序列 到 序列 的 学习 ： 语音 识别 ， 机器翻译 ！       序列 学习 的 方法 ： 用 一个 RNN （ 通常 是 LSTM ） 将 一个 序列 编码 成 一个 大 的 固定 长度 的 向量 （ Encoder ） ，     然后 再用 一个 RNN 将 该 向量 解码 成 一个 新 的 序列 （ Decoder ） 。     译码 方案 ： Beam - search   decoder 。       评估 指标 ： BLEU ？       一个   trick ：   将 序列 （ 句子 ） 反序 后 ， 加入 训练 集 。 ？       模型       标准 的 RNN 需要 将 输入 和 输出 对齐 才能 用 。     利用 标准 的   RNN   做 编码 和 译码 的 方案 ：   Cho   et   al .   [ 5 ]     这种 方案 的 问题 在于 标准 的 RNN 难以 学到 长期 依赖 。       LSTM 可以 学到 长期 依赖 ， 编码器 将 输入 序列 编码 到 一个 固定 长度 的 向量 ， 解码器 是 一个 标准 的   LSTM - LM   形式 的 解码 结构 ，     用 上述 向量 初始化 该 结构 的 初始 隐层 状态 ！ 解码 序列 直到 输出     结束 标记     才 停止 。           这 篇文章 的 创新 点 ：       编码器 和 解码器 采用 不同 的   LSTM ， 参数 不同 ， 同时 学习 两种 序列 的 结构       深层   LSTM   比 浅层 好 ， 用 了 4 层       将 输入 反序 而 输出 不 反序 ， 再 进行 训练 ，   LSTM 学得 更好 ！                   实验       WMT ’ 14   English   to   French   MT   task       解码 通过 一个 从左到右 的 简单   beam   search   方案 ， 每次 保存   B   个 最 可能 的 前缀 ！     每次 将 这 B 个 前缀 扩展 ， 然后 再 保存 最 可能 的   B   个 新 的 前缀 ， 直到 都 碰到 结束符 ！     B = 1   就 很 好 了 ！ 增加 B 的 值 ， 收益 不是 很大 ！ ？       将 输入 反序 进行 训练 带来 的 收益 很大 ！           正序 ： 1 , 2 , 3   = & gt ;   a , b , c       反序 ： 3 , 2 , 1   = & gt ;   a , b , c               the   LSTM ’ s   test   perplexity   dropped   from   5.8   to   4.7 ,   and   the   test   BLEU   scores   of   its   decoded   translations   increased   from   25.9   to   30.6           作者 给 了 一个 简单 的 解释 ： 不 反序 ， 第一个 词 .....   没看 懂       比   baseline   好 ， 但 比 最好 的 结果 还是 稍微 差点 。     baseline ： phrase - based   SMT   system ？       H .   Schwenk .   University   le   mans .   http : / / www - lium . univ - lemans . fr / ~ schwenk / cslm _ joint _ paper / ,     2014 .   [ Online ;   accessed   03 - September - 2014 ]       RNN   for   QA           一般 的   RNN   应用 :   词 序列 （ 句子 ）   = & gt ;   连续 值 或者 有限 的 离散 值       QA ：   词 序列 （ 句子 ）   = & gt ;     富 文本           Matching   Text   to   Entities :   Quiz   Bowl       对 同一个 结果 ， 需要 有 冗余 的 样本 进行 学习 。       模型 ： dependency - tree   rnn   ( dt - rnn ) ， 对 语法 变化 具有 鲁棒性 ， 同时 训练 问题 和 答案 ， 映射 到 同一个 向量 空间 。       问题 ： 词 之间 的 这个 树结构 怎么 得到 的 ？ ！ De   Marneffe   et   al . ,   2006           每 一个 关系 的 终止 节点 （ 词 ） 通过 矩阵   $ ( W _ v   \ \ in   \ \ mathbb { R } ^ { d   \ \ times   d } ) $   映射 到 隐层 。       中间 节点 也 关联 一个 词 ， 通过 下式 将 该词 和子 节点 映射 到 隐层 。           $ $     h _ n   =   ( W _ v   w _ n   + b   +   \ \ sum _ { k   \ \ in   K ( n ) }   W _ { R ( n ,   k ) }   h _ k )     $ $       权重   $ ( W _ { R ( n ,   k ) } ) $   描述 当前 词 n 与子 节点 隐层   $ ( h _ k ) $   之间 的 组合 关系 。       设 S 是 所有 的 节点 ， 给定 一个 句子 ， 设词 c 是 正确 结果 ， Z 是 所有 不 正确 的 结果 集合 。 那么 对 这 一个 样本 ， 损失 函数 为       $ $     C ( S ,   \ \ theta )   =   \ \ sum _ { s   \ \ in   S }   \ \ sum _ { z   \ \ in   Z }   L ( rank ( c ,   s ,   Z ) )   \ \ max ( 0 ,   1 -   x _ c   h _ s   +   x _ z   h _ s )   \ \ \ \     L ( r )   =   \ \ sum _ { i = 1 } ^ r   1 / i     $ $       Image   retrieval   by   Sentences       论文 ： Grounded   Compositional   Semantics   for   Finding   and   Describing   Images   with   Sentences ,   2013 ,   Richard   Socher ,     Andrej   Karpathy   ,   Quoc   V .   Le * ,   Christopher   D .   Manning ,     Andrew   Y .   Ng         DT — RNN ： CT - RNN ,   Recurrent   NN       将 图像 和 句子 映射 到 同一个 空间 ， 这样 就 可以 用 一个 来查 另外 一个 了 。       zero   shot   learning       最 简单 的 将 词 向量 变成 句子 或 短语 的 方式 是 ， 简单 地 线性 平均 这些 词 向量 ， （ 词 向量 中 的   bag   of   word ） 。     RNN   的 方法 就 没有 这些 问题 。       句子   parsed   by   the   dependency   parser   of   de   Marneffe   et   al .   ( 2006 )       每 一个 句子 被 表达 为 一个 词 ， 词 向量 序列 ，   $ ( s   =   (   ( w _ 1 ,   x _ { w _ 1 } ) ,   ( w _ 2 ,   x _ { w _ 2 } ) ,   ... ,   ( w _ n ,   x _ { w _ n } )   ) ) $ 。     parse 后 得到 树状 结构 ， 可以 用 ( 孩子 ， 父亲 ) 对来 表示   $ ( d ( s )   =   {   ( i ,   j )   } ) $ 。     最后 输入   DT - RNN   的 样本 是 两者 组成 的   ( s ,   d )       图像 特征提取 是 用   DNN ( Le   et   al . ,   2012 ) ， 利用 未 标注 的 web 图片 和 标注 的   ImageNet   训练 学 出来 的 ， dim = 4096 。     输入 ： 200x200 ， 使用 了 三个 层 ： 滤波 （ CNN ） ， pooling ( L2 ) ， local   contrast   normalization .       local   contrast   normalization :   将 输入 的 子 图块 （ 文章 中 5x5 ） 减去 均值 ， 除以 方差 进行 归一化 。   有点像   layer   nomalize .           Multimodal   Mappings             -   固定 图片 特征 4096 维       联合 训练 图片 特征向量 映射 到 联合 空间 矩阵 和 DT - RNN 参数 。                   损失 函数 ： 大 间隔 损失 函数 ， 略 。       Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions       论文 ： Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions ,     Andrej   Karpathy   ,     Li   Fei - Fei             图像 特征 ：   RCNN       文本 特征 ：   双向 RNN           将 文本 特征 和 图像 特征 映射 到 同一个 空间 ， 并 学习 图像 块 和 文本 的 对齐 向量 。       RNN 语言 模型       Recurrent   neural   network   based   language   model       动态 模型 ： 在 训练 的 时候 ， 每个 样本 在 多个 epoch 出现 ， 测试 的 时候 ， 也 更新 模型 ， 不过 一个 样本 只 出现 在 一个 epoch 中       cache   techiques 。       优化 技巧           将 出现 频率 低于 某个 阈值 的 词 映射 为 同一个 词 ， 称作 rare   token 。 条件 概率 变为 ：           $ $     p ( w _ i ( t + 1 ) |   w ( t ) ,   s ( t - 1 ) )   =   \ \ begin { cases }                                                             y _ { rare } ( t ) / C _ { rare } ,   w _ i ( t + 1 )   is   rare .   \ \ \ \                                                             y _ i ( t ) ,   otherwise                                                             \ \ end { cases }     $ $       s   是 隐层 ， 即 上下文 向量 。 因为   rare   是 多个 词 概率 之 和 ， 所以 对 某个 词 来说 ， 它 的 概率 就要 把   rare   的 概率 除以 rare 词 的 数目 。     对 这些 词 来说 ， 概率 都 是 一样 的 。       RNN   LM ：   6 小时 ；   Bengio ： 几天 ， 24 小时   sampling       A   Neural   Probabilistic   Language   Model       Yoshua   Bengio ， 2003 .       传统 的   n - gram   模型 的 问题 ， 维数 灾难 。 随着 n 增大 ， 测试 集中 的   n - gram   是 训练 集中 没有 的 概率 越来越 大 。     解决之道 ： 神经网络 模型 ， 词 的 分布式 表达 。           学习 到 了 词 的 分布 是 表达       基于 这种 词 的 表达 的 条件 概率模型 ， 语言 模型           维数 灾难 ： 建模 离散 随机变量 的 联合 分布 时 ， 10 个 变量 就 有   | V | ^ 10   个 可能 的 状态 （ 参数 ） 。     而 建模 连续变量 就 容易 一些 ， 可以 用 神经网络 ， 等等 。 （ 连续函数 一般 具有 局部 光滑 ， 即 局部 可微 ） 。       传统 n - gram 的 问题 ： 考虑 的 近邻 词 数目 太 少 ， 最好 的 结果 也 就是 trigram ；     没有 考虑 到 词 之间 的 相似性 。       维数 灾难 解决办法 ： 词 的 分布式 表达 。       n - gram   语言 模型 ， 也 会 通过 所有 的 gram 进行 平滑 。       perplexity ，   条件 概率 的 倒数 的 集合 平均值 ！       impotance   sampling :   Quick   training   of   probabilistic   neural   nets   by   importance   sampling ，   Bengio ,   2003 .       EXTENSIONS   OF   RECURRENT   NEURAL   NETWORK   LANGUAGE   MODEL       Toma   ́ s ˇ   Mikolov 。 问题 ， 计算 复杂度 太高 。 要 计算   softmax       BPTT ，   4 - 5 步 就 可以 达到 不错 的 精度 了 。       将 输出 层 分解 ， 减少 计算 量 ？ ？ ？       Opinion   Mining   with   Deep   Recurrent   Neural   Networks       将 观点 挖掘 作为 序列 标注 的 问题 。           BIO   tagging   scheme ：       B ， 表达 观点 的 开始 位置       I ， 表达 观点 的 词中       O ， 词外                   3 - 4 层后 ， 性能 也 上 不 去 了 ！       Gated   Feedback   Recurrent   Neural   Networks           多层   RNN （ 简单 RNN ，   LSTM ，   GRU 都 可以 ）       不同 层 RNN 互相 连接 ， 通过 全局   rest   门 控制 不同 层 之间 的 交互           其中 ， 第 i 层到 第 j 层 的 全局   reset   门 ， 由 当前 时刻 第   j - 1   层 的 输入 （ 对于 第一层 ， 是 x ） ，     以及 上 一 时刻 所有 的 隐层 共同 决定 ， 如下 式 ：       $ $     g ^ { i   \ \ rightarrow   j }   =   \ \ sigma ( w _ g ^ { i   \ \ rightarrow   j }   h _ t ^ { j - 1 }   +   u _ g ^ { i   \ \ rightarrow   j }   h _ { t - 1 } ^ { * } )     $ $       隐层 的 更新 ， 将 单层 隐态 更新 方程 中上 一 时刻 隐层 项 ， 变成 对 所有 隐层 通过 全局   reset   门 的 组合 。     所有 类型 的   RNN 如 LSTM ， GRU 都 适用 ， 详细 见 论文 。           试验 任务 ：       character - level   语言 模型 ， 评估 指标   bits - per - character       python   程序 结果 预测 ： 输入 一段 python 程序 ， 要求 预测 输出 结果 。 every   input   script   ends   with   a   print   statement 。   属于   sequence   to   sequence   的 问题 。 通过 调节 程序 的 难度 ， 可以 在 不同 难度 上 评估 不同 模型 的 优劣 。                   Recursive   Deep   Models   for   Semantic   Compositionality   Over   a   Sentiment   Treebank       作者 ： Richard   Socher ,   Alex   Perelygin ,   Jean   Y .   Wu ,   Jason   Chuang ,   Christopher   D .   Manning ,     Andrew   Y .   Ng     and   Christopher   Potts       Semantic   vector   spaces ： ？       在 合并 子 节点 的 时候 ， 除了 传统 RNN 的 二阶 张量 线性 项 ， 还 增加 了 三阶 张量 线性 项 ， 对 短 句子 建模 更 有效 ？ ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dlnlp-cs224d.html"},
      
      
      {"title": "CS231N - Convolutional Neural Networks for Visual Recognition", "text": "    Table   of   Contents           关于           BP   算法 与 计算 图           高效 的 BP 算法           自动 微分           参考                   神经网络 历史           感知器           三层 神经网络           RBM 深度 网络           第一个 强 结果           激活 函数           CNN                   目标 检测           HOG ( Histogram   of   Oriented   Gradient )           Deformable   Parts   Model   :   DPM                   表达 可视化           RNN           CNN   practice           Data   Augmentation   数据 增强 ：           Transfer   Learning   迁移 学习           CNN   细节           卷积 的 实现           浮点 精度                   软件包   Caffe   /   Torch   /   Theano   /   TensorFlow           Caffe           Torch           Theano           TensorFlow                   Video           无 监督 学习           Autoencoders           Variational   autoencoder           Generative   adversarial   nets                         关于       李菲菲 在 Stanford 开 的 课程 ， 见   http : / / cs231n . stanford . edu /         BP   算法 与 计算 图       一个 图 节点 实现   forward   计算 激活 函数 和   backward   计算 梯度 ， 图 的 变 对应 于 变量 。       高效 的 BP 算法       LeeCun   1998   的 论文 中 给出 了 BP 算法 的 一些 trick ：           采用 随机 梯度 下降 ， 更 快 ： 考虑 对 样本 的 10 次 复制 ， 随机 梯度 相当于 训练 了 10 次 ， 而 批量 梯度 下降 只有 1 次 ！     结果 通常 更好 ： 可以 跳出 鞍点 ， 也 可以 更大 概率 跳出 局部 最优 。 可以 随 时间 进一步 训练 ！ online   learning 。     对于 SGD ， 理论 上 最优 学习 率 随 时间 线性 下降 ！       每 一次 epoch 重新 打散 样本 ！       归一化 输入 ， 均值 接近 0 通常 收敛 更 快 ！ 同样 ， 输出 也 尽可能 是 0 均值 。 去 相关 ， 归一化 方差 ， 会 加快 收敛 。       基于 上 一个 原则 ， tanh 比 sigmoid 好 。       目标 ， 匹配 输出 激活 函数       初始化 权值 ， 权值 要 使得 激活 函数 工作 在 线性 区 ， （ 这样 才 好学 ， 否则 梯度 为 0 ） 目标 是 让 输入输出 的 方差 相同 （ 都 为 1 ） 。     当 输入 方差 为 1 的 时候 ， 输出 方差 为           $ $     \ \ sigma _ { y _ i }   =   ( \ \ sum _ j   w _ { ij } ^ 2 ) ^ { 1 / 2 }     $ $       为了 保证 输出 方差 也 为 1 ， 那么       $ $     \ \ sigma _ w   =   m ^ { - 1 / 2 }     $ $       对于 部分 连接 的 网络 （ 如 CNN ， DTNN ） ， m 应该 是 连接 的 节点 个数 。           学习 率 ， 自动 调整 衰减 。 动量 机制 ， 减少 震荡 。           $ $     \ \ Delta   w ( t + 1 )   =   \ \ eta   \ \ frac { \ \ partial   E _ { t + 1 } } { \ \ partial   w }   +   \ \ mu   \ \ Delta   w ( t )     $ $           自 适应 学习 率           自动 微分       四种 计算 梯度 的 方法 ：           手动 推导 梯度 的 公式 ， 然后 编码 实现 ： 易错 ， 费时       数值 微分 （ 有限 差分 ） ： 简单 实现 ， 但是 效率 低 ， 而且 不 精确       符号 微分 （ Mathematica ,   Maple ) ： 表达式 通常 会 比较复杂 ， 存在   expression   swell   的 问题 ， 而且 对 表达式 形式 有 要求 （ close   form ）       自动 微分 ： 可以 达到 机器 精度 ， 和 理想 的 渐进性 能 只 差 一个 常数 因子 （ 性能 牛 逼 ！ ）           参考           Automatic   differentiation   in   machine   learning :   a   survey ,   2015       Efficient   BP ,   LeeCun   1998         https : / / cs231n . github . io / optimization - 2 /           Stochastic   Gradient   Tricks             神经网络 历史       感知器       Frank   Rosenblatt   1957       $ $     y   =   f ( w   x   +   b )   \ \ \ \     f ( z )   =   1   when   z & gt ; 0   else   0     $ $       更新 权值       $ $     w _ i ( t + 1 )   =   w _ i ( t )   +   \ \ alpha   ( d _ j   -   y _ j ( t ) ) x _ { j ,   i }     $ $       相当于 下述 损失 函数   +   SGD 优化 （ 注意 这里 label 是 + 1 ， - 1 和 上面 有 区别 ， 这里 只是 便于 表达 ）       $ $     \ \ max ( 0 ,   -   d _ j   *   y _ j )     $ $       三层 神经网络       Rumelhart   et   al .   1986 ， BP 算法       RBM 深度 网络       Hinton   2006       第一个 强 结果       Context - Dependent   Pre - trained   Deep   Neural   Networks   for   Large   Vocabulary   Speech   Recognition   George   Dahl ,     Dong   Yu ,   Li   Deng ,   Alex   Acero ,   2010   MSR       Imagenet   classification   with   deep   convolutional   neural   networks     Alex   Krizhevsky ,   Ilya   Sutskever ,     Geoffrey   E   Hinton   ,   2012       激活 函数               sigmoid :   将 结果 映射 到 [ 0 , 1 ] 之间 ， 有 概率 解释 。 问题 在于 饱和 将 梯度 变为 0 了 ， 非 0 均值 。 而 非 0 均值 ， 导致 梯度 被 限制 在 各 分量 全为 正 或者 全为 负 的 区域 。     导致 收敛 变慢 。   exp   函数 计算 复杂度 较大 。               tanh :   解决 了 0 均值 的 问题           ReLU :   在 正值 区 不饱和 ， 计算 效率 较 高 ， 但是 还 是不是 0 均值 ， 负 向 梯度 为 0       Leaky   ReLU   =   max ( 0.1 x ,   x ) , 解决 负 向 梯度 饱和 问题 ，       Maxout   =   max ( w1   x   +   b1 ,   w2   x   +   b2 ) ,   基本 解决 上述 问题 ， 但是 参数 变多 了   double       ELU           $ $     x   & gt ;   0 :   x   \ \ \ \     x   & lt ;   0 :   \ \ alpha   ( \ \ exp ( x )   -   1 )     $ $       中心化 ： 减去 图像 均值 （ AlexNet ） ， 减去 每个 通道 的 均值 （ VGGNet ）       初始化 ：     “ Xavier   initialization ”   [ Glorot   et   al . ,   2010 ] ：   $ ( n _ { in }   Var ( w )   =   1 ) $       对于   ReLU ， 因为 一半 恒为 0 ， 因此 有 一个 0.5 因子 。 $ ( \ \ frac { 1 } { 2 }   n _ { in }   Var ( w )   =   1 ) $   He   et   al . ,   2015       论文 ：           Understanding   the   difficulty   of   training   deep   feedforward   neural   networks ,   Glorot   and   Bengio ,   2010       Delving   deep   into   rectifiers :   Surpassing   human - level   performance   on   ImageNet   classification   by   He   et   al . ,   2015           Batch   Normalize [ Ioffe   and   Szegedy ,   2015 ] ： 归一化 到 标准 正态分布 ， 然后 让 它 自己 学 一个 纺射 变换 。 通常 插入 在 全 连接 层 和 激活 函数 之间 。       CNN           1998 ， LeNet - 5 ,   LeCun       LeNet - 5 :   Gradient - based   learning   applied   to   document   recognition ， 1998 ， LeCun ,   Bottou ,   Bengio ,   Haffner       AlexNet :   ImageNet   Classification   with   Deep   Convolutional   Neural   Networks ， Hinton   2012 ,       ZFNet :       VGGNet :   Very   Deep   Convolutional   Networks   for   Large - Scale   Image   Recognition       GoogLeNet :   Going   Deeper   with   Convolutions       ResNet           目标 检测       任务 ： 分类   +   定位       Location   as   Regression :   输入 图片 ， 输出 4 个 坐标 ！ （ 非常简单 ） L2 损失 函数 ， 作为 回归 问题 处理 。     分类   +   定位   作为 多任务 ， 共用 同一个 CNN 做 特征提取 层 。       输入 图像   = & gt ;   CNN   = & gt ;   FC   = & gt ;   softmax / Regression       回归 层 连接 的 位置   ： 在 CNN 层 后面 （ Overfeat ，   VGG ） ；   在 全 连接 层 ( FC ) 后面 ： DeepPose ， R - CNN       检测 多个 目标 ： 共用 CNN 层 做 特征提取 层 。       姿势 估计 ： Toshev   and   Szegedy ,   “ DeepPose :   Human   Pose   Estimation   via   Deep   Neural   Networks ” ,   CVPR   2014       sliding   window :   Overfeat : Integrated   Recognition ,   Localization   and   Detection   using   Convolutional   Networks ,     ICLR   2014           在 高精度 图片 的 不同 位置 训练 模型 进行 分类 和 回归 。       将 FC 也 变成 卷积 层 ， 减少 运算量 ： Overfeat       结合 所有 的 位置 的 结果 ， 得到 最终 的 结果 （ MAX   Pool ）       实际 使用 中 ： 采用 多个 不同 位置 不同 尺寸 的 窗           通过 一个 滑动 窗 ， 目标 检测 可以 作为 一个 分类 问题 ！ 需要 大量 的 计算 匹配 ！       HOG ( Histogram   of   Oriented   Gradient )       Dalal   and   Triggs ,   “ Histograms   of   Oriented   Gradients   for   Human   Detection ” ,   CVPR   2005           在 不同 分辨率 计算 方向 梯度 直方图       略 ， 不 懂           Deformable   Parts   Model   :   DPM       Felzenszwalb   et   al ,   “ Object   Detection   with   Discriminatively     Trained   Part   Based   Models ” ,   PAMI   2010       DPM   is   CNN ?       Girschick   et   al ,   “ Deformable   Part   Models   are   Convolutional   Neural   Networks ” ,   CVPR   2015       Detection   as   Classification :       问题 ： 需要 测试 很多 位置 和 尺寸 ， 计算 量 大 ！       方案 ： 仅仅 测试 一个 很小 的 子集 ！       How   to ：       Region   Proposals :   Selective   Search       自 底向上 ， 分割 图像 ， 然后 在 不同 层级 合并 相似 区域 ， 得到 不同 层级 的 分割 结果 。       Uijlings   et   al ,   “ Selective   Search   for   Object   Recognition ” ,   IJCV   2013       其他 方法 ： EdgeBox ？       检测   Review ：     Hosang   et   al ,   “ What   makes   for   effective   detection   proposals ? ” ,   PAMI   2015       R - CNN ！   Girschick   et   al ,   “ Rich   feature   hierarchies   for     accurate   object   detection   and   semantic     segmentation ” ,   CVPR   2014           Step   1 :   Train   ( or   download )   a   classification   model   for   ImageNet   ( AlexNet )       Step   2 :   Fine - tune   model   for   detection       把 1000 个 分类 变成 20 个 目标 + 背景       扔掉 最后 一层 FC 层 ， 重新 初始化       用 正负 样本 区域 训练               Step   3 :   抽取 特征       对 所有 图片 ， 找到 感兴趣 区域       对 每 一个 区域 ， 剪切 或者 压缩 到 CNN 输入 尺寸 ， run   forward   through   CNN ， 保存 pool5 特征 到 硬盘               Step   4 :   对 每 一个 类 ， 用 上述 抽取 的 特征 ， 训练 一个 2 分类 SVM       Step   5 :   ( bbox   regression )   对   每 一个 类 ， 训练 一个 线性 回归 模型 ， 从 上述 特征 得到 box 的 偏移量 ！           目标 检测 数据 集 ： PASCAL   VOC   ( 2010 ) ，   ImageNet   Detection   ( ILSVRC   2014 ) ，   MS - COCO   ( 2014 )       评估 指标 ： “ mean   average   precision ”   ( mAP )       RCNN 问题 ：           Slow   at   test - time :   对 每 一个 区域 都 要 计算 CNN 抽取 的 特征       SVM 和 回归 都 不会 对 CNN 的 特征 进行 更新 ， 不 存在 调优       复杂 的 多 阶段 流程           Fast - RCNN ： Girschick ,   “ Fast   R - CNN ” ,   ICCV   2015           计算 慢 的 问题 ： 对 整个 图像 计算 CNN 后 的 特征 ， 共享 计算 量       end - to - end   地 训练 一次 ！           ROI ( region   of   interest )   抽取 ：           对 整个 图像 卷积 + Pooling ， 得到 高精度 的 特征       将 投影 区域 划分 为   h * w   个 格子           训练 加速 8.8 倍 ， 测试 加速 146 倍 ！       问题 ： 测试 加速 不 包过   ROI   提取 ！ ？       Faster   RCNN ： 在 最后 一层 卷积 层 加入 一层 Region   Proposal     Network   ( RPN )       Ren   et   al ,   “ Faster   R - CNN :   Towards   Real - Time   Object     Detection   with   Region   Proposal   Networks ” ,   NIPS   2015       进一步 将 test 时间 加速 10 倍 ！       表达 可视化       t - SNE   visualization ： two   images   are   placed   nearby   if   their   CNN   codes   are   close .     Laurens   van   der   Maaten   ,     Geoffrey   Hinton   ,   2008 .       Deconv 方法 ： 选择 某个 CNN 层 ， 将 该 层 的 梯度 全部 置 0 ， 除了 其中 一个 ！ 然后 BP 到 输入 ， 得到 Deconv 图像 ！   BP   to   image .       Visualizing   and   Understanding   Convolutional   Networks ,   Zeiler   and   Fergus   2013       Optimization   to   Image   方法 ： 寻找 最大化 某些 类别 的 score ！       $ $     \ \ arg   \ \ max _ { I }   S _ c ( I )   -   \ \ lambda   | | I | | _ 2 ^ 2     $ $           将 输入 层置 0 ， 即 输入 全零 图像 。       将 输出 层 的 梯度 为 单位向量 ， 某个 类别 为 1 其他 为 0 ， 然后   BP   to   image !           Deep   Inside   Convolutional   Networks :   Visualising   Image   Classification   Models   and   Saliency   Maps ,   Karen   Simonyan ,   Andrea   Vedaldi ,   Andrew   Zisserman ,   2014 .       Understanding   Neural   Networks   Through   Deep   Visualization ,   Yosinski   et   al .   ,   2015       问题 ： 给定 一个 CNN 编码 ， 能否 重构 出 原始 图像 ？       Understanding   Deep   Image   Representations   by   Inverting   Them ，   Mahendran   and   Vedaldi ,   2014       DeepDream ，     https : / / github . com / google / deepdream         Understanding   Neural   Networks   Through   Deep   Visualization ,   Jason   Yosinski ,   2015       RNN       字母 维度 的 语言 模型 ：       Image   Captioning ： 将 CNN 抽取 的 特征 ， 作为 RNN 隐层 额外 的 输入 ！ RNN 的 初始 输入 用 一个 固定 的 值 ， 后续 时刻 用前 一 时刻 的 输出 作为 输入 ！       Image   Sentence   Datasets ： Microsoft   COCO   [ Tsung - Yi   Lin   et   al .   2014 ]       RNN   在 产生 单词 的 时候 ， 关注 图像 的 部分 ： Show   Attend   and   Tell ,   Xu   et   al . ,   2015       CNN   practice       Data   Augmentation   数据 增强 ：       对 图像 进行 变换 ：           水平 翻转       随机 裁剪 和 缩放           Training :   sample   random   crops   /   scales     ResNet :           Pick   random   L   in   range   [ 256 ,   480 ]       Resize   training   image ,   short   side   =   L       Sample   random   224   x   224   patch           Testing :   average   a   fixed   set   of   crops     ResNet :     1 .   Resize   image   at   5   scales :   { 224 ,   256 ,   384 ,   480 ,   640 }     2 .   For   each   size ,   use   10   224   x   224   crops :   4   corners   +   center ,   +   flips           color   jitter ： 色彩 抖动 ？       更 多 ： Random   mix / combinations   of   :       translation       rotation       stretching       shearing ,       lens   distortions ,   …   ( go   crazy )                   加 噪声 ！     训练 ： 添加 随机噪声 ；   测试 ： 排除 噪声 ！       Transfer   Learning   迁移 学习           在 ImageNet 上 训练 CNN       在 小 数据 集上 ， 固定 前面 所有 层 ， 只 改变 最后 一层 参数 ！ 相当于 用 CNN 做 特征提取 ， 没有 调优 ！       在 中等 数据 集上 ， 固定 前面 大多数 层 ， 只 改变 后面 少许 层 参数 ， 进行 调优 ！           CNN   Features   off - the - shelf :   an   Astounding   Baseline   for   Recognition ， [ Razavian   et   al ,   2014 ]       DeCAF :   A   Deep   Convolutional   Activation   Feature   for   Generic   Visual   Recognition ,   2014 .       CNN   细节       多个 小 滤波器 堆叠 比 一个 大 滤波器 好 ！ 因为 可以 用 较 少 的 参数 ， 得到 相同 的 非线性 ！ （ 用 最后 一层 的 神经元 所 能 看到 的 输入 像素 个数 来 度量 ？ ）     并且 计算 量 更 小 ！       1x1   大小 的 滤波器 ， 用来 降维 ！ ？ GoogleNet ！       用 两个 1xN 和 Nx1 的 滤波器 ， 代替 一个 NxN 的 滤波器 ？ ！ ！ 减少 参数       Szegedy   et   al ,   “ Rethinking   the   Inception   Architecture   for   Computer   Vision ”       卷积 的 实现       将 多个 卷积 计算 变成 一个 矩阵 乘法 运算 ！   im2col ， 需要 大 额外 的 内存           设 图像 特征 map 为   H   M   C   维 ， D 个 卷积 滤波器 维度 为 K   K   C   维 。       将 图像 reshape 成   ( K ^ 2   C ) * N 维 矩阵 ， 而 将 滤波器 变为 ( K ^ 2   C ) * D 维 矩阵 ， 然后 计算 矩阵 乘法 ， 最后 将 D * N 维 结果 再 reshape 为 给定 的 大小 。           FFT 实现 ： 对于 小 的 滤波器 没有 提升 ！     Vasilache   et   al ,   Fast   Convolutional   Nets   With   fbfft :   A   GPU   Performance   Evaluation       Strassen ’ s   矩阵 乘法 算法 ！ 加速 。     Lavin   and   Gray ,   “ Fast   Algorithms   for   Convolutional   Neural   Networks ” ,   2015       GPU ： NVIDIA   is   much   more   common   for   deep   learning       CEO   of   NVIDIA :   Jen - Hsun   Huang     ( Stanford   EE   Masters   1992 )       CPU :     Few ,   fast   cores   ( 1   -   16 ) ,     Good   at   sequential   processing .       GPU :     Many ,   slower   cores   ( thousands ) ,     Originally   for   graphics ,     Good   at   parallel   computation       GUDA   vs   OpenCL .       Udacity :   Intro   to   Parallel   Programming       GPU   非常适合 矩阵 乘法 ！       多 GPU 训练 ：           模型 并行 ： FC 全 连接 层       数据 并行 ： CNN 层           Alex   Krizhevsky ,   “ One   weird   trick   for   parallelizing   convolutional   neural   networks ”       Google ： 分布式   CPU   训练 ！ 数据 并行   and   模型 并行 ！       Large   Scale   Distributed   Deep   Networks ,     Jeff   Dean     et   al . ,   2013       Google ： 异步   and   同步       Abadi   et   al ,   “ TensorFlow :   Large - Scale   Machine   Learning   on   Heterogeneous   Distributed   Systems ”       GPU   -   CPU   通信 瓶颈 ：           CPU ： 数据 预取   +   data   augment       GPU ： forward / backward           CPU   -   disk   瓶颈 ： 磁盘   = & gt ;   SSD       GPU   memory   瓶颈 ：     Titan   X :   12   GB   & lt ; -   currently   the   max 。     GTX   980   Ti :   6   GB       AlexNet :   ~ 3GB   needed   with   batch   size   256       浮点 精度           大多数 编程 环境 ： 64bit   双 精度       CNN ： 32bit   单精度       16bit   半 精度 将 成为 新 的 标准 ！ cuDNN   已经 支持 ！           最低 精度 能 到 多少 ？     16bit   定点   with   随机   round ！       Gupta   et   al ,   “ Deep   Learning   with   Limited   Numerical   Precision ” ,   ICML   2015       10bit   激活 函数 ， 12bit 参数 更新 ！     Courbariaux   et   al ,   “ BinaryNet :   Training   Deep   Neural   Networks   with   Weights   and   Activations   Constrained   to   + 1   or   - 1 ” ,   arXiv   2016       未来 ： binary   network ？       软件包   Caffe   /   Torch   /   Theano   /   TensorFlow       Caffe           U .   C .   Berkeley       C++       Has   Python   and   MATLAB   bindings       Good   for   training   or   finetuning   feedforward   models           主要 类 ：           Blob :   存储 数据       Layers ： 将 底层 Blob 变成 顶层 Blob       Net ： 很多 Layers       Solver ： 使用 梯度 更新 权值           Protocol   Buffers :   \" Typed   Json \"   from   Google       训练 和 调优 ： 不 需要 写 代码 ！       Caffe :   Model   Zoo ， 预 训练 好 的 模型 ！       提供 Python 接口       not   good   for   RNN       Torch           NYU   +   IDIAP       C   and   Lua       Used   a   lot   a   Facebook ,   DeepMind           Tensors ：   ndarray       not   good   for   RNN       Theano             Yoshua   Bengio   ’ s   group   at   University   of   Montreal       High - level   wrappers :     Keras   ,   Lasagne           计算 图 ！       问题 ： 每次 更新 权值 需要 将 权值 和 梯度 移 到   CPU   计算 ！     可以 通过   shared _ variable   得到 解决 ！       TensorFlow           From   Google       Tensorboard   for   可视化           目前 还 比较慢 ！       Video       feature   based   方法 （ 运动 识别 ） ：           Dense   trajectories   and   motion   boundary   descriptors   for   action   recognition     Wang   et   al . ,   2013       检测 不同 尺度 的 图像 的 特征 点       跟踪 特征 点   optical   flow       在 局部 坐标 中 抽取   HOG / HOF / MBH   特征       相关 文献 ：       [ G .   Farneb ä ck ,   “ Two - frame   motion   estimation   based   on   polynomial   expansion , ”   2003 ]       [ T .   Brox   and   J .   Malik ,   “ Large   displacement   optical   flow :   Descriptor   matching   in   variational   motion   estimation , ”   2011 ]       [ J .   Shi   and   C .   Tomasi ,   “ Good   features   to   track , ”   CVPR   1994 ]       [ Ivan   Laptev   2005 ]                       Action   Recognition   with   Improved   Trajectories     Wang   and   Schmid ,   2013       Spatio - Temporal   Conv ：       [ 3D   Convolutional   Neural   Networks   for   Human   Action   Recognition ,   Ji   et   al . ,   2010 ]       Sequential   Deep   Learning   for   Human   Action   Recognition ,   Baccouche   et   al . ,   2011       [ Large - scale   Video   Classification   with   Convolutional   Neural   Networks ,   Karpathy   et   al . ,   2014 ]       3D   VGGNet   :   [ Learning   Spatiotemporal   Features   with   3D   Convolutional   Networks ,   Tran   et   al .   2015 ]                   无 监督 学习       Autoencoders           Encoder   and   Decoder ：       线性   +   非线性 激活 函数 ( sigmoid )       Deep   全 连接       ReLU   CNN               loss   function :   L2       使用 Encoder 初始化 神经网络       逐层 训练 ： Greedy   training ： RBM   2006     Hinton   。 现在 不再 流行 了 ， 因为   ReLU ,   合理 的 初始化 ， batchnorm ,   Adam   etc   easily   train     from   scratch       生成 样本 ！           Variational   autoencoder       Kingma   and   Welling ,   “ Auto - Encoding     Variational   Bayes ” ,   ICLR   2014           intuition ： $ ( z ) $ 以 概率   $ ( p _ { \ \ theta * } ( x | z ) ) $ 产生 图片 样本 $ ( x ) $ ， z   可以 使 类别 ， 属性 等 ！       problem ： 在 不 知道 z 的 情况 下 ， 估计 参数 $ ( \ \ theta ) $       prior ： $ ( p ( z ) ) $ 是 标准 高斯分布       condition ： $ ( p ( x | z ) ) $ 是 对角 高斯分布 ！ 用 神经网络 预测 均值 和 方差           Generative   adversarial   nets       Goodfellow   et   al ,   “ Generative     Adversarial   Nets ” ,   NIPS   2014  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cs231n.html"},
      
      
      {"title": "Decoupled Neural Interfaces using Synthetic Gradients", "text": "    Table   of   Contents           论文 导读           导言                 论文 导读       这 篇文章 是   Google   Deepmind   团队 的 一片 文章 ， 改进 了 BP 算法 在 Leayer   层面 上 的 串行 计算 结构 。       导言       神经网络   BP   算法 的 几个   Lock ：           Forward   Locking ： 每一 模块 （ 层 ） 必须 等到 前面 的 层 全部 计算 结束 才能 得到 输入 数据 。       Update   Locking ： 每 一个 模块 都 要 等到 forward 模式 下 依赖 的 模块 计算 完成 才能 更新 。       Backwards   Locking ： 所有 模块 要 等到 forward 模式 和 backward 模式 依赖 的 所有 模块 计算 完成 后 才能 更新 。           现有 神经网络 更新 权值 的 问题 ： Update   Locking           多个 异步 模块 构成 的 系统       分布式 模型           本 论文 主要 工作 是 移 除了   Update   Locking   依赖 。 权值 更新 方法 ：       $ $     \ \ frac { \ \ partial   L } { \ \ partial   \ \ theta _ i }   =   f _ { Bprop } ( ( h _ i , x _ i , y _ i , \ \ theta _ i ) , ( h _ { i + 1 } , x _ { i + 1 } , y _ { i + 1 } , \ \ theta _ { i + 1 } ) , ... )   \ \ frac { \ \ partial   h _ i } { \ \ partial   \ \ theta _ i }   \ \ \ \     \ \ approx   \ \ hat { f } _ { Bprop } ( h _ i )   \ \ frac { \ \ partial   h _ i } { \ \ partial   \ \ theta _ i }     $ $       这里 下标 表示层 数 （ 或者 模型 序号 ） ， $ ( x ,   y ) $ 表示 输入 和 监督 ？ h 表示层 的 输出 ， 该 方法 用 一个 神经网络 对 第一 部分 建模 ， 而且 只 采用 该层 输出 的 局部 信息 ！       Decoupled   Neural   Interface   ( DNI ) .   预测 误差 仅 依赖 与 该 层 的 输出 ， 因而 一旦 该层 计算 完毕 ， 就 可以 马上 得到 BP 误差 ！     该 梯度 仅 跟 输出 h 有关 ， 成为   synthetic   gradients   ( 合成 梯度 ) ， 可以 利用 这个 合成 梯度 立即 更新 该层 网络 权值 ！     从而 移 除了   Update   Locking   和   Backwards   Locking ！       利用 同样 思想 ， 移除 Forward   Locking ? 预测 输入 ？           J .   Baxter   and   P .   L .   Bartlett .   Direct   gradient - based   reinforcement   learning .   In   Circuits   and   Systems ,       Proceedings .   ISCAS   2000   Geneva .   The   2000   IEEE   International   Symposium   on ,   volume   3 ,   pages     271 – 274 .   IEEE ,   2000 .         Y .   Bengio   .   How   auto - encoders   could   provide   credit   assignment   in   deep   networks   via   target   propagation .     arXiv   preprint   arXiv : 1407.7906 ,   2014 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/synthetic-gradients.html"},
      
      
      {"title": "Deep & Wide model : Google", "text": "    Table   of   Contents           关于                 关于       Google   宽广 度 模型       用 深度 模型 学习 特征 组合 ， 将 离散 特征 全部   embedding     到 低维 向量 ， 加上 连续 特征 一起 ，     用 DNN 建模 。 深度 模型 和 浅层 模型 联合 优化 。       线下 AUC 和 线 上 效果 不 一致 ？ 单独 深度 模型 线下 AUC 略低 ， 但是 线上 效果 确 提高 了 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-wide.html"},
      
      
      {"title": "Deep Forest: Towards An Alternative to Deep Neural Networks", "text": "    Table   of   Contents           关于           gcforest           Cascade   Forest           Multi - Grained   Scanning           试验 结果                   Ref                 关于       论文 ： Deep   Forest :   Towards   An   Alternative   to   Deep   Neural   Networks ， 南 大 周志华       gcforest               Cascade   Forest       gcforest 由 多层 构成 ， 每 一层 是 多个 决策树 森林 的 集成 。     我们 用 两个 完全 随机 树 森林 和 两个 随机 森林 ， 利用 不同 类型 的 树来 增加 集成 的 多样性 ，     因为 集成 学习 中 的 多样性 是 至关重要 的 。           完全 随机 树 森林 ： 1000 个 完全 随机 树 构成 。       完全 随机 树 ： 树 的 每 一个 节点 随机 选择 一个 特征 进行 分裂 ， 生长 树 直到 每个 叶子 结点 只有 一类 或者 样本量 不 多于 10 个 。       随机 森林 ： 1000 个 决策树 ， 每棵 树 都 是 随机 选择   $ ( \ \ sqrt { d } ) $   个 特征 ， 选择   gini   增益 最大 的 进行 分裂 。           对 每 一个 样本 ， 每个 森林 将 输出 其 在 每个 类 的 概率分布 向量 （ 通过 平均 每棵 树 的 概率分布 向量 得到 ， 每棵 树是 通过 统计 该 叶子 结点 上 的 分布 得到 ） 。       将 所有 森林 输出 的 分布 向量 链接 成 一个 大 的 向量 （ augmented   features   ） ， 并 和 原始 向量 拼接 到 一起 后 输入 下 一级 继续 学习 ！       为了 避免 过 拟合 ， 类 向量 通过 k - fold 交叉 验证 产生 的 ， 将 每个 样本 作为 训练 集 的   k - 1   次 结果 平均 !       每 一层 训练 结束 后 ， 将 在 验证 集上 计算 score ， 如果 score 没有 明显增加 ， 那么 训练 流程 将会 停止 ！     也就是说 层数 是 训练 过程 中 确定 的 。       Multi - Grained   Scanning       对于 序列 数据 、 图像 数据 等 ， 借鉴 了 卷积 的 思想 ， 利用 滑动 窗将 一个 样本 变成 多个 样本 ， 提取 特征 ！     将 一个 样本 变成 多个 小 尺寸 的 样本 ， 这个 跟 多 示例 学习 中 的   bag   generate 一致   [ Wei   and   Zhou ,   2016 ] 。     然后 把 每个 小 尺寸 样本 预测 出来 的 类 向量 拼接 到 一起 变成 一个 大 的 向量 ， 作为 原始 样本 的 组合 输出 特征 ， 叠加 到 下 一层 。     可以 采用 多个 滑动 窗 ， 从而 获取 跟 多 不同 尺寸 的 特征 ， 类似 于小波 分析 。 这些 扫描 后 的 特征 作为   Cascade   Forest   的 输入 特征 ， 进行 学习 。           X . - S .   Wei   and   Z . - H .   Zhou .   An   em -   pirical   study   on   image   bag   generators   for   multi - instance   learning .   Machine   Learning ,   105 ( 2 ) : 155 – 198 ,   2016 .                           试验 结果       试验 配置 ， gcforest 都 是 用 基本相同 的 配置 。 具体 结果 参看 原始 论文 ， 这里 列举 一些 结论 。           gcforest   在 所有 的 试验 都 显示 出 强大 的 建模 能力 ， 尤其 是 在 小 数据 集上 的 效果 ， 在 IMDB 情感 分类 这样 级别 的 数据 集 也 和 目前 最好 的 CNN 效果 略优 。       Multi - Grained   Scanning   对 图像 数据 和 序列 数据 非常 必要 ， 对 最终 效果 影响 特别 明显 ， 在 GTZAN 和 sEMG 数据 及 上 效果 相差 10 个点 ！       运行 速度 快 ， 都 采用   CPU   版本 时 ， gcforest 大约 只 需要   MLP   的   1 / 4   时间 ， 接近   GPU   版本 的   MLP       更 大规模 的 数据 集 的 效果 和   CNN   的 对比 尚有 待 研究 ， 例如 和   res150   对比 ？ ！           Ref       F .   T .   Liu ,   K .   M .   Ting ,   Y .   Yu ,   and   Z . - H .   Zhou .   Spectrum   of   variable - random   trees .   Journal   of   Ar -   tificial   Intelligence   Research ,   32 : 355 – 384 ,   2008 .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/gcforest.html"},
      
      
      {"title": "Deep Learning Book - Bengio", "text": "  关于  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-learning-bengio.html"},
      
      
      {"title": "Deep learning 环境搭建大坑", "text": "  关于       暂时 记录 了 windows 和 mac 系统 下 遇到 的 问题 。       MAC   SIP 权限 问题       经常 因为 SIP 无法 取得 最高 权限 导致 安装 出错 ， 关闭 SIP 权限 的 方法 ：     1 .   重启 MAC ， 在 重启 过程 中 按住 command ＋ R 键 ， 进入 安全 模式 ， 因为 只有 在 安全 模式 下 才能 关闭 SIP 安全 特性     2 .   在 顶部 菜单 打开 终端 ， 在 终端 输入 命令     csrutil   disable   ， 可以 查看 这个 命令 。 开启 可以 使用   enable   参数 。     3 .   重启 系统 。       theano       tensorflow       linux   安装 pip 可能 找 不到 对应 的 版本 ， 需要 从 tensor 的 github 提供 的 带   non   的 whl 包 安装 。     可能 安装 后 ， import 会报 glibc 版本 找 不到 ， 需要 下载   gnu / glibc   对应 的 版本 ， 编译 并 安装 ！       glibc   安装 ， 在 一个 方便 的 镜像 下载 glibc 源码 ， 安装             mkdir   build     cd     build   .. / configure   - - prefix   =   path - to - install   make     & amp ;     make   install      ", "tags": "machine-learning", "url": "/wiki/machine-learning/env.html"},
      
      
      {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "text": "    Table   of   Contents           关于           语音 识别 现状                 关于       深度 学习 在 语音 识别 中 的 应用 。 参考文献             Hinton   G   E   ,   Deng   L ,     Yu   D   ,   et   al .   Deep   Neural   Networks   for   Acoustic   Modeling   in   Speech   Recognition [ J ] .   IEEE   Signal   Processing   Magazine ,   2012 ,   29 ( 6 ) .           语音 识别 现状           GMM - HMM       GMM   表达   HMM   态 之间 的 关系 ；       输入 的 表达 ： MFCC （ Mel - frequency   cepstral   coefficients ） ， PLPs （ perceptual   linear   predictive   coefficients ） ；     以及 一阶 和 二阶 时域 差分 ；                       每 一个 音节 （ ？ ） 用 一个 态   s   表示 ， 输入 特征   x   到态 s 的 建模   $ ( P ( x   |   s ) ) $   采用 高斯 混合 模型   GMM ， 这是 一个 生成 模型 。 （ 利用 EM 算法 ， 很 容易 拟合 数据 ）       用隐 马尔科夫 模型 建 模态 转移       DNN   替换   GMM ： GMM 不能 很 好 的 建模 低维 非线性 流形 。 DNN 直接 建模 条件 概率   $ ( P   ( s   |   x ) ) $ ， 然后 通过 贝叶斯 法则 得到   $ ( P ( x | s )   =   P ( s | x )   *   P ( x )   /   P ( s ) ) 。 态 的 标注 通过 基本 的   HMM - GMM   得到 ？           TIMIT   database ；   LVCSR               逐层 的 训练   RBM ， 第一层 隐层 保持 二进制 （ 硬 判决 的 noise 可以 作为 正则 防止 过 拟合 ） ， 其他 层 隐层 都 用 实值 的 概率 值 。           实值 数据 （ MFCC ） 建模 ： 高斯 贝 努力   RBM （ Gaussian – Bernoulli   RBM   ( GRBM ) ） ， 能量 函数 为 ：           $ $     E ( v ,   h )   =   \ \ sum _ { i   \ \ in   vis }   \ \ frac { ( v _ i   -   a _ i ) ^ 2 } { 2   \ \ sigma _ i ^ 2 }   -   \ \ sum _ { j   \ \ in   hid }   b _ j   h _ j   -   \ \ sum _ { i , j }   \ \ frac { v _ i } { \ \ sigma _ i }   h _ j   w _ { ij }     $ $       两个 条件 分布 为 ：       $ $     p ( h _ j | v )   =   \ \ text { logistic } ( b _ j   +   \ \ sum _ i   \ \ frac { v _ i } { \ \ sigma _ i }   w _ { ij } )     \ \ \ \     p ( v _ i | h )   =   \ \ mathcal { N } ( a _ i   +   \ \ sigma _ i   \ \ sum _ j   h _ j   w _ { ij } ;   \ \ sigma _ i ^ 2 )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dl-asr.html"},
      
      
      {"title": "Deeplearning4j", "text": "    Table   of   Contents           分布式 训练           在   Spark   上 运行 训练任务           注意事项                         分布式 训练       DL4J   支持 分布式 训练 ， 加快 训练 速度 ， 目前 仅 支持 数据 并行 ， 参数 平均 方式 的 训练 ！               TrainingMaster       tm       =       new       ParameterAveragingTrainingMaster     .     Builder     (     int       dataSetObjectSize     )                               ...       (     your       configuration       here     )                               .     build     ( ) ;                 在   Spark   上 运行 训练任务       注意事项           需要 指定 内存 ， 否则 会 报 各种 错误 ， 方法 和 参数设置 见 官方 文档     https : / / deeplearning4j . org / spark   ，     从官 网上 抄下来 的 一个 例子 ：                 - - class   my . class . name . here   - - num - executors   4   - - executor - cores   8   - - executor - memory   4G   - - driver - memory   4G   - - conf   & quot ; spark . executor . extraJavaOptions = - Dorg . bytedeco . javacpp . maxbytes = 5368709120 & quot ;   - - conf   & quot ; spark . driver . extraJavaOptions = - Dorg . bytedeco . javacpp . maxbytes = 5368709120 & quot ;   - - conf   spark . yarn . executor . memoryOverhead = 6144      ", "tags": "machine-learning", "url": "/wiki/machine-learning/deeplearning4j.html"},
      
      
      {"title": "DNC: Hybrid computing using a neural network with dynamic external memory", "text": "    Table   of   Contents           关于                 关于       论文 ： Hybrid   computing   using   a   neural   network   with   dynamic   external   memory       Google   Deepmind 在 nature 上 的 一篇 文章 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dnc.html"},
      
      
      {"title": "fast text Facebook开源词向量工具背后的理论", "text": "    Table   of   Contents           关于           论文 导读           Enriching   Word   Vectors   with   Subword   Information           摘要           导言           subword   模型           结论                   Bag   of   Tricks   for   Efficient   Text   Classification           模型 结构                                 关于       fast   text   开源 工具 见   https : / / github . com / facebookresearch / fastText   。     主要 涉及 的 两篇 文章 是 ：           P .   Bojanowski ,   E .   Grave * ,   A .   Joulin ,     T .   Mikolov   ,   Enriching   Word   Vectors   with   Subword   Information       A .   Joulin ,   E .   Grave ,   P .   Bojanowski ,     T .   Mikolov   ,   Bag   of   Tricks   for   Efficient   Text   Classification           论文 导读       Enriching   Word   Vectors   with   Subword   Information       摘要       论文 的 基本 思想 ： 由于 之前 的 词 向量 方法 都 是 以 词为 单位 进行 学习 ， 而 通常 一个 词 的 有 很多 不同 的 形态 ！ （ 不能 用 stem 将 形态 标准化 么 ？ ）     为此 ， 作者 提出 一种 基于 skip - gram 结构 的 新 方法 ， 词 被 表达 为 多个   n - gram   字母 的 bag 。     对 每 一个 n - gram 学习 一个 向量 ， 而词 向量 用 这些 n - gram 的 向量 求和 得到 。     这种 方法 非常 快 ！ 作者 在 5 种 不同 的 语言 中 训练 词 向量 ， 评估 了 在 word   similarity   and   analogy     tasks 上 的 效果 ！       导言       subword   模型       在 skip - gram 模型 中 ， 两个 词 之间 相似 度用 两个 词词 向量 的 内基 表示 $ ( score   =   s ( w , c )   =   w ^ T   c ) $ 。     这个 模型 将 一个 词 的 n - gram   g 单独 赋予 一个 向量 $ ( z _ g ) $ ， 而词 向量 用 该词 所有 的 n - gram 向量 之 和 来 表示 。     不同 词 之间 ， 共享 n - gram 向量 ！ 那么 $ ( s ( w , c )   =   \ \ sum   z _ g ^ T   v _ c   ) $ 。     词 本身 也 加到 了 n - gram 集合 中 ， 并且   词 向量 和 n - gram 向量 不同 ， 比如 as 作为 单词 和 作为 paste 的 n - gram 是 不 共享 向量 的 ！       n - gram   词典 的 设计 ， 论文 只 保留 了 长度 为 3 - 6 的 n - gram 。 为 开始 位置 和 结束 位置 添加 特殊字符 ， 用以 区分 前缀 和 后缀 。       限制 内存 ： 将 n - gram   hash   到   1   to   K ， 论文 中 K 取 200W ！       效率 提升 ： 最最 常 出现 的 P 个 单词 ， 不 使用 n - gram ！       结论       对   rare   words ,   morphologically   rich   languages   and   small   training   datasets   提升 明显 。       实现 性能比 skip - gram   baseline   慢 1.5 倍 。 105k   words / second / thread   VS   145k   words / second / thread   for   the   baseline 。       Bag   of   Tricks   for   Efficient   Text   Classification       论文 ： Bag   of   Tricks   for   Efficient   Text   Classification ,   Armand   Joulin ,   Edouard   Grave ,   Piotr   Bojanowski ,     Tomas   Mikolov         fastText   在 多 核 CPU 上 ， 训练 超过 10 亿 的 单词 ， 不到 10 分钟 ！     分类 50W 句子 ， 312K 个 类别 ， 只 需要 不到 1 分钟 ！       现有 的 神经网络 建模 句子 的 表达 ， 速度慢 ：           [ Bengio   et   al . 2003 ]       Ronan   Collobert   and   Jason     Weston .   2008 .   A   unified   architecture   for   natural   language     processing :   Deep   neural   networks   with   multitask     learning .   In   ICML .           线性 模型 建模 ， 速度 快 ：   ( Mikolov   et   al . ,   2013 ;   Levy   et   al . ,   2015 ) .     通过 加入 n - gram   信息 ， 可以 将 线性 模型 的 性能 提高 到 和 深度 模型 接近 ， 但是 速度 快 几个 量级 ！       模型 结构       baseline ： 将 句子 表达 为 bag   of   word ， 然后 训练 一个 线性 模型   LR   or   linearSVM 。     线性 模型 不能 共享 权值 ， 导致 泛化 能力 不 强 。               Hierarchical   softmax   优化 ， 略       N - gram   features ： n - gram   特征 加入 会 极大 影响 速度 ， 采用   hash   trick 解决 ！       trick 方案 论文 ： Strategies   for   training   large   scale   neural   network   language     models .          ", "tags": "machine-learning", "url": "/wiki/machine-learning/fast-text.html"},
      
      
      {"title": "FFM", "text": "    Table   of   Contents           关于           原理                 关于       相比 于 逻辑 回归 ， FM 是 一个 二阶 模型 。       原理       线性 模型 ： 逻辑 回归       $ $     \ \ phi ( w ,   x )   =   w ^ T   x   \ \ \ \     y   =   sigmoid ( \ \ phi ( w ,   x ) )     $ $       考虑 所有 二阶 组合       $ $     \ \ phi ( w ,   x )   =   x ^ T   W   x     $ $       上 式 计算 量 为 O ( n ^ 2 ) ， 通过 将 交叉 项 变换 为 平方差 ， 然后 合并 同类项 ， 可以 将 计算 量减少 到 O ( n ) 线性 时间 复杂度 。       FM ： 将   W   分解 为 两个 低 秩 矩阵 的 乘积 ！       $ $     \ \ phi ( w ,   x )   =   x ^ T   W ^ T   W   x ,   W   \ \ in   \ \ mathbb { R } ^ { k   \ \ times   n }   \ \ \ \     W   =   ( w _ 1 ,   w _ 2 ,   ... ,   w _ n ) ,   w _ i   \ \ in   \ \ mathbb { R } ^ { k }     $ $       FFM ： 对 每 一个 特征 ， 将 它 划分 到 某个 feild ， 一个 特征 和 不同 field 的 特征 交叉 时 ， 使用 不同 的   $ ( w ) $ 。       $ $     W _ { i , j }   =   w _ { i ,   f _ j } ^ T   w _ { j ,   f _ i }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ffm.html"},
      
      
      {"title": "FTRL", "text": "    Table   of   Contents               参考 ： Follow - the - Regularized - Leader   and   Mirror   Descent :   Equivalence   Theorems   and   L1   Regularization ， H .   Brendan   McMahan     Google ,   Inc .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ftrl.html"},
      
      
      {"title": "GAN: 生成对抗网络", "text": "    Table   of   Contents           GAN           理论 结论                         GAN       论文 ： Generative   Adversarial   Nets ， an   J .   Goodfellow ,   Jean   Pouget - Abadie ∗ ,   Mehdi   Mirza ,   Bing   Xu ,   David   Warde - Farley ,   Sherjil   Ozair † ,   Aaron   Courville ,   Yoshua   Bengio ‡ ， 2014           生成 模型 G ， 对于 随机 noise   z ， 分布 $ ( p _ z ( z ) ) $ ， 生成 一个 样本   $ ( G ( z ;   \ \ theta _ g ) ) $       判别 模型 D ， 是 一个 常规 的 二 分类 模型 ， 输出 是 样本 来自 真实 数据 的 概率 ， $ ( D ( x ;   \ \ theta _ d ) ) $       极大 似然 估计 ， 来自 样本 的 认为 是 正例 ， 来自 生成 模型 的 认为 是 负例       minmax   value   function   $ ( V ( D ,   G ) ) $           $ $     \ \ min _ G   \ \ max _ D   V ( D ,   G )   =   \ \ mathbf { E }   _   { x   \ \ in   p _ { data } ( x ) } [ \ \ log   D ( x ) ]   +   \ \ mathbf { E }   _   { x   \ \ in   p _ { z } ( z ) } [ \ \ log ( 1 - D ( G ( z ) ) ) ]     $ $       训练 算法 ：               理论 结论           全局 最优 ： $ ( p _ g   =   p _ { data } ) $       对 固定 的 G ，           $ $     V ( G , D )   =   \ \ int _ x   dx   p _ { data } ( x )   \ \ log ( D ( x ) )   +   p _ g ( x )   \ \ log ( 1   -   D ( x ) )     $ $       容易 验证 ， 最优 的 D 满足       $ $     D ^ *   _   G ( x )   =   \ \ frac { p _ { data } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) }     $ $       最优   value   function 为       $ $     C ( G )   =   \ \ mathbf { E }   _   { x   \ \ in   p _ { data } ( x ) } [ \ \ log   \ \ frac { p _ { data } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) } ]   +   \ \ mathbf { E }   _   { x   \ \ in   p _ { z } ( z ) } [ \ \ log ( \ \ frac { p _ { g } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) } ) ]   \ \ \ \     =   -   \ \ log   4   +   2   JSD ( p _ { data }   | |   p _ g )     $ $       JSD   是   Jensen – Shannon   divergence .   上 式 最优 的 结果 是   $ ( - \ \ log4 ) $ ， 当   $ ( p _ g   =   p _ { data } ) $   取得 。           算法   Algorithm   1   的 收敛性      ", "tags": "machine-learning", "url": "/wiki/machine-learning/gan.html"},
      
      
      {"title": "Keras 深度学习库", "text": "    Table   of   Contents           关于           快速 入门   Step   by   step           配置           Sequential   model           Sequential   model   的 属性 和 方法           The   Merge   layer           编译 模型           训练           模型 评估           模型 预测           序列 模型 的 例子                   functional   API           多 输入 多 输出 模型           Shared   layers   共享 层           layer   node           一些 例子                   Layers           对 Layer 的 抽象           内置 的 核心 Layers                   TensorFlow   API           模型 可视化           sklearn   API                 关于       据说 pylearn2 停止 开发 了 ， 当时 觉得 pylearn2 虽然 编码 少 ， 但是 配置 和 文档 使用 不便 ， 而且 和 其他 库 的 融合     也 不 方便 。 后来 看到 有人 推荐   keras   ， 了解 了 一下 ， 发现 很 不错 。 他 的 底层 编译 采用   theano   ， 现在 也 加入     了   tensorflow   的 支持 。 并且 还 可以 与   scikit - learn   融合 ， 将   keras   的 模型 包装 成   scikit - learn       里面 的 模型 。 基于 这两点 ， 决定 学习 这个 库 ， 初试 了 一下 ， 感觉 很 不错 。       快速 入门   Step   by   step       快速 入门教程 参考 官方 文档   http : / / keras . io         Step1 .   创建   Sequential   模型 ， 通过   Sequential . add   方法 添加 层 。               from       keras . models       import       Sequential       model       =       Sequential     ( )         from       keras . layers       import       Dense     ,       Activation       model     .     add     (     Dense     (     output _ dim     =     64     ,       input _ dim     =     100     ) )       model     .     add     (     Activation     (     & quot ; relu & quot ;     ) )       model     .     add     (     Dense     (     output _ dim     =     10     ) )       model     .     add     (     Activation     (     & quot ; softmax & quot ;     ) )                 Step2 .   编译 模型 ， 可以 指定 优化 方法 和 损失 函数 等               model     .     compile     (     loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,       optimizer     =     &# 39 ; sgd &# 39 ;     ,                                 metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )                 Step3 .   调用   fit   方法 训练 模型 ， 这里 采用 随机 生成 的 数据               x _ train       =       np     .     random     .     randn     (     1000     ,     100     )       y _ labels       =       np     .     random     .     randint     (     0     ,     10     ,     size     =     (     1000     , ) )       y _ train       =       np     .     zeros     ( (     1000     ,     10     ) )       y _ train     [     range     (     1000     ) ,       y _ labels     ]       =       1         model     .     fit     (     x _ train     ,       y _ train     ,       nb _ epoch     =     5     ,       batch _ size     =     20     )                 Step   4 .   模型 预测 ， 这里 采用 另一组 随机 生成 的 数据               x _ test       =       np     .     random     .     randn     (     1000     ,     100     )       y _ labels       =       np     .     random     .     randint     (     0     ,     10     ,     size     =     (     1000     , ) )       y _ test       =       np     .     zeros     ( (     1000     ,     10     ) )       y _ test     [     range     (     1000     ) ,       y _ labels     ]       =       1         classes       =       model     .     predict _ classes     (     x _ test     ,       batch _ size     =     20     )       proba       =       model     .     predict _ proba     (     x _ test     ,       batch _ size     =     20     )                 配置       配置文件     ~ / . keras / keras . json             从 theano 切换 到 TensorFlow ， 将   backend   的 值 修改 为   tensorflow   即可 ， 默认 是   theano             Sequential   model       所谓   Sequential   模型 ， 就是 多个 layer 的 线性 堆叠 。 可以 通过 构造函数 创建 一个 多层 的 序列 模型 ，     也 可以 通过   . add ( )   方法 添加 层 。               ##   通过 构造函数 创建 模型 ， 参数 是   List [ Model ]       model       =       Sequential     ( [               Dense     (     32     ,       input _ dim     =     784     ) ,               Activation     (     &# 39 ; relu &# 39 ;     ) ,               Dense     (     10     ) ,               Activation     (     &# 39 ; softmax &# 39 ;     ) ,       ] )       ##   通过   . add ( )   添加 层       model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )       model     .     add     (     Activation     (     &# 39 ; relu &# 39 ;     ) )                     指定 输入 的 shape ， 通常 只有 第一层 必须 指定 ， 后面 的 层 都 可以 自动 获取       通过     input _ shape     指定 ， 不 需要 样本 大小 ， 见 例子       通过     batch _ input _ shape     指定 ， 需要 指定 样本 大小       2D   Layer   通过   input _ dim   指定 各维 大小 ， 3D   Layer 通过   input _ dim     和     input _ length     两个 参数 指定                           model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ shape     =     (     784     , ) ) )         model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       batch _ input _ shape     =     (     None     ,       784     ) ) )       #   note   that   batch   dimension   is   & quot ; None & quot ;   here ,       #   so   the   model   will   be   able   to   process   batches   of   any   size .         model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )           ##   3D       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       input _ shape     =     (     10     ,       64     ) ) )       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       batch _ input _ shape     =     (     None     ,       10     ,       64     ) ) )       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       input _ length     =     10     ,       input _ dim     =     64     ) )                 Sequential   model   的 属性 和 方法           compile   编译       fit   拟合       evaluate   评估       predict   预测       predict _ classes   预测 类别       predict _ proba   预测 概率       train _ on _ batch   在 一个 batch 上 更新 模型 ， online   learning   ?       test _ on _ batch       predict _ on _ batch       fit _ generator   从 一个 generator 而 不是 矩阵 拟合 模型 ， 可以 用来 拟合 数据 保存 在 磁盘 上 的 数据       evaluate _ generator   从 generator 评估 模型           The   Merge   layer       可以 通过   merge   Layer   将 多个 输出 融合 到 一起 。 融合 的 模式 可以 选择 ：           sum   ( default ) :   element - wise   sum       concat :   tensor   concatenation .   You   can   specify   the   concatenation   axis   via   the   argument   concat _ axis .       mul :   element - wise   multiplication       ave :   tensor   average       dot :   dot   product .   You   can   specify   which   axes   to   reduce   along   via   the   argument   dot _ axes .       cos :   cosine   proximity   between   vectors   in   2D   tensors .                   from       keras . layers       import       Merge         left _ branch       =       Sequential     ( )       left _ branch     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )         right _ branch       =       Sequential     ( )       right _ branch     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )         ##   mode = &# 39 ; concat &# 39 ;   表示 将 两个 tensor 链接 成 一个 长 的 tensor       merged       =       Merge     ( [     left _ branch     ,       right _ branch     ] ,       mode     =     &# 39 ; concat &# 39 ;     )         final _ model       =       Sequential     ( )       final _ model     .     add     (     merged     )       final _ model     .     add     (     Dense     (     10     ,       activation     =     &# 39 ; softmax &# 39 ;     ) )         final _ model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,       loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     )       final _ model     .     fit     ( [     input _ data _ 1     ,       input _ data _ 2     ] ,       targets     )         #   we   pass   one   data   array   per   model   input                 也 可以 采用 自定义 的 函数 进行 融合 。               merged       =       Merge     ( [     left _ branch     ,       right _ branch     ] ,       mode     =     lambda       x     ,       y     :       x       -       y     )                 编译 模型       在 训练 一个 模型 之前 ， 需要 先 编译 ， 通过 模型 的   compile   方法 进行 。 这个 函数 接受 3 个 参数 ：           optimizer ， 预定 义 优化 器 字符串 或者   Optimizer   实例 。 预定 义 的 优化 器有 ：         sgd           rmsprop           adagrad           adadelta           adam           adamax           nadam                     loss ， 损失 函数 ， 字符串 或者   Theano / TensorFlow   symbolic   function ， 传入 两个 参数 ： y _ true , y _ pred ，     传出 一个 标量 。 下面 列出 一部分 ， 更 多 参考 官方 文档     http : / / keras . io / objectives /             mse   均方 误差       mae       mape       msle       squared _ hinge       hinge   SVM 用 的 损失 函数       binary _ crossentropy   对数 损失 函数       categorical _ crossentropy   多 类别 对数 损失 函数                   metrics 列表 ， 注意 是 列表 。 也 接收 字符串 和 用户 定义 函数 。           accuracy                           #   for   a   multi - class   classification   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )         #   for   a   binary   classification   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )         #   for   a   mean   squared   error   regression   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; mse &# 39 ;     )                 训练       输入   np . ndarray   ， 调用   fit   训练 模型 。               #   train   the   model ,   iterating   on   the   data   in   batches       #   of   32   samples       model     .     fit     (     data     ,       labels     ,       nb _ epoch     =     10     ,       batch _ size     =     32     )                 模型 评估       调用     evaluate     方法 评估 。       模型 预测               classes       =       model     .     predict _ classes     (     X _ test     ,       batch _ size     =     32     )       proba       =       model     .     predict _ proba     (     X _ test     ,       batch _ size     =     32     )                 序列 模型 的 例子       序列 模型 的 更 多 例子 参考 官方 文档     http : / / keras . io / getting - started / sequential - model - guide /   。     这里 确实 有 很多 例子 ， 都 比较 短 。       functional   API       用来 解决   Sequential   模型   和   merge   无法 构建 的 复杂 模型 。           The   Keras   functional   API   is   the   way   to   go   for   defining   complex   models ,   such   as   multi - output   models ,   directed   acyclic   graphs ,   or   models   with   shared   layers .               一个   Layer   是 一个 callable 实例 ， 传入 一个 tensor ， 输出 一个 tensor       输入 tensor   和 输出 tensor   可以 用来 定义 一个 模型 ， 与   theano   的 函数 一样       上述 定义 的 模型 可以 和 Sequential   模型 一样 训练                   from       keras . layers       import       Input     ,       Dense       from       keras . models       import       Model         #   this   returns   a   tensor       inputs       =       Input     (     shape     =     (     784     , ) )         #   a   layer   instance   is   callable   on   a   tensor ,   and   returns   a   tensor       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     inputs     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       predictions       =       Dense     (     10     ,       activation     =     &# 39 ; softmax &# 39 ;     ) (     x     )         #   this   creates   a   model   that   includes       #   the   Input   layer   and   three   Dense   layers       model       =       Model     (     input     =     inputs     ,       output     =     predictions     )       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )       model     .     fit     (     data     ,       labels     )         #   starts   training                     所有 模型 都 是 callable ， 所以 可以 重用 一个 模型 。 利用   TimeDistributed   ， 可以 将 图像 的 模型 应用 到 video 处理 。                   x       =       Input     (     shape     =     (     784     , ) )       #   this   works ,   and   returns   the   10 - way   softmax   we   defined   above .       y       =       model     (     x     )         from       keras . layers       import       TimeDistributed         #   input   tensor   for   sequences   of   20   timesteps ,       #   each   containing   a   784 - dimensional   vector       input _ sequences       =       Input     (     shape     =     (     20     ,       784     ) )         #   this   applies   our   previous   model   to   every   timestep   in   the   input   sequences .       #   the   output   of   the   previous   model   was   a   10 - way   softmax ,       #   so   the   output   of   the   layer   below   will   be   a   sequence   of   20   vectors   of   size   10 .       processed _ sequences       =       TimeDistributed     (     model     ) (     input _ sequences     )                 多 输入 多 输出 模型       例如 下图                       from       keras . layers       import       Input     ,       Embedding     ,       LSTM     ,       Dense     ,       merge       from       keras . models       import       Model         #   headline   input :   meant   to   receive   sequences   of   100   integers ,   between   1   and   10000 .       #   note   that   we   can   name   any   layer   by   passing   it   a   & quot ; name & quot ;   argument .       main _ input       =       Input     (     shape     =     (     100     , ) ,       dtype     =     &# 39 ; int32 &# 39 ;     ,       name     =     &# 39 ; main _ input &# 39 ;     )         #   this   embedding   layer   will   encode   the   input   sequence       #   into   a   sequence   of   dense   512 - dimensional   vectors .       x       =       Embedding     (     output _ dim     =     512     ,       input _ dim     =     10000     ,       input _ length     =     100     ) (     main _ input     )         #   a   LSTM   will   transform   the   vector   sequence   into   a   single   vector ,       #   containing   information   about   the   entire   sequence       lstm _ out       =       LSTM     (     32     ) (     x     )         #   Here   we   insert   the   auxiliary   loss ,   allowing   the   LSTM   and   Embedding   layer       #   to   be   trained   smoothly   even   though   the   main   loss   will   be   much   higher   in   the   model .       auxiliary _ loss       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ,       name     =     &# 39 ; aux _ output &# 39 ;     ) (     lstm _ out     )         #   concat   lstm _ out   和   auxiliary _ input   作为 后续 模型 的 输入       auxiliary _ input       =       Input     (     shape     =     (     5     , ) ,       name     =     &# 39 ; aux _ input &# 39 ;     )       x       =       merge     ( [     lstm _ out     ,       auxiliary _ input     ] ,       mode     =     &# 39 ; concat &# 39 ;     )         #   we   stack   a   deep   fully - connected   network   on   top       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )         #   and   finally   we   add   the   main   logistic   regression   layer       main _ loss       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ,       name     =     &# 39 ; main _ output &# 39 ;     ) (     x     )         #   创建 这个 多 输入 多 输出 模型       model       =       Model     (     input     =     [     main _ input     ,       auxiliary _ input     ] ,       output     =     [     main _ loss     ,       auxiliary _ loss     ] )         #   编译 模型       #   We   compile   the   model   and   assign   a   weight   of   0.2   to   the   auxiliary   loss .       #   To   specify   different   loss _ weights   or   loss   for   each   different   output ,       #   you   can   use   a   list   or   a   dictionary .   Here   we   pass   a   single   loss   as   the   loss   argument ,       #   so   the   same   loss   will   be   used   on   all   outputs .       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,       loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   loss _ weights     =     [     1 .     ,       0.2     ] )       #   训练 模型       model     .     fit     ( [     headline _ data     ,       additional _ data     ] ,       [     labels     ,       labels     ] ,                           nb _ epoch     =     50     ,       batch _ size     =     32     )         #   因为 我们 为 每 一个 输出 层 设置 了 name ， 所以 也 可以 通过 字典 而 不是 list 指定 参数       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     {     &# 39 ; main _ output &# 39 ;     :       &# 39 ; binary _ crossentropy &# 39 ;     ,       &# 39 ; aux _ output &# 39 ;     :       &# 39 ; binary _ crossentropy &# 39 ;     } ,                                   loss _ weights     =     {     &# 39 ; main _ output &# 39 ;     :       1 .     ,       &# 39 ; aux _ output &# 39 ;     :       0.2     } )         #   and   trained   it   via :       model     .     fit     ( {     &# 39 ; main _ input &# 39 ;     :       headline _ data     ,       &# 39 ; aux _ input &# 39 ;     :       additional _ data     } ,                           {     &# 39 ; main _ output &# 39 ;     :       labels     ,       &# 39 ; aux _ output &# 39 ;     :       labels     } ,                           nb _ epoch     =     50     ,       batch _ size     =     32     )                 Shared   layers   共享 层       比如 训练 一个 模型 ， 预测 两个 tweets 是否 来自 同一个 人 ， 首先 可以 用 LSTM 将 两个 tweet 转换 为 两个 向量 ，     而 这个 LSTM 对 两个 tweet 都 能 用 ， 所以 可以 将 这个 LSTM 层 共享 。               from       keras . layers       import       Input     ,       LSTM     ,       Dense     ,       merge       from       keras . models       import       Model         tweet _ a       =       Input     (     shape     =     (     140     ,       256     ) )       tweet _ b       =       Input     (     shape     =     (     140     ,       256     ) )         #   this   layer   can   take   as   input   a   matrix       #   and   will   return   a   vector   of   size   64       shared _ lstm       =       LSTM     (     64     )         #   when   we   reuse   the   same   layer   instance       #   multiple   times ,   the   weights   of   the   layer       #   are   also   being   reused       #   ( it   is   effectively   * the   same *   layer )       encoded _ a       =       shared _ lstm     (     tweet _ a     )       encoded _ b       =       shared _ lstm     (     tweet _ b     )         #   we   can   then   concatenate   the   two   vectors :       merged _ vector       =       merge     ( [     encoded _ a     ,       encoded _ b     ] ,       mode     =     &# 39 ; concat &# 39 ;     ,       concat _ axis     = -     1     )         #   and   add   a   logistic   regression   on   top       predictions       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ) (     merged _ vector     )         #   we   define   a   trainable   model   linking   the       #   tweet   inputs   to   the   predictions       model       =       Model     (     input     =     [     tweet _ a     ,       tweet _ b     ] ,       output     =     predictions     )         model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )       model     .     fit     ( [     data _ a     ,       data _ b     ] ,       labels     ,       nb _ epoch     =     10     )                 layer   node       当 输入 或者 输出 只有 一个 时 ， 可以 通过     . input _ shape     和     . get _ output ,   . output ,   . output _ shape     获取 输入输出 的 信息 。     当有 多个 的 时候 ， 需要 用     . get _ output _ at ,   . get _ input _ shape _ at     替代 。       一些 例子       一些 前沿 的 例子 ， 见   http : / / keras . io / getting - started / functional - api - guide /             inception   modeule       residual   connection                   from       keras . layers       import       merge     ,       Convolution2D     ,       Input         #   input   tensor   for   a   3 - channel   256x256   image       x       =       Input     (     shape     =     (     3     ,       256     ,       256     ) )       #   3x3   conv   with   3   output   channels   ( same   as   input   channels )       y       =       Convolution2D     (     3     ,       3     ,       3     ,       border _ mode     =     &# 39 ; same &# 39 ;     )       #   this   returns   x   +   y .       z       =       merge     ( [     x     ,       y     ] ,       mode     =     &# 39 ; sum &# 39 ;     )                     Shared   vision   model       Visual   question   answering   model       Video   question   answering   model           Layers       对 Layer 的 抽象       一个 layers 需要 有 以下 方法 ：           -     . get _ weights ( )       -     . set _ weights ( )       -     . get _ config ( )         一个 Layers 可以 通过 构造函数 创建 ， 也 可以 通过 config 创建 ， 采用   layer _ utils   包中 的   layer _ from _ config ( )   函数 。       对于 单 节点 的 layer ， 可以 通过 这些 属性 获取 输入输出             -     . input       -     . output       -     . input _ shape       -     . output _ shape         对于 多 节点 的 layer ， 则 需要 使用 这些 方法 ：             -     . get _ input _ at ( idx )       -     . get _ output _ at ( idx )       -     . get _ input _ shape _ at ( idx )       -     . get _ output _ shape _ at ( idx )         内置 的 核心 Layers             Dense     简单 的 全 连接 网络层 ， 至少 需要 一个     output _ dim     参数 ， 对于 非 输入 层 ， 会 自动 获得 输入 的 维数 ；     如果 是 输入 层 ， 还 需要 指定   input _ dim   参数 。 重要 的 参数 ：         activation     激活 函数 ， 默认 是 线性 函数 ，   a ( x ) = x   ， 即 没有 非线性 变换 ， 可以 指定 激活 函数 为 预定 义 的 非线性 函数 或者 自定义 的   element - wise   的 符号 函数 。 预定 义 函数 可以 通过 字符串 指定 ， 常用 的 有 ：   sigmoid ,   relu ,   tanh ,   softmax ,   hard _ sigmoid ,   softsign ,   softplus                   Activation ( name )     name   是 激活 函数 的 名字 。 既然 Dense 可以 指定 activation 参数 ， 为什么 还要 一个 激活 层 ？ ！         Dropout ( p )     Dropout   层 ， 参数 是 dropout 的 概率 。     Dropout :   A   Simple   Way   to   Prevent   Neural   Networks   from   Overfitting           Flatten ( )     将 多维 特征 展开 为 一维 特征 ， 不会 影响 样本 维度 。 常用 在 卷积 网络 。         Reshape ( hape )     shape : Tuple ， 将 特征 尺寸 reshape ， 不 影响 样本 维度 。         Permute ( dims )     dims : Tuple [ int , int , ... ]   将 维度 重新 变换 ， 如果 dims 是 两个 元素 ， 相当于 转置 。                   model       =       Sequential     ( )       model     .     add     (     Permute     ( (     2     ,       1     ) ,       input _ shape     =     (     10     ,       64     ) ) )       #   now :   model . output _ shape   = =   ( None ,   64 ,   10 )       #   note :   ` None `   is   the   batch   dimension                       RepeatVector ( n : Int )     将 输入 重复 n 次 ，                   model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     32     ) )       #   now :   model . output _ shape   = =   ( None ,   32 )       #   note :   ` None `   is   the   batch   dimension         model     .     add     (     RepeatVector     (     3     ) )       #   now :   model . output _ shape   = =   ( None ,   3 ,   32 )                       Merge ( layers : List [ Layer ] ,   mode : String | Function ,   ... )     融合 层 。         Lambda ( func : Function ,   output _ shape : Tuple ,   args : Dict )     将 任意 符号 函数 应用 到 之前 的 层                   #   add   a   x   - & gt ;   x ^ 2   layer       model     .     add     (     Lambda     (     lambda       x     :       x       * *       2     ) )                       ActivityRegularization ( l1 = 0.0 ,   l2 = 0.0 )     添加 正则 项 ？ 怎么 添加 的 ？         Masking     不 懂 ， 貌似 跟 LSTM 层有 关系         Highway     也 不 懂 ， 貌似 跟 LSTM 层 有关         MaxoutDense     maxout   层 ， 是 线性 层 ， 不像   Dense   ， 不能 添加 激活 函数 ， 需要 在 后面 添加 激活 函数 层 。         TimeDistributedDense     不 懂 ， 貌似 在 RNN 中 有用           TensorFlow   API       模型 可视化       利用 模块     keras . utils . visualize _ util     里面 的 工具 函数 。             plot ( model ,   to _ file = filename ,   show _ shapes = False ,   show _ layer _ names = True )     保存 到 文件         model _ to _ dot ( model ,   show _ shapes = False ,   show _ layer _ names = True ) . create ( format = ' dot ' )     输出 为 dot 绘图 格式 ， 也 可以 指定   format   为 svg 等 格式 。 然后 利用   IPython . display     模块 输出 为 SVG 图像 。                   from       keras . utils . visualize _ util       import       plot       plot     (     model     ,     show _ shapes     =     True     )         from       IPython . display       import       SVG       from       keras . utils . visualize _ util       import       model _ to _ dot         SVG     (     model _ to _ dot     (     model     ,       show _ shapes     =     True     )     .     create     (     prog     =     &# 39 ; dot &# 39 ;     ,       format     =     &# 39 ; svg &# 39 ;     ) )                 sklearn   API       将   Keras   模型 封装 成 sklearn   的 API 。 两个 封装 API ， 分别 是 分类器 和 回归 器             keras . wrappers . scikit _ learn . KerasClassifier ( build _ fn = None ,   * * sk _ params )   ,   which   implements   the   sklearn   classifier   interface ,         keras . wrappers . scikit _ learn . KerasRegressor ( build _ fn = None ,   * * sk _ params )   ,   which   implements   the   sklearn   regressor   interface .             build _ fn     需要 返回 一个 模型 ， sk _ params   是 模型 参数 和   fit / predict   参数 ， 另外 需要 模型 所有 参数 都 存在 默认 参数 。     也 接受   fit ,   predict ,   predict _ proba ,   and   score   函数 的 参数 。       用 sklearn   API 封装 后 ， 就 可以 利用 sklearn 的 Gridsearch 等 工具 进行 调参 了 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/keras.html"},
      
      
      {"title": "Lambda Rank", "text": "    Table   of   Contents           背景           Rank   Net           参考                 背景       排序 问题 和 分类 回归 问题 本质 差异 在于 优化 的 目标 不同 ， 除此之外 ， 分类 回归 问题 的 模型 应该 都 能 用 。     但是 排序 问题 存在 一些 非 连续 优化 目标 ， 如 NDCG ， 直接 优化 该 目标 并 不 容易 。 Lambda   Rank 是 一种 近似 优化 方法 。       排序 问题 归根结底 也 是 对 每个 样本 打分 ， 目标 是 使得 应该 排 在 前面 的 样本 得分 更高 ！     通常 排序 会 有 一个 GROUP ， GROUP 之间 的 序 关系 没有 意义 ， 有 意义 的 是 GROUP 内部 的 序 关系 。     例如 搜索 排序 中 ， query 就是 一个 GROUP ， 对 给定 的 一个 GROUP ， 将 另外 一种 实体 e 进行 排序 。       一般 将 二元 组 ( q ,   e ) 看做 一个 样本 ， 如果 考虑 个性化 ， 还要 加上 用户 这个 实体 。 总而言之 ， 将 这些 实体 多元 组 看做 一个 样本 ，     提取 一些 列 特征 $ ( x _ i ) $ ， 然后 构建 一个 模型 $ ( f ) $ 对 它 进行 打分 $ ( f ( x ) ) $ 。       Rank   Net       将 样本 $ ( U _ i ) $ 排 在 $ ( U _ j ) $ 前面 的 概率 $ ( P ( U _ i   \ \ succ   U _ j ) ) $ 建模 成 得分 差值 的 sigmoid 函数 ！       $ $     P _ { ij }   =   P ( U _ i   \ \ succ   U _ j )   =   \ \ frac { 1 } { 1   +   e ^ { - \ \ sigma ( s _ i   -   s _ j ) } }   \ \ \ \     s _ i   =   f ( x _ i )     $ $       通过 对 所有 可 比较 的 样本 对 计算 此 概率 值 ， 然后 进行 极大 似然 估计 得到 损失 函数 。     由于 一个 样本 $ ( x _ i ) $ 会 同时 出现 在 多个 样本 对 中 ， 损失 函数 对 $ ( s _ i ) $ 的 梯度 $ ( \ \ lambda _ i ) $ 会 跟 多个 样本 对 有关 ，       $ $     \ \ lambda _ i   =   \ \ frac { \ \ partial   l } { \ \ partial   s _ i }   =   \ \ sum _ k   \ \ frac { \ \ partial   l _ k } { \ \ partial   s _ i }     $ $       参考           From   RankNet   to   LambdaRank   to   LambdaMART :   An   Overview      ", "tags": "machine-learning", "url": "/wiki/machine-learning/lambda-rank.html"},
      
      
      {"title": "Layer Normalization", "text": "    Table   of   Contents           关于           摘要           导言           不变性 分析           Geometry   of   parameter   space   during   learning           GLM   的 几何 分析                   实验 结果                 关于       见 论文     http : / / cn . arxiv . org / abs / 1607.06450         摘要       Batch   Normalization   难以 应用 在 RNN ， 因为 是 基于 输入 的 batch 进行 归一化 ，     而 RNN 输入 每次 都 只有 一个 ！       导言       分布式 神经网络 ：   Jeffrey   Dean   ,   Greg   Corrado ,   Rajat   Monga ,   Kai   Chen ,   Matthieu   Devin ,   Mark   Mao ,   Andrew   Senior ,   Paul   Tucker ,   Ke   Yang ,   Quoc   V   Le ,   et   al .   Large   scale   distributed   deep   networks .   In   NIPS ,   2012 .           In   addition   to   training   time   improvement ,   the   stochasticity   from   the   batch   statistics   serves   as   a   regularizer   during   training .           实际上 就是 讲   BN   对 batch 的 归一化 变成 了 对 神经网络 某 一层 进行 归一化 。           BN ： 一个 batch 使用 相同 的 均值 和 方差 ， 但是 不同 神经元 不同 ！ 对 batch 大小 有 要求 ， 不能 太小 ！ 对于 RNN ， 需要 在 每 一个 时隙 单独 保存 均值 和 方差 参数 ！       LN ： 同 一层 神经元 使用 相同 的 均值 和 方差 ， 但是 不同 样本 不同 ！ 对 batch 大小 没有 要求 ， 适用 于 RNN ， 不 需要 单独 保存 参数 ， 隐层 数目 不能 太小 吧           $ $     \ \ mu ^ l   =   \ \ frac { 1 } { H }   \ \ sum _ { i = 1 } ^ H   a _ i ^ l   \ \ \ \     \ \ sigma ^ l   =   \ \ sqrt { \ \ frac { 1 } { H }   \ \ sum _ { i = 1 } ^ H   ( a _ i ^ l - \ \ mu ^ l ) ^ 2   } .     $ $     H 代表 该层 的 神经元 个数 ！       对于 标准 的 RNN ， $ ( a ^ t   =   W _ { hh } h ^ { t - 1 }   +   W _ { xh }   x ^ t ) $ 。       $ $     h ^ t   =   f \ \ left [ \ \ frac { g } { \ \ sigma }   \ \ odot   ( a ^ t - \ \ mu ^ t ) +   b   \ \ right ]     $ $       $ ( b ,   g ) $   是 两个 参数 ， 需要 学习 ， 和 BN 中 类似 。       不变性 分析       BN ， LN ， WN ( weight   Normalization )   都 可以 用 下述 形式 表示 ：       $ $     h _ i   =   f ( \ \ frac { g _ i } { \ \ sigma _ i } ( a _ i - \ \ mu _ i )   +   b _ i )     $ $       对于 WN ， $ ( \ \ mu = 0 ,   \ \ sigma   =   | |   w | | _ 2 ) $                   Weight   matrix   re - scaling   and   re - centering :   对 权值 矩阵 进行 变换 ： $ ( W   \ \ rightarrow   \ \ delta   W   +   \ \ gamma ) $       Weight   vector   re - scaling   and   re - centering :   指对 权值 矩阵 的 某 一个 向量 做 上述 仿射变换 ， BN 和 WN 是 不 变得 ， 因为 他们 每个 神经元 都 有 自己 的 归一化 参数 ， 而 LN 是 变得 ， 因为 它 所有 神经元 共用 同 一组 规划 参数 。       Data   re - scaling   and   re - centering ： 指对 数据 集 的 归一化 操作 ， LN 对 单个 数据 的 rescaling 也 是 不变 的 ！ （ 因为 它 归一化 的 时候 仅 考虑 一个 样本 ！ ）           Geometry   of   parameter   space   during   learning       Under   the   KL   divergence   metric ,   the   parameter   space   is   a   Riemannian   manifold ？ ！ ！       KL 距离 下 的 黎曼 度规 可以 用 Fisher 信息 矩阵 的 二阶 近似       $ $     ds ^ 2   =   D _ { KL } ( P ( y | x ; \ \ theta )   | |   P ( y | x ;   \ \ theta   +   \ \ delta ) )   \ \ approx   \ \ frac { 1 } { 2 } \ \ delta ^ T   F ( \ \ theta )   \ \ delta   \ \ \ \     F ( \ \ theta )   =   \ \ mathbb { E } _ { x \ \ sim   P ( x ) ,   y \ \ sim   P ( y | x ) }   \ \ left [     \ \ frac { \ \ partial   \ \ log   P ( y | x ; \ \ theta ) } { \ \ partial   \ \ theta }   \ \ frac { \ \ partial   \ \ log   P ( y | x ; \ \ theta ) } { \ \ partial   \ \ theta } ^ T   \ \ right ]     $ $       GLM   的 几何 分析       广义 线性 模型 对数 似然 函数 表示 为 LN 的 输入 a 的 形式       $ $     \ \ log   P ( y | x ; w , b )   =   \ \ frac { ( a + b )   y   -   \ \ eta ( a   +   b ) } { \ \ phi }   +   c ( y ,   \ \ phi )     \ \ \ \     \ \ mathbb { E } [ y |   x ]   =   f ( a + b )     =     f ( w ^ T   x   +   b ) ,   \ \ \ \     Var [ y |   x ]   =   \ \ phi   f ' ( a + b )     $ $       这里   $ ( f ) $   是 函数 $ ( \ \ eta ' ) $ ， 为 GLM 的 均值 函数 。     假设 有 多个 独立 的 response 变量 ， $ ( y   =   [ y _ 1 ,   ... ,   y _ H ] ) $ ， 对应 多分 模型 参数 $ ( \ \ theta   =   [ w _ 1 ^ T ,   b _ 1 ,   ... ,   w _ H ^ t ,   b _ H ^ T ] ^ T ) $ ，     将 对数 似然 函数 代入 Fisher 信息 矩阵 ， 并 对 $ ( y ) $ 求 期望 ， 可 得       $ $     F ( \ \ theta )   =   \ \ mathbb { E } _ { x \ \ sim   P ( x ) }   \ \ left [     \ \ frac { Cov [ y | x ] } { \ \ phi ^ 2 }   \ \ otimes   \ \ left [   \ \ begin { matrix }                             xx ^ T   & amp ;   x   \ \ \ \                             x ^ T   & amp ;   1                     \ \ end { matrix }   \ \ right ]   \ \ right ]     $ $       采用 归一化 方法 后 ， 将 额外 的 参数 g 加到 参数 列表 中 ， $ ( \ \ theta   =   vec ( [ W ,   \ \ mathbf { b } ,   \ \ mathbf { g } ] ^ T ) ) $                   for   the   same   parameter   update   in   the   normalized   model ,   the   norm   of   the   weight   vector   effectively   controls   the   learning   rate   for   the   weight   vector .       have   an   implicit   “ early   stopping ”   effect   on   the   weight   vectors   and   help   to   stabilize   learning   towards   convergence .       Riemannian   metric   along   the   magnitude   of   the   incoming   weights   for   the   standard   GLM   is   scaled   by   the   norm   of   its   input ,   whereas   learning   the   gain   parameters   for   the   batch   normalized   and   layer   normalized   models   depends   only   on   the   magnitude   of   the   prediction   error .   Learning   the   magnitude   of   incoming   weights   in   the   normalized   model   is   therefore ,   more   robust   to   the   scaling   of   the   input   and   its   parameters   than   in   the   standard   model .           归一化 改变 曲率 ？ 还 学习 到 了 输入 幅度 ？ ！       实验 结果       在 多个 任务 上 进行 试验 ： 对 RNN 和 全 连接 网络 有效 ， 可以 加速 收敛 ， 但是 CNN 上 的 结果 不如 BN ， 作者 认为 CNN 神经元 之间 的 统计 特性 相差太大 导致 的 ，     因为 每个 神经元 都 只 链接 上 一层 一小块 区域 ， 从而 导致 同 一层 的 神经元 统计 特性 相差 很大 ！ 但是 作者 没有 报道 具体 结果 ， LN 比 BN 差 多少 也 不得而知 。           Order   embeddings   of   images   and   language ：   Ivan   Vendrov ,   Ryan   Kiros ,   Sanja   Fidler ,   and   Raquel   Urtasun .   Order - embeddings   of   images   and   language .     ICLR ,   2016 .       Teaching   machines   to   read   and   comprehend ： Karl   Moritz   Hermann ,   Tomas   Kocisky ,   Edward   Grefenstette ,   Lasse   Espeholt ,   Will   Kay ,   Mustafa   Suleyman ,     and   Phil   Blunsom .   Teaching   machines   to   read   and   comprehend .   In   NIPS ,   2015 .       Skip - thought   vectors ： Ryan   Kiros ,   Yukun   Zhu ,   Ruslan   R   Salakhutdinov ,   Richard   Zemel ,   Raquel   Urtasun ,   Antonio   Torralba ,   and   Sanja     Fidler .   Skip - thought   vectors .   In   NIPS ,   2015 .       Modeling   binarized   MNIST   using   DRAW ： K .   Gregor ,   I .   Danihelka ,   A .   Graves ,   and   D .   Wierstra .   DRAW :   a   recurrent   neural   network   for   image   generation .   arXiv : 1502.04623 ,   2015 .       Handwriting   sequence   generation ： Marcus   Liwicki   and   Horst   Bunke .   Iam - ondb - an   on - line   english   sentence   database   acquired   from   handwritten   text   on   a   whiteboard .   In   ICDAR ,   2005 .       Permutation   invariant   MNIST      ", "tags": "machine-learning", "url": "/wiki/machine-learning/layer-normalization.html"},
      
      
      {"title": "Learn to Rank", "text": "    Table   of   Contents           关于           PointWise   方法                 关于       Learn   to   Rank           Learning   to   Rank   for   Information   Retrieval ,   Tie - Yan   Liu           PointWise   方法       把 排序 问题 当做 单 样本 回归 或 分类 问题 ，  ", "tags": "machine-learning", "url": "/wiki/machine-learning/l2r.html"},
      
      
      {"title": "machine learning resource", "text": "    Table   of   Contents           Conference           Group                 Conference           NIPS     https : / / papers . nips . cc /         ICML     http : / / icml . cc /         CVPR , ICCV     http : / / www . cv - foundation . org /         ACL       AAAI     http : / / www . aaai . org / Awards / paper . php         ECCV           Group           Deepmind :     https : / / deepmind . com / research / publications /         LAMDA :   Nanjing   Univsity   & amp ;         https : / / cs . nju . edu . cn / zhouzh / zhouzh . files / publication / publication . htm           http : / / lamda . nju . edu . cn / CH . Pub . ashx                 LeCun :     https : / / arxiv . org / find / cs / 1 / au : + LeCun _ Y / 0 / 1 / 0 / all / 0 / 1        ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-link.html"},
      
      
      {"title": "Machine Learning: A probabilistic perspective", "text": "    Table   of   Contents           关于           Logistic   Regression           模型 拟合           Bayesian   logistic   regression           Drivation   of   BIC           高斯 近似           后验 预测           Online   learning                   广义 线性 模型 和 指数 族   GLM           指数 族 分布           重要性 ：           定义           极大 似然 估计           贝叶斯 统计           最大 熵 原理                   广义 线性 模型   GLM           Probit   regression           Multi - task   learning           Learn   to   rank                   Latent   linear   models           因子分析           混合 因子分析   ( Hinton   et   al .   1997 )           因子分析 的 EM 算法                   PCA           Classical   PCA           SVD           PPCA   ( Tipping   and   Bishop   1999 )                         关于       这 本书 从 概率 的 角度 阐述 机器 学习 相关 理论 ， 角度 比较 有意思 ！ 贝叶斯 学派 ？ ！       Logistic   Regression           生成 模型 ， 建模 $ ( p ( x ,   y ) ) $ ， 然后 利用 贝叶斯 公式 得到 后验 概率 $ ( p ( y | x ) ) $ .       判别 模型 ， 直接 建模 $ ( p ( y | x ) ) $ .           逻辑 回归 模型 ： $ (     p ( y | x ,   w )   =   Bernoulli ( y |   sigm ( w ^ T   x )       ) $       模型 拟合       极大 似然 MLE ：       负 对数 似然 函数 ( 也 叫 交叉 熵 )       $ $     NLL ( w )   =   -   \ \ sum _ { i = 1 } ^ N   \ \ left [         y _ i     \ \ log   \ \ mu _ i     ( 1 - y _ i )   \ \ log   ( 1 - \ \ mu _ i )           \ \ right ]     $ $       另外 一种 写法 ： $ ( y   \ \ in   \ \ { - 1 ,   + 1 \ \ } ) $ ， 那么 $ ( p ( y )   =   \ \ frac { 1 } { 1 + \ \ exp ( y   w ^ T   x ) } ) $ 。     所以       $ $     NLL ( w )   =   -   \ \ sum _ { i = 1 } ^ N   \ \ log ( 1 + \ \ exp ( y _ i   w ^ T   x ) )     $ $       closed   form :   由 常数 ， 变量 ， 通过 四则运算 ， n 次方 ， 指数 ， 对数 ， 三角函数 ， 反 三角函数 ， 经过 有限 次 运算 和 符合 得到 的 表达式 （ 通常 没有 极限 运算 ）       一个 问题 被 称为 可解 的 （ P 问题 ） ， 表示 可以 用闭 形式 解决 ！ ？       很多 累积 分布 没有 闭 形式 ， 但是 可以 通过 误差 函数 、 gamma 函数 表达 ！               NLL   无法 表达 为闭 形式 ？ 所以 ， 采用 数值 优化 ， 需要 计算 梯度 和 海森 矩阵 ！       $ $     g   =   X ^ T   ( \ \ mu   -   y )   \ \ \ \     H   =   \ \ sum _ i   \ \ mu _ i   ( 1 - \ \ mu _ i )   x _ i   x _ i ^ T   =   X   S   X ^ T     $ $       H   是 正定 的 ， 所以 NLL 是 强 凸函数 ， 且 存在 唯一 解 。       最速 下降 ， 学习 率 通过 线性 搜索 寻找 。 精确 线性 搜索 的 问题 ： zig - zag ， 当 初始 梯度 和 末端 梯度 正交 ， 现象 明显 。     $ ( \ \ eta _ k   =   \ \ arg   \ \ min _ { \ \ eta & gt ; 0 }   f ( \ \ theta   +   \ \ eta   d ) ) $   d 是 下降 的 负 梯度方向 。     精确 的 线性 搜索 会 使得 $ ( d ^ T   g   =   0 ) $ ， g 是 搜索 到 的 最佳 位置 ， 函数 f 的 梯度 ， 要么 g = 0 ， 要么 互相 垂直 ！       减少   zig - zag   的 方法 ， 动量 方法   momentum   ( $ ( \ \ theta _ k   -   \ \ theta _ { k - 1 } ) $ ) 。 heavy   ball   method       $ $     \ \ theta _ { k + 1 }   =   \ \ theta _ k   -   \ \ eta _ k   g _ k   +   \ \ mu _ k   ( \ \ theta _ k   -   \ \ theta _ { k - 1 } )   \ \ \ \     0   \ \ le   \ \ mu _ k   \ \ le   1     $ $       另外 一种 方法 是 共轭 梯度 法 ， 二次 目标 函数 $ ( f   =   \ \ theta   A ^ T   \ \ theta ) $ 常用 在 线性系统 ， 非线性 系统 不 常用 ？ ！       牛顿 法 ， 二阶 方法 ， 步长 $ ( d _ k   =   - H _ k ^ { - 1 }   g _ k ) $ 。 要求 目标 函数 强凸 ， 海森 矩阵 才 会 可逆 ！       解决方案 ： 直接 用 共轭 梯度 法 求解 最优 步长 $ ( H   d   =   - g ) $ 。 线性方程 等价 于 无约束 优化 二次 规划 $ ( 1 / 2   | |   Hd   +   g | | ^ 2 ) $ 。     利用 共轭 梯度 （ CG ） ， 迭代 到 负 曲率 的 地方 停止 迭代 ！       iteratively   reweighted   least   squares   or   IRLS   优化   for   逻辑 回归 。     在 牛顿 法中 ，       $ $     w _ { k + 1 }   =   w _ k   -   H ^ { - 1 } g _ k   \ \ \ \             =   ( X ^ T   S _ k   X ) ^ { - 1 }   X ^ T   S _ k   z _ k   \ \ \ \     z _ k   =   Xw _ k   +   S _ k ^ { - 1 } ( y - \ \ mu _ k )     $ $       恰好 是 带权 最小 二乘 问题   $ ( ( z   -   X ^ T   w ) ^ T   S   ( z   -   X ^ T   w ) ) $   的 解 。       拟 牛顿 法 ： BFGS ， L - BFGS       MAP   估计 导致 正则 项 ， 高斯 先验 ： l2 正则 。       多 分类 逻辑 回归 ， 最大 熵 模型       Bayesian   logistic   regression       需要 计算   $ ( p ( w | \ \ mathcal { D } ) ) $ ， 从而 得到 $ ( p ( y | x ,   \ \ mathcal { D } )   =   p ( y | x ,   w )   p ( w | \ \ mathcal { D } ) ) $ 。       问题 ： 逻辑 回归 没有 一个 方便 的 共轭 先验 （ 和 似然 函数 形式 相同 的 分布 ） 。     一些 解决方案 ： MCMC ， variational   inference ， expectation   propagation ( Kuss   and   Rasmussen   2005 )       拉普拉斯 近似 ， 高斯 近似 ： 假定 （ 对 数据 本身 建立 生成 模型 $ (   \ \ theta   = & gt ;   \ \ mathcal { D }     ) $ ） 后验 概率 为       $ $     p ( \ \ theta |   \ \ mathcal { D } )   =   \ \ frac { 1 } { Z }   e ^ { - E ( \ \ theta ) }     $ $       $ ( E ( \ \ theta ) ) $   叫做 能量 函数 ， 等于 $ (   -   p ( \ \ theta ,   \ \ mathcal { D } )   ) $ ， 而 $ ( Z   =   p ( \ \ mathcal { D } ) ) $ 。     利用 泰勒 级数 ， 将 能量 函数 在 最低 能量 值 $ ( \ \ theta * ) $ 附近 展开 到 二阶 项 。       $ $     E ( \ \ theta )   \ \ approx   E ( \ \ theta * )   +   ( \ \ theta   -   \ \ theta * ) ^ T   g   +   \ \ frac { 1 } { 2 }   ( \ \ theta   -   \ \ theta * ) ^ T   H   ( \ \ theta   -   \ \ theta * )     $ $       g 是 能量 函数 在 最低 能量 位置 的 梯度 ， 等于 0 ， 所以       $ $     \ \ hat { p } ( \ \ theta |   \ \ mathcal { D } )   \ \ approx   \ \ frac { 1 } { Z }   e ^ { - E ( \ \ theta * ) }   \ \ exp   \ \ left [     - \ \ frac { 1 } { 2 }   ( \ \ theta   -   \ \ theta * ) ^ T   H       ( \ \ theta   -   \ \ theta * )   \ \ right ]   \ \ \ \     =   \ \ mathcal { N } ( \ \ theta   |   \ \ theta *   ,   H ^ { - 1 } )   \ \ \ \     Z   =   p ( \ \ mathcal { D } )   \ \ approx   e ^ { - E ( \ \ theta * ) }   ( 2 \ \ pi ) ^ { D / 2 }   | H | ^ { - 1 / 2 }     $ $       最后 一个 式子 是 对 边际 分布 的     拉普拉斯 近似     ， 因此 ， 把 第一个 式子 称作 对 后验 概率 的 拉普拉斯 近似 （ 其实 更 应该 称作     高斯 近似   ） 。     高斯 近似 通常 是 一个 较 好 的 近似 ， 随着 样本数 目的 增加 ， 中心 极限 定理 可以 保证 ！ （   saddle   point   approximation     in   physics )       Drivation   of   BIC       利用 高斯 近似 ， 边际 分布 的 对数 似然 函数 为 ( 去掉 不 相关 常数 ) ：       $ $     p ( \ \ mathcal { D } )   \ \ approx   \ \ log   p ( \ \ mathcal { D } | \ \ theta ^ *   )   +   \ \ log   p ( \ \ theta ^ * )   -   \ \ frac { 1 } { 2 }   \ \ log   | H |     $ $       $ (   p ( \ \ mathcal { D } | \ \ theta ^ *   )   ) $   常 称作     Occam   factor   ， 用作 模型 复杂度 的 度量 ， 如果 假定 均匀分布 先验 ， 即 $ (   p ( \ \ theta ^ )   \ \ varpropto   1       ) $ ， 那么 可以 简化 为 极大 似然 估计 ， 可以 用 MLE 的 值 $ (   \ \ hat { \ \ theta }   ) $ 替换 $ (   \ \ theta ^ *     ) $ 。       对于 第三项 ， 有   $ (   H   =   \ \ sum _ i   H _ i ,   H _ i   =   \ \ nabla   \ \ nabla   \ \ log   p ( \ \ mathcal { D } _ i   |   \ \ theta )   ) $ ，     假定 每个 $ (   H _ i   =   \ \ hat { H }   ) $ ， 是 一个 常数 矩阵 （ 此时 样本分布 是 什么 情况 ？ ） ， 那么       $ $     \ \ log   | H |   =   \ \ log   | N   \ \ hat { H } |   =   \ \ log   ( N ^ D   \ \ hat { H } )   \ \ \ \             =   D   \ \ log   N   +   \ \ log   | \ \ hat { H } |     $ $       其中 $ ( D ) $ 是 参数 空间 的 维度 ， H 是 满 秩 的 。 最后 一项 与 N 无关 ， 可以 作为 常数 丢弃 ！ 那么 可得 BIC   score       $ $     \ \ log   p ( \ \ mathcal { D } )   =   \ \ log   p ( \ \ mathcal { D } | \ \ hat { \ \ theta } )   -   \ \ frac { D } { 2 }   \ \ log   N     $ $       高斯 近似       近似 先验   $ (     p ( w )   =   \ \ mathcal { N } ( 0 ,   V _ 0 )       ) $ ， 后验 概率 为       $ $     p ( w   |   \ \ mathcal { D } )   \ \ approx   \ \ mathcal { N } ( w | \ \ hat { w } ,   H ^ { - 1 } )     $ $       其中 $ ( \ \ hat { w } ) $ 是 极大 似然 估计值 ， $ (   \ \ hat { w }   =   \ \ arg   \ \ min _ w   E ( w )   ) $ ， $ (   E ( w )   =   -   \ \ log   p ( \ \ mathcal { D }   |   w )   -   \ \ log   p ( w )   ) $ 。     而   $ ( H   =   \ \ nabla   \ \ nabla   E ( w )   | _ { w ^ * } ) $ 。       也就是说 我们 之前 用 极大 似然 估计 出来 的 参数 ， 是 参数 后验 分布 的 期望值 ！       当 数据 是 线性 可分 的 情况 下 ， 极大 似然 估计 的 模型 参数 $ ( w ) $ 将 可以 是 任意 大 的 向量 ！ sigmoid 函数 就 变成 了 阶跃 函数 ！       后验 预测       没有 正则 项 的 预测 $ ( p ( y | x ,   \ \ hat { w } ) ) $ 是 对 参数 的 极大 似然 估计 。     带 正则 项 的 预测 $ ( p ( y | x ,   \ \ hat { w } ) ) $   是 对 参数 的 最大 后验 轨迹 。     这 两者 预测 出来 的 数值 都 只是 在 一个 参数 点 的 条件 概率 ！ ！     这个 点 可以 是 极大 似然 估计 出来 的 ， 也 可以 是 最大 后验 估计 出来 的 ！     如果 要 得到 在 数据 集上 的 后验 概率 ， 需要 计算       $ $     p ( y |   x ,   \ \ mathcal { D } )   =   \ \ int   p ( y | x , w )   p ( w |   \ \ mathcal { D } )   dw     $ $       但是 ， 这个 积分 没 难以 求解 ， 一个 简单 的 近似 是 用 w 的     后验 均值   ！       $ $     p ( y |   x ,   \ \ mathcal { D } )   \ \ approx   p ( y |   x ,   \ \ mathbb { E } [ w ] )     $ $       $ ( \ \ mathbb { E } [ w ] ) $ 称作 贝叶斯 点 ！                 蒙特卡洛 近似   ， 即 随机 采样 一些 w ， 近似 积分 ！       $ $     p ( y = 1 |   x ,   \ \ mathcal { D } )   \ \ approx   \ \ frac { 1 } { S }   \ \ sum _ { s = 1 } ^ S   sigm ( ( w ^ s ) ^ T   x )     $ $       $ ( w ^ s   \ \ sim   p ( w | \ \ mathcal { D } ) ) $   采样 自后验 分布 ！ 高斯 近似 下 ， 就 相当于 采样 高斯分布 ！     采样 多个 样本 时 ， 不但 可以 得到 较 好 的 概率 估计值 ， 可以 得到 输出 概率 的 置信区间 ！ ！ ！         probit   近似     当 w 的 后验 分布 用 高斯 近似 $ ( \ \ mathcal { N } ( w | m _ N ,   V _ N ) ) $ 时 ， 可以       $ $     p ( y = 1 |   x ,   \ \ mathcal { D } )   \ \ approx   \ \ int   sigm ( w ^ T   x )   p ( w | \ \ mathcal { D } )   dw   \ \ \ \                     =   \ \ int   sigm ( a )   \ \ mathcal { N } ( a | \ \ mu _ a ,   \ \ sigma _ a ^ 2 )       \ \ \ \     a   =   w ^ T   x   \ \ \ \     \ \ mu _ a   =   \ \ mathbb { E } [ a ]   =   m _ N ^ T   x   \ \ \ \     \ \ sigma _ a ^ 2   =   x ^ T   V _ N   x .     $ $       将 sigmoid 函数 用 probit 函数 近似 ， probit 函数 是 标准 正态分布 的 累积 分布 函数 ！     和 sigmoid 函数 非常 接近 ， 见上图 ！     采用 这种 近似 后 ， 前述 积分 可以 得到 解析 表达式 ！       $ $     \ \ int   sigm ( a )   \ \ mathcal { N } ( a | \ \ mu _ a ,   \ \ sigma _ a ^ 2 )   \ \ approx   sigm ( k ( \ \ sigma ^ 2 ) \ \ mu )   \ \ \ \     k ( \ \ sigma ^ 2 )   =   ( 1 + \ \ pi   \ \ sigma ^ 2 / 8 ) ^ { - \ \ frac { 1 } { 2 } } .     $ $       此时 ， 由于 k 小于 1 ， 因此 相当于 在 极大 似然 估计 的 概率 上 ， 在 横轴 进行 缩放 ！ 通过 $ ( \ \ sigma ) $ 控制 过 拟合 ？ ！     但是 判决 面 并 没有 变 ！         Residul   analysis ( outlier   detection )       在 回归 问题 中 ， 计算 残差 $ ( r _ i   =   y _ i   -   \ \ hat { y _ i } ) $ ， 其中 模型 估计值 $ ( \ \ hat { y _ i }   =   \ \ hat { w } ^ T   x _ i ) $ .     该 残差 应该 服从 正态分布 $ ( \ \ mathcal { N } ( 0 ,   \ \ sigma ^ 2 ) ) $ ， 从而 可以 通过   qq - plot   得到 离异 值 ？ ！       分类 问题 可以 采用 另外 的 方法 ！       Online   learning       广义 线性 模型 和 指数 族   GLM       指数 族 分布       指数 族 分布 ： 高斯 ， Bernoulli ,     gamma 分布 等 ！     非 指数 族 例子 ： Student   t   分布 ,   均匀分布       重要性 ：           finite - sized   sucient   statistics .               under   certain   regularity   conditions ,   the   exponential   family   is   the   only   family   of   distributions   with   finite - sized   sucient   statistics ,   meaning   that   we   can   compress   the   data   into   a   fixed - sized   summary   without   loss   of   information .   This   is   particularly   useful   for   online   learning ,   as   we   will   see   later .                   唯一 存在 共轭 先验 的 分布 族               least   set   of   assumptions                   The   exponential   family   can   be   shown   to   be   the   family   of   distributions   that   makes   the   least   set   of   assumptions   subject   to   some   user - chosen   constraint           定义       概率密度函数       $ $     p ( x )   =   h ( x )   \ \ exp ( \ \ theta ^ T   \ \ phi ( x )   -   A ( \ \ theta ) )   \ \ \ \             =   \ \ frac { 1 } { Z ( \ \ theta ) } h ( x )   e ^ { \ \ theta ^ T   \ \ phi ( x ) }     $ $       $ ( Z ) $ 是 归一化 因子 ， $ ( A ) $ 是 对数 归一化 因子 ， $ ( \ \ theta ) $ 被称作 自然 参数 。     $ ( \ \ phi ( x )   \ \ in   \ \ mathcal { R } ^ d ) $ 被 称为 充分 统计 向量 ( vector   of   sucient   statistics ) 。     如果 $ ( \ \ phi ( x ) = x ) $ ， 称为 自然 指数 族 。 $ ( \ \ theta   \ \ rightarrow   \ \ eta ( \ \ theta ) ) $ 。       Log   partition   function   性质 ：           $ ( A ( \ \ theta ) ) $ 的 一阶导 是 $ ( \ \ phi ( x ) ) $ 的 期望 ， 而 二阶 导是 其 协方差 矩阵 ！ 即 对数 矩母 函数 ！           极大 似然 估计       N 个 iid 的 样本 的 对数 似然 函数 为 ：       $ $     l ( \ \ theta )   =   \ \ theta ^ T   \ \ sum _ i   \ \ phi ( x _ i )   -   N   A ( \ \ theta )   +   constant     $ $       第一项 是 $ ( \ \ theta ) $ 的 线性 函数 ， 第二项 由于 其 二阶 导是 协方差 矩阵 ， 非负 ， 所以 也 为 凸函数 ， 因此 指数 族 N 个 独立 同 分布 样本 的 对数 似然 函数 为 凸函数 ，     因而 通过 极大 似然 估计 参数 ， 可以 很 容易 得到 最优 解 ！ 并且 由       $ $     \ \ nabla _ { \ \ theta }   l   =   0   \ \ \ \     \ \ nabla _ { \ \ theta }   A   =   E   \ \ phi ( X )     $ $       可 得 期望 匹配 条件       $ $     E   \ \ phi ( X )   =   \ \ frac { 1 } { N }   \ \ sum _ i   \ \ phi ( x _ i )     $ $       例如 对   Bernoulli   分布 样本 ， $ ( \ \ phi ( X ) =   \ \ mathbb { I } ( X = 1 ) ) $ ， 因此 要求 模型 预测 的 期望值 要 和 样本均值 相同 ！         Pitman - Koopman - Darmois   theorem     理论 ： 在 一些 约束 下 ， 指数 族 是 为 一个 分布 族 ， 其 充分 统计 量 是 有限 的 （ 不随 样本 个数 增加 而 增加 ） ！     例如 这里 充分 统计 量 数目 不随 N 增长 ， 一直 为   N   和   $ ( \ \ sum _ i   \ \ phi ( x _ i ) ) $ ！       贝叶斯 统计       似然 函数 具有 如下 形式 ：       $ $     p ( \ \ mathcal { D } | \ \ theta )   \ \ varpropto   g ( \ \ theta ) ^ N   \ \ exp ( \ \ eta ( \ \ theta ) ^ T   s _ N )     $ $       这里 $ ( s _ N ) $ 是 N 个 样本 之 和 ， 而 在 canonical   parameters 形式 下 为 ：       $ $     p ( \ \ mathcal { D } | \ \ eta )   \ \ varpropto   g ( \ \ eta ) ^ N   \ \ exp ( N   \ \ eta ^ T   \ \ bar { s }   -   N   A ( \ \ eta ) )     $ $       选择 共轭 先验 与 似然 函数 具有 相同 的 形式 ， 将会 使得 贝叶斯 分析 简化 ， 指数 族 是 唯一 具有 这种 性质 的 分布 族 ！ （ 不是 因为 封闭 形式 的 原因 么 ？ ）       $ $     p ( \ \ eta | v _ 0 ,   \ \ bar { \ \ tau _ 0 } )   \ \ varpropto   g ( \ \ eta ) ^ N   \ \ exp ( v _ 0   \ \ eta ^ T   \ \ bar { \ \ tau _ 0 }   -   v _ 0   A ( \ \ eta ) )     $ $       那么 后验 分布 将 为       $ $     p ( \ \ theta | \ \ mathcal { D } )   =   p ( \ \ theta |   v _ 0   + N   ,   \ \ tau _ 0   +   s _ N )     \ \ \ \     p ( \ \ eta | \ \ mathcal { D } )   =   p ( \ \ eta |   v _ 0   + N   ,   \ \ frac { v _ 0   \ \ bar { \ \ tau _ 0 }   +   N   \ \ bar { s } } { v _ 0   +   N } )     $ $       最大 熵 原理       最大 熵 原理 是 说 ， 要 选择 分布 使得 熵 最大 ， 在 约束条件 ： 对 一些 特殊 函数 （ 特征 ） 期望值 和 样本均值 相同 ！       $ $     \ \ max   - \ \ sum _ x   p ( x )   \ \ log ( p ( x ) )   \ \ \ \     s . t .   \ \ sum _ x   f _ k ( x ) p ( x )   =   F _ k ,   k = 1 , ... , m   \ \ \ \     \ \ sum _ x   p ( x ) = 1     $ $       利用 拉格朗 日 对偶 ， 及 KKT 条件 ， 易知 分布 要 满足       $ $     p ( x )   =   \ \ frac { 1 } { Z }   \ \ exp ( \ \ lambda _ k   f _ k ( x ) )     $ $       最大 熵 分布 是 指数 族 分布 ！ ！ 这个 分布 也 叫   Gibbs   分布 ！       广义 线性 模型   GLM       GLM ： 均值 函数 是 输入 的 线性组合 ， 然后 加上 一个 非线性 变换 ！ 输出 是 指数 族 分布 的 模型 ！       设 标量 响应 变量 满足 分布 ：       $ $     p ( y _ i |   \ \ theta ,   \ \ sigma ^ 2 )   =   \ \ exp   \ \ left [ \ \ frac { y _ i   \ \ theta   -   A ( \ \ theta ) } { \ \ sigma ^ 2 }   +   c ( y _ i ,   \ \ sigma ^ 2 )   \ \ right ]     $ $       $ ( \ \ theta ) $ 是 自然 参数 ， $ ( \ \ sigma ) $ 是 dispersion   parameter （ 通常 是 1 ） ， A 是   partition   function   和 指数 族 里面 的   A   一样 ！     例子 ， 逻辑 回归自然 参数 是 对数 发生 比   $ ( \ \ theta   =   \ \ log   \ \ frac { \ \ mu } { 1 - \ \ mu } ) $ ， 其中 均值 $ ( \ \ mu   =   \ \ mathbb { E } y = p ( y = 1 ) ) $ 。     均值 到 自然 参数 的 转换 函数 $ ( \ \ phi ) $ 被 指数 族 分布 函数 形式 唯一 确定 。               这个 转换 函数 的 逆函数 $ ( \ \ mu   =   \ \ phi ^ { - 1 } ( \ \ theta )   =   A ' ( \ \ theta ) ) $ 。     该值 是 输入 的 线性 变化 加上 一个 非线性 变换 $ ( g ^ { - 1 } ) $ ！       $ $     \ \ mu _ i   =   g ^ { - 1 } ( \ \ eta _ i )   =   g ^ { - 1 } ( w ^ T   x _ i )     $ $       该 非线性 变化 也 叫 均值 函数 ， 其 反函数 $ ( g ) $ 称作   link   function !   例如 在 逻辑 回归 中 均值 函数 就是 sigmoid 函数 ！     均值 函数 的 一种 选取 方式 是 $ ( g = \ \ phi ) $ ， 得到 canonical   link   function ， 此时 自然 参数 就是 输入 的 线性组合 $ ( \ \ theta _ i   =   w ^ T   x _ i ) $ ！     从而       $ $     p ( y _ i |   \ \ theta ,   w ,   \ \ sigma ^ 2 )   =   \ \ exp   \ \ left [ \ \ frac { y _ i   w ^ T   x _ i   -   A ( w ^ T   x _ i ) } { \ \ sigma ^ 2 }   +   c ( y _ i ,   \ \ sigma ^ 2 )   \ \ right ]     $ $       例子 ：             线性 回归   ， $ ( y _ i   \ \ in   \ \ mathbb { R } ,   \ \ theta _ i   =   \ \ mu _ i   =   w ^ T   x _ i ,   A ( \ \ theta ) = \ \ theta ^ 2 / 2 ) $         binomial 回归   ， $ ( y _ i \ \ in { 0 , 1 , ... , N _ i } ,   \ \ pi _ i   =   sigm ( w ^ T   x _ i ) ,   \ \ theta   =   \ \ log ( \ \ pi _ i   /   ( 1 - \ \ pi _ i ) )   =   w ^ T   x ) $         泊松 回归   ， $ ( y _ i   \ \ in   \ \ mathcal { N } ^ + ,   \ \ mu _ i   =   exp ( \ \ theta ) ,   \ \ theta = w ^ T   x ) $           $ $     \ \ mathbb { E } [ y | x , w , \ \ sigma ^ 2 ]   =   \ \ mu _ i   =   A ' ( \ \ theta )   \ \ \ \     Var [ y | x , w , \ \ sigma ^ 2 ]   =   \ \ sigma _ i ^ 2   =   \ \ sigma ^ 2   A ' ' ( \ \ theta )   \ \ \ \     $ $       极大 似然 估计 和 最大 后验 估计       对数 似然 函数 为 为 ：       $ $     l ( w )   =   \ \ log   p ( \ \ mathcal { D } | w )   =   \ \ frac { 1 } { \ \ sigma ^ 2 } \ \ sum _ { i = 1 } ^ N   l _ i   \ \ \ \     l _ i   =   \ \ theta _ i   y _ i   -   A ( \ \ theta _ i )     $ $       当 使用   canonical   link   函数 时 ， 对数 似然 函数 的 梯度 为 ：       $ $     \ \ nabla _ w   l   =   \ \ frac { 1 } { \ \ sigma ^ 2 }   \ \ sum _ { i = 1 } ^ N   ( y _ i   -   \ \ mu _ i )   x _ i     $ $       他 是 用 误差 对 样本 加权 ， 然后 求和 得到 ！ 上式 为 0 的 时候 ， 就是 最大 熵 方法 的 约束条件 ： 特征 的 经验 均值 和 期望值 相等 ！       嗨 森 矩阵 为 ：       $ $     H   =   X ^ T   S   X   \ \ \ \     S   =   diag \ \ { ...   \ \ frac { d \ \ mu _ i } { d \ \ theta _ i }   ... \ \ }     $ $       Fisher   scoring   method ?       带 高斯 先验 的 最大 后验 相当于 增加 了 l2 正则 项 ！       Probit   regression       将 逻辑 回归 里面 的 sigmoid 函数 换成 一般 的 函数 ： $ ( f :   \ \ mathbb { R }   \ \ rightarrow   [ 0 , 1 ] ) $     而 均值 函数 $ ( g ^ { - 1 }   =   \ \ Phi ) $ 是 标准 正态 cdf 。 这个 函数 跟 sigmoid 函数 很 像 ， 都 是 S 型函数 ！               隐 变量 解释 ： 设 存在 两个 隐 变量 $ ( u _ { 0 , i } ,   u _ { 1 , i } ) $ ， 随机 选择 模型       $ $     u _ { 0 , i }   =   w _ 0   ^ T   x _ i   +   \ \ delta _ { 0 , i }   \ \ \ \     u _ { 1 , i }   =   w _ 1   ^ T   x _ i   +   \ \ delta _ { 1 , i }     \ \ \ \     y _ i   =   \ \ mathbb { I } ( u _ { 1 , i }   & gt ;   u _ { 0 , i } )     $ $       对 两隐 变量 作差 ， 可 得 差分 随机 选择 模型 ， 令 $ ( \ \ epsilon _ i   =   \ \ delta _ { 1 , i }   -   \ \ delta _ { 0 , i } ) $ ， 且 假定 服从 标准 正态分布       $ $     p ( y _ i = 1 | x _ i ,   w )   =   \ \ Phi ( w ^ T   x _ i )     $ $       推广 到 有序 的   probit   回归 ， 响应 变量 是 多个 且 有序 的 情况 下 ！       Multinomial   probit   models ： 直接 看 模型 数学公式       $ $     z _ { ic }   =   w ^ T   x _ { ic }   +   \ \ epsilon _ { ic }   \ \ \ \     \ \ epsilon   \ \ sim   \ \ mathcal { N } ( 0 ,   R )   \ \ \ \     y _ i   =   \ \ arg   \ \ max _ c   z _ { ic }     $ $       Multi - task   learning       如果 不同 任务 的 输入 相同 ， 目标 相关 的 情况 下 ， 可以 同时 训练 这 几个 模型 ， 同时 优化 参数 ， 可以 得到 更好 的 性能 ！           multi - task   learning   ( Caruana   1998 )       transfer   learning   ( e . g . ,   ( Raina   et   al .   2005 ) )       learning   to   learn   ( Thrun   and   Pratt   1997 )           Hierarchical   Bayes   for   multi - task   learning ：     设 响应 变量 为 $ ( y _ { ij } ) $ ， i 为 样本 指标 ， j 为 任务 指标 ， 关联 的 特征向量 为 $ ( x _ { ij } ) $ 。     用 GLM 建模   $ ( \ \ mathbb { E } [ y _ { ij } | x _ { ij } ]   =   g ( x _ { ij } ^ T   \ \ beta _ j ) ) $ 。     由于 有 多个 任务 需要 多个 $ ( \ \ beta ) $ 。     可以 单独 训练 每 一个 模型 ， 但 在 实际 问题 中 ， 比如 商品 偏好 模型 ， 由于 长尾 分布 的 原因 ， 某些 商品 数据 很多 ， 而 其他 的 很少 ！     对于 数据 少 的 模型 ， 训练 很 困难 。 可以 通过 隐 变量 ， 让 这些 数据共享 ！     具体做法 是 控制 模型 参数 的 先验 分布 ： $ ( \ \ beta _ j   \ \ sim   \ \ mathcal { N } ( \ \ beta _ * ,   \ \ sigma _ j ^ 2   I ) ,   \ \ beta _ *   \ \ sim   \ \ mathcal { N } ( \ \ mu ,   \ \ sigma _ * ^ 2   I ) ) $     每个 模型 的 参数 通过 先验 分布 参数 $ ( \ \ beta _ * ) $ 联系 到 一起 。     可以 通过 交叉 验证 选取 $ ( \ \ mu ,   \ \ sigma _ j ,   \ \ sigma _ * ) $ 。       案例 ：   个性化 垃圾邮件 过滤         对 每 一个 用户 需要 训练 一个 模型 参数 $ ( \ \ beta _ j ) $ ， 但 由于 用户 标记 通常 很少 ， 因此 难以 单独 训练 ， 可以 采用   multi - task   learning 。       $ $     \ \ mathbb { E } [ y _ i   |   x _ i ,   u = j ]   =   ( \ \ beta _ * ^ T   +   w _ j ) ^ T   x _ i     $ $       这里 $ ( w _ j   =   \ \ beta _ j   -   \ \ beta _ * ) $ 用来 估计 个性化 的 部分 ！       Learn   to   rank       查询 q 和 文档 d 的 相关性 ， 标准 方法 ： bag   of   word   概率 语言 模型 （ 这 不是 朴素 贝叶斯 吗 ？ ）       $ $     sim ( q ,   d )   =   p ( q   |   d )   =   \ \ Pi _ { i = 1 } ^ n   p ( q | q _ i )     $ $       The   pointwise   approach :   对 每 一个 q 和 文档 d ， 生成 一个 特征向量 $ ( x ( q ,   d ) ) $ ，     学习 模型 $ ( p ( y = 1 |   x ( q , d ) ) ) $ ， 用 概率 进行 排序 。       pairwise ：   学习   $ ( p ( y _ { jk } = 1 |   x ( q ,   d _ j ) ,   x ( q ,   d _ k ) ) ) $ ， $ ( y _ { jk } = 1 ) $ 表示 文档 j 相关度 大于 文档 k 相关度 ，     一种 建模 方法 是 ：       $ $     p ( y _ { jk } = 1 |   x ( q ,   d _ j ) ,   x ( q ,   d _ k ) )   =   sigm ( x ( q ,   d _ j )   -   x ( q ,   d _ k ) )     $ $       关键 工作 ： RankNet   ( Burges   et   al .   2005 )       好处 ， 人 比较 两个 文档 哪个 更 相关 比 打分 更 容易 ， 因而 标注 数据 准确性 更高 ？       The   listwise   approach ： 学习 一个 排列 $ ( \ \ pi ) $ 。 分布       $ $     p ( \ \ pi | s )   =   \ \ Pi _ { j = 1 } ^ m   \ \ frac { s _ j } { \ \ sum _ { u = j } ^ m   s _ n }     $ $       $ ( s _ j     =   s ( \ \ pi ^ { - 1 } ( j ) ) ) $   是 文档 排列 在 第 j 个 位置 的 时候 的 score ！     例如 $ ( \ \ pi   =   ( A ,   B ,   C ) ) $ ， 那么 排列 的 概率 为 A 排在 第一 的 概率 乘以 B 排在 第二 的 概率 乘以 C 排在 第三 的 概率       $ $     p ( \ \ pi | s )   =   \ \ frac { s _ A } { s _ A + s _ B + s _ C }   \ \ times   \ \ frac { s _ B } { s _ B + s _ C }     \ \ times   \ \ frac { s _ C } { s _ C }     $ $       而 这个 score 可以 通过 模型 学习 $ ( s ( d )   =   f ( x ( q ,   d ) ) ) $ ， 通常 取为 线性 模型 $ ( w ^ T   x ) $ 。       ListNet   Cao   et   al .   2007         Latent   linear   models       因子分析       实值 隐 变量   $ ( z _ i   \ \ in   \ \ mathbb { R } ^ L ) $ ， 其 先验 分布 假设 为 高斯 （ 后面 会 假设 为 其他 分布 ）       $ $     p ( z _ i )   =   \ \ mathcal { N } ( z _ i   |   \ \ mu _ 0 ,   \ \ Sigma _ 0 )     $ $       观测 变量   $ ( x _ i   \ \ in   \ \ mathbb { R } ^ D ) $ ， 假设 其 服从 高斯分布 ， 其 均值 是 隐 变量 的 线性 函数 ！       $ $     p ( x _ i   |   z _ i ,   \ \ theta )   =   \ \ mathbb { N } ( W   z _ i   +   \ \ mu ,   \ \ Phi )     $ $       W   被 称为   factor   loading   matrix ，   而应 变量 称为 因子 ， 被 强制 要求 为 能够 解释 观测 变量 之间 的 相关性 ， 此时 $ ( \ \ Phi ) $ 是 对角 的 ！     一个 特例 是 $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $ ， 为   Probabilistic   Principal   Components   Analysis 。           因子分析 作为 协方差 的 低 秩 矩阵 分解           边际 分布       $ $     p ( x _ i   |   \ \ theta )   =   \ \ mathbb { N } ( x _ i   |   W \ \ mu _ 0   +   \ \ mu , \ \ Phi   +   W \ \ Sigma _ 0   W ^ T )     $ $       不是 一般性 ， 可以 假设 $ ( \ \ mu _ 0   =   0 ,   \ \ Sigma _ 0   =   1 ) $ ， 一般 情况 可以 作 变量 代换 得到 这种 形式 ， 即 要求 隐 变量 z 是 标准 正态分布 ， 且 独立 同 分布 。     因此 可 得 协方差 矩阵 为       $ $     \ \ text { Cov } ( x )   =   W   W ^ T   +   \ \ Phi     $ $       从 上式 可以 看到 ， 因子分析 只 使用 了   O ( LD   +   D ) 个 参数 ， $ ( \ \ Phi ) $   是 对角 的 ！       因子分析 的 目的 是 为了 通过 隐 变量 z 得到 有用 的 信息 ， 其后 验 分布 为       $ $     p ( z _ i |   x _ i ,   \ \ theta )   =   \ \ mathcal { N } ( z _ i   |   m _ i ,   \ \ Sigma _ i )     \ \ \ \     \ \ Sigma _ i   =   ( \ \ Sigma _ 0   +   W ^ T   \ \ Phi ^ { - 1 }   W ) ^ { - 1 }       \ \ \ \     m _ i   =   \ \ Sigma _ i ( W ^ T   \ \ Phi ^ { - 1 }   ( x _ i   -   \ \ mu )   +   \ \ Sigma _ 0 ^ { - 1 }   \ \ mu _ 0 )     $ $       m   被称作 隐 因子   or   latent   score .         W 不是 唯一 的 ！ ！     例如 可以 通过 做 一个 正交变换 R ， RW   仍然 是 有效 的 ！ 解决 方法 ， 增加 约束 。           要求 W 是 正交 的 ， 然后 按照   latent   factor   的 方差 排序 ：   PCA       要求 W 是 下 三角 ， 即 每个 观测 变量 只 与 前面 的 因子 有关       稀疏 约束 ： l1   regularization   ( Zou   et   al .   2006 ) ,   ARD   ( Bishop   1999 ;   Archambeau   and   Bach   2008 ) ,   or   spike - and - slab   priors   ( Rattray   et   al .   2009 ) .       Choosing   an   informative   rotation   matrix .   varimax       非 高斯 先验   for   隐 变量 ： ICA           因子 旋转 ：     http : / / www . cis . pku . edu . cn / faculty / vision / zlin / Courses / DA / DA - Class7 . pdf         混合 因子分析   ( Hinton   et   al .   1997 )       因子 模型 假设 数据 是 嵌入 在 低维 线性 流形 之中 ！ 而 实际上 大多数 时候 是 曲线 流形 ！     曲线 流形 可以 通过 分片 线性 流行 近似 。               有 K 个 FA 模型 ， 对应 维度 为 $ ( L _ k ) $ ， 参数 $ ( W _ k ) $ ，     隐 变量   $ ( q _ i   \ \ in   \ \ { 1 , 2 , ... , K \ \ } ) $ 。       $ $     p ( x _ i |   z _ i ,   q _ i   =   k ,   \ \ theta )   =   \ \ mathcal { N } ( x _ i   |   \ \ mu _ k   +   W _ k   z _ i ,   \ \ Phi )   \ \ \ \     p ( z _ i |   \ \ theta )   =   \ \ mathcal { N } ( z _ i |   0 ,   I )   \ \ \ \     p ( q _ i |   \ \ theta )   =   \ \ text { Cat } ( q _ i |   \ \ pi )     $ $       因子分析 的 EM 算法       在 E 步 ， 根据 当前 的 参数 ， 对 每 一个 数据 计算 它 来自 cat   c 的 概率 ：       $ $     r _ { ic }   =   p ( q _ i = c |   x _ i ,   \ \ theta )   \ \ propto   \ \ pi _ { c }   \ \ mathcal { N } ( x _ i | \ \ mu _ c ,   W _ cW _ c ^ T   +   \ \ Phi )   \ \ \ \     $ $       在 M 步 ， 利用 E 步 估计 的 c ， 可以 分别 计算 出 每个 cat 的 参数 $ ( \ \ mu _ c ,   W _ c ) $ ， 以及 新 的 $ ( \ \ pi _ { c } ) $ 。       PCA       ( Tipping   and   Bishop   1999 ) :   当   $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $   并且   W   矩阵 是 正交 的 ， 随着 $ ( \ \ sigma ^ 2   \ \ rightarrow   0 ) $ ，     模型 就 变为 principal   components   analysis （ PCA ，   KL 变化 ） 了 ！ $ ( sigma ^ 2   & gt ; 0   ) $ 的 版本 成为 概率 PCA （ PPCA ） 。       Classical   PCA       最小化 重构 误差       $ $     \ \ min   J ( W ,   Z )   =   \ \ sum _ { i = 1 } ^ N   | |   x _ i   -   \ \ hat { x _ i } | | ^ 2   =   | |   X   -   WZ ^ T   | | ^ 2 _ F   \ \ \ \     s . t .   W ^ T   W   =   I _ L   .     $ $       F 表示 Frobenius 范数 。       SVD       略 ： truncated   SVD       PPCA   ( Tipping   and   Bishop   1999 )       因子 模型 中 ， 当   $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $ ， 并且 W 为 正交 阵 ， 那么 观测 数据 的 对数 似然 函数 为       $ $     \ \ log   p ( X |   W ,   \ \ sigma ^ 2 )   & amp ; =   -   \ \ frac { N } { 2 }   \ \ log   | C |   -   \ \ frac { 1 } { 2 }   \ \ sum _ { i = 1 } ^ N   x _ i ^ T   C ^ { - 1 }   x _ i   \ \ \ \       & amp ; =     -   \ \ frac { N } { 2 }   \ \ log   | C |   -   \ \ frac { 1 } { 2 }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/machine-learning-a-probabilistic-perspective.html"},
      
      
      {"title": "MaxOut 网络", "text": "    Table   of   Contents           关于           Maxout   networks   论文 导读           导言           Review   of   Dropout           Maxout   模型           效果           Keras   实现                         关于       2013 年 ，   Ian   Goodfellow ,   Yoshua   Bengio   提出 maxout 网络 ， 号称 只 需要 两个 隐层 ， 就 可以 逼近 任意 分片 连续函数 。       Maxout   networks   论文 导读       论文 地址     maxout   networks         导言       Dropout   是   Hinton   2012   年 提出 的 一种 简单 的 方法 ，   模型 平均 ？ ！           We   argue   that   rather   than   using   dropout   as   a   slight     performance   enhancement   applied   to   arbitrary   models ,     the   best   performance   may   be   obtained   by   directly     designing   a   model   that   enhances   dropout ’ s   abilities   as     a   model   averaging   technique           Review   of   Dropout       对于 给定 的 输入 向量 $ ( v ) $ ， 预测 输出 $ ( y ) $ ， 一系列 的 隐层   $ ( \ \ vec { h }   =   {   h ^ { ( 1 ) } ,   ... ,   h ^ { ( L ) }   } ) $ 。     Dropout   用 变量 $ ( v ,   \ \ vec { h } ) $   的 一部分 变量 ， 训练 一组 模型 （ 这是 随机 置 0 的 高级 理解 ！ ！ ？ ） 。     这些 模型 采用 同一个 模型 参数   $ ( \ \ theta   \ \ in   \ \ mathcal { M } ) $ ， 和 一个二元   mask   变量   $ ( \ \ mu ) $ 。     对于 每 一个 样本 ， 对 不同 的 随机 参数 $ ( \ \ mu ) $ ， 我们 通过 $ ( log   p ( y | v ;   \ \ theta ,   \ \ mu ) ) $   的 梯度 训练 一个     子 模型 。       Dropout   很 像   bagging   的 方法 ， 在 数据 集 的 很多 子集 上 训练 一组 模型 。     不同 的 是 ，   Dropout   每 一个 模型 只 训练 一步 ， 并且 不同 模型 共享 一个 参数 。     因此 ， 在 训练 的 时候 ， 每 一步 需要 比较 大 的 影响 （ 把 步长 调大 ？ ） 。       Maxout   模型       Maxout   模型 也 是 一个 多层 的 前馈 结构 ， 它 的 基本 单元 是   Maxout   unit 。     对 给定 的   $ ( x   \ \ in   \ \ mathbb { R } ^ d ) $ ， 一个   maxout   隐层 为 ：       $ $     h _ i ( x )   =   \ \ max _ { j   \ \ in   [ 1 ,   k ] }   z _ { i , j }   \ \ \ \     z _ { i ,   j }   =   x ^ T   W _ { . ij }   +   b _ { ij } ,   W   \ \ in   \ \ mathbb { R } ^ { d   \ \ times   m   \ \ times   k } ,   b   \ \ in   \ \ mathbb { R } ^ { m \ \ times   k }     $ $       相当于 $ ( k ) $ 个 常规 的 纺射 变换 （ 一个 线性 变化 $ ( W ) $   加上 平移   $ ( b ) $ ） ， 然后 取 每 一个 模型 的 最大值 作为 最终 的 输出 。       单个   Maxout   单元 可以 理解 为 ， 对 凸 的 分片 连续函数 的 近似 。           Stone - Weierstrass   函数 逼近 理论 说 ， 紧集 （ 闭 区间 的 推广 ） 上 的 连续函数 可以 用 连续 的 分片 线性 函数 ( PWL )     一致 逼近   。       Wang   2004   的 理论 表明 ， 任意 连续 分片 线性 函数 可以 表达 为 两个 凸 的 分片 线性 函数 之差 。           个人 总结 ： Maxout   模型 很 简单 ， 理论 也 容易 理解 ， 就是 要 逼近 效果 好 ， 需要 $ ( k ) $ 值 足够 大 ， 这 将 会 导致 参数 个数 随 $ ( k ) $ 增大 ， 而 线性 增长 。       效果           MNIST   手写 数字 数据 集 两层   Conv .   maxout   +   dropout   得到 最佳 效果   0.45 %           Keras   实现         https : / / github . com / fchollet / keras / blob / master / keras / layers / core . py # L807                 def       call     (     self     ,       x     ,       mask     =     None     ) :               #   no   activation ,   this   layer   is   only   linear .               output       =       K     .     dot     (     x     ,       self     .     W     )               if       self     .     bias     :                       output       + =       self     .     b               output       =       K     .     max     (     output     ,       axis     =     1     )               return       output        ", "tags": "machine-learning", "url": "/wiki/machine-learning/maxout-network.html"},
      
      
      {"title": "Memory Network", "text": "    Table   of   Contents           关于           Memory   Network           A   MemoryNN   Implement   for   Text           基本 模型           训练           词 序列 作为 输入           EFFICIENT   MEMORY   VIA   HASHING           MODELING   WRITE   TIME           对于 unseen 词 的 处理           精确 匹配           结果                   End - To - End   Memory   Networks           Neural   Turing   Machine           基础 研究           Recurrent   Neural   Networks           神经 图灵机 结构           寻址 机制                   实验 结果           未 解决 的 问题                 关于       RNN 相当于 给 神经网络 加 了 记忆 单元 ， 做个 类比 ， 普通 的 前馈 神经网络 （ DNN ， CNN ） 就 好像 是 组合 逻辑电路 ；     而 增加 了 记忆 单元 的   RNN   ( LSTM   etc ) 就 好像 是 时序 逻辑电路 。 有 了 记忆 单元 的 神经网络 就 能够 像 图灵机     一样 记忆 、 推断 等 高级 能力 ， 而 不仅仅 是 像 组合 逻辑 那样 只能 学 一个 数学 函数 ！       Memory   Network           [ 1 ]   Memory   Network ,   Jason   Weston ,   Sumit   Chopra   & amp ;   Antoine   Bordes ,   Facebook   AI   Research   2015 .       [ 2 ]   End - To - End   Memory   Networks ,   Sainbayar   Sukhbaatar ,   Arthur   Szlam ,   Jason   Weston ,   Rob   Fergus   2015 .           RNN   的 缺点 在于 记忆 单元 太小 ， 所以 记忆 能力 很 弱 ， 比如 最 简单 的 问题 ： 输入 一串 单词 ， 输出 刚 看到 的 单词 ， RNN 都 难以实现 ( Zaremba   & amp ;   Sutskever ,   2014 )       结构 ： 一个 记忆 单元 集合   $ (       m   =   ( m _ 1 , m _ 2 ,   ... )       ) $ ， 4 个 模块 ：           I   :   input   feature   map ,   将 输入 转换 为 中间 特征 表达 。       G   :   generalization ,   根据 新 的 输入 更新 存储单元 的 内容 。       O   :   output   feature   map ,   根据 当前 的 输入 和 存储单元 内容 ， 生成 输出 结果 的 特征 空间 表达 。       R   :   response ,   将 输出 的 特征 空间 表达 转换 为 目标 形式 ， 例如 一个 文本 ， 或者 一个 动作 。           给定 一个 输入   x , 例如 一个 句子 ， 一个 视频 ，           首先 将 输入 转换 为 中间 表达   $ (   I ( x )   ) $       对 新 的 输入 x ， 更新 内存   $ ( m _ i   =   G ( m _ i ,   I ( x ) ,   m ) ,   \ \ forall   i ) $       对 新 的 输入 x   和 内存 m ， 计算 输出 向量   $ (   o   =   O ( I ( x ) ,   m )   ) $       最后 解码 输出 序列   r   =   R ( o )           A   MemoryNN   Implement   for   Text       如果 上述 四个 组件 是 用 神经网络 来 实现 ， 就 称作   Memory   NN ， 这些 组件 可以 用 任何 机器 学习 的 方法 ！       基本 模型       输入 是 一个 句子 ( I ) ， 文本 被 存在 下 一个 空 的 记忆 单元 ， 不 做 任何 处理 ( G ) ， O   模块 根据 输入   x   找到 k 个 有关 的 记忆 单元 ，       $ $     o _ j   =   O _ j ( x ,   m )   =   \ \ arg \ \ max _ { i = 1 , ... , N }   s _ O ( [ x ,   m _ { o _ 1 } ... , m _ { o _ { i - 1 } } ] ,   m _ i )     $ $       最终 输出 给   R   的 是   $ (   [ x ,   m _ { o _ 1 } , .. , m _ { o _ k } ]     ) $ ， 在 这个 例子 中 ， R 根据 这些 记忆 单元 和 输入 ，     输出 一个 最 匹配 的 单词       $ $     r   =   \ \ arg \ \ max _ { w   \ \ in   W }   s _ R ( [ x ,   m _ { o _ 1 } , .. , m _ { o _ k } ] ,   w )     $ $       这个 任务 可以 用来 实现 单个 单词 的 问答 问题 。     两个 匹配 函数 $ ( s _ O ,   s _ R ) $ 都 可以 用 下列 方式 建模       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U ^ T   U   \ \ phi _ y ( y )     $ $       U   是 要 学习 的 参数 ， $ ( \ \ phi ) $ 是 对 输入 的   embedding   函数 。       训练       学习 最优 的 参数   $ ( U _ O ,   U _ R ) $   最小化 损失 函数 ， 损失 函数 包含 2 部分 ( k = 2 ) ，           选择 最 匹配 的 记忆 单元 带来 的 风险       选择 最佳 response 带来 的 风险 ， 如果 用 RNN 就 直接 用 RNN 的 损失 函数 就行           上述 风险 都 用 大 间隔 损失 函数 来 刻画 ！       词 序列 作为 输入       没有 区分   statements   和   questions ， 因此 需要 学习 一个 分割 函数 ， 将 词 序列 分割 为 两 部分 。       $ $     seg ( c )   =   W _ { swg } ^ T   U _ S   \ \ Phi _ { seg } ( c )     $ $       上述 三个 参数 分别 是 ： 分割 模型 的 线性 权重 参数 ， 将 特征 空间 映射 到 embedding 空间 的 线性变换 矩阵 （ 即 查找 表 ） ， 输入 词 的 特征向量 （ 高维 稀疏 ） ， c 是 词袋 词典 。       EFFICIENT   MEMORY   VIA   HASHING       利用   hash   trick   加速 内存 查找 ： 将 输入   I ( x )   hash 到 一个 或 多个 桶 ， 只 将 输入 和 同一个 桶 内 的   memory   进行 打分 计算 。     两种   hash   策略 ：           直接   hash   词 ， 因此 对于 一个 句子 ， 只 需 比较 至少 共享 一个 词 的   memory       对词 向量 做   k - means   聚类 ， 对 一个 句子 ， 比较 的 是 和 至少 一个 词 在 同一个 类别 的 memory ， 这种 方法 考虑 了 词 的 语义 。           MODELING   WRITE   TIME       将   memory   写 的 时间 也 记录下来 。   对 内存   $ ( m _ j ) $   将 索引   j   （ 假定 没有 内存 更新 ， 索引 和 时间 一一对应 ） 也 编码 进   $ ( \ \ Phi _ x ,   \ \ Phi _ y ) $ 。 具体做法 ， 将   $ ( \ \ Phi ) $   扩充 三维 ， 都 设置 为 0 ， 对 输入   x   ， 当前 两个 内存   y ,   y ' ,   计算 分数       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U _ { O _ t } ^ T   U _ { O _ t }   ( \ \ phi _ y ( y )   -   \ \ phi _ y ( y ' )   +   \ \ phi _ t ( x ,   y ,   y ' )   )     $ $       其中 $ ( \ \ phi _ t ( x ,   y ,   y ' ) ) $ 只有 扩充 的 三维 不为 0 ， 其他 都 为 0 ， 扩充 的 三维 分别 用于 指示           whether   x   older   than   y ,       whether   x   older   than   y ' ,       whether   y   older   than   y ' ,           实际上 将 原来 的   poinwise   变为   pairwise 了 ， 并 加入 了 时间 先后 关系 信息 。 选出 比较 完胜 的 y 作为 输出 。       对于 unseen 词 的 处理       利用 语言 模型 ， 用 周围 的 词 预测 这个 词 可能 是 什么 词 ， 那么 这个 unseen 的 词 就 和 这个 预测 的 结果 的 词 相似 。     将 词 特征 表达 从   3 | W |   增加 到   5 | W | ， 增加 的 两个 部分 分别 表达 这个 词 的 左 上下文 和 后 上下文 。       精确 匹配       embedding   因为 将 词 降维到 低维 连续 向量 ， 所以 无法 做 精确 匹配 。     解决 的 办法 是 为   score   增加 一项 精确 匹配 score 。       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U ^ T   U   \ \ phi _ y ( y )   +   \ \ lambda   \ \ phi _ x ( x ) ^ T     \ \ phi _ y ( y )     $ $       结果                   可以 看出   只用   embedding   特征 和 之前 最好 结果 差不多 ， 稍低 ， 但是 加上   BOW   特征 ， 效果 马上 就 提升 了 9 个点 。       不同 hash 策略 的 影响 ， word   hash   实现 了 1000 倍 的 加速 但是 精度 减少 很 明显 ， cluster   hash   在 精度 只 减少 1 个点 的 情况 下 ， 实现 了 80 倍 的 加速 。                       可以 看到 时间 特征 对 提升 非常 显著       输出 选取 的 内存 个数 k 对 效果 提升 也 非常 显著       MemNN   完胜   RNN ， LSTM           End - To - End   Memory   Networks       Neural   Turing   Machine       图灵机 三个 基本 机制 ： 元素 操作 （ 算数 计算 ） ， 逻辑 流控制 （ 分支 ） ， 外部 存储 。     现代 机器 学习 模型 侧重 建模 复杂 数据 ， 缺乏 流控制 和 存储 。       RNN   是 图灵 完备 的 ， 即 可以 实现 图灵机 的 所有 操作 ， 即可 用 图灵机 完成 的 事情 ， RNN 有 这种 潜能 可以 解决 ！       NTM 之于 RNN ， 就 如 图灵机 之 于 有限 状态机 ！ 最大 的 区别 在于 前者 有 接近 无限 的 存储空间 ！       NTM   通过   attention   机制 实现 内存 的 读取 和 写入 操作 。       基础 研究       这 一部分 主要 是从 心理学 、 神经科学 、 语言学 等 角度 阐述   working   memory   和   NTM   的 相关 原理 和 解释 ！     总之 一句 话 ： memory 是 非常 重要 的 。       working   memory   在 心理学 中 用于 解释 短期 信息处理 能力           Baddeley ,   A . ,   Eysenck ,   M . ,   and   Anderson ,   M .   ( 2009 ) .   Memory .   Psychology   Press .       Miller ,   G .   A .   ( 1956 ) .   The   magical   number   seven ,   plus   or   minus   two :   some   limits   on   our   capacity   for   processing   information .   Psychological   review ,   63 ( 2 ) : 81 .           Recurrent   Neural   Networks       与 隐 马尔科夫 模型 比较 ， 前者 只有 有限 个 离散 的 状态 ， 而   RNN   的 状态 是   distributed ， 具有 无限 个 状态 ， 具有 更大 的 计算能力 ！           state ： 隐 马尔科夫 模型 具有 有限 个 离散 状态 ； RNN 具有 无限 个 分布式 状态       状态 转移 概率 ： 隐 马尔科夫 模型 通过 转移 概率 矩阵 建模 ， 依赖于 当前 状态 和 当前 的 输入 ； RNN 通过 隐层 建模 ， 可以 是 简单 的 RNN 隐层 单元 ， LSTM 单元 ， GRU ， 甚至 更 复杂 的 多层 结构 ， 依赖于 当前 状态 和 当前 的 输入 。       输出 ： 只 依赖于 当前 的 状态 ， 通常 用 生成 模型 建模 这个 条件 概率 ； RNN 则 用 （ 单层 或 多层 ） 神经网络 建模 这个 条件 概率 。       模型 训练 ： 隐 马尔科夫 模型 根据 输出 的 结果 ， 用 维特 比 算法 解码 出 状态 ， 转移 概率 和 条件 概率 通过 EM 算法 优化 得到 （ 参考 语音 识别 ） ； RNN 则 是 端 到 端 用 梯度 下降 联合 优化 所有 参数 得到 。           LSTM   解决 RNN 梯度 消失 和 爆炸 的 问题 是 通过 嵌入 一个 理想 积分器 ？               RNN   的 一些 应用 场景 ：           语音 识别       Graves ,   A . ,   Mohamed ,   A . ,   and   Hinton ,   G .   ( 2013 ) .   Speech   recognition   with   deep   recurrent   neural   networks .   In   Acoustics ,   Speech   and   Signal   Processing   ( ICASSP ) ,   2013   IEEE   International   Conference   on ,   pages   6645 – 6649 .   IEEE .       Graves ,   A .   and   Jaitly ,   N .   ( 2014 ) .   Towards   end - to - end   speech   recognition   with   recurrent   neural   networks .   In   Proceedings   of   the   31st   International   Conference   on   Machine   Learn -   ing   ( ICML - 14 ) ,   pages   1764 – 1772 .                           神经 图灵机 结构               神经 图灵机 包括 两个 基础 结构 ： 神经网络 控制器   controller ， memory   bank .     控制器 通过 输入 向量 和 输出 向量 和 外界 交互 ， 和 一般 的 神经网络 不同 的 是 ， 他 还 会 选择性 地 和 内存 交互 。     和 内存 交互 的 部件 成为   head ， 读写 头 。 读写 头 通过   attention   机制 ， 对 不同 的 内存 读取 的 值 赋予 不同 的 权重 ！       读写 头要 读 所有 的 内存 岂 不是 很 慢 ， 如何 实现 稀疏 的 读 和 写 ？     几个 参数 ：           $ (   \ \ mathbf { M } _ t   ) $   $ ( N   \ \ times   M ) $   尺寸 的 内存 矩阵 ， M 是 每个 内存 向量 的 尺寸 ， N 是 内存 向量 的 个数       $ (   \ \ mathbf { w } _ t   ) $   是 读头 给出 的 每个 内存 的 权重 向量 ， 他 应该 满足 概率 约束条件 ， 非负 ， 和 为 1 .       读头 读 内存 后 返回 的 结果 为           $ $     \ \ mathbf { r } _ t   =   \ \ sum _ i   w _ t ( i )   \ \ mathbf { M } _ t ( i )     $ $       $ (   \ \ mathbf { M } _ t ( i )   ) $   是 行向量 ， 也 就是 一个 内存 单元 。       写入 过程 ： 借鉴 了   LSTM   的 设计 ， 写 过程 包括 两个 部分 ， forget   和   add .           擦除 向量   $ ( \ \ mathbf { e } _ t ) $   M   个 元素 全为 0 - 1 之间 ， 设 写入 权重 为 $ ( w _ t ( i ) ) $ 内存 更新 方程 为           $ $     \ \ tilde { \ \ mathbf { M } } _ t ( i )   =   \ \ mathbf { M } _ { t - 1 } ( i ) [ \ \ mathbf { 1 }   -   w _ t ( i ) \ \ mathbf { e } _ t ]     $ $           add   向量   $ ( \ \ mathbf { a } _ t ) $ ， 用 add   向量 更新 擦 出后 的 内存           $ $     \ \ mathbf { M } _ t ( i )   =   \ \ tilde { \ \ mathbf { M } } _ t ( i )   +   w _ t ( i )   \ \ mathbf { a } _ t     $ $           擦除 向量 和 add 向量 每 一维 都 是 独立 的 。           寻址 机制       即 确定 读写 权重 $ ( w _ t ) $                       两种 基本 机制 ：           基于 内容 的 寻址 ： 找 和 控制器 发出 的 值 最 相似 的 位置 ， Hopfield   networks   1982 ： 简单 、 可以 获取 内存 的 精确 值 ； 不 适合 算术 问题 ， 例如 计算   $ ( x + y ) $ ， 寻址 跟 内容 无关       基于 位置 的 寻址 ， 可以 看做 基于 内容 的 寻址 的 特例 ， 因为 位置 也 可以 看做 内容 的 一部分                   基于 内容 的 寻址 原理 ： 每 一个   head （ 读 或者 写 ） 先生 成 一个 长度 为   $ ( M ) $   的   key   vector   $ ( \ \ mathbf { k } _ t ) $ ， 通过 这个 向量 和 内存 中 的 所有 向量 进行 比较 ， 计算 相似 度   $ ( K [ · ,   · ] ) $ ， 相似 度 在 所有 的 内存 上 归一化 ， $ ( \ \ beta _ t ) $   是 缩放 因子 。 论文 中 相似 度 度量 采用 向量 的 余弦 相似 度               $ $     w _ t ^ c ( i )   =   \ \ frac { \ \ exp \ \ left ( \ \ beta _ t   K [ \ \ mathbf { k } _ t ,   \ \ mathbf { M } _ t ( i ) ] \ \ right ) } { \ \ sum _ j   \ \ exp \ \ left ( \ \ beta _ t   K [ \ \ mathbf { k } _ t ,   \ \ mathbf { M } _ t ( j ) ] \ \ right ) }     $ $           基于 位置 的 寻址 ： 每 一个   head   生成 一个 标量   interpolation   gate   $ ( g _ t   \ \ in   ( 0 ,   1 )   ) $ ， 利用 这个 门去 控制 当前 权重 向量 $ ( \ \ mathbf { w }   t ^ c ) $   和 历史 权重 $ ( \ \ mathbf { w }   ) $ 混合 生成 门限 权重           $ $     \ \ mathbf { w } _ { t } ^ g   =   g _ t   \ \ mathbf { w } _ t ^ c   +   ( 1   -   g _ t )   \ \ mathbf { w } _ { t - 1 }     $ $       经过 插值 后 ， head 生成 一个 shift   weighting   $ ( \ \ mathbf { s } _ t ) $ ， 它 是 一个 向量 ， 刻画 了 移动 的 所有 可能 的 整数 上 的 一个 分布 。 这个 权重 可以 通过 一个   softmax   层 近似 ， 也 可以 输出 单个 标量 ， 例如 6.7 代表 $ ( s _ t ( 6 )   =   0.3 ,   s _ t ( 7 )   =   0.7 ) $ ， 其他 都 为 0 .   利用 这个 向量 对 插值 后 的 向量 加权 ， 可以 表达 为 一个 循环 卷积       $ $     \ \ tilde { w } _ t ( i )   =   \ \ sum _ { j = 0 } ^ { N - 1 }   w _ t ^ g ( j )   s ( i   -   j )     $ $       为了 让 寻址 更加 sharp ， head   还 生成 一个 变量   $ ( \ \ gamma _ t   \ \ ge   1 ) $ ， 对 卷积 后 的 权重 做 变换 得到 最终 的 权重       $ $     w _ t ( i )   =   \ \ frac { \ \ tilde { w } _ t ( i ) ^ { \ \ gamma _ t } } { \ \ sum _ j   \ \ tilde { w } _ t ( j ) ^ { \ \ gamma _ t } }     $ $       实验 结果       未 解决 的 问题           擦除 向量 $ ( e _ t ) $ 和 add 向量 $ ( a _ t ) $ 怎么 确定 ？       输入 数据 如何 影响 读写 头 的 权重 ？ 输入 数据 和   key   vector   $ ( k _ t ) $   有 什么 关系 ？      ", "tags": "machine-learning", "url": "/wiki/machine-learning/memory-network.html"},
      
      
      {"title": "MXNET学习", "text": "    Table   of   Contents           Rabit           checkpoint           Allreduce   and   Lazy   Preparation                         Rabit       教程 地址 ：   https : / / github . com / dmlc / rabit / tree / dev / guide         RABIT ： Reliable   Allreduce   and   Broadcast   Interface       两个 关键 的 操作 ： Allreduce   和   Broadcast         Allreduce   :   和 不同 的 reduce 不同 ， 以   max   操作 为例 ， reduce 将 很多 个 节点 的 值 变成 一个 最大 的 值 ，     而   allreduce   将 每 一个 节点 都 变成 最大值 ； 这 在 机器 学习 操作 中 经常 用到 ， 比如 LBFGS 同步 所有 节点     的 梯度 为 全局 梯度 ！         Broadcast   ： 和   Spark   的 broadcast 一样 ， 将 一个 节点 的 值 广播 到 所有 的 节点 ！       参考   KMEANS   的 例子 ， 了解 这 两个 操作 ：   https : / / github . com / dmlc / wormhole / blob / master / learn / kmeans / kmeans . cc         checkpoint       用于 每 一次 迭代 保存 模型 ， 以此 保证 容错 能力 ， API ：   LoadCheckPoint   ,     CheckPoint         Allreduce   and   Lazy   Preparation               Allreduce     & lt ;     operator     & gt ;     (     pointer _ of _ data     ,       size _ of _ data     ) ;        ", "tags": "machine-learning", "url": "/wiki/machine-learning/mxnet.html"},
      
      
      {"title": "ND4J", "text": "    Table   of   Contents           关于           指南                 关于       ND4J   JVM 上 的 Numpy       指南  ", "tags": "machine-learning", "url": "/wiki/machine-learning/nd4j.html"},
      
      
      {"title": "Node2Vec", "text": "    Table   of   Contents           关于           摘要           特征 学习 框架           邻居 节点 的 搜索 策略                         关于       论文 ： node2vec :   Scalable   Feature   Learning   for   Networks       摘要           将 一个 网络 中 的 节点 变成 一个 低维 的 连续 向量 ， 作为 其他 模型 的 输入 特征 。       通过 最大化 网络邻居 的 似然 函数 。       基于 特征值 分解 的 线性 或 非线性 降维 方法 在 实际 的 大规模 数据 应用 中 ， 计算 太慢 ？ 并且 性能 还 不好 ！       目标 函数 ， 保证 邻居 节点 依然 相近 ； 保证 具有 相似 结构 的 节点 的 嵌入 向量 也 相近 ！                       目标 函数 ： maximize   the   likelihood   of   pre -   serving   network   neighborhoods   of   nodes   in   a   d - dimensional   feature   space 。       2 阶   random   walk   方法 产生 节点 的 网络 上 的 邻居 样本 。       node2vec 可 扩展 到 边       用 学到 的 向量 去 做 分类 任务 的 特征 ， 结果 比 其他 方法 好 很多 ， 并且 这种 方法 很鲁棒 ！ 即使 缺少 边 也 没 问题 。       可 扩展 到 大规模   node ！       基于 特征值 分解 的 方法 难以 扩展 到 大规模 ？ suffer   from   both   computational   and   statistical   performance   drawbacks ； 不够 鲁邦 ！ 不能           特征 学习 框架           网络 ： $ ( G   =   ( V ,   E ) ) $       学习 目标 ： $ ( f   :   V   \ \ rightarrow   R ^ d ) $ ， 实际上 是 一个 $ ( | V |   \ \ times   d ) $ 参数       采样 策略 $ ( S ) $ 生成 的 节点 $ ( u ) $ 的 网络邻居   $ ( N _ S ( u )   \ \ in   V ) $       极大 似然 估计 ：           $ $     \ \ max _ f   \ \ sum _ { u   \ \ in   V }   \ \ log   Pr ( N _ S ( u ) |   f ( u )   )     $ $           几个 假设 ：       条件 独立 ： $ (   Pr ( N _ S ( u ) |   u   )   =   \ \ Pi _ { n _ i   \ \ in   N _ S ( u ) }   Pr ( n _ i | f ( u ) )   ) $       对称性 ： 特征 空间 中 ， 两个 互为 邻居 的 边 有 对称 效应 ：                   $ $     Pr ( n _ i | f ( u ) )   =   \ \ frac { \ \ exp ( f ( n _ i )   \ \ dot   f ( u ) ) } { \ \ sum _ { v \ \ in   V }   f ( v )   \ \ dot   f ( u ) }     $ $       和   word2vec   一样 ， 可以 通过 负 采样 来 优化 分母 的 计算 量 ！           最大 的 问题 是 对 领居 节点 的 采样 ， skip - gram 是 通过 一个 固定 宽度 的 滑动 窗 ， 网络 由于 不是 线性 的 ， 比较 麻烦 。 不 一定 是 直接 邻居 可以 当 邻居 ， 这 取决于 采样 策略   $ ( S ) $           邻居 节点 的 搜索 策略           经典 的 搜索 策略 ：       BFS ： 宽度 优先 搜索 ， 找 直接 相连 的 节点       DFS ： 深度 优先 搜索               node2vec   的 搜索 策略 综合 了 这 两种 策略                       对于 源 节点 $ ( u ) $ ， 通过   random   walk   ( 马尔科夫 链 )   采样   l   长度 的 邻居 节点   $ ( c _ i ,   c _ 0 = u ) $       条件 概率 为 ：           $ $     P ( c _ i = x | c _ { i - 1 } = v )   =   \ \ begin { cases }             \ \ frac { \ \ pi _ { vx } } { Z } ,   if   ( v , x )   \ \ in   E .   \ \ \ \             0 ,   otherwise     \ \ end { cases }     $ $       其中 $ ( \ \ pi _ { vx } ) $ 是 未 归一化 的 概率 ， Z 是 归一化 常数 。 对于 最 简单 的 情况 ， 可以 用边 的 权重 作为 未 归一化 概率     $ ( \ \ pi _ { vx }   =   w _ { vx } ) $ 。       对于 2 阶   random   walk ， 未 归一化 概率 和 权重 之间 的 关系 为 ： $ ( \ \ pi _ { vx }   =   \ \ alpha _ { pq } ( t , x ) w _ { vx } ) $     t 是 上 一个 节点 ， v 是 当前 节点 ， x 是 下 一个 可能 的 节点 ， 系数       $ $     \ \ alpha _ { pq } ( t ,   x )   =   \ \ begin { cases }                             \ \ frac { 1 } { p } ,   d _ { tx }   =   0 ,   \ \ \ \                             1 ,   d _ { tx } = 1 , \ \ \ \                             \ \ frac { 1 } { q } ,   d _ { tx } = 2 .     \ \ end { cases }     $ $       $ ( d _ { tx } ) $ 是 两个 节点 的 距离 ， p 是 return 参数 ， q 是 in - out 参数 。           random   walk   的 好处 ：       可以 减少 邻居 的 存储空间 到 $ ( O ( a ^ 2 | V | ) ) $ . 本来 是 $ ( O ( E ) ) $ 。       每 一次 产生 的 链 可以 复用 ， 因为 马尔科夫 性 。              ", "tags": "machine-learning", "url": "/wiki/machine-learning/node2vec.html"},
      
      
      {"title": "Numerical Optimization", "text": "    Table   of   Contents           关于           8 .   拟 牛顿 法                 关于       8 .   拟 牛顿 法  ", "tags": "machine-learning", "url": "/wiki/machine-learning/numerical-optimization.html"},
      
      
        
        
      
      {"title": "方程求解方法汇总", "text": "    Table   of   Contents           线性方程           高斯消 元法           迭代 解法                   非线性 方程           不动点 迭代           线性 搜索                         线性方程       高斯消 元法       高斯消 元法 求解 线性方程 是 最为 经典 的 方法 ， 基本 思想 就是 一次 消 去 多个 变量 ， 直到 留下 一个 ， 从而 解 出 结果 。 用 矩阵 的 语言 描述 ， 就是 对 线性方程       $ $     Ax   =   b     $ $       的 矩阵 A 和 b 构成 的 增广 矩阵 [ A   b ] 做 一系列 的 行 变换 $ ( P _ i ,   i = 1 , ... , k ) $ ， 将 矩阵 A 变成 只有 对角线 为 1 或 0 的 单位 阵       $ $     P _ k   P _ { k - 1 }   ...   P _ 1   A   x   =   P _ k   P _ { k - 1 }   ...   P _ 1   b   \ \ \ \     x   =   P   b     $ $       $ ( P   =   P _ k   P _ { k - 1 }   ...   P _ 1 ) $ 是 一系列 的 航 变换 ， 将 A 矩阵 变为 单位 阵 ， 实际上 就是 A 矩阵 的 逆 ！     高斯消 元法 计算 复杂度 是 $ ( O ( n ^ 3 ) ) $ 。       迭代 解法         Jacobi   迭代   ： 将 系数 矩阵 A 分解 为 $ ( A   =   D   +   B ) $ ， 其中 D 是 对角 阵 ， B 是 对角线 上 为 0 的 矩阵 。 那么 有       $ $     x   =   -   D ^ { - 1 }   B   x   +   D ^ { - 1 }   b     $ $       上 式 可以 把 x 看做 迭代 方程 的 不动点 ， 这 就是 Jacobi 迭代法 。 收敛 的 充分条件 是 $ ( | | D ^ { - 1 } B | | & lt ; 1 ) $ ， 如果 取 无穷 范数 ， 则 表示 A 矩阵 是 主 对角 元 最大 的 矩阵 ， 即 $ ( | a _ { ii } |   & gt ;   | a _ { ij } | ,   \ \ forall   i , j ) $ 。 一般 的 方差 ， 总 可以 通过 简单 的 行 初等变换 变成 主 对角 元 最大 的 矩阵 ！         Gauss - Seidel 迭代   ： 将 矩阵 分解 为 $ ( A   =   L   +   U ) $ ， 其中 L 是   有   对角 元素 的 下 三角 阵 ， U 是   没有   对角 元 的 上 三角 阵 。 迭代 方程 为       $ $     L   x _ { k + 1 }   =   -   U   x _ k   +   b     $ $       每 一次 迭代 是 在 解 稀疏 矩阵 是 下 三角 矩阵 的 线性方程 ， 可以 在 $ ( O ( n ^ 2 ) ) $ 时间 复杂度 内 求解 ！ 迭代 速度 高于 Jacobi 迭代 。       非线性 方程       不动点 迭代       与 线性方程 的 迭代 方法 类似 ， 构造 不动点 方程 ， 使得 迭代 是 压缩 映象 ， 那么 不断 迭代 该 方程 可以 得到 不动点 ， 就 是非 线性方程 的 解 ！       $ $     x   =   g ( x )     $ $       线性 搜索       二分法 、 牛顿 法等 方法 。 实际上 牛顿 法 求解 方程 可以 看做 梯度 是 f 的 原函数 的 最优化 问题 。  ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/equation-solve.html"},
      
      
      {"title": "约束凸优化", "text": "    Table   of   Contents           等式 约束 问题           内点法                 等式 约束 问题       对于 线性方程 等式 约束 问题       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   \ \ \ \     s . t .   \ \ quad   & amp ;   A   x   = b     \ \ end { align }     $ $       根据 KKT 最优 条件 可知 最优 解 满足 方程       $ $     Ax ^   *   =   b ,   \ \ nabla   f ( x ^   *   )   +   A   v ^   *   =   0     $ $       后 一个 式子 实际上 是 说 ， 在 最优 解处 ， f 的 梯度 在 A 列 向量 的 子 空间 中 ！ A 是 向量 的 时候 ， 有个 直观 的 几何 解释 ！ 即 f 的 等 高面 与 超平面 相切 ！         等式 约束 二次 规划     有 闭式 解 ， 上述 条件 变成 了 一个 线性方程 问题 ！ 设 $ ( f ( x )   =   1 / 2   x ^ T   P   x   +   q ^ T   +   r ) $ ， 那么 KKT 条件 变为       $ $     Ax ^   *   =   b ,   P   x ^   *     +   A   v ^   *   +   q   =   0     $ $       这是 一个 线性方程 ， 可以 利用 线性方程 求解 方法 如 高斯消 元法 、 迭代 算法 等 求解 。     如果 方程 有解 ， 那么 所有 的 解 都 是 可行 解 ！ 如果 方程 无 解 ， 那么 表示 原 问题 没有 最小值 ！         消除 等式 约束     方法 ： 先 求解 线性方程 得到 通解   $ ( \ \ { Fz   +   \ \ hat { x } |   z   \ \ in   R ^ { n - p } \ \ } ) $ ， 然后 代入 目标 函数 ， 消除 等式 约束 ！         求解 对偶 问题     :   对偶 问题 是 无约束 优化 问题 。       $ $     \ \ min _ v   b ^ T   v   +   f ^   *   ( - A ^ T   v )     $ $         牛顿 方法     即 在 迭代 点 附近 用 一个 二次 函数 近似 f ( x ) ， 这样一来 ， 在 每次 迭代 的 时候 ， 就是 在 求解 等式 约束 二次 规划 ， 有 闭式 解 ！ 从而 可以 很 方便 求 出 下降 步长 ！       内点法       凸 优化 问题       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   \ \ \ \     s . t .   \ \ quad   & amp ;   f _ i ( x )   \ \ le   0   \ \ \ \                           & amp ;   Ax = b     \ \ end { align }     $ $       定义 示性 函数       $ $     I _ - ( x )   =   \ \ begin { cases }                     0   \ \ quad   x   \ \ le   0   \ \ \ \                     \ \ infty     \ \ quad   x   & gt ;   0                     \ \ end { cases }     $ $       通过 这个 示性 函数 ， 可以 将 不等式 约束 去       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   +   \ \ sum _ i   I _ - ( f _ i ( x ) ) \ \ \ \     s . t .   \ \ quad   & amp ;   Ax = b     \ \ end { align }     $ $       对数 壁垒 函数 ： 由于 示性 函数 不可 微 ， 可以 用 可微 函数 近似 ， 一种 选择 是 采用 如下 对数函数       $ $     \ \ hat { I _ - } ( x )   =   -   \ \ frac { 1 } { t }   \ \ log ( - x )     $ $       随着 参数 t 趋近 于 无穷大 ， 对 示性 函数 的 近似 度 越来越 好 ！ 在 这种 壁垒 函数 选取 下 ， 最优 解 可以 通过 牛顿 法 求解 ， 最优 解 跟 t 有关 ， $ ( x ( t ) ) $ 随 t 变动 而 形成 的 轨迹 叫做     中心 路径   。 并且 有       $ $     f ( x ^   *   ( t )   )   -   p   ^   *   & lt ;   m / t     $ $       m 是 不等式 约束 的 个数 。 p 是 最优 f 值 ， $ ( x ^   *   ( t )   ) $ 是 壁垒 函数 近似 的 最优 解 ！ 因此 ， 随着 t 增大 ， 可以 控制 误差 在 指定 的 范围 内 ！          ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/interior-point-method.html"},
      
      
      {"title": "近似算法", "text": "    Table   of   Contents           投影 算子           Moreau   分解           近似 梯度 法           forward - backward   splitting           投影 梯度 算法           交替 投影 算法                   投影 算子 计算           二次 函数           标量 函数           纺射集 投影           半 空间           集合 的 支持 函数           范数           L2 范数           L1 范数                           软 阈值 算法           L1 范数 正则           L2 范数           L21 范数                   软 阈值 算法 实现           交替 方向 乘子 ADMM           多个 函数           相关 资料                 投影 算子       点到 闭凸 集合 的 投影       $ $     prox _ C ( x )   =   \ \ arg \ \ min _ y   I _ C ( y )   +   \ \ frac { 1 } { 2 } | | x   -   y | | ^ 2     $ $       其中 $ ( I _ C ) $ 是 集合 C 的 示性 函数       $ $     I _ C ( x )   =   \ \ begin { cases }                     0 ,   \ \ quad   x   \ \ in   C   \ \ \ \                     \ \ infty ,   \ \ quad   x   \ \ not \ \ in   C                     \ \ end { cases }     $ $       几何 解释 就是 点 x 到 集合 C 的 投影 就是 C 中到 x 最近 的 点 ！       把 示性 函数 替换成 一般 的 凸函数 f ， 可以 得到 一般 的 投影 算子       $ $     prox _ f ( x )   =   \ \ arg \ \ min _ y   f ( y )   +   \ \ frac { 1 } { 2 } | | x   -   y | | ^ 2     $ $       不动点 方程 ： 如果 一个点 x * 在 f 的 投影 下 是 他 自己 ， 那么 根据 上 式 ， 第二项 为 0 ， 所以       $ $     x ^   *   =   prox _ f ( x ^ *   )   =   \ \ arg \ \ min _ y   f ( x )     $ $       几何 解释 就是 在 f 的 最小值 等 高 面上 的 点 的 投影 是 它 自己 ！       由于 点 x 到 投影 点 $ ( prox _ f ( x ) ) $ 的 方向 向量 $ ( x   -   prox _ f ( x ) ) $ 与 等 高面 垂直 （ 投影 的 几何 解释 ） ， 因此 ， 投影 操作 可以 看做 梯度 下降 的 推广 ！     投影 是 往 函数 f 的 较 小值 等 高 面上 进行 投影 ！       令 $ ( p   =   prox _ { \ \ lambda   f } ( x ) ) $ ，       $ $     p   =   \ \ arg \ \ min _ y   \ \ frac { 1 } { 2 }   | | y   -   x | | ^ 2   +   \ \ lambda   f ( y )   \ \ \ \     0   \ \ in   p - x   +   \ \ lambda   \ \ partial   f ( p )   \ \ \ \     x   \ \ in   ( I   +   \ \ lambda   \ \ partial   f )   ( p )   \ \ \ \     p   =   ( I   +   \ \ lambda   \ \ partial   f ) ^ { - 1 }   ( x )     $ $       注意 ， 次梯度 的 逆 有 唯一 的 像 ！ 所以 投影 算子 与 次梯度 关系 为       $ $     prox _ { \ \ lambda   f }     =   ( I   +   \ \ lambda   \ \ partial   f ) ^ { - 1 }     $ $       如果 $ ( \ \ lambda ) $ 很小 且 f 存在 常规 梯度 ， 那么 可以 近似 为       $ $     prox _ { \ \ lambda   f } ( v )   \ \ approx   v   -     \ \ lambda   \ \ nabla   f ( v )     $ $       也就是说     投影 操作 是 梯度 下降 的 一种 推广   ！ 实际上 ， 可以 看做 一种 前向 梯度 下降 ， 即 下降 的 梯度 不是 在 当前 点 v 计算 得到 的 ， 而是 在 下降 的 目标 点 p 计算 得到 的 ！       如果 投影 操作 计算 方便 （ 有 简单 的 解析 解 ） ， 那么 用 投影 操作 做 优化 可以 取代 梯度 下降 ， 并且 可以 应用 到 梯度 下降 没法用 的 场景 — — 梯度 不 存在 的 函数 优化 ！       近似 点 算法 ： 求函数 f 的 最小值 ， 利用 投影 算子 是 压缩 算子 ， 且 投影 算子 的 不动点 是 f 的 最小值 点 性质 可 得 迭代 近似 点 算法       $ $     x _ { n + 1 }   =   prox _ { \ \ lambda   f } ( x _ n )     $ $       前面 说 到 投影 算子 就 相当于 梯度 下降 的 推广 ， 那么 近似 点 算法 可以 看做 梯度 下降 求 最小值 的 推广 ！       Moreau   分解       点 v 可以 分解 为 投影 和 共轭 投影 之 和       $ $     v   =   prox _ f ( v )   +   prox _ { f ^   *   } ( v )   \ \ \ \     f ^   *   ( y )   =   \ \ sup _ x   \ \ left ( y ^ T   x   -   f ( x ) \ \ right )     $ $       这个 分解 可以 看做 几何 中 的 正交 分解 ！       近似 梯度 法       forward - backward   splitting       如果 目标 函数 存在 不可 微分 部分 ， 可以 将 目标 函数 分解 为 两 部分 ， 一部分 可以 微分 ， 另 一部分 不可 微分       $ $     \ \ min _ x   f ( x )   +   g ( x )     $ $       假设 f 可微 ， g 是 不可 微 部分 。 若 $ ( p ) $ 是 最优 解 ， 那么 根据 最优 条件 可知       $ $     0   \ \ in   \ \ lambda   \ \ nabla   f ( p   )   +   \ \ lambda   \ \ partial   g ( p   )   \ \ \ \     0   \ \ in   \ \ lambda   \ \ nabla   f ( p )   -   p   +   p +   \ \ lambda   \ \ partial   g ( p   )   \ \ \ \     ( I   -   \ \ lambda   \ \ nabla   f )   ( p )   \ \ in   ( I   +   \ \ lambda   \ \ partial   g )   ( p   )   \ \ \ \     p   =   ( I   +   \ \ lambda   \ \ partial   g ) ^ { - 1 }   ( I   -   \ \ lambda   \ \ nabla   f )   ( p )   \ \ \ \     p   =   prox _ { \ \ lambda   g } \ \ left (   p   -   \ \ lambda   \ \ nabla   f ( p )     \ \ right )     $ $       上述 不动点 方程 给出 了 优化 迭代 步骤 ，   先 按着 可微 函数 梯度 下降 ， 然后 对 不可 微 函数 做 投影 下降   ！       如果 两步 都 采用 投影 来 做       $ $     p   =   prox _ {   g } \ \ left (   prox _ f   ( p )   \ \ right )     $ $       就是   backward - backward   splitting .       投影 梯度 算法       求解 约束 优化 问题       $ $     \ \ min _ { x   \ \ in   C }   f _ 2 ( x )     $ $       迭代 算法       $ $     x _ { k + 1 }   =   P _ C \ \ left ( x _ k   -   \ \ gamma   \ \ nabla   f _ 2 ( x _ k ) \ \ right )     $ $       $ ( P _ C ) $ 是 到 凸集 的 投影 ， 该 算法 的 集合 意义 是 先 沿着 f2 的 梯度方向 下降 ， 让 后 将 结果 点 投影 到 集合 C 中 ， 以 保证 解 不会 离开 约束 区域 ！       交替 投影 算法       求解 约束 优化 问题       $ $     \ \ min _ { x \ \ in   C }   \ \ frac { 1 } { 2 }   d _ D ^ 2 ( x )     $ $       $ ( d _ D ( x ) ) $ 是 点 x 到 凸集 D 的 距离 ， 采   backward - backward   分割 ， 可 得 迭代 算法       $ $     x _ { k + 1 }   =   P _ C ( P _ D ( x _ k ) )     $ $       该 算法 的 几何 图像 是 ， 交替 像 两个 凸集 C 和 D 进行 投影 ， 直到 收敛 ！       投影 算子 计算       常规 的 计算方法 是 直接 根据 定义 ， 求解 优化 问题 ：       $ $     \ \ min _ y   f ( y )   +   \ \ frac { 1 } { 2 \ \ lambda } | | y   -   x | | ^ 2     $ $                         $ ( dom   f ) $ 有限       $ ( dom   f   =   R ^ n ) $                       f 光滑       投影 梯度 法 、 内点法       梯度 下降               f 不 光滑       投影 次梯度 法       次梯度 法                   二次 函数       $ ( f ( x )   =   1 / 2x ^ T   A   x   +   b ^ T   x   + c   ) $ ，       $ $     prox _ { \ \ lambda   f }   ( v ) =   ( I   +   \ \ lambda   A ) ^ { - 1 } ( y   -   \ \ lambda   b )     $ $       如果   A   =   I ,   且 只有 二次 项 ， 即 $ ( f   =   | |   \ \ cdot   | |   _   2 ^ 2 ) $ ， 那么 投影 算子 表现 为 shrinkage   operator ， 直观 来看 就是 把 做 了 一个 衰减 ！       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ frac { v } { 1   +   \ \ lambda }     $ $       标量 函数       如果 f 是 标量 函数 ， 自变量 是 单 变量 ， 那么 很 容易 求得       $ $     v   \ \ in   \ \ lambda   \ \ partial   f ( x )   +   x   \ \ \ \     x   = prox _ { \ \ lambda   f } ( v )   =   ( 1   +   \ \ partial   f ) ^ { - 1 }   v     $ $       当 $ ( f ( x )   =   | x | ) $ 时 ， 有       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ max ( | v |   -   \ \ lambda   ,   0 )   sgn ( v )     $ $       即软 阈值 （ soft   thresholding ） 操作 ！       纺射集 投影       集合 $ ( C   =   \ \ { x |   Ax   =   b   \ \ } ) $ , 投影 操作 为       $ $     \ \ Pi _ C ( v )   =   v   -   A ^ \ \ dagger ( Ax   -   b )     $ $       $ ( \ \ dagger ) $ 是   伪逆   。       当 A 是 一个 向量 时 ， 相当于 投影 到 超平面 ，       $ $     \ \ Pi _ C ( v )   =   v   -   \ \ frac { a ^ Tx - b } { | | a | | _   2 ^ 2 }   a     $ $       半 空间       集合 $ ( C   =   \ \ { x |   a ^ Tx   \ \ le   b   \ \ } ) $ , 投影 操作 为       $ $     \ \ Pi _ C ( v )   =   v   -   \ \ frac { ( a ^ Tx - b )   _   +   } { | | a | |   _   2 ^ 2 }   a     $ $       即 要 减掉 向量 在 另外 一边 的 分量 ！       集合 的 支持 函数       集合 C 的 支持 函数 定义 为       $ $     S _ c ( x )   =   \ \ sup _ { y \ \ in   C } y ^ T   x     $ $       结合 解释 是 ， 以 x 为 外法 向量 的 点 就是 支持 点 ！ 支持 函数 与 示性 函数 共轭 $ ( S _ C ^   *   =   I _ C   ) $ 。     根据 Moreau 分解 ， 知       $ $     prox _ { \ \ lambda   S _ c } ( v )   =   v   -   \ \ lambda   \ \ Pi _ C ( v / \ \ lambda )     $ $       注意 最后 一个 式子 的 几何 理解 ！       范数       如果 函数 $ ( f   =   | |   \ \ cdot   | | ) $ ， 那么 对 偶函数 $ ( f ^   *   =   I _ B ) $ ，   B 是 对偶 范数 的 单位 球 ！       $ $     prox _ { \ \ lambda   f } ( v )   =   v   -   \ \ lambda   \ \ Pi _ B ( v / \ \ lambda )     $ $       L2 范数       L2 范数 的 对偶 范数 还是 L2 范数 ， L2 单位 球 就是 欧式 空间 的 单位 球 ！ 所以       $ $     \ \ Pi _ B ( v )   =   \ \ begin { cases }                             v / | | v | |   _   2 ,   \ \ quad   | | v | |   _   2   & gt ;   1 ,   \ \ \ \                             v ,   \ \ quad   | | v | |   _   2   \ \ le   1                             \ \ end { cases }     $ $       所以 L2 范数 的 投影 为       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ begin { cases }                             ( 1   -   \ \ lambda / | | v | |   _   2 )   v ,   \ \ quad   | | v | |   _   2   & gt ;   \ \ lambda ,   \ \ \ \                             0 ,   \ \ quad   | | v | |   _   2   \ \ le   \ \ lambda                             \ \ end { cases }     $ $       也 就是 将 原始 向量 v 沿着 v 方向 减少 $ ( \ \ lambda ) $ 长度 ， 除非 减到 0 向量 ！ 也 叫做   block   soft   thresholding 操作 ！       L1 范数       L1 范数 的 对偶 范数 是 $ ( L _ { \ \ infty } ) $ ， 对应 的 单位 球是 单位 立方体 ， 投影 为       $ $     \ \ Pi _ B ( v )   =   \ \ begin { cases }                             sgn ( v _ i ) ,   \ \ quad   | v   _   i |   & gt ;   1 ,   \ \ \ \                             v _ i ,   \ \ quad   | v _ i |   \ \ le   1                             \ \ end { cases }     $ $       所以       $ $     v _ { i + 1 }   =   \ \ begin { cases }             v _ i   -   \ \ lambda ,   \ \ quad   v _ i & gt ; \ \ lambda   \ \ \ \             - v _ i   +   \ \ lambda ,   \ \ quad   v _ i & lt ; - \ \ lambda   \ \ \ \             0 ,   \ \ quad   other             \ \ end { cases }     $ $               软 阈值 算法       L1 范数 正则       L1 正则 问题       $ $     \ \ min _ x   f ( x )   +   \ \ lambda   | x | _   1     $ $       利用 近似 梯度 法 ， 令 $ ( g = | \ \ cdot |   _   1 ) $ 有 迭代 算法       $ $     z _ k   =   x _ k   -   \ \ eta     \ \ nabla   f ( x _ k )   \ \ \ \     x _ { k + 1 }   =   prox _ { g }   z _ k     $ $       最后 一步 是 一个 投影 ， 根据 定义       $ $     prox _ { \ \ lambda   g }   ( v )   =   \ \ arg   \ \ min _ x   \ \ lambda   | x |   _   1   +   \ \ frac { 1 } { 2 } | | x   -   v | | ^ 2     $ $       对 上式 微分 得       $ $     v _ i   -   x _ i   \ \ in   \ \ lambda   \ \ partial   | x _ i | ,   i = 1 , ...     $ $       上 式 表明 下降 量 不 超过 $ ( \ \ lambda ) $ ， 如果 $ ( v _ i ) $ 绝对值 大于 $ ( \ \ lambda ) $ 那么 下降 不会 越过 不可 微分 点 ， 可以 按照 正常 的 梯度 下降 ，     但是 如果 小于 ， 那么 只能 下降 到 0 ， 才能 保证 上 式 成立 ！       $ $     x _ i   =   \ \ begin { cases }             v _ i   -   \ \ lambda ,   \ \ quad   v _ i & gt ; \ \ lambda   \ \ \ \             - v _ i   +   \ \ lambda ,   \ \ quad   v _ i & lt ; - \ \ lambda   \ \ \ \             0 ,   \ \ quad   other             \ \ end { cases }     $ $       这 表明 ， 加 了 L1 正则 项 ， 相当于 将 阈值 为 $ ( \ \ lambda ) $ 以内 的 分量 都 置 0 ， 以上 的 都 减小 $ ( \ \ lambda ) $ 。       L2 范数       L21 范数       软 阈值 算法 实现       求解 问题       $ $     \ \ min _ x   \ \ frac { 1 } { 2 }   | | Ax   -   b | | ^ 2   +   \ \ lambda _ 2   | | x | |   _   2 ^ 2   +   \ \ lambda _ 1   | | x | |   _   1     $ $       可以 看到 ， 算法 很快 就 收敛 了 ， 并且 L1 正则 很 容易 得到 稀疏 解 ！               import       numpy       as       np         ##   问题 100 维       #         n       =       100       A       =       np     .     random     .     randn     (     n     *     10     ,       n     )       #   超定 方程       xx       =       np     .     random     .     randn     (     n     )       mask       =       np     .     random     .     rand     (     n     )     & lt ;     0.8       xx       =       xx       *       mask       print       &# 39 ; ground   truth : &# 39 ;     ,       xx       b       =       np     .     matmul     (     A     ,       xx     )       +       np     .     random     .     randn     (     n     *     10     )       *       0.1       #   加入 一些 噪声         & gt ; & gt ; & gt ;       Output     :               ground       truth     :       [       0 .                         0.42171383       -     0.94716965         0.54640995         0.67948742         0.88857648               0.14209921       -     0.70685128         0.43310285       -     1.2423563           0.27323468       -     0.23007387               1.53049499       -     0.03076339       -     0 .                         0.53795911         1.1853585           0.66830622               0.0813316         -     0.39508658         0.75451939         0.53967945       -     0 .                       -     0.79440595               0.10416921       -     0.76577241         2.21847433       -     1.38163076         0.6089114           1.18767332               -     1.28999937         0.65445551       -     0.3248272         -     0.88002173       -     0.82729771         0.47309462               -     0.8384278         -     1.66928355         0.85613791         0.31921217         2.51727067         1.11885762               0.38646877         0.32068998         0 .                         1.02912399       -     0.4607417         -     0.84519518               -     0 .                         0.34949314         0.56150765         0.08035849         1.812666           -     1.23004836               1.65564242         0.23581581       -     0.03529459       -     0.33258733       -     0.65909872       -     1.1317373               -     0.46223132         0.97113475         0 .                         0.17753836       -     0 .                         0 .               0.08929848         0.02685682         0 .                       -     0 .                         1.86521051       -     1.02918525               1.39816556       -     0.32507115         0.20111102       -     0 .                       -     1.81123986         0.18043876               0 .                       -     0.84625861         0.8709556           0 .                         0.65961205         2.35225572               0 .                       -     0.04910171       -     1.35667457       -     1.45385942         0.15419398         1.1789595               0.7340732         -     0 .                         0.85819805         0.57832173         0.49845621         0 .                       -     0.6047393               0.99361454         0.45679531         0.28392374     ]           #   目标 函数       def       f     (     x     ,       lab     =     1     ,       lab2     =     0.01     ) :               return       0.5     *     np     .     sum     ( (     np     .     matmul     (     A     ,       x     )       -       b     )     * *     2       )     +       lab       *       np     .     sum     (     np     .     abs     (     x     ) )       +       lab2     *     np     .     sum     (     x     *     x     )         #   除了 L1 正则 项外 的 梯度       def       grad     (     x     ,       lab2     =     0.01     ) :               return       np     .     matmul     (     A     .     T     ,       np     .     matmul     (     A     ,       x     )       -       b     )       +       lab2     *     x         #   软 阈值 迭代 算法         x       =       np     .     random     .     randn     (     n     )       mu       =       0.001       lamb       =       0.01       lab2       =       0.01         for       i       in       range     (     100     ) :               #   STEP1 ： 梯度 下降               x       - =       grad     (     x     ,       lab2     )       *       mu                 #   STEP2 ： 投影 操作 ， 也 就是 软 阈值 操作               for       j       in       range     (     len     (     x     ) ) :                       if       x     [     j     ]       & gt ;         lamb     :                               x     [     j     ]       - =       lamb                       elif       x     [     j     ]       & lt ;       -       lamb     :                               x     [     j     ]       + =       lamb                       else     :                               x     [     j     ]       =       0.0                 #   打印 中间 迭代 结果               if       i       %       10       = =       0     :                       print       i     ,       &# 39 ; loss = &# 39 ;     ,       f     (     x     ,       lamb     ,       lab2     )         print       x         & gt ; & gt ; & gt ;       Output     :               0       loss     =       7532.16879981               10       loss     =       11.0285759018               20       loss     =       10.9840030328               30       loss     =       10.9842744582               40       loss     =       10.9842770859               50       loss     =       10.9842771092               60       loss     =       10.9842771094               70       loss     =       10.9842771094               80       loss     =       10.9842771094               90       loss     =       10.9842771094               [       0 .                         0.41005474       -     0.93416517         0.52872225         0.66901803         0.87710187               0.1254697         -     0.69410934         0.42094427       -     1.23128115         0.25644887       -     0.22262353               1.51692245       -     0.02018758         0 .                         0.53116499         1.17967392         0.64459272               0.06787308       -     0.38043066         0.74631218         0.52152388         0 .                       -     0.79768153               0.09135038       -     0.75555311         2.20245087       -     1.36570672         0.59778203         1.17212485               -     1.27509079         0.63889237       -     0.31797818       -     0.87161519       -     0.81344888         0.464237               -     0.82969714       -     1.66337076         0.83671452         0.30766759         2.50494207         1.1034511               0.38148637         0.31093457         0 .                         1.02118932       -     0.45228788       -     0.8302418           0 .               0.32929066         0.54006833         0.06742762         1.80640673       -     1.2229943           1.64774712               0.21735476       -     0.02296309       -     0.3222897         -     0.65161403       -     1.10748401       -     0.44978335               0.95982422         0 .                         0.16099326       -     0.00337374         0 .                         0.0845288               0.01798603         0 .                         0 .                         1.84882179       -     1.01734222         1.39139325               -     0.31544124         0.18957113         0 .                       -     1.80153297         0.17600738         0 .               -     0.83960423         0.86347204         0 .                         0.64546066         2.35318802         0 .               -     0.03208243       -     1.34212428       -     1.44773413         0.14815159         1.17458127         0.72367534               0 .                         0.84649559         0.56543654         0.47495661         0 .                       -     0.59702874               0.98713984         0.4507846           0.27214572     ]                 交替 方向 乘子 ADMM       求 目标 可分解 优化 问题       $ $     \ \ min _ { x , z }   f ( x )   +   g ( z )   \ \ \ \     s . t .   \ \ quad   Ax   +   Bz   =   c     $ $       利用 增广 拉格朗 日 乘子       $ $     L _ { \ \ rho } ( x ,   z ,   y )   =   f ( x )   +   g ( z )   +   y ^ T ( Ax   +   Bz   -   c )   +   \ \ rho / 2   | | Ax + Bz - c | | ^ 2     $ $       利用 拉格朗 日 对偶 问题 的 性质 可知 ， 在 强 对偶 条件 下 ， 最优 解为       $ $     x   *   ,   y   *   ,   z   *   =   \ \ arg \ \ max _ y   ( \ \ arg \ \ min _ { x , z }   L ( x , z ,   y )   )     $ $       于是 有 交替 迭代 算法 ( ADMM ) :       $ $     x _ { k + 1 }   =   \ \ arg \ \ min _ x   L _ { \ \ rho } ( x ,   z _ k ,   y _ k )   \ \ \ \     z _ { k + 1 }   =   \ \ arg \ \ min _ z   L _ { \ \ rho } ( x _ { k + 1 } ,   z ,   y _ k )   \ \ \ \     y _ { k + 1 }   =   y _ k   +   \ \ rho   ( Ax _ { k + 1 }   + Bz _ { k + 1 }   -   c )     $ $       其中 前 两个 式子 实际上 是 两个 投影 算子 ， 最后 一个 式子 是 用 梯度 上升 求 对 偶函数 的 最大值 ！       当 A = - B = I 时 ， 可 得到 无约束 最优化 问题 $ ( \ \ min _ x   f ( x )   +   g ( x ) ) $ 的 求解 算法       $ $     x _ { k + 1 }   =   prox _ { \ \ lambda _ k   f } ( z _ k   -   u _ k )   \ \ \ \     z _ { k + 1 }   =   prox _ { \ \ lambda _ k   f } ( x _ { k + 1 }   +   u _ k )   \ \ \ \     u _ { k + 1 }   =   u _ k   +   x _ { k + 1 }   -   z _ { k + 1 }     $ $       多个 函数       优化 如下 问题 ， 其中 这些 函数 都 有 可能 是 不 光滑 的       $ $     \ \ min   f _ 1 ( x )   +   ...   +   f _ m ( x )     $ $       将 问题 转化 为 约束 优化 问题       $ $     \ \ min _ { x   \ \ in   D }     f ( x )   \ \ \ \     f ( x )   =   f _ 1 ( x _ 1 )   +   ...   +   f _ m ( x _ m )   \ \ \ \     x   =   ( x _ 1 ,   ... ,   x _ m ) ,   D =   { ( x _ 1 , ... , x _ m )   |   x _ 1 = ... = x _ m }     $ $       相关 资料           比较 全面 介绍 近似算法 的 书 ； Parikh   N ,   Boyd   S .   Proximal   algorithms [ J ] .   Foundations   and   Trends ®   in   Optimization ,   2014 ,   1 ( 3 ) :   127 - 239 .       近似算法 综述 论文 ： Combettes   P   L ,   Pesquet   J   C .   Proximal   splitting   methods   in   signal   processing [ M ] / / Fixed - point   algorithms   for   inverse   problems   in   science   and   engineering .   Springer ,   New   York ,   NY ,   2011 :   185 - 212 .       分布式 ADMM 经典 论文 ： Boyd   S ,   Parikh   N ,   Chu   E ,   et   al .   Distributed   optimization   and   statistical   learning   via   the   alternating   direction   method   of   multipliers [ J ] .   Foundations   and   Trends ®   in   Machine   Learning ,   2011 ,   3 ( 1 ) :   1 - 122 .       压缩 感知 书籍 ： Foucart   S ,   Rauhut   H .   A   mathematical   introduction   to   compressive   sensing [ M ] .   Basel :   Birkh ä user ,   2013 .      ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/proximal-algorithm.html"},
      
      
      
      {"title": "Optimization Methods for Large-Scale Machine Learning", "text": "    Table   of   Contents           关于                           关于       了解 分布式 梯度 优化 方法 ， 来源于 综述 文章 ：     Leon   Bottou ,   Frank   E   Curtis ,   Jorge   Nocedal ,   Optimization   Methods   for   Large - Scale   Machine   Learning ,   2016 ，     发表 于   ICML2016 。        ", "tags": "machine-learning", "url": "/wiki/machine-learning/optimization-method-for-large-scale-machine-learning.html"},
      
      
      {"title": "OWL-QN: Scalable Training of L1-Regularized Log-Linear Models", "text": "    Table   of   Contents           关于           问题           记号           OWL - QN   算法                 关于       论文 导读 ： Scalable   Training   of   L1 - Regularized   Log - Linear   Models ,   Galen   Andrew ,   Jianfeng   Gao     Microsoft   Research   ,   2007 .       问题       优化 问题 ：       $ $     f   ( x )   =   l ( x )   +   r ( x )   \ \ \ \     r ( x )   =   \ \ sum   | x _ i |       $ $       l1   范数 优化 ， 在 0 处 不可 导 。       记号       左右 偏导       $ $     \ \ partial _ i ^ +   f ( x )   =   \ \ lim _ { \ \ alpha   \ \ rightarrow   0 ^ + }   \ \ frac { f ( x   +   \ \ alpha   e _ i )   -   f ( x ) } { \ \ alpha }     $ $           方向 导数   $ ( f ' ( x ;   d ) ) $   表示 在 方向 d   对应 的 方向 。       符号 函数   $ ( \ \ sigma ( x ) ) $       投影 函数           $ $     \ \ pi _ i ( x ;   y )   =   \ \ begin { cases }                                     x _ i   & amp ;   if   \ \ sigma ( x _ i )   =   \ \ sigma ( y _ i )     \ \ \ \                                     0       & amp ;   \ \ text { otherwise }                                     \ \ end { cases }     $ $       OWL - QN   算法       在 一个 象限 中 ， 二阶 近似 是 成立 的 ， 且 二阶 梯度 矩阵 与 正则 项 无关 。       对 任意 符号 向量   $ ( \ \ xi   \ \ in   \ \ { - 1 ,   0 ,   1   \ \ } ^ n ) $ ， 定义       $ $     \ \ Omega _ { \ \ xi }   =   \ \ {   x   \ \ in   R ^ n   |   \ \ pi ( x ,   \ \ xi )   =   x   \ \ }     $ $       在 象限 $ ( \ \ Omega _ { \ \ xi } ) $ 中 ， 目标 函数 可以 写为       $ $     f ( x )   =   l ( x )   +   C   \ \ xi ^ T   x .     $ $       将 这个 函数 扩展 到 整个 空间 得到 扩展 函数 $ ( f _ { \ \ xi } ) $ ， 对于 这个 函数 ， 可以 用拟 牛顿 法 求解 。       伪 梯度 ： 其实 就是 将 某个 方向 不可 微点 的 梯度 在 该 方向 的 分量 置 为 搜索 方向 的 方向 梯度 ； 或置 0 （ xi = 0 点 ） ， 或 正 梯度 或负 梯度 .       $ $     \ \ Diamond _ i   f ( x )   =   \ \ begin { cases }                                     \ \ partial _ i ^ +   f ( x )   & amp ;   \ \ partial _ i ^ +   f ( x )   & lt ;   0     \ \ \ \                                     \ \ partial _ i ^ -   f ( x )       & amp ;   \ \ partial _ i ^ -   f ( x )   & gt ; 0     \ \ \ \                                     0           & amp ;     \ \ text { otherwise }                                     \ \ end { cases }     $ $               与 正常 L - BFGS 算法 的 改动 地方 。           用伪 梯度 替代 梯度       下降 方向 投影 到 伪 梯度 子 空间 ， 即 和 伪 梯度 不同 符号 的 分量 全部 置 0       采用 约束 线性 搜索 ， 以 保证 搜索 不会 超过 该 象限       计算 向量 y 只 需要 损失 函数 就 可以 ， 不用 正则 项           约束 线性 搜索 ： 在 非 0 值 ， 约束 在 原来 的 象限 和 0 值 搜索 ； 在 0 值 约束 在 负 梯度方向 ！       $ $     \ \ xi _ i   =   \ \ begin { cases }                                     \ \ sigma ( x _ i ^ k )   & amp ;   x _ i ^ k   \ \ neq   0     \ \ \ \                                     \ \ sigma ( -   \ \ Diamond _ i   f ( x ^ k ) )       & amp ;   x _ i ^ k   = 0                                     \ \ end { cases }       \ \ \ \     p ^ k   =   \ \ pi ( d ^ k ;   \ \ xi _ k )     \ \ \ \     x ^ { k + 1 }   =   \ \ pi ( x ^ k   +   \ \ alpha   p ^ k ;   x ^ k )     \ \ \ \     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/qwlqn.html"},
      
      
        
        
      
      {"title": "Facebook 预测广告点击率的实践经验", "text": "    Table   of   Contents           Facebook   预测 广告 点击率 的 实践经验           摘要           引言                         Facebook   预测 广告 点击率 的 实践经验       摘要       在线 广告 允许 广告 客户 只 为 可以 测量 的 用户 反应 出价 和 支付 ， 比如 点击 广告 。   所以 ， 在线 广告 系统 中 点击 预测 系统 是 大多数 在线 应用 的 核心 。 在 Facebook 上 ， 每天 活跃 用户 超过 7.5 亿 ， 活跃 广告 客户 有 100 多万 ， 点击率 预估 在 Facebook 广告 中是 一个 具有 挑战性 的 机器 学习 任务 。 本文 介绍 了 一个 结合 决策树 与 逻辑 回归 的 模型 ， 它 超越 了 其中 任何 一个 单独 的 方法 ， 效果 提升 了 超过 3 ％ ， 这个 提升 对 整个 广告 系统 的 效果 有显 着 的 影响 。 然后 我们 探索 一些 基本参数 如何 影响 我们 系统 的 最终 预测 性能 。 我们 发现 ， 最 关键 的 是 要 有 正确 的 特征 ：   那些 有关 用户 历史 信息 或 广告 领域 的 其他 类型 的 特征 。 一旦 我们 有 正确 的 特征 和 正确 的 模型 （ 决策树 加 逻辑 回归 ） ， 其他 因素 影响 很小 （ 在 大规模 的 情况 下 ， 虽然 很小 的 改善 也 是 重要 的 ） 。 选择 最佳 处理 数据 新鲜度 ， 学习 速率 和 数据 采样 只 稍微 提高 了 一点点 ， 远 不及 添加 一个 高 价值 的 特征 ， 或 选择 正确 的 模型       引言       数字 广告 是 一个 价值 数十亿美元 的 行业 并且 每年 大幅 增长 。 在 大多数 在线 广告 平台 ， 广告 的 分配 是 动态 的 ， 基于 用户 观察 后 的 反馈 为 用户 的 兴趣 量身 打造 的 。 机器 学习 在 预估 候选 广告 给 用户 的 效果 中起 着 核心作用 ， 并 增加 了 市场 效率 。     2007 年 ， Varian   [ 11 ] 和 Edelman [ 4 ] 等 人 的 开创性 论文 描述 了 由 Google 和 Yahoo 的 出价 ( bid ) 和 每次 点击 付费 的 先驱 工作 !   同年 ， 微软 也 是 在 此基础 上 基于 竞价 模型 构建 一个 赞助 搜索 市场 [ 9 ] 。   广告 拍卖 的 效率 取决于   对 点击 预测 的 准确度 和 校准 。   该   点击 预测 系统 需要 具有 鲁棒性 和 自适应性   能够 从 大量 的 数据 中 学习 。   目标   本文 的 目的 是 分享 来自 实验 的 见解   考虑 到 这些 要求 并 执行   对 现实 世界 的 数据 。   在 赞助 搜索 广告 中 ， 用户 查询 用于   检索 候选 广告 ， 显式 或 隐式 地   匹配 查询 。   在 Facebook 上 ， 广告 没有 关联   用 查询 ， 而是 指定 人口 和 兴趣   定位 。   因此 ， 广告 的 数量   有 资格 在 用户 访问 Facebook 时 可以 显示   大于 赞助 搜索 。   为了 处理 大量 的 候选 广告   请求 ， 每当 用户 触发 广告 请求   访问 Facebook ， 我们 将 首先 建立 一个 级联 的 分类器   增加 计算成本 。   在 本文 中 ， 我们 重点 介绍   级联 分类器 的 末级 点击 预测 模型 ，   那 就是 为 最终 集合 产生 预测 的 模型   的 候选 广告 。   我们 发现 一个 结合 决策树 的 混合 模型   逻辑 回归 优于 这 两种 方法 之一   靠 自己 超过 3 ％ 。   这种 改善 有 重大意义   影响 整个 系统 的 性能 。   一些   基本参数 影响 最终 的 预测 效果   -   我们 的 系统 。   如 预期 的 那样 最 重要 的 事情   是 要 有 正确 的 功能 ：   形成 关于 用户 或 广告 主宰 其他 类型 的 功能 ，   功能 。   一旦 我们 有 正确 的 功能 和 正确 的 模式   （ 决策树 加 逻辑 回归 ） ， 其他 因素 发挥作用   小角色 （ 尽管 小小的 改进 很 重要   规模 ） 。   为 数据 新鲜度 选择 最佳 处理 方式 ，   学习 率 模式 和 数据 采样 改善 模型   稍微 ， 虽然 比 添加 高 价值 功能 少得 多 ，   或者 选择 合适 的 模型 开始 。   我们 首先 概述 了 我们 在 Sec -   在 第 3 部分 我们 评估 不同 的 概率 线性     第 2 页     分类器 和 多种 在线 学习 算法 。   在 合约   线性 分类 文本 我们 继续 评估 影响   功能 转换 和 数据 新鲜度 。   受到 了 启发   吸取 的 实际 经验教训 ， 特别 是 数据 新鲜度   和 在线 学习 ， 我们 提出 了 一个 模型 架构 ，   企业 在线 学习 层 ， 而 公平 地 生产   紧凑型 号 。   第 4 节 描述 了 一个 关键 组件   为 在线 学习 层 ， 在线 木匠 ， 一个   可以 生成 实况 的 实验性 基础设施   实时 训练 数据流 。   最后 ， 我们 提出 交易 记忆 和 准确性 的 方法   计算 时间 ， 并 应付 大量 的 培训   数据 。   在 第五 部分 中 ， 我们 描述 了 一些 实用 的 方法 ，   包含 大规模 应用程序 的 时间 和 延迟   在 第 6 节 我们 深入 到 了 训练 数据 之间 的 权衡   数量 和 准确性 。   2 . 实验 安装   为了 达到 严格 和 可控 的 实验 ， 我们   通过 选择 任意 一周 准备 训练 数据   在 2013 年 第四季度 。 为了 保持 不变   培训 和 测试数据 在 不同 的 条件 下 ，   削减 了 与 观察 到 的 类似 的 训练 数据   线上 。   我们 把 存储 的 数据 分成 训练 和   测试 并 使用 它们 来 模拟 在线 数据 ，   在线 培训 和 预测 。   相同 的 培训 / 测试数据   被 用作 本文 所有 实验 的 测试 平台 。   评估 指标 ： 因为 我们 最 关心 的   这些 因素 对 机器 学习 模型 的 影响 ，   我们 直接 使用 预测 的 准确性 而 不是 度量   与 利润 和 收入 有关 。   在 这项 工作 中 ， 我们 使用 Normal -   熵 （ NE ） 和 校准 作为 我们 的 主要 评估   度量 。   归一化 熵 更 准确 ，   熵 相当于 每次 印象 的 平均 对数 损失   除以 每次 印象 的 平均 对数 损失   如果 一个 模型 预测 了 背景 点击率   （ CTR ） 为 每个 印象 。   换句话说 ，   由 背景 的 熵 归一化 的 对数 对数 损失   CTR 。   背景 点击率 是 平均 经验 点击率   的 训练 数据 集 。   这 可能 会 更 多 的 描述   -   将 这个 度量 称为 归一化 对数 （ Normalized   Logarithmic ）   失利 。   值越 低 ， 预测 越 好   由 模型 制作 。   这个 正常化 的 原因 是   背景 CTR 越 接近 0 或 1 ，   更 容易 实现 更好 的 日志 丢失 。   除以 en -   背景 CTR 的 回归 使得 NE 不 敏感   背景 点击率 。   假设 一个 给定 的 训练 数据 集   N 个 具有 标签 的 示例 y   i   2   { 1 ， +   1 } 和 估计 的 概率   -   点击 的 能力   我 在 哪里 我 =   1 ， 2 ， ...   N 。   平均 经验值   CTR 作为 p   NE   =   1   N   P   n   我 =   1   （   1   +   y   我   2   log （ p   i   ） +   1y   i   2   日志 （ 1   p   i   ） ）   （ p ⇤ log （ p ） + （ 1   p ） ⇤ log （ 1   P ） ）   （ 1 ）   NE 基本上 是 计算 相对 信息 的 一个 组成部分 ，   增益 （ RIG ） 和 RIG   =   1   NE   图 1 ： 混合 模型 结构   输入 功能   通过 增强 的 决策树 进行 转换 。   每个 单独 的 树 的 输出 被 视为 一个   将 分类 输入 特征 映射 到 稀疏 线性 分类器 。   提升 的 决策树 被 证明 是 非常 强大 的   功能 转换 。   校准 是 平均 估计 点击率 和   经验性 点击率   换句话说 ， 这是 数字 的 比例   预期 点击 次数 达到 实际 观察 到 的 点击 次数 。   校准 是 一个 非常 重要 的 指标 ， 因为 准确 和   经过 精心 校准 的 CTR 预测 对 成功 至关重要   在线 竞价 和 拍卖 。   校准 不同   从 1 开始 ， 模型 越 好 。   我们 只 报告 校准   在 不 重要 的 实验 中 。   请 注意 ， ROC 区域 （ AUC ） 也 是 相当 不错 的   不 考虑 排名 质量 的 度量   校准 。   在 现实 的 环境 中 ， 我们 预计 ，   词典 是 准确 的 ， 而 不是 仅仅 得到 最佳 的 选择 。   错误 的 排列 顺序 ， 以 避免 潜在 的 不足 交付 或 过度 使用 ，   交货 。   NE 衡量 预测 的 好处 ，   明确 地 反映 了 校准 。   例如 ， 如果 一个 模型 over -   预测 2 倍 ， 我们 应用 全球 乘数 0.5 来 解决   校准 后 ， 对应 的 NE 也 会 有所改善   即使 AUC 保持 不变 。   详见 [ 12 ]   研究 这些 指标 。   3 . 预测 模型 结构   在 本节 中 ， 我们 提出 一个 混合 模型 结构 ：   提升 的 决策树 和 概率   -   抽动 稀疏 线性 分类器 ， 如图 1 所示 。 在 Sec -   我们 显示 决策树 是 非常 强大 的 输入   功能 转换 ， 这 大大增加 了 ac -   概率 线性 分类器 的 精度 。   在 3.2 节 我们   展示 如何 更 新鲜 的 训练 数据 导致 更 准确 的 预处理 ，   新词语 。   这 激发 了 使用 在线 学习 的 想法   方法 来 训练 线性 分类器 。   在 3.3 节中 ，   削减 了 两个 家庭 的 在线 学习 变量   概率 线性 分类器 。   我们 评估 的 在线 学习 计划 是 基于     第 3 页     随机 梯度 下降 （ SGD ） 算法 [ 2 ] 适用 于   稀疏 线性 分类器 。   经过 功能 转换 后 ，   广告 印象 是 以 结构化 向量 x   = 给出 的   （ e   i   1   ， ... ， e   i   n   ） 其中 e   i 是 第 i 个 单位 矢量 ， i   1   ， ... ， i   n   是 n 个 分类 输入 特征 的 值 。   在 里面   训练 阶段 ， 我们 也 假设 我们 得到 了 一个 二进制   指示 点击 或 不 点击 的 标签 y   2   { + 1 , 1 } 。   给定 一个 标记 的 广告 印象 （ x ， y ） ， 让 我们 表示 线性   活动 权重 的 组合   s （ y ， x ， w ） =   y · w   T   x   =   y   ñ   X   J   =   1   w   j ， i   j   ，   （ 2 ）   其中 w 是 线性 点击 分数 的 权重 向量 。   在 最 先进 的 贝叶斯 在线 学习 计划 中   概率 回归 （ BOPR ） 描述 [ 7 ] 的 可能性 和   事先 给出   p （ y   |   x ， w ） =   ✓ s （ y ， x ， w ） ◆ ，   p （ w ） =   ñ   ÿ   k   =   1 时   N （ W · K ; μ K ， 2   k   ） ，   （ t ） 是 标准 的 累积 密度 函数   正态分布 和 N （ t ） 是 密度 函数   标准 正态分布 。   在线 培训 已经 实现   通过 期望 传播 与 时刻 匹配 。   最终 的 模型 由 均值 和 方差 组成   权重 向量 的 近似 后验 分布   W 上 。   BOPR 算法 中 的 推理 是 计算   p （ w   |   y ， x ） 并 将 其 投影 到 最 接近 因子 分解 的 Gaus -   （ w ） 的 sian 近似 。   因此 ， 更新 算法   可以 单独 用 所有 的 更新 方程 来 表示   意义 和 方差 的 非 零组件 x （ 见 [ 7 ] ） ：   μ I   J   μ I   J   +   Y ·   2   我   j   ^ · v ✓   s （ y ， x ， μ ）   ^   ◆ ，   （ 3 ）   2   我   j   2   我   j   · “ 1   2   我   j   ^   2   · w ✓   s （ y ， x ， μ ）   ^   ◆ ＃ ，   （ 4 ）   ^   2   =   2   +   ñ   X   J   =   1   2   我   j   。   （ 5 ）   这里 ， 校正 函数 v 和 w 由 v （ t ） ： = 给出   N （ t ） / （ t ） 和 w （ t ） ： =   v （ t ） · [ v （ t ） +   t ] 。   这个 推断 可以   被 视为 信念 向量 μ 和 的 SGD 方案 。   我们 将 BOPR 与 可能性 函数 的 SGD 进行 比较   p （ y   |   x ， w ） =   sigmoid （ s （ y ， x ， w ） ） ，   其中 sigmoid （ t ） =   exp （ t ） / （ 1   +   exp （ t ） ） 。   由此 产生 的 al -   算法 通常 被 称为 Logistic 回归 （ LR ） 。   推断   -   在 这个 模型 中 计算 对数 的 导数 ，   可能性 和 步行 每个 坐标 依赖 步长   这个 梯度 的 方向 ：   w   i   j   瓦特   I   J   +   Y · ⌘ I   J · G （ S （ Y ， X ， W ） ） ，   （ 6 ）   其中 g 是 所有 非零 com -   并且 由 g （ s ） 给出 ： =   [ y （ y   +   1 ） /   2   y · sigmoid （ s ） ] 。   请 注意 ， （ 3 ） 可以 被 看作 是 一个 每 坐标 梯度 de -   香味 等 （ 6 ） 上 的 平均 矢量 μ ， 其中 步长 ⌘ I   J   是 由 信念 不确定性 自动控制 的 。   在   3.3 小节 将 介绍 各种 步长 函数 ⌘   并 与 BOPR 进行 比较 。   上述 的 基于 SGD 的 LR 和 BOPR 都 是 流式 的   学习者 一个 接 一个 地 适应 训练 数据 。   3.1 决策树 特征 转换   有 两种 简单 的 方法 来 转换 输入 功能   的 线性 分类器 ， 以 提高 其 准确性 。   对于   连续 的 功能 ， 学习 非线性 的 简单 技巧   转化 是 为了 将 这个 特征 和 待处理 的 垃圾箱   dex 作为 一个 分类 特征 。   线性 分类器 有效   学习 特征 的 分段 恒定 非线性 映射 。   学习 有用 的 bin 边界 是 很 重要 的 ， 并且 有   许多 信息 最大 限度 地 做到 这 一点 。   第二个 简单 而 有效 的 转换 包括   构建 元组 输入 功能 。   对于 分类 特征 ，   蛮力 方法 包括 采取 笛卡尔 产品 ，   uct ， 即 创建 一个 新 的 分类 特征   作为 原始 特征 的 所有 可能 值 的 值 。   不   所有 的 组合 都 是 有用 的 ， 而 那些 不是 可以 的   剪掉 了 。   如果 输入 功能 是 连续 的 ， 可以 这样 做   联合 装箱 ， 例如 使用 kd 树 。   我们 发现 ， 推动 决策树 是 一个 强大 的 ， 非常   便捷 的 方式 来 实现 非线性 和 元组 transfor -   我们 刚刚 描述 的 那种 。   我们 对待 每个 指标 ，   个人 树 作为 一个 分类 特征 ， 作为 值   叶子 的 索引 最终 落入 。 我们 使用 1 -   这种 类型 的 特征 的 - K 编码 。   例如 ， 考虑   图 1 中 的 增强 树 模型 有 2 个子 树 ， 其中   第一个 子树有 3 个 叶子 和 第二个 叶子 。   如果   实例 结束 于 第一个 子树 中 的 叶子 2 和 叶子 1 中   第二个 子树 ， 将 整体 输入 到 线性 分类器 中   是 二进制 向量 [ 0 , 1 , 0 , 1 , 0 ] ， 其中 前 3 个 条目   对应 第一个 子树 的 叶子 ， 最后 2 个   那些 第二个 子树 。   推动 决策树 我们   使用 遵循 梯度 增压 机 （ GBM ） [ 5 ] ， 其中   使用 经典 的 L   2   - TreeBoost 算法 。   在 每个 学习   -   迭代 ， 创建 一个 新 的 树来 模拟 残差   以前 的 树木 。   我们 可以 理解 提升 的 决策树   作为 一个 监督 的 特征 编码   将 实值 向量 转换 为 紧凑 二进制 值   向量 。   从根 节点 到 叶 节点 的 遍历 表示   某些 特征 的 规则 。   拟合 一个 线性 分类器   二元 向量 本质 上 是 学习 权重 的 集合   规则 。   提升 的 决策树 以 批处理 方式 进行 培训 。   我们 进行 实验 来 显示 包括 树 的 效果   作为 线性 模型 的 输入 。   在 这个 实验 中   我们 比较 两个 logistic 回归 模型 ，   变换 和 其他 与 平原 （ 非 转换 ）   特征 。   我们 也 只 使用 一个 提升 的 决策树 模型   比较 。   表 1 显示 了 结果 。   树状 特征 转换 有助于 减少 标准化 的 En -   相对 于 标准化 来说 超过 3.4 ％   没有 树 变换 的 模型 的 熵 。   这是 一个   非常 显着 的 相对 改善 。   作为 参考 ，   ical   feature   engineering   experiment   will   shave   off 夫妇   几十 ％ 的 相对 NE 。   有趣 的 是 看到     第 4 页     表 1 ： Logistic 回归 （ LR ）   锡永树 （ 树 ） 做出 强有力 的 组合 。   我们   评估 他们 的 归一化 熵 （ NE ）   相对 于 只有 树 的 模型 。   模型 结构 NE （ 仅限于 树 ）   LR   + 树木   96.58 ％   只有 LR   99.43 ％   树 只   100 ％   （ 参考 ）   图 2 ： 预测 精度 作为 一个 函数   训练 和 测试 之间 的 延迟 以天 为 单位 。   Accu -   活动 表示 为 归一化 熵 相对 于   最坏 的 结果 ， 得到 的 树木 模型   推迟 6 天 。   孤立 使用 的 LR 和 Tree 模型 具有 相似性 ，   rable 预测 准确性 （ LR 是 好 一点 ） ， 但 它 是   他们 的 组合 产生 了 一个 精确 的 飞跃 。   中 的 收益   预测 准确性 显 着 ;   供 大家 参考   特征 工程 实验 只能 减少   归一化 的 熵 由 百分之几 分 之一 。   3.2 数据 新鲜度   点击 预测 系统 通常 部署 在 动态 环境 中 ，   数据分布 随 时间 变化 的 数据 。   我们   研究 训练 数据 新鲜度 对 预测 每个 人 的 预测 效果 ，   formance 。   要 做到 这 一点 ， 我们 在 一个 特定 的 日子 里 训练 模型   并 在 连续 的 几天 进行 测试 。   我们 运行 这些 实验   对于 一个 推动 的 决策树 模型 和 一个 逻辑 的   具有 树 变换 的 输入 特征 的 回归 模型 。   在 这个 实验 中 ， 我们 训练 一天 的 数据 ， 并 进行 评估   连续 六天 计算 归一化   每个 熵 。   结果 如图 2 所示 。   预测 精度 明显降低 ， 因为 两种 模式   训练 和 测试 集 之间 的 延迟 增加 。   对于 这 两个 mod -   可以 看出 NE 可以 减少 近似值   从 每周 的 培训 到 每天 的 培训 只有 1 ％ 。   这些 发现 表明 ， 每天 都 值得 重新 培训   基础 。   一种 选择 是 重复 日常 工作   可能 会 批量 培训 模型 。   所 需要 的 时间   再 培训 推动 的 决策树 根据 不同 的 因素 而 有所不同   如 培训 的 例子 数量 ， 树木 的 数量 ，   每棵 树上 的 叶子 数量 ， cpu ， 内存 等 。 可能 需要   超过 24 小时 ， 建立 一个 数百人 的 助推 模型   数以亿计 的 实例 中 ，   GLE 核心 CPU 。   在 实际 情况 下 ， 可以 完成 培训   在 几个 小时 内 通过 多 核心 的 充分 并发   整体 拥有 大量 内存 的 机器   训练 集 。   在 下 一节 我们 考虑 一个 替代 方案 。   推动 决策树 可以 每天 或 每个 cou -   几天 的 时间 ， 但 线性 分类器 可以 在 近处 训练   通过 使用 一些 在线 学习 的 风味 实时 。   3.3 在线 线性 分类器   为了 最大 限度 地 提高 数据 的 新鲜度 ， 一种 选择 是 训练   在线 分类器 ， 即 直接 作为 标签   广告 印象 到达 。   在 即将 到来 的 第四 部分 中 ，   扫描 一个 可以 实时 生成 的 基础设施   训练 数据 。   在 本节 中 ， 我们 将 评估 几种 方法   为 基于 SGD 的 在线 学习 设置 学习 率 ，   gistic 回归 。   然后 ， 我们 将 最好 的 变体 与 在线 比较   学习 BOPR 模型 。   就 （ 6 ） 而言 ， 我们 探讨 以下 选择 ：   1 . 每 坐标 学习 率 ：   迭代 t 的 ture 被 设置 为   ⌘ t   ， i   =   ↵   +   qP   t   j   =   1   r   2   J ， I   。   是 两个 可调 参数 （ 在 [ 8 ] 中 提出 ） 。   2 . 权重 平方根 学习 率 ：   ⌘ t   ， i   =   ↵   pn   t ， i   ，   其中 n   t ， i 是 具有特征 的 总 训练 实例   我 直到 迭代 t 。   3 . 重量 学习 率 ：   ⌘ t   ， i   =   ↵   不   ， 我   。   4 . 全球 学习 率 ：   ⌘ t   ， i   =   ↵   PT 。   5 . 不断 学习 率 ：   ⌘ t   ， i   = ↵ 。   前 三个 方案 分别 设置 学习 率   特征 。   最后 两个 使用 相同 的 速度 为 所有 功能 。   所有   可调 参数 通过 网格 搜索 （ optima ） 进行 优化   详见 表 2 . ）   我们 将 学习 率 下降 0.00001 连续   学习 。   我们 用 相同 的 数据 训练 和 测试 LR 模型   以上 学习 率 计划 。   实验 结果 是   如图 3 所示 。   从 以上 结果 可以 看出 ， SGD 以 每 坐标 学习 为主   速率 达到 最好 的 预测 精度 ， 用 NE   al -   比 使用 每 重量 学习 率低 5 ％     第 5 页     表 2 ： 学习 速率 参数   学习 率 模式   参数   每 坐标   0.1   =   0.1 ， =   1.0   重量 平方根   =   0.01   每 重量   =   0.01   全球   =   0.01   不变   0.000   =   0.0005   图 3 ： 不同 学习 的 实验 结果   LR 新加坡元 汇率 。   X 轴 cor -   回应 不同 的 学习 率 计划 。   我们   在 左侧 绘制 标定 y -   轴 ， 而 归一化 的 熵 则 用   右边 的 第二 y 轴 。   表现 最差 。   这个 结果 是 符合 结论 的 ，   sion   [ 8 ] 。   新元 与 重量 平方根 和 恒定   学习 率 达到 相似 和 略 差 的 NE 。   该   其他 两种 方案 比 以前 差 很多   版本 。   全球 学习 率 主要 由于 失败   每个 功能 上 的 训练 实例 数量 不 平衡 。   由于 每个 培训 实例 可能 包含 不同 的 功能 ，   一些 受欢迎 的 功能 ， 接受 更 多 的 培训 ，   立场 比 别人 。   根据 全球 学习 率 计划 ，   对于 具有 较 少 实例 的 特征 的 学习 率 ，   折痕 太快 ， 并 防止 收敛 到 最佳 状态   重量 。   虽然 每 重量 学习 费率 计划 ad -   打扮 这个 问题 ， 它 仍然 失败 ， 因为 它 减少 了   所有 功能 的 学习 速度 太快 。   培训 终止   模型 收敛 到 次 优点 的 地方 太早 了 。   这 就 解释 了 为什么 这个 方案 的 性能 最差   在 所有 的 选择 之中 。   有意思 的 是 BOPR 更新 方程   （ 3 ） 平均数 与 每 坐标 学习 最 相似   SGD 的 LR 版本 。   有效 的 学习 率   BOPR 是 针对 每个 坐标 而定 的 ， 取决于   与 每个 个体 相关 的 体重 的 后 验方 差   协调 ， 以及 给予 什么 标签 的 “ 惊喜 ”   模型 会 预测 [ 7 ] 。   我们 进行 了 一个 实验 ，   通过 每个 坐标 SGD 和 BOPR 训练 LR 的 形式 。   我们 训练 LR 和 BOPR 模型   排序 器 ”   Online ' Joiner ”   培训师 ”   功能 { x }   点击 { y }   楷模   { x ， y }   广告   图 4 ： 在线 学习 数据 / 模型 流程   数据 并 评估 下 一次 的 预测 性能   天 。   结果 如表 3 所示 。   表 3 ： 每 坐标 在线 LR 与 BOPR   模型 类型   NE （ 相对 于 LR ）   LR   100 ％   （ 参考 ）   BOPR   99.82 ％   也许 正如 人们 所 期望 的 那样 ， 鉴于 质量 的 相似性   更新 公式 ， BOPR 和 LR 用 SGD 进行 培训   每 坐标 学习 率有 非常 相似 的 预测 ，   无论是 NE 还是 校准 都 可以 提高 性能   （ 未 在 表格 中 显示 ） 。   LR 比 BOPR 的 一个 优点 是 模型 的 大小   是 一半 ， 因为 只有 一个 相关 的 权重   稀疏 特征值 ， 而 不是 均值 和 方差 。   去   等待 实施 ， 更 小 的 模型 大小 可能   导致 更好 的 缓存 局部性 ， 从而 更快 的 缓存 查找 。   在   在 预测 时间 计算 费用 的 条款 ， LR   模型 只 需要 一个 内部 产品 的 功能 vec -   tor 和 权重 矢量 ， 而 BOPR 模型 需要 两个   方差 向量 和 均值 向量 的 内积   与 特征向量 。   BOPR 比 LR 更 重要 的 一个 优点 是 ，   贝叶斯 公式 ， 它 提供 了 一个 完整 的 预测 分布   -   点击 概率 。   这 可以 用来 com -   预测 分布 的 百分位 数 ， 可以   用于 探索 / 开发 学习 计划 [ 3 ] 。   4 . 在线 数据 连接器   前 一节 确定 了 更新 的 训练 数据   导致 预测 准确度 提高 。   它 还 提出 了 一个   线性 分类器 层 的 简单 模型 架构   在线 培训 。   本 部分 介绍 了 一个 实验 系统 ，   用于 训练 线性 分类器 的 实时 训练 数据 ，   通过 在线 学习 更加 困难 。   我们 将 这个 系统 称为   “ 在线 木匠 ” ， 因为 它 的 关键 操作 是 加入   标签 （ 点击 / 不 点击 ） 到 培训 输入 （ 广告 印象 ） 中   一个 在线 的 方式 。   类似 的 基础设施 用于 流   例如 在 Google 广告 系统 [ 1 ] 中 学习 。   在线 木匠 输出 实时 训练 数据流   到 一个 名为 Scribe 的 基础设施 [ 10 ] 。   而 积极 的     第 6 页     标签 （ 点击 ） 是 明确 的 ， 没有 这样 的 事情   用户 可以 按 “ 不 点击 ” 按钮 。   为此 ， 一个   印象 被 认为 是 否定 否定 点击 标签   用户 在 固定 之后 没有 点击 广告 ， 并且 充分 地   看到 广告 后 很长 一段时间 。   的 长度   等待时间 窗口 需要 仔细 调整 。   使用 等待时间 过长 会 延迟 实时 训练 ，   增加 数据 并 增加 分配 给 缓存 的 内存   等待 点击 信号 的 印象 。   太短 了   时间 窗口 导致 一些 点击 丢失 ， 因为   相应 的 印象 可能 已经 被 刷新 ，   作为 未 点击 的 贝壳 。   这 对 “ 点击 覆盖 ” 产生 了 负面影响   所有 点击 成功 加入 展示 的 比例 。   因此 ， 网上 木匠 系统 必须 取得 平衡   新近 度 和 点击率 之间 。   没有 完整 的 点击 覆盖 意味着 实时 培训   -   一套 将 是 有 偏见 的 ： 经验性 的 点击率 是 有点   低于 事实 的 真相 。   这 是因为 一小部分   的 标记 为 未 点击 的 展示 次数 将会 是 la -   如果 等待时间 足够 长 ， 可以 点击 。   然而 在实践中 ， 我们 发现 很 容易 减少 这种 情况   对 等待 窗口 百分比 的 小数点 偏差   大小 ， 导致 可 管理 的 内存 要求 。   在   此外 ， 这个 小 偏差 可以 测量 和 纠正 。   可以 找到 更 多 关于 窗口 大小 和 效率 的 研究   在 6 点钟 ] 。   在线 木匠 被 设计 来 执行 分布式   流到 流 加入 广告 展示 和 广告 点击 uti -   将 请求 ID 作为 连接 的 主要 组件   谓词 。   每次 用户 每次 生成 一个 请求 ID 时 ，   在 Facebook 上 形成 一个 触发 刷新 的 动作   他们 所 接触 到 的 内容 。   原理图 数据 和 模型   显示 了 在线 加工者 随后 在线 学习 的 流程   如图 4 所示 。 初始 数据流 是 当 用户 产生 的   访问 Facebook 并 向 排名 提出 请求 ，   没有 广告   广告 被 传回 给 用户 的 设备   并行 使用 每个 广告 和 相关 的 功能   排名 印象 被 添加 到 印象 流中 。   如果 用户 选择 点击 广告 ， 该 点击 将 被 添加   到 点击 流 。   实现 流到 流 的 连接   系统 利用 一个 HashQueue 组成 一个 First - In -   先出 队列 作为 缓冲 窗口 和 快速 的 散列图   随机 访问 标签 展示 次数   一个 HashQueue   在 键值 对 上 有 三种 操作 ： 排队 ，   出列 和 查找 。   例如 ， 要 排队 一个 项目 ， 我们   将 项目 添加 到 队列 的 前面 并 在 中 创建 一个 键   散列 映射 ， 其值 指向 队列 的 项目 。   只有 完整 的 加入 窗口 过期 后 ， 标签 才 会 显示   印象 被 发射 到 训练 流 。   如果 没有 点击   它 将 作为 负面 标记 的 例子 发射 出去 。   在 这个 实验 设置 中 ， 教练员 不断 学习   从 培训 流程 中 发布 新款 车型 ，   对于 Ranker 而言 。   这 最终 形成 一个 严密 的 封闭   循环 的 机器 学习 模型 的 变化 ，   可以 捕获 分布 或 模型 性能 ，   了解 到 ， 并 在短期内 继续 纠正 。   实验 一个 重要 的 考虑 因素   实时 训练 数据 生成 系统 是 需要 的   建立 防止 异常 的 保护 机制   腐败 在线 学习 系统 。   让 我们 来 简单 介绍 一下   例 。   如果 点击 流 由于 某些 原因 而 变得 陈旧   数据 基础设施 问题 ， 网上 木匠 将 产生 train -   这些 数据 具有 非常 小 的 甚至 为 零 的 经验性 点击率 。   由此 ， 实时 教练 将 开始   错误 地 预测 非常低 ， 或者 接近 零 概率   单击 。   广告 的 预期 价值 自然 取决于   估计 的 点击 概率 ， 以及 一个 结果   错误 地 预测 非常低 的 点击率 是 系统 可能   展示 广告 展示 次数 减少 。   异常 检测   -   重刑 机制 可以 在 这里 帮助 。   例如 ，   从 网上 木匠 matically 断开 在线 教练   如果 实时 训练 数据分布 突然 改变 。   5 . 包含 内存 和 延迟   5.1 增强 树 的 数量   模型 中 的 树越 多 ， 所 需 的 时间 就 越长   做 一个 预测 。   在 这个 部分 ， 我们 研究 了 这个 效果   估计 精度 的 推动 树 数量 。   我们 改变 树 的 数量 从 1 到 2   000 ， 并 训练   模型 在 一天 的 数据 ， 并 测试 预测 per -   在 第二天 的 表演 。   我们 只 限于 此   每棵 树上 有 12 片 叶子 。   与 之前 的 实验 类似 ，   我们 使用 归一化 熵 作为 评估 指标 。   该   实验 结果 如图 5 所示 。   图 5 ： 增强 次数 的 实验 结果   树木 。   不同 系列 对应 不同 的 子 系列   楷模 。   x 轴 是 助推 树 的 数量 。   Y 轴 是 归一化 的 熵 。   热带植物 减少 ， 因为 我们 增加 了 增加 树木 的 数量 。   然而 ， 增加 树木 的 收益 会 减少 重新 生长 ，   转 。   NE 的 改进 几乎 都 来自 前 500 名   树木 。   最后 一千 棵 树 使 NE 降低 不到 0.1 ％ 。   此外 ， 我们 看到 子 模型 的 归一化 熵   2 棵 树 开始 退化 后 ， 1000 棵 树 。   之所以 这样 做 ，   现象 是 过度 配合 。   由于 子 模型 的 训练 数据   2 比子 模型 0 和 1 小 4 倍 。   5.2 提升 功能 重要性   特征 计数 是 另 一个 模型 特征 ，   在 估算 精度 和 计算 之间 取舍 ，   性能 。   为了 更好 地 理解 功能 的 效果   算上 我们 首先 将 特征 重要性 应用 于 每个 特征 。   为了 衡量 我们 使用 的 功能 的 重要性   统计 促进 特征 重要性 ，     第 7 页     确定 归因于 特征 的 累计 损失 减少 量 。   在 每个 树 节点 结构 中 ， 选择 一个 最佳 特征   分割 以 最大化 平方 误差 减少 。   由于 一个 功能 ，   可以 用于 多种 树木 ， （ 增强 特征   重要性 ） 为 每个 功能 是 通过 求和 来 确定 的   所有 树木 的 特定 特征 的 总 减少 量 。   通常 情况 下 ， 少数 特征 贡献 了 主   -   而 其余 特征 具有 解释性 权力   只有 边际 贡献 。   我们 看到 这种 模式   当 绘制 功能 的 数量 与 他们 的 cumu -   图 6 中 的 特征 重要性 。   图 6 ： 提升 功能 重要性   X 轴 cor -   响应 功能 的 数量 。   我们 画 功能   在 左侧 小学 对数 的 重要性   y 轴 ， 而 累积 特征 的 重要性 是   与 右边 的 副 y 轴 一起 显示 。   从 以上 的 结果 可以 看出 ， 前十名 的 特点 是   负责 总 功能 重要性 的 大约 一半 ，   而 最近 的 300 个 功能 贡献 不到 1 ％ 的 功能   重要性 。   基于 这个 发现 ， 我们 进一步 做 实验   只 保留 了 前 10 , 20 , 50 , 100 和 200 个 功能 ，   并 评估 表现 如何 影响 。   的 结果   实验 如图 7 所示 。 从图 中 我们 可以 看出   可以 看到 归一化 的 熵 有 类似 的 递减   返回 属性 ， 因为 我们 包含 更 多功能 。   下面 我们 将 对 这个 实用性 进行 一些 研究   的 历史 和 背景 特征 。   由于 数据 sen -   敏度 的 性质 和 公司 政策 ， 我们 不 能够   揭示 在 我们 实际 使用 的 功能 细节 。 有些 EX -   充足 的 上下文 特征 可以 是 一天 的 当地 时间 ， 天   周等 历史 特征 可以 是 累计数   的 点击 广告 ， 等等 。   5.3 历史 特点   在 升压 模式 中 使用 的 功能 可 分为   分为 两种 类型 ： 上下文 特征 和 历史 特点 。   语境 功能 的 价值 完全 取决于 CUR -   租 有关 的 上下文 信息 ， 其中 广告 是   示出 的 ， 诸如 由 用户 或 CUR - 所 使用 的 设备   租 页面 ， 用户 上 。 相反 ， 历史   功能 依赖于 广告 或 用户 以前 的 互动 ，   例如 ， 通过 广告 的 点击率 在 上周 ， 或   平均 点击 通过 用户 的 速度 。   图 7 ： 结果 与 顶部 fea - 推进 模型   功能 。 我们 利用 校准 在 左侧 革命制度党   玛丽 y 轴 ， 虽然 示出 归一化 的 熵   与 右手 侧 次级 y 轴 。   在 这 一部分 ， 我们 研究 如何 系统 的 性能   取决于 两个 类型 的 特征 。 首先 ， 我们 检查   这 两种 类型 的 特点 的 相对 重要性 。 我们 这样 做 是 通过   按 重要性 排序 的 所有 功能 ， 然后 计算 出 per -   在 第 k 个 重要 特征 历史 特色 centage 。   结果 在 图 8 中由该 结果 所示 ， 我们 可以 看到   图 8 ： 结果 历史风貌 百分比 。   X 轴 对应 于 特征 数 。 Y 轴   给 的 历史 特点 在 顶部 K - 百分比   重要 特征 。   该 历史风貌 提供 了 相当 多 的 explana -   保守党 功率 比 上下文 特征 。 排名 前 10 的 特征 或   -   按 重要性 dered 都 是 历史 特色 。 之间   排名 前 20 位 的 特点 ， 也 有 只有 2 上下文 特征 ， 尽管   历史 特征 占据 的 特征 大致 75 ％ 在   此 数据 集 。 为了 更好 地 理解 的 比较 值   从 各 类型 合计 ， 我们 班 列车 2 个 助推 功能   荷兰 国际 集团 车型 只有 上下文 特征 ， 只有 历史   功能 ， 那么 这 两个 型号 的 完整 比较   模型 的 所有 功能 。 结果 示于 表 4 。   从表中 ， 我们 可以 再次 确认 ， 在 他 的   -   汇总   torical 功能 发挥 出比 上下文 特征 发挥 更大 的 作用 。     第 8 页     表 4 ： 具有 不同 类型 的 推进 fea - 模型   功能   特征 NE 的 类型 （ 相对 于 语境 ）   所有   95.65 ％   历史 的   96.32 ％   上下文   100 ％   （ 参考 ）   没有 只 上下文 特征 ， 我们 衡量 4.5 ％ 的 损失   预测 精度 。 相反 ， 没有 上下文   功能 ， 我们 遭受 的 预测 精度 小于 1 ％ 的 损失 。   应该 注意 到 ， 上下文 功能 都 非常 的 IM   portant 处理 冷启动 问题 。 对于 新 用户 和   广告 ， 上下文 特征 是 必不可少 的 一种 合理   点击率 预测 。   在 下 一步 ， 我们 评估 与 他 的   -   只有 在 训练 的 模型   在 连续 torical 功能 或 上下文 特征   周来 测试数据 新鲜度 的 特征 依赖 。   该   结果 示于 图 9 。   图 9 ： 结果 datafreshness 用于 不同 类型   的 功能 。 X 轴 是 而 y 轴上 的 评估 日期   是 归一化 熵 。   从图 中 我们 可以 看到 ， 与 情境 模型   功能 更 依赖 比 历史数据 新鲜度   特征 。   这 是 与 我们 的 直觉 线 ， 因为 历史 fea -   功能 形容 长期 积累 的 用户 行为 ，   比 上下文 特征 更加 稳定 。   6 . 海量 训练 数据 期 应对   Facebook 的 广告 印象 数据 的 一整天 可以 包含   大量 实例 。 请 注意 ， 我们 不能   透露 实际 数字 ， 因为 它 是 保密 的 。 但 小   一天 的 有 价值 的 数据 的 部分 可以 有 几百   数以百万计 的 实例 。 用于 控制 的 一种 常用 技术   培训 成本 降低 了 培训 的 数据量 。   在 这 一节 中 ， 我们 评估 两种 技术 下 采样   数据 ， 均匀 子 采样 和 负 向下 采样 。   在   每次 我们 训练 了 一套 增强型 树 模型 与 600 棵 树 情况   并 同时 使用 校准 和 标准化 评估 这些   熵 。   6.1 统一 的 二次 抽样   培训 行 统一 欠 采样 是 一个 诱人 的 AP -   proach 减少 数据量 ， 因为 它 是 既 容易   实现 并且 可以 使用 所 产生 的 模型 与   -   出来 的 二次 抽样 训练 数据 和 修改 都   非 二次 抽样 的 测试数据 。 在 这 一部分 ， 我们 评估 了 一组   大约 成倍增加 二次 采样率 。   对于   每 一个 速度 ， 我们 培养 了 增强型 树 模型 ， 在 这个 采样   率 从 基部 数据 集 。 我们 改变 二次 采样率   在 { 0.001 , 0.01 ， 0.1 ％ ， 0.5 ％ ， 1 } 。   对 数据量 的 结果 在 图 10 中 它 是 在 示   图 10 ： 数据量 试验 的 结果 。   该   X 轴 对应 于 训练 实例 数 。   我们 利用 校准 在 左侧 主   y 轴 ， 而 归一化 的 熵 被 示为 具有   右手 侧 次级 y 轴 。   与 我们 的 直觉 线 更 多 的 数据 带来 更好 的 per -   formance 。 此外 ， 数据 表明 体积 dimin -   ishing 在 预测 准确度 方面 的 回报 。 只用   数据 的 10 ％ ， 则 归一化 的 熵 仅 1 ％ reduc -   重刑 在 相对 于 整个 训练 数据 集 的 性能 。   在 这个 采样率 校准 没有 显示 性能   减少 。   6.2 负下 采样   一流 的 失衡 已经 研究 了 许多 研究 人员 和   已 被 证明 对 perfor - 显著 影响   学习 模式   -   曼斯 。 在 这 一部分 中 ， 我们 调查   使用 负 向下 采样 的 解决 类 不 平衡   问题 。   我们 经验 与 不同 的 负 试验   下 采样率 来 测试 的 预测 精度   学习 模型 。 我们 而 变化 { 0.1 ， 0.01 ， 0.001 ， 0.0001 } 的 速率 。   实验 结果 示于 图 11 。   从 结果 中 我们 可以 看到 ， 负 下降 SAM -   pling 率 对 的 性能 显著 影响   训练 模型 。 最佳 的 性能 与 neg - 实现   ative 下行 采样率 设置 为 0.025 。   6.3 型号 重新 校准   负 采样 可以 加快 培养 和 提高   模型 的 性能 。 需要 注意 的 是 ， 如果 一个 模型 在 数据 训练     第 9 页     图 11 ： 阴性 实验 结果 比较 下降   采样 。 X 轴 对应 于 不同 nega -   略去 下行 采样率 。 我们 利用 校准 的   左手 侧主 y 轴 ， 而 归一化 的   熵 被 示出 具有 右手 侧 次级   y 轴 。   设置 有 负下 采样 ， 也 校准 预   文辞 的 采样 空间 。 例如 ， 如果 aver -   取样 前 年龄 的 点击率 是 0.1 ％ ， 我们 做 一个 负 0.01   下 采样 ， 经验 CTR 将 成为 约 10 ％ 。   我们 需要 重新 校准 的 实际 流量 实验 模型   并 取回 与 q 中 的 0.1 ％ 的 预测 =   p   P   + （ 1P ） /   w 的   其中 p 是 下 采样 空间 和 瓦特 的 预测   负 采样率 。   7 . 讨论   我们 从 实验   -   提出 了 一些 实用 的 经验教训   荷兰 国际 集团 与 Facebook 的 广告 数据 。 这 一直 激励 着 一个 有 前途 的   对于 点击 预测 混合 模型 架构 。   • 数据 新鲜度 的 问题 。 值得 一 至少 再 培训   日常 。 在 本文 中 ， 我们 更进一步 讨论   各种 在线 学习 方案 。 我们 还 提出   基础设施 ， 使 生成 的 实时 培训   数据 。   • 转型 实值 输入 功能 与 提升   决策树 显著 提高 预测 AC -   概率 线性 分类 的 curacy 。   这 激励 着   该 串接 升压 混合 模型 架构   决策树 和 稀疏 线性 分类 。   • 最 优惠 的 在线 学习 方法 ： LR 每个 坐标   学习 率 ， 这 最终 是 类似 的 在 per -   与 BOPR   formance ， 并 执行 胜过 一切   正在 研究 其他 LR   SGD 方案 。 （ 表 4 ， 图 12 ）   我们 已经 描述 的 技巧 来 维持 内存 和 延迟 CON 组   tained 在 大规模 机器 学习 应用   • 我们 已经 提出 的 数量 之间 的 权衡   提高 决策树 和 准确性 。 有利 的 是 ，   保持 树木 小 ， 以 保持 计算 的 数量   记忆 包含 。   • 提振 决策树 给 做 的 一种 便捷 方式   通过 功能 重要 手段 特征选择 。   一   可 积极 地 减少 的 活性 特征 的 数量   而 只有 适度 的 伤害 预测 精度 。   • 我们 分析 了 使用 历史 fea - 的 影响   功能 与 上下文 特征 的 组合 。 对于 广告   和 用户 提供 的 历史 ， 这些 功能 提供 了 优于   预测 性能比 背景 特征 。   最后 ， 我们 已经 讨论 了 欠 采样 培训 方式   数据 ， 都 均匀 地 而且 更 有趣 在 偏置   办法 只有 负面 例子 子 采样 。  ", "tags": "machine-learning/paper-translate", "url": "/wiki/machine-learning/paper-translate/2018-01-01-ctr-facebook-2014.html"},
      
      
      
      {"title": "Parameter Server for Distributed Machine Learning", "text": "    Table   of   Contents           关于           一个 优化 问题                 关于       论文 ： Parameter   Server   for   Distributed   Machine   Learning ， NIPS2014 。       一个 优化 问题       近似 梯度 法 ：       最小化 下述 问题       $ $     f ( w )   +   h ( w )     $ $       定义 近似 操作 ：       $ $     \ \ text { Prox } _ { \ \ gamma } ( x )   =   \ \ arg   \ \ min _ { y   \ \ in   \ \ mathbb { X } }   h ( y )   +   \ \ frac { 1 } { 2   \ \ gamma }   | |   x   -   y   | | ^ 2     $ $       f 是 可微 的 ， h 有 可能 不可 微 ， 那么 ， 对于 学习 率   $ ( \ \ gamma _ t   & gt ;   0 ) $   迭代 方程 是 ： ？       $ $     w ( t   +   1 )   =   \ \ text { Prox } _ { \ \ gamma _ t }   [ w ( t )   -   \ \ gamma _ t   \ \ nabla   f ( w ( t ) ) ]     \ \ text { for   }   t   \ \ in   \ \ mathbb { N }     $ $       一个 博客 ：     http : / / blog . csdn . net / lanyanchenxi / article / details / 50448640             异步 更新 ， 机器 学习 算法 具有 容错 能力      ", "tags": "machine-learning", "url": "/wiki/machine-learning/ps.html"},
      
      
      {"title": "Pattern Recognition and Machine Learning - Bishop", "text": "    Table   of   Contents           关于           引言           曲线拟合                         关于       Bishop 写得 PRML 无疑 是 机器 学习 领域 的 权威 著作 ， 是 加强 机器 学习 理论知识 必看 的 书籍 之一 。     这里 是 我 在 阅读 这 本书 时 记录 的 笔记 和 自己 的 一些 初步 思考 。       引言       曲线拟合       设 曲线拟合 的 目标 变量 为 $ ( t ) $ ， $ ( N ) $ 个 训练 集 为     $ (   \ \ mathbf { x }   =   ( x _ 1 ,   ... ,   x _ N ) ^ T   ) $ ， 对应 的 目标值 为     $ (   \ \ mathbf { t }   =   ( t _ 1 ,   ... ,   t _ N ) ^ T   ) $ 。 假设 估计 误差 服从     高斯分布     $ $     p ( t | x ,   \ \ mathbf { w } ,   \ \ beta )   =   \ \ mathcal { N }   ( t |   y ( x ,   \ \ mathbf { w } ) ,   \ \ beta ^ { - 1 } )       $ $     这里 的 $ ( \ \ mathbf { w } ) $ 是 待 估计 的 参数 ， $ ( \ \ beta ^ { - 1 } ) $ 对应 于     估计 误差 的 方差 。 此时 ， 按照 最大 似然 准则 估计 的 参数 就是 最小 二乘     法 的 结果 ， 也 就是 最小化 均方 误差 。 而 $ ( \ \ beta ^ { - 1 } ) $ 的 最大 似然 估计 为     模型 预测值 与 实际 值 的 均 方差 。     $ $     \ \ frac { 1 } { \ \ beta }   =   \ \ frac { 1 } { N }   \ \ sum _ { n = 1 } ^ N   [ y ( x _ n , \ \ mathbf { w } _ { ML } )   -   t _ n ] ^ 2 .     $ $       如果 我们 对 参数 的 具有 一定 先验 知识 ， 可以 进一步 采用 最大 后验 概率 估计 ，     得到 更好 的 结果 。 作为 一个 特例 ， 如果 认为     $ $     p ( \ \ mathbf { w } | \ \ alpha )   =   \ \ mathcal { N } ( \ \ mathbf { w }   |   0 ,   \ \ alpha ^ { - 1 }   \ \ mathbf { I } )       \ \ \ \                                             =   ( \ \ frac { \ \ alpha } { 2 \ \ pi } ) ^ { ( M + 1 ) / 2 }   \ \ exp ( - \ \ frac { \ \ alpha } { 2 }   \ \ mathbf { w } ^ T   \ \ mathbf { w } ) .     $ $     那么 此时 的 最大 后验 概率 估计 为 带 $ ( L _ 2 ) $ 正则 项 的 估计 ，     它 最小化     $ $     \ \ frac { \ \ beta } { 2 }   \ \ sum _ { n = 1 } ^ N   [ y ( x _ n ,   \ \ mathbf { w } )   - t _ n ] ^ 2   +   \ \ frac { \ \ alpha } { 2 }   \ \ mathbf { w } ^ T   \ \ mathbf { w }     $ $     也就是说 正则 项是 对模型 参数 的 一种 先验 知识 ， $ ( L _ 2 ) $ 正则 项 代表 高斯 先验 。     那 $ ( L _ 1 ) $ 代表 laplace 先验 ( 待 求证 ) ？  ", "tags": "machine-learning", "url": "/wiki/machine-learning/prml.html"},
      
      
      {"title": "Pixel RNN & CNN", "text": "  关于           参考 论文 ：       Den   Oord   A   V ,   Kalchbrenner   N ,   Kavukcuoglu   K ,   et   al .   Pixel   Recurrent   Neural   Networks [ C ] .   international   conference   on   machine   learning ,   2016 :   1747 - 1756 .       Conditional   Image   Generation   with   PixelCNN   Decoders                   Pixel   RNN           对 自然 图像 建模 ， 是 无 监督 学习 中 的 一个 重要 任务 。       Piexel   RNN   将 此 问题 当做 一个 序列 预测 的 问题 ， 即 利用 前面 的 像素 ( 作为 上下文   Context ) 预测 后 一个 像素 的 离散 概率 ，     建立 条件 概率模型 。       采用 残差 连接 ， 12 层       预测 离散 值 ， 直接 采用 一层   softmax   ， 作为 一个多 分类 的 问题 。       上下文 可以 是 自 左上角 到 右下角 ， 顺序 的 上下文 ； 也 可以 是 多 尺度 的 上下文 。           图像   $ ( x _ { i , j } ,   i , j   =   1 , 2 , ... , n ) $   作为 一个 序列   $ ( x _ i ,   i = 1 , ... , n ^ 2 ) $ ，     联合 概率       $ $     p ( x _ 1 ,   .. ,   x _ n )   =   \ \ Pi _ { i = 1 } ^ { n ^ 2 }   p ( x _ i | x _ 1 , ... , x _ { i - 1 } )     $ $           并且 ， 不同 通道 （ RGB 三个 通道 ） 不仅 依赖 前面 的 序列 ， 还 依赖 与 其他 通道 。 分布 的 计算 可以 并行 ？ ！ ！ ！           Pixel   CNN       能够 建模 任意 向量 的 条件 概率 ； 如果 对 图像 的   class   label   建模 ， 可以 生成 多样 的 真实 场景 下 的 不同 动物 ， 目标 ， 场景 等 ；     如果 对 图像 的 CNN   embedding 向量 建模 条件 概率 ， 可以 生成 一个 没有 见到 过 的 人 的 其他 侧面 、 姿势 下 的 画像 ！       Pixel   CNN   也 可以 作为 图像   autoencoder   的 一个 强大 的 解码器 ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pix-rnn-cnn.html"},
      
      
      {"title": "PMML: 用XML描述机器学习模型", "text": "    Table   of   Contents           关于           通用 结构           Header           MiningBuildTask           DataDictionary           DataField                   TransformationDictionary           MODEL - ELEMENT           MiningModel           functionName           Segmentation                   RegressionModel                   Extension           其他           modelName           基本 数据类型                         关于       PMML 是 用 XML 来 描述 数据挖掘 模型 的 一种 通用 可 交换 格式 ， 利用 PMML 可以 将 各种 工具 生成 的 模型 很 方便 的 发布 到 生产 环境 ！     目前 著名 的   sklearn   和   R 中 的 模型 ， 都 支持 导出 为 PMML 格式 ！       http : / / dmg . org / pmml /         通用 结构           采用 XML 格式 ， 只有 一个 根 节点                   & lt ; PMML       version =     & quot ; 4.1 & quot ;               xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 1 & quot ;               xmlns : xsi =     & quot ; http : / / www . w3 . org / 2001 / XMLSchema - instance & quot ;     & gt ;       & lt ; / PMML & gt ;                     一个 PMML 文件 可以 包含 多个 模型 ， 一个 应用 系统 可以 通过 名字 选择 模型 ， 否则 选择 第一个 模型           Header       可以 包含     copyright ,   description ,   Applilcation ,   name ,   version ,   Annotation   。     例子 ：               & lt ; Header       copyright =     & quot ; www . tracholar . com & quot ;     / & gt ;                 MiningBuildTask       包含 任意 XML 值 ， 描述 训练 模型 的 配置 ， 不是   PMML   必须 。       DataDictionary       定义 用于 模型 输入 的 数据 的 类型 ， 范围 等 。     可以 用 在 多个 模型 当中 。         DataField               name     必须 唯一 ！         opType     操作 类型 ， 表明 是 连续 ， 离散 特征         dataType     数据类型     https : / / www . w3 . org / TR / xmlschema - 2 /   ， 外加 几个 新 类型     timeSenconds ,   dateDaysSince [ aYear ] ,   dateTimeSecondsSince [ aYear ]           Value             TransformationDictionary       对 数据 变换 :           Normalization ： 归一化 ， 输入 可以 是 连续 或 离散 值       Discretization ： 将 连续 值 变成 离散 值       Value   mapping ： 将 离散 值 映射 为 离散 值       Text   Indexing ： 对 给定 的 term 赋予 一个 频率 值 ？       Functions ： 函数 映射       Aggregation ： 聚合 函数 ， 例如 求和       Lag ： 使用 之前 的 值 ？           MODEL - ELEMENT       MiningModel       functionName       PMML   定义 了 5 中 挖掘 函数 ， 每个 模型 可以 有 一个 属性     functionName     用来 指定 functionName 。     可以 取值 为 ：           associationRules       sequences       classification       regression       clustering       timeSeries       mixed           Segmentation             multipleModelMethod     多个 模型 融合 方法 ：         majorityVote     投票         modelChain     模型 链 ， 前 一个 模型 的 结果 作为 后 一个 模型 的 输入         average     平均       selectFirst ,   selectAll ,   majorityVote ,   modelChain ,   etc               Segment ，   id   1 - based   index ： 为什么 要 有 一个     & lt ; True / & gt ;             RegressionModel       回归 模型           MiningSchema ： 定义 建模 的 字 段       Output ： 定义 一个 模型 的 输出 结果 值 ， 使用 OutputField 定义 名字 ， predictedValue 字段 表明 输出 的 是   raw   predicted   value .       Target           Extension       其他       modelName       用来 指定 模型 民资       基本 数据类型       NUMBER ,   INT - NUMER ,   REAL - NUMBER ,   PROB - NUMBER ,   PERCENTAGE - NUMBER ,   FIELD - NAME ,   ARRAY  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pmml.html"},
      
      
      {"title": "Rank 常用方法和模型", "text": "    Table   of   Contents           关于           逻辑 回归           rankSVM           lambda   rank                 关于       排序 是 机器 学习 算法 在 业界 用 得 比较 多 的 场景 ， 很多 问题 也 可以 转化 为 排序 的 问题 。       逻辑 回归       例如 点击率 预估 ， 用户 偏好 等 。 排序 问题 被 当做 二 分类 预测 问题 ， 用 概率 值 进行 排序 ， 一般 会 关注   AUC   指标 。       rankSVM       直接 优化   AUC ， 并 借鉴   SVM   大 间隔 的 思想 ， 构造   rank   loss :       $ $     loss   =     $ $       lambda   rank  ", "tags": "machine-learning", "url": "/wiki/machine-learning/rank.html"},
      
      
      {"title": "Recurrent neural network based language model", "text": "    Table   of   Contents           历程           模型                 历程           Bengio   采用 神经网络 做 统计 语言 模型 。 前馈 神经网络   +   固定窗 长度     Yoshua   Bengio ,   Rejean   Ducharme   and   Pascal   Vincent .   2003 .   A     neural   probabilistic   language   model .   Journal   of   Machine   Learning     Research ,   3 : 1137 - 1155       Goodman   在 Bengio   的 基础 上 进行 发展 ， 发现 这种 简单 模型 比 混合 了 其他 多种 方法 的 模型 都 要 好 。     Goodman   Joshua   T .   ( 2001 ) .   A   bit   of   progress   in   language   modeling ,     extended   version .   Technical   report   MSR - TR - 2001 - 72 .       Schwenk   发现 基于 神经网络 的 模型 ， 能够 显著 提升 语音 识别 任务 ， 在 几个 任务 中 比 最好 的 系统 都 要 好 。     Holger   Schwenk   and   Jean - Luc   Gauvain .   Training   Neural   Network     Language   Models   On   Very   Large   Corpora .   in   Proc .   Joint   Conference     HLT / EMNLP ,   October   2005 .           这种 方法 唯一 的 缺点 是 ， 需要 采用 固定 的 窗 长度 ， 一般 在 5 - 10 。     递归 神经网络 理论 上 能够 记忆 任意 长 的 信息 ， 解决 了 这个 问题 。       另外 一种 能够 实现 长期 依赖 的 方法 ： 随机 梯度 下降 ？     Yoshua   Bengio   and   Patrice   Simard   and   Paolo   Frasconi .   Learning     Long - Term   Dependencies   with   Gradient   Descent   is   Difficult .     IEEE   Transactions   on   Neural   Networks ,   5 ,   157 - 166 .       模型       首先 采用 一个 简单 的 递归 神经网络 ， 也 叫 Elman 网络 ：     Jeffrey   L .   Elman .   Finding   Structure   in   Time .   Cognitive   Science ,     14 ,   179 - 211       输入 $ ( x ( t ) ) $ 为 当前 词 向量 $ ( w ( t ) ) $ 和 上 一 时刻 隐层 状态 $ ( s ( t - 1 ) ) 连接成 的 新 向量 ， 这里 用   +   表示 链接 ， 不是 求和 ：     用 隐层 的 状态 来 代表 上下文 信息 。       $ $     x ( t )   =   w ( t )   +   s ( t - 1 )     \ \ \ \     s _ j ( t )   =   sigmoid ( \ \ sum _ i   x _ i ( t )   u _ { ji } )     \ \ \ \     y _ k ( t )   =   softmax ( \ \ sum _ j   s _ j ( t )   v _ { kj } )     $ $       模型 训练 ： 标准 的   BP   +   SGD ， 一 开始 学习 率 $ ( \ \ alpha = 0.1 ) $ ， 每 一个 epoch 之后 ， 在 验证 集上 检验 ， 如果 验证 集 的 对数 似然比 增加 了 ，     就 继续 训练 ， 如果 没有 明显 的 改善 ， 就 将 学习 率 减半 $ ( \ \ alpha _ { new }   =   \ \ alpha   /   2   ) $ 。 如果 之后 仍然 没有 明显 的 改善 ， 就 停止 训练 。     一般 在 10 - 20 个 epoch 就 能 收敛 。       作者 的 模型 没有 明显 的 过 拟合 ， 即使 在 使用 正则 项 的 情况 下 ， 也 没有 明显 的 收益 。       误差 是 基于 交叉 熵 计算 的 ， 即 交叉 熵 的 导数 ：       $ $     err ( t )   =   desired ( t )   -   y ( t )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/recurrent-neural-network-based-language-model.html"},
      
      
        
        
      
      {"title": "Asynchronous Methods for Deep Reinforcement Learning", "text": "    Table   of   Contents           简介           要点                 简介       强化 学习 经典 论文 , 异步 实现 方法 。 Deepmind   & amp ;   Google 。       要点           之前 的   Deep   RL   算法 需要 经验 回放 ( experience   replay ) 解决 样本 时间 相关性 的 问题 ,   例如 DQN ;         经验 回放 有 两个 问题 :   1 是 需要 大 内存 存放 经验 ( 通常 非常 大 ) ,   2 是 用 经验 回放 , 那么 线上 策略 与 学习策略 不 一致 , 也 就是 要用 off - policy 学习 方式 。       异步 并行 的 学习 方式 可以 很 好 的 解决 样本 的 时间 相关性 问题 , 因为 同时 运行 很多 个 episode , 每 一个 都 是 独立 的 , 不 存在 时间 相关性 。       使用 了 单机 多卡 ( GPU ) 的 并行 方案       asynchronous   advantage   actor - critic   ( A3C ) 效果 最优       The   General   Reinforcement   Learning   Architecture   ( Gorila )       多机 分布式 异步 训练       每个 actor 独立 训练 和 使用 经验 回放       每 一个 learner 从 所有 的 经验 中 采样 , 计算 DQN   loss       梯度 异步 与 PS 交互 , 更新       模型 参数 定期 同步 到 每 一个   actor - learner       100   actor - learner ,   30   parameter   server ,   130 台 机器               使用 n 步 TD 的 方法 更新 参数      ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/a3c.html"},
      
      
      {"title": "延迟回报分解", "text": "    Table   of   Contents           Bias - Variance   for   MDP                 Bias - Variance   for   MDP       动作 值 函数 的 偏差 和 方差       $ $     q ^ { \ \ pi } ( s , a )   =   \ \ sum     $ $  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rudder.html"},
      
      
      {"title": "强化学习2018年论文阅读笔记", "text": "    Table   of   Contents           Robust   DQN                 Robust   DQN           Stabilizing   Reinforcement   Learning   in   Dynamic   Environment   with   Application   to   Online   Recommendation ,   Shi - Yong   Chen ,   Yang   Yu   etc       南京大学 跟 阿里巴巴 合作 的 论文 ,   KDD18       传统 RL 方法 在 静态 环境 中 比较 适合 , 本文 方法 主要 解决 在 动态 环境 ( 环境参数 分布 会 随 时间 变化 ) 中 , 实现 稳定 的 回报 ( reward ) 估计 :   Robust   DQN ( Double   DQN ) 。       相比 于 Atari 游戏 、 alpha   go , 实际 的 推荐 问题 中 最大 的 不同 是 高度 动态 , 环境参数 和 分布 随 时间 的 变化 , 使得 对 reward 的 估计 方差 很大 , 并且 有 偏 :   因为 reward 的 增大 可能 是 策略 提升 带来 的 , 也 可能 是 时间 变化 带来 的 。       两个 关键 策略 改进       解决 方差 较大 : 用 分层 采样 回放 ( stratified   sampling   replay )   代替 传统 的 经验 回放 。 使用 一个 先验 的 用户 分布 , 根据 这个 先验 分布 采样 每 一个 batch       解决 时变 偏差 : 实现 近似 的   regret   reward               分层 采样 回放       通过 长时间 的 数据 统计 , 选出 一些 用户 分布 稳定 的 属性 : 性别 、 年龄 、 地理信息 , 通过 这些 属性 对 用户 分层       在 回放 的 时候 , 用 固定 比例 的 分层 采样 代替 对 短期 用户 的 均匀 采样               Approximate   Regretted   Reward       利用 一个 离线 模型 的 线 上 回报 估计 环境 随 时间 的 动态 变动 ,   然后 利用 估计 出来 的 环境 变动 补偿 线上 的 DQN       用   $ ( \ \ tile ( r ) _ t   =   r _ t   -   r _ b ) $   代替   $ ( r _ t ) $       r _ b   估计 方法 :   随机 选择 一些 用户 , 利用 离线 训练 好 的 模型 进行 决策 , 然后 实时 统计 这些 用户 的 平均 reward 作为   r _ b               应用 案例 :   Tip   推荐       Q ( S ,   a )   将 动作 ( 20000 种 Tip , 对应 动作 空间 20000 ) embedding 后 , 作为 神经网络 的 输入       $ ( r   =   r _ 1   +   \ \ alpha   r _ 2   +   \ \ beta   r _ 3 ) $         第一 部分 回报 是 点击 回报 $ ( r _ 1   =   I   *   ( 1   +   \ \ rho   *   e ^ { - x } ) $   I 代表 是否 点击 , x 是 当前 tip 展示 的 页面 数量       第二 部分 回报 是 描述 用户 点击 tip 的 偏好 ,   $ ( r _ 2   =   I   *   e ^ { - y } ) $   y 是 用户 最近 100 次 PV 中 对 tip 的 点击 次数       第三 部分 回报 是 用户 交易   $ ( r _ 3   =   c ) $   c 代表 是否 交易                      ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl2018-paper-reading.html"},
      
      
      {"title": "强化学习简介", "text": "    Table   of   Contents           关于           动态 规划 ( DP ) 方法           蒙特卡洛 ( MC ) 方法           Importance   Sampling                   时间 差分 ( TD ) 方法           SARSA 方法           Q - learning           期望 SARSA 方法           Double   Q - learning                   n - step   TD 方法           TD ( $ ( \ \ lambda ) $ ) 方法           策略 梯度 理论                 关于       Reinforcement   Learning :   An   introduction   读书笔记       动态 规划 ( DP ) 方法       当 环境 已知 时 ， 即 状态 转移 概率 $ ( P ( s ' | s ,   a ) ) $ 和 回报 $ ( r ( s ,   a ) ) $ 也 知道 的 情况 下 ， 根据 HJB 方程       $ $     V ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ( s ' ) ]     $ $       因为 求 最大值 操作 的 存在 ， 上述 方程 是 值 函数 V 的 非线性 方程 ， 所以 无法 直接 求解 。     动态 规划 方法 求解 值 函数 方法 有 两种 ： 值 迭代 与 策略 迭代 。       采用 值 迭代 的 理论依据 是非 线性方程 的 迭代 求解 方法 ， 从 HJB 方程 来看 ， 值 函数 V 可以 看做 右边 非线性 算子 的 不动点 ， 容易 验证 当 $ ( \ \ gamma & lt ; 1 ) $ 时 ， 该 非线性 算子 是 压缩 映象 ， 值 迭代 比 收敛 于 不动点 ！ 它 也 可以 看做 往前 一步 的 期望值 作为 目标 ， 进行 迭代 。       $ $     V _ { k + 1 } ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V _ k ( s ' ) ]     $ $       另 一种 方法 是 交替 优化 策略 和 值 函数 ， 好处 是 在 策略 固定 时 ， HJB 方程 没有 max 操作 ， 是 线性方程 ！     策略 迭代 分为 两步         策略 评估   ： 将 选择 动作 的 策略 固定 ， 求解 策略 的 值 函数 ， 因为 策略 固定 ， 非线性 的 HJB 方程 变成 线性方程 了 ！ 所以 可以 采用 线性方程 的 所有 求解 方法 进行 求解 ， 比如 高斯消 元法 、 雅克 比 迭代法 等等 。       $ $     V ^ { \ \ pi } ( s )   =   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ^ { \ \ pi } ( s ' ) ] ,   a = \ \ pi ( a | s )     $ $         策略 提升   ， 对于 上述 评估 出来 的 值 函数 ， 提升 策略       $ $     \ \ pi ' ( s )   =   \ \ arg \ \ max _ a   Q ^ { \ \ pi } ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   r ( s ,   a )   +   \ \ gamma   V ^ { \ \ pi } ( s ' )     $ $       策略 提升 可以 保证 值 函数 序列 是 单调 递增 序列 ！       蒙特卡洛 ( MC ) 方法       当 环境 未知 的 时候 ， 无法 采用 动态 规划 方法 求解 ， 需要 根据 经验 数据 进行 评估 。     蒙特卡洛 法 通过 在线 学习 的 方法 ， 将 整个 动作 序列 执行 至 终态 ， 根据 实际 获得 的 总 回报 $ ( G   =   \ \ sum _ { t = 0 } ^ T   \ \ gamma ^ t   r ( s _ t ,   a _ t ) ) $ 来 进行 策略 评估 ！ 当 执行 多次 之后 ， $ ( G ) $ 的 平均值 可以 作为 该 策略 下 ， 初始状态 s 的 值 函数 。 这个 过程 也 可以 用 迭代 的 方法 描述       $ $     Q ^ { \ \ pi } ( s ,   a )   \ \ leftarrow   Q ^ { \ \ pi } ( s ,   a )   -   \ \ alpha [ Q ^ { \ \ pi } ( s ,   a )   -   G ]   \ \ \ \     \ \ alpha   =   1 / N     $ $       该 迭代 过程 可以 看做 用 观测 到 的 回报 G 作为 学习 目标 的 随机 梯度 下降       $ $     J ( Q ^ { \ \ pi } )   =   \ \ frac { 1 } { 2 } \ \ sum _ i   ( Q ^ { \ \ pi } ( s ,   a )   -   G _ i ) ^ 2     $ $       蒙特卡洛 法 的 特点 ， 必须 等到 动作 序列 执行 完毕 后 ， 才能 进行 评估 。     但是 一个 序列 可以 更新 多个 状态 的 值 ， 利用 马尔科夫 链 的 性质 ， 从 这 条链 中间 任何 一个 状态 开始 ， 都 可以 得到 该 状态 的 一个 值 函数 的 采样 值 ！       为了 有效 地 估计 出 Q 函数 ， 对 每个 状态 - 动作 对 都 需要 产生 多个 样本 ， 因此 初始状态 需要 随机 从 可能 的 状态 - 动作 对 中 随机 选择 ！               由于 所有 的 初始状态 - 动作 都 是 随机 的 ， 所以 初始 动作 比较 无效 ！ 但是 又 不能 按照 当前 的 策略 选择 初始 动作 ， 那样 将会 导致 很多 状态 - 动作 对 没有 样本 ！       为了 解决 这个 问题 ， 可以 限制 $ ( \ \ pi ( a | s )   & lt ;   1   -   \ \ epsilon   +   \ \ epsilon / | A ( s ) | ) $ ， 也就是说 不让 策略 只 选择 最优 的 动作 ， 还 以 一定 的 概率 选择 其他 动作 ， 这样 初始 动作 的 选择 也 可以 采用 当前 最优 策略 来选 了 ！ 但是 ， 这样一来 ， 收敛 后 的 策略 并 不是 最优 策略 了 ， 只是 近 最优 策略 ！       Importance   Sampling       另外 一种 解决方案 是 利用 采样 ( Importance   Sampling ) 实现 off - policy ， 也 就是 评估 的 策略 不是 线上 运行 的 策略 ！ 前面 的 方法 评估 的 策略 就是 线上 运行 的 策略 ， 叫做 on - policy 方法 。       假设 评估 的 策略 是 $ ( \ \ pi ( a | s ) ) $ ， 而线 上 运行 的 策略 是 $ ( b ( a | s ) ) $ ， 那么 对于 动作 - 状态 轨迹 $ ( \ \ tau   =   ( A _ 1 ,   S _ 2 ,   A _ 2 ,   ... ,   S _ T ) ) $ ， 两种 策略 产生 该 轨迹 的 概率 之比为       $ $     \ \ rho _ { t : T - 1 }   =   \ \ Pi _ t ^ { T - 1 }   \ \ frac { \ \ pi ( A _ t | S _ t ) } { b ( A _ t | S _ t ) }     $ $       那么 ， 根据 用 策略 $ ( b ) $ 得到 的 经验 数据 ， 可以 估计 在 策略 $ ( \ \ pi ) $ 下 的 值 函数       $ $     V ( s )   =   \ \ frac { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 }   G _ t } { | B ( s ) | }   \ \ \ \     V ( s )   =   \ \ frac { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 }   G _ t } { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 } }     $ $       $ ( B ( s ) ) $ 是 状态 s 所处 的 时间 集合 。 前 一种 估计 无偏 但是 高 方差 ， 后 一种 有 偏 但是 低 方差 ， 但是 偏差 会 随着 样本数 增加 而 趋近 于 0 ！ 推荐 后 一种 估计 。       时间 差分 ( TD ) 方法       蒙特卡洛 方法 需要 策略 执行 到 终止 状态 才能 评估 策略 ， TD 方法 只 需要 1 步 ！ 核心思想 在于 自助 法 ， 它 对值 函数 的 估计 是       $ $     G _ t   =   R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } )     $ $       即用 原来 的 值 函数 取代 了 后面 所有 的 回报 ！ 并 采用 常数 学习 率 ，       $ $     V ( S _ t )   \ \ leftarrow   V ( S _ t )   +   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } )   -   V ( S _ t ) ]     $ $       上述 迭代 可以 看做 最小化   $ ( 1 / 2 | | G _ t   -   V ^   *   ( S _ t ) | | ^ 2 ) $   进行 随机 梯度 下降 ！     误差 项 $ ( \ \ delta _ t   =   G _ t   -   V ( S _ t ) ) $ 称作   TD   error 。       SARSA 方法       Sarsa 方法 就是 用 TD 方法 对 动作 值 函数 Q ( s , a ) 进行 学习 ！       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   Q ( S _ { t + 1 } ,   A _ { t + 1 } )   -   Q ( S _ t ,   A _ t ) ]     $ $       可以 看到 ， Sarsa 方法 用到 了 两个 状态 和 两个 动作 ， 这 两个 动作 都 是 采样 自 Q 函数 策略 。 如果 动作 选取 有 探索 ， 那么 Q 就 不是 最优 解 ， 一般 可以 随着 学习 的 推进 ， 不断 减小 探索 到 某个 很小 的 值 ， 可以 得到 近 最优 解 。       SARSA 可以 看做 用   $ ( Q ( S _ { t + 1 } ,   A _ { t + 1 } )   ) $   来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ ！       $ $     Q ^   *   ( S _ t ,   A _ t )   =   R _ { t + 1 }   +   \ \ gamma   V ^   *   ( S _ { t + 1 } )   \ \ approx   R _ { t + 1 }   +   \ \ gamma   Q ( S _ { t + 1 } ,   A _ { t + 1 } )     $ $               Q - learning       也 是 一种 TD 学习 方法 ， 而且 是 off - policy ， 与 SARSA 不 一样 的 是 ， 它 只用 到 了 一个 初始 动作 ， 不 需要 根据 Q 函数 策略 采样 的 第二个 动作 ， 这是 最大 的 区别 ！ 也 是 Q - learning 可以 用 off - policy 学到 最优 策略 的 关键 ！       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   \ \ max _ a   Q ( S _ { t + 1 } ,   a )   -   Q ( S _ t ,   A _ t ) ]     $ $       Q - learning 可以 看做 用   $ ( \ \ max _ a   Q ( S _ { t + 1 } ,   a )   ) $   来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ ！       $ $     Q ^   *   ( S _ t ,   A _ t )   =   R _ { t + 1 }   +   \ \ gamma   V ^   *   ( S _ { t + 1 } )   \ \ approx   R _ { t + 1 }   +   \ \ gamma   \ \ max _ a   Q ( S _ { t + 1 } ,   a )     $ $       期望 SARSA 方法       SAESA 方法 用 当前 策略 的 采样 动作 $ ( A _ { t + 1 } ) $ 的 动作 值 函数 近似 $ ( V ^   *   ( s ' ) ) $ ， 因此 是 有 偏 而且 方差 很大 ！     Q - learning 则 用 $ ( \ \ max _ a   Q ( s ' ,   a ) ) $ 近似 $ ( V ^   *   ( s ' ) ) $ ， 也 是 有 偏 的 方差 也 大 ， 而且 因为 取 max 导致 有 过高估计 的 问题 。     期望 SARSA 则 用 当前 Q 函数 在 当前 策略 下 的 期望值   $ ( \ \ sum _ { a }   \ \ pi ( a | s ' )   Q ( s ' ,   a ) ) $   近似 $ ( V ^   *   ( s ' ) ) $ ， 因为 用到 期望值 ， 所以 可以 减少 方差 ！ 迭代 方程 是       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   \ \ sum _ a   \ \ pi ( a | S _ { t + 1 } )   Q ( S _ { t + 1 } ,   a )   -   Q ( S _ t ,   A _ t ) ]     $ $       Double   Q - learning       解决 求 max 操作 的 过高估计 问题 ， 用 两个 Q 函数 ， 交替 学习 ， 每次 学习 时 ， 用 Q2 来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ 作为 目标 ， 对 Q1 进行 梯度 下降       $ $     Q _ 1 ( S _ t ,   A _ t )   \ \ leftarrow   Q _ 1 ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   Q _ 2 ( S _ { t + 1 } ,   a )   -   Q _ 1 ( S _ t ,   A _ t ) ]   \ \ \ \     a   =   \ \ arg \ \ max _ a   Q _ 1 ( S _ { t + 1 } ,   a )     $ $       n - step   TD 方法       在 单步 TD 方法 中 ， 指向 前 看 了 一步 就 得出 $ ( V ^ { \ \ pi }   ( S _ t ) ) $ 的 估计值 ， 实际上 也 可以 看多步 ， 这样 可以 估计 更加 准确       $ $     G _ { t : t + n }   =   R _ { t + 1 }   +   ...   +   \ \ gamma ^ { n - 1 }   R _ { t + n }   +   \ \ gamma ^ n   V ( S _ { t + n } )     $ $       可以 看出 单步 TD 方法 就是 n = 1 的 情况 ， 而 蒙特卡洛 方法 可以 看做   $ ( n   \ \ rightarrow   \ \ infty ) $   的 极限 ！     n - step   TD 估计 偏差 来自 于 最后 一项 ， 用 当前 的 值 函数 估计 最优 值 函数 ， 因为 前面 的 系数 是 $ ( \ \ gamma ^ n ) $ 是 指数 衰减 的 ， 所以 n 步 TD 方法 的 偏差 随 指数 衰减 ！       n - step   TD 方法 迭代 方程 是       $ $     V ( S _ t )   \ \ leftarrow   V ( S _ t )   +   \ \ alpha [ G _ { t : t + n }   -   V ( S _ t ) ]     $ $                 SARSA   ： 将 SARSA 对 Q 的 估计 用 n 步 回报 替换 就 可以 得到 n 步 SARSA 方法       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha \ \ left (   [ R _ { t + 1 }   +   ...   +   \ \ gamma ^ { n - 1 } R _ { t + n }   +   \ \ gamma ^ n   Q ( S _ { t + n } ,   A _ { t + n } ) ]   -   Q ( S _ t ,   A _ t )   \ \ right )     $ $         Q - learning   ： 同理 将 Q - learning 对 Q 的 估计 用 n 步 回报 替换 就 可以 得到 n 步 Q - learning 方法 ！       TD ( $ ( \ \ lambda ) $ ) 方法       基本 出发点 ： 将 n 步 回报 加权 平均 作为 对 回报 的 估计 ， 距 现在 越久 的 权重 越小 。       $ $     G _ t ^ { \ \ lambda }   =   ( 1 - \ \ lambda ) \ \ sum _ { n = 1 } ^ { \ \ infty }   \ \ lambda ^ { n - 1 }   G _ { t : t + n }     $ $       可以 看出 当 $ ( \ \ lambda   =   0 ) $ 时 ， 就是 单步 TD 方法 ， 当 $ ( \ \ lambda   =   1 ) $ 时 ， 就是 蒙特卡洛 方法 。       后 向 更新 算法 ， 迭代 地 利用   TD   error 进行 更新       $ $     Z _ t ( s )   =   \ \ begin { cases }             \ \ gamma   \ \ lambda   Z _ { t - 1 } ( s ) ,   \ \ quad   s   \ \ neq   S _ t   \ \ \ \             \ \ gamma   \ \ lambda   Z _ { t - 1 } ( s )   +   1 ,   \ \ quad   s   =   S _ t             \ \ end { cases }   \ \ \ \     \ \ delta _ t   =   R _ { t + 2 }   +   \ \ gamma   V _ t ( S _ { t + 1 } )   -   V _ t ( S _ t )   \ \ \ \     V _ t ( S _ t )   \ \ leftarrow   V _ { t - 1 } ( S _ t )   +   \ \ alpha   \ \ delta _ t   Z _ t ( S _ t )     $ $       策略 梯度 理论       直接 建模 策略 函数       $ $     \ \ nabla   J ( \ \ theta )   =   \ \ sum _ s   \ \ mu _ { \ \ pi } ( s )   \ \ sum _ a   q _ { \ \ pi } ( s ,   a )   \ \ nabla _ { \ \ theta }   \ \ pi ( a | s , \ \ theta )   =   E _ { \ \ tau   \ \ sim   \ \ pi _ { \ \ theta } }   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tau )   r ( \ \ tau )     $ $       两种 表述 ， 前面 一个 等式 是 状态 - 动作 表述 ， 后 一个 等式 是 状态 - 动作 序列 表述 。 策略 梯度 法 实际上 相当于 用 回报 作为 样本 权重 的 极大 似然 估计 。       REINFORCE ：   蒙特卡罗 策略 梯度         $ $     \ \ nabla   J ( \ \ theta )   =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t \ \ sum _ a     q _ { \ \ pi } ( S _ t ,   a )   \ \ nabla _ { \ \ theta }   \ \ pi ( a | S _ t , \ \ theta )         \ \ right ]     \ \ quad   \ \ text { ( 对 状态 采样 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   q _ { \ \ pi } ( S _ t ,   A _ t )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )         \ \ right ]             \ \ quad   \ \ text { ( 对 动作 采样 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   G _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )         \ \ right ]             \ \ quad   \ \ text { ( 用 采样 的 回报 替换 q 函数 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   ( G _ t   -   V ( S _ t ) )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )   \ \ right ]     $ $       等 二个 等式 ， 用到 了 关系       $ $     Eq _ { \ \ pi } ( S _ t ,   A _ t ) \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )     =   \ \ sum _ a     \ \ pi ( a | S _ t ) q _ { \ \ pi } ( S _ t ,   a ) \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )     $ $       最后 一个 等式 是因为 被 剪掉 的 部分 期望值 为 0 .       因此 ， 可以 利用 单个 样本 $ ( ( S _ t ,   A _ t ) ) $ 估计 策略 梯度 ， 得到 策略 梯度 随机 梯度 上升 迭代 公式       $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ { t }   +   \ \ alpha   \ \ gamma ^ t   G _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta _ t )     $ $       这里 的 $ ( G _ t ) $ 采用 的 是 单条 链 的 最终 回报 ， 所以 是 用 蒙特卡罗 法 估计 策略 梯度 ！       用 $ ( \ \ delta _ t   =   G _ t   -   V ( S _ t ;   w ) ) $ 替换 $ ( G _ t ) $ 可以 减小 梯度 估计 的 方差 ， 并且 使用 TD 方法 估计 $ ( G _ t   =   R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } ;   w ) ) $ 。 这 就是     Actor   Critic     算法 ：       $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ { t }   +   \ \ alpha   \ \ gamma ^ t   \ \ delta _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta _ t )     $ $          ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl-intro-book.html"},
      
      
      {"title": "强化学习算法实现(I)", "text": "    Table   of   Contents           环境 构建           值 迭代           策略 迭代           蒙特卡罗 法           Q - Learning                 环境 构建       MDP 的 环境 是 指 转移 概率 $ ( P ( s ' |   s ,   a ) ) $ 和 回报 函数 $ ( r ( s ,   a ) ) $ ！     在 环境 已知 的 动态 规划 算法 中 ， 用于 迭代 算法 当中 。 在 环境 位置 的 TD 方法 中 用于 模拟 。       构建 一个 简单 的 环境 ， 有 nS 个 状态 ， 0 ， 1 ， ... ， nS - 1 ； 其中 nS - 1 是 终止 状态 。     该 环境 下 一共 两个 动作 ： 0 向 左 运动 ， 1 向 右 运动 ， 每个 动作 都 有 概率 p0 不动 ， p1 的 概率 会往 反 方向 运动 ,   1 - p0 - p1 概率 正常 运动 。                   0       1       2       ...       9                       ← · →       ← · →       ← · →       ← · →       终点                           import       numpy       as       np         nS       =       10       nA       =       2         # 不要 改 这个 参数       Done       =       nS       -       1       p0       =       0.1       p1       =       0.1       P       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       #   转移 概率       R       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       -       1.0       #   回报 都 是 - 1       gamma       =       1         for       s       in       range     (     nS     ) :               if       s       = =       Done     :       #   终止 态 转移 概率 都 为 0                       continue               for       a       in       range     (     nA     ) :                       inc       =       a       *       2       -       1       #   步长                                       P     [     s     ,       a     ,       s     ]       + =       p0       #   不 动                       P     [     s     ,       a     ,       max     (     0     ,       s       -       inc     ) ]       + =       p1       #   反 方向                       P     [     s     ,       a     ,       max     (     0     ,       s       +       inc     ) ]       + =       1       -       p0       -       p1       #   正常 运动                 值 迭代       迭代法 求解 非线性 方程 ， 值 迭代 迭代 方程       $ $     V ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ( s ' ) ]     $ $               V       =       np     .     zeros     (     nS     )       for       it       in       range     (     1000     ) :               converage       =       True               for       s       in       range     (     nS     ) :                       if       s       = =       Done     :         #   终止 状态 不 迭代                               V     [     s     ]       =       0                               continue                         #   HJB   非线性 方程                       maxV       =       max     (     sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     )                                                       )       for       a       in       range     (     nA     )                                             )                         if       V     [     s     ]       ! =       maxV     :       #   测试 值 迭代 是否 收敛                               converage       =       False                               V     [     s     ]       =       maxV               if       converage     :                       break         print       &# 39 ; iteral   steps : &# 39 ;     ,       it       print       V         & gt ; & gt ; & gt ;       iteral       steps     :       57       [     -     12.65306123       -     11.40306123         -     9.99681123         -     8.57102998         -     7.14280732           -     5.71427949         -     4.28571351         -     2.85714276         -     1.42857142           0 .                     ]                 策略 迭代           策略 迭代 分 两步       第一步 固定 策略 ， 求解 值 函数 ， 叫 策略 评估 ， 因为 策略 固定 了 ， HJB 方程 由 非线性 方程 变成 线性方程 ， 可以 采用 迭代 解法 或者 高斯消 元法 求解 ；       第二步 叫 策略 提升 ， 对值 函数 构造 新 的 更优 策略 ！                           pi       =       np     .     zeros     (     nS     ,       dtype     =     int     )       # 初始 策略 全部 往 左         for       it       in       range     (     100     ) :               V       =       np     .     zeros     (     nS     )               #   策略 评估 ， 解 线性方程 ， 雅克 比 迭代法               for       _       in       range     (     100     ) :                       converage       =       True                       for       s       in       range     (     nS     ) :                               if       s       = =       Done     :                                       V     [     s     ]       =       0                                       continue                                 #   HJB   线性方程                               a       =       pi     [     s     ]                               v       =       sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     ) )                                 if       V     [     s     ]       ! =       v     :                                       converage       =       False                                       V     [     s     ]       =       v                       if       converage     :                               break                 #   策略 提升               converage       =       True               for       s       in       range     (     nS     ) :                       maxA       =       np     .     argmax     ( [     sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     ) )       for       a       in       range     (     nA     ) ] )                       if       maxA       ! =       pi     [     s     ] :                               converage       =       False                               pi     [     s     ]       =       maxA               print       &# 39 ; iter &# 39 ;     ,     it     ,       &# 39 ; pi   = &# 39 ;     ,     pi                 if       converage     :                       break         print       &# 39 ; pi   = &# 39 ;     ,       pi       print       &# 39 ; V   = &# 39 ;     ,       V         & gt ; & gt ; & gt ;       iter       0       pi       =       [     0       0       0       0       0       0       0       1       1       0     ]       iter       1       pi       =       [     0       0       0       0       0       1       1       1       1       0     ]       iter       2       pi       =       [     0       0       0       1       1       1       1       1       1       0     ]       iter       3       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       4       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       V       =       [     -     12.65306123       -     11.40306123         -     9.99681123         -     8.57102998         -     7.14280732           -     5.71427949         -     4.28571351         -     2.85714276         -     1.42857142           0 .                     ]                 蒙特卡罗 法       当 环境 未知 时 ， 即 转移 概率 未知 ， 无法 利用 HJB 方程 求解 值 函数 。 蒙特卡罗 法 通过 运行 到 结束 得到 回报 ， 去 更新 动作 值 函数 — — Q 函数 ！               Q       =       np     .     zeros     ( (     nS     ,       nA     ) )       pi       =       np     .     random     .     randint     (     0     ,       nA     ,       nS     )         def       go _ next     (     s     ,       a     ) :               r       =       np     .     random     .     rand     ( )               i       =       0               p       =       0               while       True     :                       if       r       & lt ;       p       +       P     [     s     ,       a     ,       i     ] :                               return       i                       p       + =       P     [     s     ,       a     ,       i     ]                       i       + =       1               return       len     (     P     [     s     ,       a     ] )         alpha       =       0.01       for       it       in       range     (     1000     ) :               if       it       %       50       = =       0     :                       print       &# 39 ; iter &# 39 ;     ,       it     ,       &# 39 ; pi = &# 39 ;     ,       pi                 #   策略 评估 ： 根据 目前 策略 仿真 一条 状态 - 动作 路径 ， 更新 Q 函数               for       s       in       range     (     nS     ) :                       for       a       in       range     (     nA     ) :                               #   仿真 一条 状态 - 动作 路径                               history       =       [ ]                               ss       =       s                               while       ss       ! =       Done     :                                       ss _ next       =       go _ next     (     ss     ,       a     )                                       history     .     append     ( (     ss     ,       a     ,       R     [     ss     ,     a     ,     ss _ next     ] ,       ss _ next     ) )                                       ss       =       ss _ next                                       a       =       pi     [     ss     ]       # 更新 动作                                       # print   ss                                 Gt       =       0                               #   对 出现 的 所有 ( s ,   a ) 对 更新 Q 函数 ， 复用 这 条 路径                               for       i       in       reversed     (     range     (     len     (     history     ) ) ) :                                       ss     ,       aa     ,       rr     ,       _       =       history     [     i     ]                                       Gt       =       gamma       *       Gt       +       rr                                       Q     [     ss     ,       aa     ]       + =       alpha       *       (     Gt       -       Q     [     ss     ,       aa     ] )                 #   策略 提升 ： 根据 更新 后 的 Q 函数 ， 更新 策略               for       s       in       range     (     nS     ) :                       pi     [     s     ]       =       np     .     argmax     (     Q     [     s     ,       : ] )       print       &# 39 ; V = &# 39 ;     ,       np     .     max     (     Q     ,       axis     =     1     )       print       &# 39 ; pi = &# 39 ;     ,       pi         & gt ; & gt ; & gt ;       iter       0       pi     =       [     1       0       1       0       0       1       1       0       1       0     ]       iter       50       pi     =       [     0       1       0       1       0       1       1       0       1       0     ]       iter       100       pi     =       [     0       1       0       1       0       1       1       0       1       0     ]       iter       150       pi     =       [     1       0       1       1       0       1       1       0       1       0     ]       iter       200       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       250       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       300       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       350       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       400       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       450       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       500       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       550       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       600       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       650       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       700       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       750       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       800       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       850       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       900       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       950       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       V     =       [     -     12.60595021       -     11.20499113         -     9.82580041         -     8.41347716         -     6.9761059           -     5.5099145           -     4.19408818         -     2.89429492         -     1.45856045           0 .                     ]       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]                 Q - Learning       Q - learning 是 TD 方法 的 一种 ， 也 是 用 得 最 多 的 一种 ， 因为 他 可以 很 方便 off - policy 学习 ， 不 需用 借助 重要性 采样 。     相比 蒙特卡罗 法 可以 不 需要 等到 运行 结束 就 可以 更新 Q 函数 。               Q       =       np     .     zeros     ( (     nS     ,       nA     ) )       pi       =       np     .     random     .     randint     (     0     ,       nA     ,       nS     )         def       go _ next     (     s     ,       a     ) :               r       =       np     .     random     .     rand     ( )               i       =       0               p       =       0               while       True     :                       if       r       & lt ;       p       +       P     [     s     ,       a     ,       i     ] :                               return       i                       p       + =       P     [     s     ,       a     ,       i     ]                       i       + =       1               return       len     (     P     [     s     ,       a     ] )         alpha       =       0.01       epsilon       =       0.9       #   探索       for       it       in       range     (     100     ) :               if       it       %       10       = =       0     :                       print       &# 39 ; iter &# 39 ;     ,       it     ,       &# 39 ; epsilon = &# 39 ;     ,       epsilon     ,       &# 39 ; V [ 0 ] = &# 39 ;     ,       max     (     Q     [     0     ] )                 #   根据 目前 策略 仿真 一条 状态 - 动作 路径 ， 并 同时 更新 Q 函数               for       s       in       range     (     nS     ) :                       if       s       = =       Done     :                               continue                       for       a       in       range     (     nA     ) :                               #   仿真 一条 状态 - 动作 路径                               ss       =       s                               while       ss       ! =       Done     :                                       ss _ next       =       go _ next     (     ss     ,       a     )                                       Gt       =       R     [     ss     ,     a     ,     ss _ next     ]       +       gamma       *       max     (     Q     [     ss _ next     ,       : ] )                                       Q     [     ss     ,       a     ]       + =       alpha       *       (     Gt       -       Q     [     ss     ,       a     ] )         #   Q - learning   迭代 步骤                                       ss       =       ss _ next                                         a       =       pi     [     ss     ]       # 选择 动作                                       if       np     .     random     .     rand     ( )       & lt ;       epsilon     :       #   探索                                               a       =       np     .     random     .     randint     (     0     ,       nA     )                                       # print   ss                 epsilon       =       max     (     0.01     ,       epsilon       *     0.99     )         for       s       in       range     (     nS     ) :               pi     [     s     ]       =       np     .     argmax     (     Q     [     s     ,       : ] )         print       &# 39 ; V = &# 39 ;     ,       np     .     max     (     Q     ,       axis     =     1     )       print       &# 39 ; pi = &# 39 ;     ,       pi         & gt ; & gt ; & gt ;       iter       0       epsilon     =       0.9       V     [     0     ]     =       0.0       iter       10       epsilon     =       0.813943867508       V     [     0     ]     =       -     5.50430863461       iter       20       epsilon     =       0.736116243838       V     [     0     ]     =       -     9.58192560014       iter       30       epsilon     =       0.665730336049       V     [     0     ]     =       -     11.372140159       iter       40       epsilon     =       0.602074582713       V     [     0     ]     =       -     12.0806787351       iter       50       epsilon     =       0.544505460424       V     [     0     ]     =       -     12.3537691746       iter       60       epsilon     =       0.492440978152       V     [     0     ]     =       -     12.5909546081       iter       70       epsilon     =       0.44535479364       V     [     0     ]     =       -     12.657326121       iter       80       epsilon     =       0.402770892387       V     [     0     ]     =       -     12.5917214982       iter       90       epsilon     =       0.36425877541       V     [     0     ]     =       -     12.7107205284       V     =       [     -     12.70433436       -     11.41177551         -     9.8564413           -     8.59335991         -     7.20906867           -     5.70400336         -     4.41177384         -     2.82674456         -     1.35446586           0 .                     ]       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]        ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl-python-a.html"},
      
      
      {"title": "最优控制简介", "text": "    Table   of   Contents           关于           最优控制 问题           动态 规划                 关于       An   introduction   to   Mathematical   Optimal   Control   Theory   读书笔记       最优控制 问题       受控 微分 动力系统       $ $     \ \ begin { align }     \ \ dot { x } ( t )   & amp ; =   f ( x ( t ) ,   \ \ alpha ( t ) ) ,   t & gt ; 0   \ \ \ \     x ( 0 )   & amp ; =   x ^ 0     \ \ end { align }     $ $       Payoffs   函数       $ $     P [ \ \ alpha ( \ \ cdot ) ]   =   \ \ int _ 0 ^ T   r ( x ,   \ \ alpha )   dt   +   g ( x ( T ) )     $ $       g 是 终态 回报 ， 最优控制 就是 寻找 是 上式 最大 的 控制 函数 ！       当 f 是 线性 函数 时 ， 即 系统 是 线性 控制系统 ， 存在 bang - bang 控制 是 最优控制 ！ 即 $ ( | \ \ alpha |   =   1 ) $       最优控制 实际上 相当于 系统 模型 已知 的 强化 学习 问题 ！ 并且 是 确定性 系统 ， 该 系统 用 微分方程 描述 ！       动态 规划       值 函数 $ ( V ( x ,   t ) ) $       $ $     V ( x ,   t )   =   \ \ sup _ { \ \ alpha   \ \ in   A }   P _ { x ,   t }   =   \ \ int _ t ^ T   r ( x ,   \ \ alpha )   ds   +   g ( X ( T ) )     $ $       值 函数 满足 哈密顿 - 雅克 比 - 贝尔曼 方程 HJB       $ $     V _ t   +   \ \ max _ { a   \ \ in   A } {   f ( x ,   a )   \ \ cdot   \ \ nabla _ x   V   +   r ( s ,   a )     }   =   0     $ $       解释 ： 即 在 最优控制 下 ， 单位 时间 值 函数 的 减小 量 等于 回报 ！       动态 规划 求解 步骤 是 ， 先 根据 HJB 方程 求 出值 函数 ， 然后 根据 值 函数 设计 控制策略 ！  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/optimal-control.html"},
      
      
      {"title": "策略梯度理论", "text": "    Table   of   Contents           关于           主要 结论           策略 梯度 基本 理论           几个 概念           策略 梯度 公式                         关于       策略 梯度 理论 早期 经典 论文 笔记 ： Sutton   R   S ,   McAllester   D   A ,   Singh   S   P ,   et   al .   Policy   gradient   methods   for   reinforcement   learning   with   function   approximation [ C ] / / Advances   in   neural   information   processing   systems .   2000 :   1057 - 1063 .       其中   RS   Sutton   就是 《 Reinforcement   Learning   -   An   Introduction 》 的 作者       主要 结论           策略 函数 的 梯度 可以 用 经验 数据 中 估计 出来 的 动作 - 值 函数 ( 即 Q 函数 ) 或者 优势 函数 ( Advantage   function ， 即 A 函数 ) 近似 。       证明 了 策略 梯度 近似 可以 收敛 到 局部 最优 策略 ， 这 也 是 策略 梯度 相比 于值 函数 近似 方法 的 优势 ， 非线性 函数 的 值 函数 近似 方法 无法 保证 收敛 到 局部 最优 ！ （ 不过 线性 函数 还是 可以 的 ）           策略 梯度 基本 理论       用 $ ( \ \ rho ) $ 表示 策略 的 性能 测度 ， 即 期望 回报 ， 用 $ (   \ \ theta   ) $ 表示 策略 函数 的 参数 ， 即 $ (   \ \ pi   =   \ \ pi _ { \ \ theta } ( a |   s )       ) $ 。 那么 策略 梯度 为       $ $     \ \ Delta   \ \ theta   \ \ approx   \ \ alpha   \ \ frac { \ \ partial   \ \ rho } { \ \ partial   \ \ theta }     $ $       因此 ， 当 策略 梯度 趋于 0 时 ， 可以 保证 $ (   \ \ rho   ) $ 达到 极小值 ！       相比 值 函数 近似 方法 ， 策略 梯度 参数 的 很小 变动 只会 带来 很小 的 策略 上 的 改变 ， 而值 函数 近似 方法 可能 带 啦 跳跃式 的 改变 ！       几个 概念       策略 的 长期 期望 单步 回报       $ $     \ \ rho ( \ \ pi )   =   \ \ lim _ { n   \ \ rightarrow   \ \ infty }   \ \ frac { 1 } { n }   E [ r _ 1   +   r _ 2   +   ...   +   r _ n |   \ \ pi ]   =     \ \ sum _ s   d ^ { \ \ pi } ( s   ) \ \ sum _ a   \ \ pi ( s ,   a )   R ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   \ \ sum _ { kt1 } ^ { \ \ infty } E [     r _ t   - \ \ rho ( \ \ pi ) | s _ 0 = s ,   a _ 0 = a ,   \ \ pi ]     $ $           $ (   R ( s ,   a )   =   E [ r _ { t + 1 }   |   s _ t   =   s ,   a _ t   =   a ]   ) $ 是 状态 s 下 采取 动作 a 的 单步 期望 回报 。       $ (   d ^ { \ \ pi } ( s   )   =   \ \ lim _ { n   \ \ rightarrow   \ \ infty }   Pr ( s _ t   = s   | s _ 0 ,   \ \ pi )   ) $   是 在 该 策略 下 状态 s 的 稳态 分布 。           这里 应该 是 认为 这个 马尔科夫 链是 没有 终止 态 的 ， 所以 期望 回报 会 等于 稳态 分布 的 单步 平均值 。       策略 的 长期 折扣 回报       $ $     \ \ rho ( \ \ pi )   =   E [ \ \ sum _ { t = 1 } ^ { \ \ infty }     \ \ gamma ^ { t - 1 }   r _ t | s _ 0 ,   \ \ pi ]   =     \ \ sum _ s   d ^ { \ \ pi } ( s   ) \ \ sum _ a   \ \ pi ( s ,   a )   R ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   E [ \ \ sum _ { k = 1 } ^ { \ \ infty }     \ \ gamma ^ { k - 1 }   r _ { t + k } | s _ t = s ,   a _ t = a ,   \ \ pi ]     $ $           $ (   d ^ { \ \ pi } ( s   )   =   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   Pr ( s _ t   = s   | s _ 0 ,   \ \ pi )   ) $   是 在 该 策略 下 状态 s 的 累积 折扣 次数           这 一种 定义 是 比较 常用 的 一种 ， 马尔科夫 链 可以 有 终止 状态 。       策略 梯度 公式       $ $     \ \ frac { \ \ partial   \ \ rho } { \ \ partial   \ \ theta }   =   \ \ sum _ s   d ^ { \ \ pi } ( s )   \ \ sum _ a   \ \ frac { \ \ partial   \ \ pi ( s ,   a ) } { \ \ partial   \ \ theta }   Q ^ { \ \ pi } ( s ,   a )     $ $  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/pg-function-approximation.html"},
      
      
      {"title": "阿里强化学习应用", "text": "    Table   of   Contents          ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/ali-ebook.html"},
      
      
      
      {"title": "sklearn - python 机器学习库", "text": "    Table   of   Contents           预处理 模块           LabelBinarizer           LabelEncoder                   决策树           树 的 可视化                   特征 离散 化                 预处理 模块         LabelBinarizer         在 one - vs - all 里面 经常 用 ， 将 数值 或者 字符 类型 的 label ， 转换 为 一个 向量 ， 向量 每 一维 对应 其中 一个 label 值 ，     就 像 one - hot 编码 。               & gt ; & gt ; & gt ;       from       sklearn       import       preprocessing       & gt ; & gt ; & gt ;       lb       =       preprocessing     .     LabelBinarizer     ( )       & gt ; & gt ; & gt ;       lb     .     fit     ( [     1     ,       2     ,       6     ,       4     ,       2     ] )       LabelBinarizer     (     neg _ label     =     0     ,       pos _ label     =     1     ,       sparse _ output     =     False     )       & gt ; & gt ; & gt ;       lb     .     classes _       array     ( [     1     ,       2     ,       4     ,       6     ] )       & gt ; & gt ; & gt ;       lb     .     transform     ( [     1     ,       6     ] )       array     ( [ [     1     ,       0     ,       0     ,       0     ] ,                     [     0     ,       0     ,       0     ,       1     ] ] )                 属性   classes _   保存   fit   后 的 类 列表 ， 提供 正 变换   transform   ， 逆变换   inverse _ transform   。         LabelEncoder         用来 将 数值 类型 或者 字符串 类型 编码 为 数字   0   到   n _ classes - 1 。               & gt ; & gt ; & gt ;       from       sklearn       import       preprocessing       & gt ; & gt ; & gt ;       le       =       preprocessing     .     LabelEncoder     ( )       & gt ; & gt ; & gt ;       le     .     fit     ( [     1     ,       2     ,       2     ,       6     ] )       LabelEncoder     ( )       & gt ; & gt ; & gt ;       le     .     classes _       array     ( [     1     ,       2     ,       6     ] )       & gt ; & gt ; & gt ;       le     .     transform     ( [     1     ,       1     ,       2     ,       6     ] )       array     ( [     0     ,       0     ,       1     ,       2     ]     ...     )       & gt ; & gt ; & gt ;       le     .     inverse _ transform     ( [     0     ,       0     ,       1     ,       2     ] )       array     ( [     1     ,       1     ,       2     ,       6     ] )                 决策树       树 的 可视化       利用   tree . export _ graphviz   函数 ， 可以 将 树 导出 为 图 文件 。 借助于   IPython   可以 在 notebook 里面     将 树 可视化 显示 出来 。 需要 安装   graphviz   命令行 工具 ， 在 mac 里面 可以 通过 brew 安装 ， 命令 是   brew   install   graphviz   。     如果 你 在 终端 里面 输入   dot   ， 没有 提示 找 不到 命令 ， 那么 就 安装 好 了 。               from       IPython . display       import       Image       from       StringIO       import       StringIO       import       pydot       from       sklearn       import       tree         def       tree2png     (     clf     ,       cols     ) :               dot _ data       =       StringIO     ( )               tree     .     export _ graphviz     (     clf     ,       out _ file     =     dot _ data     ,       feature _ names     =     cols     ,     filled     =     True     ,       rounded     =     True     ,                                                                     special _ characters     =     True     )               g       =       pydot     .     graph _ from _ dot _ data     (     dot _ data     .     getvalue     ( ) ) [     0     ]               return       Image     (     g     .     create _ png     ( ) )                 特征 离散 化       从 scikit - learn   0.1 . 7 版本 开始 可以 通过 树 分类器 的   . apply       方法 ， 获取 输出 叶子 节点 的 序号 ， 然后 利用   onehot   转换器 转换 为     稀疏 的 离散 特征 。       利用 决策树 分类器 对 特征 进行 离散 化 的 例子               from       sklearn       import       tree       from       sklearn . tree       import       DecisionTreeClassifier         clf       =       DecisionTreeClassifier     (     min _ samples _ leaf     =     10     )       clf     .     fit     (     X _ train     ,       Y _ train     )           from       sklearn . preprocessing       import       OneHotEncoder       onehot       =       OneHotEncoder     ( )         ##   因为 决策树 apply 输出 是 一个 一位 数组 ， 所以 要 reshape       ##   如果 是 randomforest 或者 gbdt ， 那么 输出 的 是 一个二维 数组 ，       ##   每 一个 决策树 有 一个 输出 。 这里 都 是 说 二 分类器 。       leaf _ numbers       =       clf     .     apply     (     X _ train     )     .     reshape     ( (     -     1     ,     1     ) )       onehot     .     fit     (     leaf _ numbers     )       onehot _ features       =       onehot     .     transform     (     leaf _ numbers     )        ", "tags": "machine-learning", "url": "/wiki/machine-learning/sklearn.html"},
      
      
      {"title": "sklearn 源码阅读计划", "text": "    Table   of   Contents           关于           基础 类   base . py           线性 模型           base . py           逻辑 回归                   特征 选取           参考                 关于       sklearn 是 python 的 机器 学习 库 ， 来源于 Google   summer   的 一个 项目 ， 个人 认为 是 一个 设计 地 非常 好 的 软件 ，     对模型 的 抽象 很 不错 。 另一方面 ， 在 学习 理论 的 同时 ， 自己 写 代码 是 一个 加深 理解 的 好 方式 ， 而 阅读 源码 也 是     一种 很 好 的 方式 ， 而且 还 能 学习 别人 的 实现 和 优化 技巧 。       这个 源码 阅读 计划 可能 持续 较长 ， 内容 会 总结 在 wiki 中 ， 最后 形成 文章 在 我 的 博客 中 出现 。       基础 类   base . py       sklearn 使用 5 个 基础 类 ， 对模型 的 功能 进行 抽象 。 用户 可以 基于 这些 类 实现 自己 的 模型 ， 使得 可以 像 操作 sklearn     里面 的 其他 模型 那样 调用 。 基础 类 放在 包   sklearn . base   中           BaseEstimator ， 要求 构造方法 能够 设置 模型 参数 ， 并且 实现     get _ params     和     set _ params     两个 方法 。       ClassifierMixin ， 实现   score   方法 ， 返回   accuracy _ score ， 所有 的 分类器 的 Mixin 类       RegressorMixin ， 实现   score   方法 ， 返回   r2 _ score ， 所有 回归 器 的 Mixin 类       ClusterMixin ， 实现   fit _ predict   方法 ， 返回   聚类 标签 ， 所有 聚类器 的 Mixin 类       BiclusterMixin ， 两聚类 Mixin 类       TransformerMixin ， 实现   fit _ transform   方法 ， 转换器 Mixin 类       MetaEstimatorMixin ， MetaEstimator   Mixin 类           estimator 有个 属性   _ estimator _ type   ， 用来 表示 他 是 分类器 还是 回归 器 。 这是 通过 两个 Mixin 类来 实现 的 。       在   base   模块 中 ， 还 提供 一个   clone   函数 ， 用于 克隆 一个 现有 的 estimator 。       线性 模型         base . py                   LinearModel     线性 模型 的 基础 类           继承     BaseEstimator   ， 提供 抽象 方法     fit     需要 子类 实现 。       它 实现 了     predict     方法 ， 用来 预测 ， 该 方法 直接 输出     _ decision _ function     的 结果 ！       它 的 核心 就是 这个     _ decision _ function   ， 代码 很 简单     safe _ sparse _ dot ( X ,   self . coef _. T ,   dense _ output = True )   +   self . intercept _   。     就是 对 特征向量   X   做 一个 仿射变换     y   =   W   X   +   b   。 此外 ， 还有 一个     _ set _ intercept     用来 设置 偏置 $ ( b   =   \ \ bar { y }   -   w   \ \ bar { x } ) $ 。       在 实现 的 时候 ， 会 对 数据 做 中心化 处理 ， 有 一个 私有 的 中心化 函数     _ conter _ data   。                     LinearClassifierMixin     线性 分类器   Mixin ， 只 处理 预测           核心 的 函数 是     decision _ function     返回 样本 到 超平面 的 有 向 距离 ， 和 上面 的 函数 一样 （ ？ 为什么 分开 写 ） 。         predict     如果 是 两 分类 ， 返回     ( scores   & gt ;   0 ) . astype ( np . int )   ， 多 分类 则 返回 距离 最大 的 那个 index     scores . argmax ( axis = 1 )   ，     最后 会 将 结果 映射 会 label 标签 的 值 。         _ predict _ proba _ lr     LR 的 私有 方法 ， 输出 概率 ！ 代码 采用   inplaced   优化 空间 ， 可以 看   这个 代码                       SparseCoefMixin     稀疏 系数   Mixin   类 。 稀疏 稀疏 和 普通 系数 互相 转换 。 L1   正则 化 需要 继承 这个 类             densify     方法 将 稀疏 系数 转为 普通 向量 。         sparsify       转为 稀疏 系数                     LinearRegression     线性 回归 模型 ， 继承     LinearModel ,   RegressorMixin   。             residues _     采用     @ property     修饰 实现 只读 属性 ！ 0.19 要 废弃 这个 属性           fit     方法       是 核心 。 对于 稀疏 数据 ，                   逻辑 回归       特征 选取       参考             官方 文档           github 源码        ", "tags": "machine-learning", "url": "/wiki/machine-learning/sklearn-source.html"},
      
      
      {"title": "tensorflow google 开源机器学习库", "text": "    Table   of   Contents           TensorFlow   白皮书           基本概念           优化           工具                   核心 图 数据结构           基本 数据类型           优化           tf . distrib . learn   框架           CPU   vs   GPU           TIPS                 TensorFlow   白皮书       第一代 分布式 机器 学习 框架 ：   DistBelief 。     第二代 ： TensorFlow ， 通用 的 计算 框架 ！       基本概念           TensorFlow   的 计算 被 表达 为 一个 有向图 ( graph ) — — 计算 图 ， 它 由 很多 节点 ( Node ) 构成       每 一个 节点 有 0 个 或者 多个 输入 ， 0 个 或者 多个 输出 ， 表达 了 一个 计算 操作 实例       正常 边上 流动 的 值 被称作 张量 ( tensor )       特殊 边 ： control   dependencies ： 没有 数据流 过 这些 边 ， 用来 控制 依赖 关系 的       操作 （ Operation ） ： 对 计算 的 抽象 ， 例如 矩阵 乘法 ， 加法 等 。 操作 可以 有 属性 ， 所有 的 属性 必须 被 指定 ， 或者 在 图 构建 的 时候 能够 推断 出来       内核 （ Kernel ） ： 操作 的 一种 特殊 实现 ， 能够 在 特定 的 设备 （ 如 CPU ， GPU ） 上 运行       会话 （ Session ） ： client 程序 与   TensorFlow   系统 交互 的 方式 ，   一般 创建 一次 ， 然后 调用     run     方法 执行 计算 图 的 计算 操作       变量 （ Variable ） ： 大多数   tensor   在 一次 计算 后 就 不 存在 了 ， 变量 在 整个 计算 图 计算 过程 中 ， 可以 一直 保持 在 内存 。 Variable   操作 返回 一个 句柄 ， 指向 该 类型 的 可变 张量 。 对 这些 数据 的 操作 可以 通过 返回 的 句柄 进行 ， 例如   assign ,   assignadd 操作 。 一般 用来 保存 模型 参数 ！       实现 ： 单机 ， 分布式 。 client   通过   session   提交 计算 任务 ， master 通过   worker   执行 计算 操作                           设备 ： device ， 如 CPU 或者 GPU ； 每 一个   worker   关联 一个 或 多个 设备 。 每个 设备 都 一个 一个 类型 ， 和 一个 名字 ， 如     / job : localhost / device : cpu : 0       或者     / job : worker / task : 17 / device : gpu : 3   。 其他 设备 类型 可以 通过 注册 的 机制 加入 ！               单机 执行           多机 执行 ： 多机 通信 方式 ： TCP   or   RDMA       容错 ： 一旦 检测 到 错误 ， 就 重新 开始 ； 变量 （ Variable ） 会 定期 的 保存   chekpoint 。       梯度 计算 ： 会 创建 一个 子图 ， 计算 梯度       控制流 ： 支持   条件 跳转 ， switch ， 以及 循环       输入 节点 ： client 通过 feed 灌入 数据 ， 或者 直接 定义 输入 节点 直接 访问 文件 （ 效率 更好 ）       队列 ： 让子 图 异步 执行 的 特性 ！ FIFO 队列 ， shuffle   队列       容器 （ container ） ： 用于 存 长期 可变 状态 ， 例如 变量           优化           Common   Subexpression   Elimination ， 公共 子 表达式 消除       通过 控制流 ， 延迟 recevier 节点 的 通信 ， 减少 不必要 的 通信 资源 消耗       异步   kernel       采用 深度 优化 的 库 ： BLAS ， cuBLAS ， convolutional ， Eigen （ 已 扩展 到 支持 任意 维度 的 张量 ）       有损压缩 ， 通信 的 时候 采用 有损压缩 传递数据       数据 并行 ， 模型 并行 ！           工具           TensorBoard ： 训练 过程 可视化 ， 计算 图 结构 的 可视化           核心 图 数据结构       class   tf . Graph       基本 数据类型           constant                   a       =       tf     .     constant     (     5.0     )       b       =       tf     .     constant     (     6.0     )       c       =       a       *       b       with       tf     .     Session     ( )       as       sess     :               print       sess     .     run     (     c     )               print       c     .     eval     ( )                 #   just   syntactic   sugar   for   sess . run ( c )   in   the   currently   active   session !                     Session         tf . InteractiveSession ( )         default   session               Variables ， 用来 表示 模型 参数 。       “ When   you   train   a   model   you   use   variables   to   hold   and     update   parameters .   Variables   are   in - memory   buffers     containing   tensors ”   -   TensorFlow   Docs .                           W1       =       tf     .     ones     ( (     2     ,     2     ) )       W2       =       tf     .     Variable     (     tf     .     zeros     ( (     2     ,     2     ) ) ,       name     =     & quot ; weights & quot ;     )       R       =       tf     .     Variable     (     tf     .     random _ normal     ( (     2     ,     2     ) ) ,       name     =     & quot ; random _ weights & quot ;     )                 使用 函数   tf . initialize _ all _ variables ( )   参数 初始化 ， 初始值 在 定义 的 时候 给出 。     如果 要 对 变量 作用域 里面 的 所有 变量 用 同一个 初始化 方法 ， 可以 在 定义 作用域 的 时候 指定 。     参考   https : / / www . tensorflow . org / versions / r0 . 7 / how _ tos / variable _ scope / index . html # initializers - in - variable - scope                 with       tf     .     variable _ scope     (     & quot ; foo & quot ;     ,       initializer     =     tf     .     constant _ initializer     (     0.4     ) ) :               v       =       tf     .     get _ variable     (     & quot ; v & quot ;     ,       [     1     ] )               assert       v     .     eval     ( )       = =       0.4         #   Default   initializer   as   set   above .                 变量 的 更新 ， 使用 方法   tf . add   ,     tf . assign   等 方法               state       =       tf     .     Variable     (     0     ,       name     =     & quot ; counter & quot ;     )       new _ value       =       tf     .     add     (     state     ,       tf     .     constant     (     1     ) )       update       =       tf     .     assign     (     state     ,       new _ value     )       with       tf     .     Session     ( )       as       sess     :               sess     .     run     (     tf     .     initialize _ all _ variables     ( ) )               print       sess     .     run     (     state     )               for       _       in       range     (     3     ) :                       sess     .     run     (     update     )                       print       sess     .     run     (     state     )         0       1       2       3                 利用   tf . convert _ to _ tensor   方法 可以 将 数值 变量 转换 为 张量 。           placeholder 用来 输入 数据 ， 通过   feed _ dict   字典 将 输入 数据 映射 到 placeholder .                   input1       =       tf     .     placeholder     (     tf     .     float32     )       input2       =       tf     .     placeholder     (     tf     .     float32     )       output       =       tf     .     mul     (     input1     ,       input2     )       with       tf     .     Session     ( )       as       sess     :               print       sess     .     run     ( [     output     ] ,       feed _ dict     =     {     input1     : [     7 .     ] ,       input2     : [     2 .     ] } )         tf     .     placeholder     (     dtype     ,       shape     =     None     ,       name     =     None     )                 注意 ， 这里 有个 大坑 ， 这个 字典 的 键 是 一个   op   ， 而 不是 一个 字符串 ！ ！               变量 作用域 ，   variable _ scope   ,     get _ variable _ scope   ,     get _ variable   .       scope . reuse _ variables ( )     可以 使得 该 作用域 的 变量 重复使用 ， 在 RNN 实现 中 很 有用 。         声明 重复 利用 的 时候 ，   get _ variable   的 时候 不是 创建 一个 变量 ， 而是 查询 保存 的 那个 变量 。               word   embedding               优化       当 得到 损失 函数 之后 ， 可以 通过   tf . train . Optimizer   优化 工具 来 进行 优化 ， 实际 优化 的 时候 使用 的 是 他 的 子类 ，     如   GradientDescentOptimizer   ,     AdagradOptimizer   ,   or     MomentumOptimizer   。     优化 obj 通常 有 以下 几个 重要 方法 可以 使用 。             train _ op   =   minimize ( loss )   ， 直接 最小化 损失 函数         compute _ gradients ( loss )   ， 计算 梯度         train _ op   =   apply _ gradients ( grad )   ， 应用 梯度 更新 权值             tf . distrib . learn     框架       一个 高级 机器 学习 框架           模型 基本 接口 ， 与 sklearn 很 像         init   ( )   初始化         fit     拟合         evaluate     评估         predict     预测                   CPU   vs   GPU           Q :   自己 代码 在 实现 上 有 什么 区别 呢 ？           TIPS             tf . reshape ( some _ tensor ,   ( - 1 ,   10 ) )   将 数据 重新 划分 为 10 列 的 元素 ， 第一 维自 适应         tf . device     指定 CPU 或者 GPU         tf . add _ to _ collection ( name ,   value )   将 value 保存 为 名字 为 name 的 共享 集合 中 ， 供 后面 使用 .       tf . get _ collection ( name )   ， 获取 存储 的 值       tensorflow 里面 的 标量 和   shape = [ 1 ]   是 不同 的 ， 请 注意 。           [ 1 ]     https : / / www . tensorflow . org /       [ 2 ]     http : / / cs224d . stanford . edu / lectures / CS224d - Lecture7 . pdf    ", "tags": "machine-learning", "url": "/wiki/machine-learning/tensorflow-google.html"},
      
      
      {"title": "Theano", "text": "    Table   of   Contents           theano 中 的 broadcast           theano 两层 神经网络 代码 例子           tips                 theano 中 的 broadcast       theano 和 numpy 中 的 broadcast 不同 ，     theano 中 需要 对 能够 broadcast 的 变量 进行 编译 前 申明 ，     默认 情况 下 ， theano 中 的   matrix / tensor   和 所有 的   shared   variable   都 不 不 可以     broadcast 的 ，   vector   和   matrix   进行 计算 时 是 可以 进行 broadcast 的 。       shared   variable   可以 在 创建 的 时候 声明 可以 broadcast 。               bval       =       np     .     array     ( [ [     10     ,       20     ,       30     ] ] )       bshared       =       theano     .     shared     (     bval     ,       broadcastable     =     (     True     ,       False     ) )                 theano 两层 神经网络 代码 例子               def       nn _ cost _ func     ( ) :               import       theano               from       theano       import       tensor       as       T               w1       =       T     .     dmatrix     (     &# 39 ; w1 &# 39 ;     )               b1       =       T     .     dvector     (     &# 39 ; b1 &# 39 ;     )               w2       =       T     .     dmatrix     (     &# 39 ; w2 &# 39 ;     )               b2       =       T     .     dvector     (     &# 39 ; b2 &# 39 ;     )               x       =       T     .     dmatrix     (     &# 39 ; x &# 39 ;     )               y       =       T     .     dmatrix     (     &# 39 ; y &# 39 ;     )               h       =       T     .     nnet     .     sigmoid     (     T     .     dot     (     x     ,     w1     )     +     b1     )               yp       =       T     .     nnet     .     softmax     (     T     .     dot     (     h     ,       w2     )     +     b2     )               J       =       T     .     mean     (     T     .     sum     (     y       *       T     .     log     (     yp     ) ,       axis     =     1     ) )               dJ       =       T     .     grad     (     J     , [     w1     ,     b1     ,     w2     ,     b2     ] )               fJ       =       theano     .     function     ( [     x     ,     y     ,     w1     ,     b1     ,     w2     ,     b2     ] ,       J     )               fdJ       =       theano     .     function     ( [     x     ,     y     ,     w1     ,     b1     ,     w2     ,     b2     ] ,       dJ     )               return       fJ     ,       fdJ                 tips           梯度 函数 输出 一个 列表 ， 如果 只有 一个 梯度 的 时候 ， 需要 这样 写                   grad     ,       =       fdJ     (     weights     ,       features     ,       ground _ truth     ,       N     ,       regularization     )        ", "tags": "machine-learning", "url": "/wiki/machine-learning/theano.html"},
      
      
      {"title": "TOPIC MODEL - 主题模型", "text": "    Table   of   Contents           主题 模型 的 意义           Latent   Dirichlet   allocation ( LDA )           参考文献                 主题 模型 的 意义       Topic   modeling   provides   methods   for   automatically   organizing ,   understanding ,   searching ,   and   summarizing   large   electronic   archives .           Discover   the   hidden   themes   that   pervade   the   collection .       Annotate   the   documents   according   to   those   themes .       Use   annotations   to   organize ,   summarize ,   and   search   the   texts .           Latent   Dirichlet   allocation ( LDA )           文档 包含 多个 主题       每 一个 主题 是 在 词上 的 一个 分布 ， 可以 表达 为 词 的 直方图       每 一个 文档 是 多个 主题 的 混合 ， 可以 表达 为 主题 的 直方图       每 一个 词 是从 某个 主题 中 采样 得到       但是 我们 只能 观察 到 文档 ， 其他 的 都 是 隐 变量 ！       我们 的 目标 是 推断出 这些 隐 变量 ！                   参考文献           Probabilistic   Topic   Models ,   ICML2012   Tutorial :     http : / / www . cs . columbia . edu / ~ blei / talks / Blei _ ICML _ 2012 . pdf       2 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/topic-model.html"},
      
      
      {"title": "VSM - From Frequency to Meaning: Vector Space Models of Semantics", "text": "    Table   of   Contents           关于           导言           语义 向量 空间 模型           文档 的 相似性 ： The   Term – Document   Matrix ， Salton   et   al .   ( 1975 )           词 的 相似性 ： The   Word – Context   Matrix           关系 的 相似性 ： The   Pair – Pattern   Matrix           Types   and   Tokens           五个 假设                   Linguistic   Processing   for   VSM           Tokenization           Normalization           Annotation                   Mathematical   Processing   for   Vector   Space   Models           频率 统计           加权 频率 变换           平滑 矩阵           比较 向量           有效 的 比较                   3 个 开源 VSM 系统           应用           其他 方法   to   语义 分析           VSM 的 未来           问题                 关于       向量 空间 模型 ( VSM ) 综述 论文 ： From   Frequency   to   Meaning :   Vector   Space   Models   of   Semantics ,   2010 .       导言           词 的 分布式 假设 （ distributional   hypothesis   ） :               words   that   occur   in   similar   contexts   tend   to   have   similar   meanings   ( Wittgenstein ,   1953 ;   Harris ,   1954 ;   Weaver ,   1955 ;   Firth ,   1957 ;   Deerwester ,   Dumais ,   Landauer ,   Furnas ,   & amp ;   Harsh -   man ,   1990 )               三类 矩阵 ： term – document ,   word – context ,   and   pair – pattern   matrices       event   frequencies   而 不是   adjacency   matrix （ 基于 词典 的 方法   wordnet ）               未来 工作 ： 新 的 矩阵 ， 高阶 张量 ！       应用 （ lead   algorithm ） ：       measuring   semantic   relatedness :   Pantel   & amp ;   Lin ,   2002a ;   Rapp ,   2003 ;   Turney ,   Littman ,   Bigham ,   & amp ;   Shnayder ,   2003 .       measuring   the   similarity   of   semantic   relations :   Lin   & amp ;   Pantel ,   2001 ;   Turney ,   2006 ;   Nakov   & amp ;   Hearst ,   2008 .                     案例         IQ 测试   or   性格 测试 ：   subject - item   matrix !       向量分析 方法 ：   因子分析   ！       向量 空间 模型 里面 向量 的 元素 来自 于     事件 频率 统计   ！ ！             Latent   Semantic   Analysis   ( LSA )     Deerwester   et   al . ,   1990 ;   Lan -   dauer   & amp ;   Dumais ,   1997         Hyperspace   Analogue   to   Language   ( HAL )     ( Lund ,   Burgess ,   & amp ;   Atchley ,   1995 ;   Lund   & amp ;   Burgess ,   1996 )           语义 向量 空间 模型           假设         statistical   semantics   hypothesis   :   statistical   patterns   of   human   word   usage   can   be   used   to   figure   out   what   people   mean .         bag   of   words   hypothesis           distributional   hypothesis           extended   distributional   hypothesis           latent   relation   hypothesis                     文档 的 相似性 ： The   Term – Document   Matrix ， Salton   et   al .   ( 1975 )           行向量 对应 于 一个 term ， 通常 是 一个 词 ； 列 向量 是 一个 document ， 例如 一个 网页 ！         bag     是 指 一个 集合 ， 不同 的 是 可以 有 重复 ， 但是 元素 的 顺序 没有 意义 ， 不同 顺序 是 等价 的 ！     一个 bag 可以 用 一个 向量 来 表示 ， 向量 每个 元素 表示 对应 的 bag 元素 出现 的 次数 ， 例如 { a , a , b , c , c , c , } 可以 表示 为 & lt ; 2 , 1 , 3 & gt ; .     一系列 的 bag 可以 用 一个 矩阵 X 表示 ， 矩阵 的 一列 对应 于 一个 bag 向量 ， 而 一行 对应 于 一个 唯一 的 元素 ， $ ( x _ { ij } ) $ 为 第 j 个 bag 中 元素 i 出现 的 次数 。       term - document 中 ， 一个 文档 被 表达 为 一个   bag   of   word ， 每 一个 列 向量 对应 于 bag 的 向量 表达 。               In   information   retrieval ,   the   bag   of   words   hypothesis   is   that   we   can   estimate   the   relevance   of   documents   to   a   query   by   representing   the   documents   and   the   query   as   bags   of   words .   ( Salton   et   al . ,   1975 )               矩阵 的 每 一列   $ ( x _ { : j } ) $ 代表 文档 $ ( d _ j ) $ 的 一种 向 量化 表达 ， 虽然 没有 考虑 词 的 顺序 、 短语 、 句子 等 语义 结构 ， 但是 仍然 在 搜索引擎 中 工作 的 很 好 ！       而 每 一行   $ ( x _ { i } ) $ 代表 term   $ ( w _ i ) $   的 一种 签名 ！ 可以 用来 度量   term   的 相似性 ！ Deerwester   et   al .   ( 1990 )       一种 解释 ： the   topic   of   a   document   will   probabilistically   influence   the   author ’ s   choice   of   words   when   writing   the   document .   直接 导致 LDA 模型 的 出现 ！           词 的 相似性 ： The   Word – Context   Matrix           行向量 是 词 ， 列 向量 是 上下文 ， context   可以 是 词 、 短语 、 句子 、 段落 、 章节 、 文档 等 更 多 可能性       上下文 可以 参考   Sahlgren ’ s   ( 2006 )   thesis       矩阵 相似 的 行向量 代表 相似 的 词 ！ 但是 主要 的 上下文 通常 是 其他 词 ！       共现 频率   Weaver   ( 1955 )   co - occurrence   frequency ， 用来 消歧意               distributional   hypothesis   in   linguistics   is   that   words   that   occur   in   similar   contexts   tend   to   have   similar   meanings   ( Harris ,   1954 )           关系 的 相似性 ： The   Pair – Pattern   Matrix           行向量 是 词 对 ， 例如   mason : stone   and   carpenter   :   wood ； 列 向量 是 词 对 出现 的 模式 ， 例如   “ X   cuts   Y   ”   and   “ X   works   with   Y   ” .   Lin   and   Pantel   ( 2001 ) ， 用来 判定 模式 的 相似性       用来 推理 ， 一个 句子 是 另 一个 句子 的 解释 。       行向量 ： 词对 的 相似性 ， Turney   et   al .   ( 2003 )                 extended   distributional   hypothesis   ,   that   patterns   that   co - occur   with   similar   pairs   tend   to   have   similar   meanings .   Lin   and   Pantel   ( 2001 )         The   latent   relation   hypothesis     is   that   pairs   of   words   that   co - occur   in   similar   patterns   tend   to   have   similar   semantic   relations   ( Turney ,   2008a )           关系 的 相似性 不能 消减 为 属性 的 相似性 （ word - context   matrix )           高阶 张量 ：       term – document – language   third - order   tensor ： 多 语言 信息检索       word – word – pattern   tensor ： 词 相似性       verb – subject – object   tensor ：                   Types   and   Tokens       token - document   matrix ， 里面 相同 的 词 但是 出现 在 不同 地方 的 词 作为 不同 的 token ；     type - duocument   matrix ， 则 把 相同 词 合并 了 。       前者 可以 用 在 词消 歧义 上 ， 一词 多义 ！       问题 ， 这种 token - document   matrix 完全 看不出 有 什么 意义 啊       五个 假设           Statistical   semantics   hypothesis ： 词 的 统计 模式 可以 用来 表明 含义       Bag   of   words   hypothesis       Distributional   hypothesis       Extended   distributional   hypothesis       Latent   relation   hypothesis           Linguistic   Processing   for   VSM       对 数据 的 预处理 ： tokenize ， normalize （ 将 词 不同 的 形式 归一化 ） ， annotate   the   raw   text （ 将 相同 的 形式 标记 为 不同 的 含义 ： eg   动词 ， 名词 ）       Grefenstette   ( 1994 ) ： 三步走 ： tokenization ,   surface   syntactic   analysis ,   and   syntactic   attribute   extraction .       Tokenization       英语 等 西班牙语 系 可以 通过 天然 的 分割 符 空格 进行 分割 ！     而 汉语 等 非 西班牙语 系则 不同 ！       精确 的 Tokenizer 还 需要 处理 标点符号 ！ 连 字符 ， multi - word   terms （ e . g . ,   Barack   Obama   and   ice   hockey ） 。     停止 词 ， 高频 却 无 意义 的 词 ， 代词 等 。 停止 词表 ： SMART   system   ( Salton ,   1971 )       Normalization           case   folding       stemming           一般而言 ， 归一化 将 导致 精确度 降低 ， 召回 率 提高 。     如果 数据量 少 ， 一定 要 用 归一化 ， 提高 召回 率 ；     但 如果 数据量 很大 ， 精确度 更 重要 ， 可以 不 归一化 ！       Annotation           part - of - speech   tagging       word   sense   tagging       parsing           降低 召回 率 ， 提高 精确度 ！       Mathematical   Processing   for   Vector   Space   Models       Lowe   ( 2001 )   4 步 走 ： 1 、 统计 频率 ， 2 、 频率 变换 （ 加权 ） ， 3 、 平滑 ， 4 、 计算 相似性 。       频率 统计       关键技术 ： Hash   Table ； 数据库 ； 搜索引擎 索引 。       加权 频率 变换           TF - IDF   用 倒 文档 频率 作为 权值       文档 长度 ： 因为 相同 的 情况 下 ， 长 文档 更 容易 被 匹配 到 ！ 因为 词多 ！       term   的 权重 ， 两个 很 相近 的 词 同时 出现 在 一个 文档 中 ， 除了 可以 将 他们 归一化 到 同一个 词 ， 也 可以 减少 他们 的 权重 ！       特征选择 也 可以 看做 一种 加权 手段 ： Forman   ( 2003 )       Pointwise   Mutual   Information （ PMI ， Church   & amp ;   Hanks ,   1989 ;   Turney ,   2001 ）       Positive   PMI （ PPMI ） ： 将 PMI 小于 0 的 值置 0 ！ 当用 word - context 矩阵 度量 语义 相似性 地 时候 ， 效果 更好 ！           假设 word - context   矩阵   F ， 行向量 $ ( f _ i ) $ ， 列 向量 $ ( f _ { : j } ) $ 。 新 矩阵   X   是 PPMI 矩阵 ， 定义 为       $ $     p _ { ij }   =   \ \ frac { f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     p _ { i * }   =   \ \ frac { \ \ sum _ j   f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     p _ { *   j }   =   \ \ frac { \ \ sum _ i   f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     pmi _ { ij }   =   \ \ log \ \ left (   \ \ frac { p _ { ij } } { p _ { i * }   p _ { *   j } }     \ \ right )         \ \ \ \     x _ { ij }   =   \ \ begin { cases }             pmi _ { ij }   & amp ;   if   pmi _ { ij }   & gt ;   0   \ \ \ \             0         & amp ;     \ \ text { otherwise }     \ \ end { cases }     $ $       PMI   的 问题 ， 对 小 概率 事件 有 偏 ！ 特例 ： 当 i 和 j 统计 依赖 ， $ ( p _ { ij }   =   p _ { i * }   =   p _ { *   j } ) $ ，     那么 PMI 变为   $ ( \ \ log ( 1 / p _ { i * } ) ) $ 。       一种 解决方案 是 （ Pantel   & amp ;   Lin ,   2002a ） ， 对 $ ( f _ { ij } ,   f _ { i * } ,   f _ { *   j } ) $ 进行 平滑 处理       $ $     \ \ delta _ { ij }   =   \ \ frac { f _ { ij } } { f _ { ij }   +   1 }   \ \ frac { \ \ min ( f _ { *   j } ,   f _ { i * } ) } { \ \ min ( f _ { *   j } ,   f _ { i * } )   +   1 }     \ \     newpmi _ { ij }   =   \ \ delta _ { ij }   pmi _ { ij }     $ $       另 一种 解决方案 是 对 概率 进行 拉普拉斯 平滑 ！ 即 对 每 一个 $ ( f _ { ij }   \ \ rightarrow   f _ { ij }   +   k ) $ 。       平滑 矩阵           限制 向量 成分 ： 只 保留 PMI 超过 某个 阈值 的 项 ， 其他 置 0 .       truncated   SVD ： 应用 到 document   similarity 就是   Latent   Semantic   Indexing   ( LSI ) ； 应用 到   word   similarity   就是   Latent   Semantic   Analysis   ( LSA )           rank - k   矩阵 近似 ， 最小化 富 比尼 范数 $ ( | | X   -   \ \ hat { X } | | _ F ) $ ！ ( Golub & amp ; VanLoan , 1996 )           Latent   Meaning :   Deerwester   et   al .   ( 1990 )   and   Landauer   and   Dumais   ( 1997 )   ， 认为 k 个 最大 的 奇异 值 是 隐层 语义 ， 对应 的 两个 矩阵 分别 代表 term 和 document 与 不同 隐 变量 的 相关度 。       Noise   reduction ： 是 对 矩阵 X 的 平滑 ！ Rapp   ( 2003 )       High - order   co - occurrence :   Landauer   and   Dumais   ( 1997 ) ， Lemaire   and   Denhiere   ( 2006 )           Sparsity   reduction ： 类似 于 矩阵 补全 ！               SVD 实现 ：           svdlibc :   http : / / tedlab . mit . edu / ~ dr / svdlibc /   .   Rohde       Brand ’ s   ( 2006 )       Gorrell ’ s   ( 2006 )                   高阶 张量 类似 算法 ： parallel   factor   analysis ， canonical   decomposition ， Tucker   decomposition           其他 平滑 方法 ：       Nonnegative   Matrix   Factorization   ( NMF )   ( Lee   & amp ;   Seung ,   1999 ) ,   Probabilistic   Latent   Semantic   Indexing   ( PLSI )   ( Hofmann ,   1999 ) ,   Iter -   ative   Scaling   ( IS )   ( Ando ,   2000 ) ,   Kernel   Principal   Components   Analysis   ( KPCA )   ( Scholkopf ,   Smola ,   & amp ;   Muller ,   1997 ) ,   Latent   Dirichlet   Allocation   ( LDA )   ( Blei   et   al . ,   2003 ) ,   and   Discrete   Component   Analysis   ( DCA )   ( Buntine   & amp ;   Jakulin ,   2006 ) .               SVD   隐含地 假设 词频 是 高斯分布 ， 然而 并 不是 ， PMI 比 PPMI 更 接近 高斯分布 ！           比较 向量           向量 夹角 余弦 值 ！       距离 的 度量 可以 转换 为 相似 度       距离 度量 ： 欧式 距离 ， 曼哈顿 距离 ，   Hellinger ,   Bhattacharya ,     and     Kullback - Leibler         在   Bullinaria   and   Levy   ( 2007 )   试验 中 ， 余弦 相似 度 效果 最好 ！       其他 度量 ： Dice   and   Jaccard   coe   cients   ( Manning   et   al . ,   2008 ) .           三类 ： Weeds   et   al .   ( 2004 )           high - frequency   sensitive   measures   ( cosine ,   Jensen - Shannon ,   $ ( \ \ alpha ) $ - skew ,   recall ) ,       low - frequency   sensitive   measures   ( precision ) ,   and       similar - frequency   sensitive   methods   ( Jaccard ,   Jaccard + MI ,   Lin ,   harmonic   mean ) .           有效 的 比较           稀疏 矩阵 乘法 优化       将 低于 阈值 的 项 减为 0 ， 也 可以 极大 的 减少 计算 量       分布式 实现 ： MapReduce ， Elsayed ,   Lin ,   and   Oard   ( 2008 )       随机 算法 ：       random   indexing       Locality   sensitive   hashing （ LSH ）                   3 个 开源 VSM 系统               Term - Document   Matrix ：   Lucene .   结合   Nutch ， Solr 可以 做 一个 搜索 系统 了 ！               Word – Context   Matrix :   Semantic   Vectors               Pair – Pattern   Matrix :   Latent   Relational   Analysis   in   S - Space               应用           Term – Document   Matrices ：       文档 检索 ， 跨 语言 检索 ： 截断 SVD 可以 提高 精度 和 召回 ！ ！ ！ 问题 在于 要 解决 大规模 问题 的 计算 量 ！ 其他 技巧 有 协同 过滤 和 PageRank       文档 聚类       文档 分类 ： 主题 ， 语义 ， 垃圾邮件       文章 自动 打分       文档 分割 ： 将 文档 分割 为 几个 不同 的 主题       QA   问答 系统       Call   routing ， 客服 ？               Word – Context   Matrices ：       词 相似性 ： TOEFL       词聚类       词 分类       词典 自动 生成       词消 歧义       上下文 评写 纠错       查询 扩展 ： 搜索引擎 扩展 查询 词为 相近 的 词 ： 使用   session   上下文 和 click   上下文       文本 广告 ： 点击 付费 广告 ： bidterm   扩展       信息提取 （   I     nformation     E     xtraction ) :   名字 实体 识别 （ NER ） ， relation   extraction ,   event   extraction ,   and   fact   extraction               Pair – Pattern   Matrices       关系 相似性       模式 相似性       关系 聚类       关系 分类       关系 搜索       自动 词典 生成       Analogical   mapping ： SAT 测试   a : b : : c : d                   其他 方法   to   语义 分析           概率 语言 模型       词典 ： 图           VSM 的 未来           批评 ： 没有 考虑 词 的 顺序       80 % 的 含义 来自 于 词 ！ ！ ？ Landauer   ( 2002 )           问题             随机 投影           LSH     SIMIHash 等      ", "tags": "machine-learning", "url": "/wiki/machine-learning/vsm.html"},
      
      
      {"title": "What You Get Is What You See: A Visual Markup Decompiler", "text": "    Table   of   Contents           关于           导言           Problem :   Image - to - Markup   Generation           模型   WYGIWYS                 关于       论文 ： What   You   Get   Is   What   You   See :   A   Visual   Markup   Decompiler       导言       OCR 用来 识别 并 提取 结构 信息 ： 不仅仅 要 识别 文字 ， 还要 提取 语义 。     数学 表达式 OCR 系统 ： INFTY 系统 。     需要 联合 处理 图片 和 文字 信息 。       文章 使用 的 模型 是 对模型     attention - based   encoder - decoder   model   ( Bahdanau ,   Cho ,   and   Bengio   2014 )     的 简单 扩展 。           The   use   of   attention   addi -   tionally   provides   an   alignment   from   the   generated   markup   to   the   original   source   image           数据 集 ： IM2LATEX - 100K       在线 效果 演示 ：   http : / / lstm . seas . harvard . edu / latex /         Problem :   Image - to - Markup   Generation           图像 ： $ ( x   \ \ in   \ \ mathcal { X } ) $ ， 例如 $ ( \ \ mathcal { X }   =   \ \ mathbb { R } ^ { H   \ \ times   W } ) $ 。       文本 ： $ ( y   =   ( y _ 1 ,   y _ 2 ,   ... ,   y _ C ) ;   y   \ \   in   \ \ mathcal { Y } ,   y _ i   \ \ in   \ \ Sigma ) $ 。       编译 ： $ ( \ \ mathcal { Y }   \ \ rightarrow   \ \ mathcal { X } ) $ .       需要 学习 一个 反编译器 ！       训练 ： 利用 样本 $ ( ( x ,   y ) ) $ 训练 学习 一个 反编译器 。       测试 ： 利用 模型 预测 的 $ ( \ \ hat { y } ) $ 和 编译 函数 ， 生成 一个 图像 $ ( \ \ hat { x } ) $ ， 要求 生成 的 图像 和 $ ( x ) $ 一致 。           模型   WYGIWYS                   图像 特征 抽取 ： CNN ， 没有 全 连接 层 ， 抽取 的 特征 V 尺寸 为   $ ( D   \ \ times   H '   \ \ times   W ' ) $ ， 分别 是 通道 数 ， 降维后 的 高度 和 宽度 。       编码器 ： 之前 的 ImageCaption 不 需要 这个 编码器 ， 但是 编码器 可以 学到 顺序 关系 ， 这 可以 ：       学习   markup   languages   的 从左到右 的 顺序 关系       使用 周围 的 上下文 去 编码 隐层 表达                   编码器 使用 RNN （ LSTM ） 。 隐层   feature   grid   $ ( \ \ tilde { V } _ { h , w }   =   \ \ text { RNN } ( \ \ tilde { V } _ { h , w - 1 } ,   V _ { h ,   w } ) ) $ ，     即 按行 顺序 编码 ， 对 每 一行 的 初始状态 $ ( \ \ tilde { V } _ { h , 0 } ) $ ， 也 是 通过 学习 得到 （ 怎么 训练 ？ 作为 一个 参数 一起 学 ？ ） ， 叫做   position   embedding ， 可以 表达 图像 所在位置 信息 。           解码器 ： 优点 复杂       通过 上述 编码 后 的 特征   grid   $ ( \ \ tilde { V } ) $ ， 加上 历史 隐层 向量 $ ( h _ { t - 1 } ) $ 学习 一个 注意力 向量 $ ( \ \ alpha _ t ) $       利用 注意力 向量 和 特征 矩阵   $ ( \ \ tilde { V } ) $   学习 一个 有 注意力 的 上下文 向量   $ ( c _ t ) $       利用 当前 隐态 向量   $ ( h _ t ) $   和 带有 注意力 的 上下文   $ ( c _ t ) $   学习 一个 输出 向量   $ ( o _ t ) $ ， 最终 做 softmax 变换 得到 输出 的 词 $ ( y _ t ) $ ！       隐态 更新 采用 常规 的 Decoder 方案 ， 即 上 一 时刻 的 隐态 $ ( h _ { t - 1 } ) $   加上   上 一 时刻 的 输出   $ ( o _ t ,   y _ { t - 1 } ) $                   $ $       $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/im2tex.html"},
      
      
      {"title": "word2vec", "text": "    Table   of   Contents           关于           word2vec   论文           导论           The   Skip - gram   Model           Hierarchical   Softmax           Negative   Sampling           高频词 的 负 采样                   测试数据           短语 学习                   CBOW   论文           Glove                 关于       词 向量 相关 论文 学习 。       word2vec   论文                 Mikolov ,   T .   ,   Sutskever ,   I . ,     Chen ,   K .   ,   Corrado ,   G .   S . ,   & amp ;     Dean ,   J .     ( 2013 ) .   Distributed   representations   of   words   and   phrases   and   their   compositionality .   In   Advances   in   neural   information   processing   systems   ( pp .   3111 - 3119 ) .               问题 ：           相比 直接 通过 VSM 学习 到 的 词 向量 ， 优势 是 什么 ？                   导论               创新 点 ：           针对 连续   skip - gram   模型 ： 采用 一些 介壳 提升 词 向量 质量 和 训练 速度       负 采样 技术 ：   对 高频词 负 采样 可以 显著 提升 训练 时间 （ 2 - 10 倍 的 提升 ） ， 同时 也 能 提高 低频词 的 词 向量 质量 ？                   recursive   autoencoders   ： Richard   Socher ,   Cliff   C .   Lin ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .   Parsing   natural   scenes   and   natural   language   with   recursive   neural   networks .   In   Proceedings   of   the   26th   International   Conference   on   Machine   Learning   ( ICML ) ,   volume   2 ,   2011 .               The   Skip - gram   Model       用 中间 词 预测 周围 的 词 ， 最大化 对数 似然 函数       $ $     \ \ frac { 1 } { T }   \ \ sum _ { t = 1 } ^ T   \ \ sum _ { - c   \ \ le   j   \ \ le   c ,   j   \ \ neq   0 }   \ \ log   p ( w _ { t + j } |   w _ t )   \ \ \ \     p ( w _ O |   w _ I )   =   \ \ frac { \ \ exp ( v ^ ' _ { w _ O } ^ T   v _ { w _ I } ) } { \ \ sum _ { w = 1 } ^ W   \ \ exp ( v ^ ' _ { w } ^ T   v _ { w _ I } )   }     $ $       计算 代价 正比 于 词典 规模   W （ 10 ^ 5 - 10 ^ 7 ） ， 因此 很 费时间 。       Hierarchical   Softmax       Frederic   Morin   and     Yoshua   Bengio   .   Hierarchical   probabilistic   neural   network   language   model .   In   Pro -   ceedings   of   the   international   workshop   on   artificial   intelligence   and   statistics ,   pages   246 – 252 ,   2005 .       只 需要 计算 $ ( \ \ log _ 2W ) $ 个 节点 ！     将 条件 概率 的 计算 ， 变成 多个 分类 概率 的 计算 。 可以 用 一个 二叉树 表示 出来 ， 叶子 结点 对应 词 ， 中间 节点 有 一个 参数 ！     每 一个 内部 节点 可以 看做 一个 二 分类 逻辑 回归 ， 其 参数 就是 二 分类 参数 。 这个 参数 也 要 学习 ！     文中 表示 用 哈夫曼 树 作为 这个 二叉树 可以 简单 提升 性能 。       $ $     p \ \ left (   w _ O   |   w _ I   \ \ right )   =   \ \ prod _ { j   =   1 } ^ { L ( w )   -   1 }   \ \ sigma   \ \ left (   [ n ( w , j + 1 )   =   ch ( n ( w , j ) ) ]   \ \ centerdot   v _ { n ( w , j ) } ^ { \ \ top }   v _ { w _ I }   \ \ right )     $ $       Negative   Sampling           Noise   Contrastive   Estimation   ( NCE ) ：       Michael   U   Gutmann   and   Aapo   Hyva   ̈ rinen .   Noise - contrastive   estimation   of   unnormalized   statistical   mod -   els ,   with   applications   to   natural   image   statistics .   The   Journal   of   Machine   Learning   Research ,   13 : 307 – 361 ,   2012 .       Andriy   Mnih   and   Yee   Whye   Teh .   A   fast   and   simple   algorithm   for   training   neural   probabilistic   language   models .   arXiv   preprint   arXiv : 1206.6426 ,   2012 .                   负 采样 解释 可以 看 2014 年 的 文章 ：   http : / / cn . arxiv . org / pdf / 1402.3722 v1 . pdf         可以 将 每一项 理解 为 一个二元 分类 问题 ， 正 样本 是 词 在 中心词 的 上下文 ， 而负 样本 是 不 在 中心词 上下文 的 词 。     目标 函数 相当于 让 正 样本 出现 以及 k 各负 样本 不 出现 的 联合 概率 最大化 ！       $ $     \ \ log   \ \ sigma ( v _ { w _ O } ' ^ T   v _ { w _ I } )   +   \ \ sum _ { i = 1 } ^ k   \ \ mathbb { E } _ { w _ i   \ \ sim   P _ n ( w ) }   \ \ left [     \ \ log ( - \ \ sigma ( v _ { w _ i } ' ^ T   v _ { w _ I } ' ) )   \ \ right ]     $ $       对于 小 数据 集 k 取   5 - 20   即可 ； 对于 大 数据 集 k 可以 取 小点 2 - 5 .     采样 方法   P   取   unigram   distribution   $ ( U ( w ) ^ { 3 / 4 } ) $ 最好 ， 即 正比 于 词频 的 3 / 4 次 幂 。       高频词 的 负 采样       以 概率 P 丢弃 ！       $ $     P ( w _ i )   =   1   -   \ \ sqrt { \ \ frac { t } { f ( w _ i ) } }     $ $       t   是 阈值 ， 典型值 为 $ ( 10 ^ { - 5 } ) $ ， f 是 词 频率 ！     这种 方式 不但 可以 加快速度 ， 还 能 提高 低频词 的 精度 ！ （ 不是 数据 越多越好 ？ ！ ）       测试数据           相似 推理 任务 ：       语法 相似   syntactic   analogies ：   “ quick ”   :   “ quickly ”   : :   “ slow ”   :   “ slowly ”       语义 相似   semantic   analogies ： “ Germany ”   :   “ Berlin ”   : :   “ France ”   :   ?     结果 如下 图 所示 。                           短语 学习       将 经常出现 在 一起 的 ， 而 不 经常 在 其他 上下文 出现 的 多个 词 作为 一个 token 。 例如 ： New   York   Times ；     但是 ： this   is ， 没有 作为 一个 token ！       CBOW   论文         Mikolov ,   T .   ,   Chen ,   K . ,   Corrado ,   G . ,   & amp ;   Dean ,   J .   ( 2013 ) .   Efficient   estimation   of   word   representations   in   vector   space .   arXiv   preprint   arXiv : 1301.3781 .       周围 词加 和 预测 中心词 ， Hierarchical   Softmax   哈夫曼 树 ！       Glove       Pennington ,   J . ,   Socher ,   R . ,   & amp ;   Manning ,   C .   D .   ( 2014 ,   October ) .   Glove :   Global   Vectors   for   Word   Representation .   In   EMNLP   ( Vol .   14 ,   pp .   1532 - 43 ) .           square   root   type   transformation   in   the   form   of   Hellinger   PCA   ( HPCA )   ( Lebret   and   Collobert ,   2014 )       Mnih   and   Kavukcuoglu   ( 2013 )   also   proposed   closely - related   vector   log - bilinear   models ,   vLBL   and   ivLBL ,   and   Levy   et   al .   ( 2014 )   proposed   explicit   word   embed -   dings   based   on   a   PPMI   metric .           核心思想 ， 直接 建模 共生 矩阵 ！ （ skip - gram ， CBOW 是 直接 建模 上下文 ！ 不能 利用 全局 统计 信息 ？ ）     建模 概率 比率 ， 而 不是 建模 概率 本身 ！       $ $     F ( w _ i ,   w _ j ,   \ \ hat { w } _ k )   =   \ \ frac { P _ { ik } } { P _ { jk } }     $ $       希望 学习 到 线性关系 ？ ！       $ $     F ( w _ i ,   w _ j ,   \ \ hat { w } _ k )   =   F ( w _ i   -   w _ j ,   \ \ hat { w } _ k )   \ \ \ \       =   F (   ( w _ i   -   w _ j ) ^ T   \ \ hat { w } _ k )   =   \ \ frac { P _ { ik } } { P _ { jk } }     $ $       考虑 对称性 ， 即将 共生 矩阵 行列 对换 ， 需要 保持 不变性 ！ 那么 要 F 是 群 ( R ,   + ) 到 ( R + ,   x ) 的 同态 映射       $ $     F ( ( w _ i   -   w _ j ) ^ T   \ \ hat { w } _ k )   =   \ \ frac { F ( w _ i ^ T   \ \ hat { w } _ k ) } { F ( w _ j ^ T   \ \ hat { w } _ k ) }     $ $       因此 ， F 是 指数函数 ！       进而 要求 具有 交换 对称性 ， 可以 增加 bias 实现       $ $     w _ i ^ T   \ \ hat { w } _ k   +   b _ i   +   \ \ hat { b } _ k   =   \ \ log ( X _ { ik } )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/word2vec.html"},
      
      
      {"title": "xgboost", "text": "    Table   of   Contents           关于           xgboost   论文 导读           introduction           Tree   bossting   in   a   nutsell           分裂 点 寻找 算法           系统 设计                   个人 注解           GBDT 和 XGBOOST 的 联系                   算法 细节           源码 实现           TODO           reference                 关于       xgboost   据说 是 现在 大 数据 竞赛 冠军队 的 标配 ！       xgboost   论文 导读       三个 关键点     -   large - scale     -   sparsity - aware   algorithm   for   sparse   data     -   weighted   quantile   sketch   for   approximate   tree   learning       introduction           机器 学习 和 data - driven 方法 的 成功 依赖于 两 方面 的 发展 ： 1 ， 有效 的 （ 统计 ） 模型   2 ， scalable   算法       tree   boosting   算法       P .   Li .   Robust   Logitboost   and   adaptive   base   class   ( ABC )     Logitboost .   In   Proceedings   of   the   Twenty - Sixth   Conference     Annual   Conference   on   Uncertainty   in   Artificial   Intelligence     ( UAI ’ 10 ) ,   pages   302 – 311 ,   2010 .       C .   Burges .   From   ranknet   to   lambdarank   to   lambdamart :     An   overview .   Learning ,   11 : 23 – 581 ,   2010 .       X .   He ,   J .   Pan ,   O .   Jin ,   T .   Xu ,   B .   Liu ,   T .   Xu ,   Y .   Shi ,     A .   Atallah ,   R .   Herbrich ,   S .   Bowers ,   and   J .   Q .   n .   Candela .     Practical   lessons   from   predicting   clicks   on   ads   at   facebook .     In   Proceedings   of   the   Eighth   International   Workshop   on     Data   Mining   for   Online   Advertising ,   ADKDD ’ 14 ,   2014 .         Netflix   price     J .   Bennett   and   S .   Lanning .   The   netflix   prize .   In     Proceedings   of   the   KDD   Cup   Workshop   2007 ,   pages   3 – 6 ,     New   York ,   Aug .   2007 .               Kaggle   2015   年 29 个 比赛 中 ， TOP3 队伍 中有 17 个用 了 XGBoost ， 其中 8 个用 XGBoost 直接 预测 ， 而 另外 的 用 XGBoost 和 神经网络 进行 集成 。 而 DNN 居然 只有 11 个 ！       KDDCup   2015   TOP10   队伍 都 用 了 XGBoost ！ ！       各种 比赛 中 已 解决 的 任务 包括 ：   store   sales     prediction ;   high   energy   physics   event   classification ;   web   text     classification ;   customer   behavior   prediction ;   motion   detection ;     ad   click   through   rate   prediction ;   malware   classification ;     product   categorization ;   hazard   risk   prediction ;   massive   online     course   dropout   rate   prediction .           XGBoost   创新 点 在于 ：           a   novel   tree   learning   algorithm   is   for   handling   sparse   data ;       a   theoretically   justified   weighted     quantile   sketch   procedure   enables   handling   instance   weights     in   approximate   tree   learning       Parallel   and   distributed   computing     makes   learning   faster   which   enables   quicker   model   exploration       还 可以 使用 out - of - core 计算 ， 使得 在 单机 就 可以 处理 上 亿 （ hundred   million ） 样本                   现有 的 并行 的   tree   boosting   算法 有 ：           B .   Panda ,   J .   S .   Herbach ,   S .   Basu ,   and   R .   J .   Bayardo .     Planet :   Massively   parallel   learning   of   tree   ensembles   with     mapreduce .   Proceeding   of   VLDB   Endowment ,     2 ( 2 ) : 1426 – 1437 ,   Aug .   2009 .       S .   Tyree ,   K .   Weinberger ,   K .   Agrawal ,   and   J .   Paykin .     Parallel   boosted   regression   trees   for   web   search   ranking .   In     Proceedings   of   the   20th   international   conference   on   World     wide   web ,   pages   387 – 396 .   ACM ,   2011 .       J .   Ye ,   J . - H .   Chow ,   J .   Chen ,   and   Z .   Zheng .   Stochastic     gradient   boosted   distributed   decision   trees .   In   Proceedings     of   the   18th   ACM   Conference   on   Information   and     Knowledge   Management ,   CIKM   ’ 09 .                   尚未 解决 的 问题 是 ： out - of - core   computation ,     cache - aware   and   sparsity - aware   learning     -   大神 解决 的 几个 方案 ， 后面 再 膜拜             -   T .   Chen ,   H .   Li ,   Q .   Yang ,   and   Y .   Yu .   General   functional             matrix   factorization   using   gradient   boosting .   In   Proceeding             of   30th   International   Conference   on   Machine   Learning             ( ICML ’ 13 ) ,   volume   1 ,   pages   436 – 444 ,   2013 .             -   T .   Chen ,   S .   Singh ,   B .   Taskar ,   and   C .   Guestrin .   Efficient             second - order   gradient   boosting   for   conditional   random             fields .   In   Proceeding   of   18th   Artificial   Intelligence   and             Statistics   Conference   ( AISTATS ’ 15 ) ,   volume   1 ,   2015 .       Tree   bossting   in   a   nutsell           正则 化 目标 函数           回归 树 的 数学 表示 如下 ， q 是 一个 将 特征向量 x 映射 到 树 的 叶子 节点 ， T 是 叶子 结点 个数 。     每 一个 叶子 结点 对应 一个 连续 值 $ ( w _ i ) $ ， 输出 的 是 q 映射 的 那个 叶子 结点 的 值 。       $ $     F   =   { f ( x )   =   w _ { q ( x ) }   }   ( q   :   R ^ m   →   { 1 , 2 , ... , T } ,   w   ∈   R ^ T )     $ $       树 ensemble 之后 的 输出 是 融合 每 一棵树 的 结果 后 的 输出 ( 直接 求和 ！ ！ ？ ？ )       $ $     \ \ hat { y }   =   \ \ phi ( x _ i )   =   \ \ sum _ { k = 1 } ^ K   f _ k ( x _ i ) ,   f _ k   \ \ in   F     $ $       添加 正则 项后 的 目标 函数 为       $ $     L ( \ \ phi )   =   \ \ sum _ i   l ( y _ i ;   \ \ hat { y } _ i )   +   \ \ sum _ k   \ \ Omega ( f _ k )     \ \ \ \     where   \ \ Omega ( f )   =   \ \ gamma   T   +   \ \ frac { 1 } { 2 }   \ \ lambda   | | w | | ^ 2     $ $       这个 损失 函数 也 在 Regularized   greedy   forest   ( RGF )     model   出现 过 ， 参看 这 篇文章       T .   Zhang   and   R .   Johnson .   Learning   nonlinear   functions     using   regularized   greedy   forest .   IEEE   Transactions   on     Pattern   Analysis   and   Machine   Intelligence ,   36 ( 5 ) ,   2014 .       上面 的 目标 函数 比   RGF   模型 简单 ， 更 容易 并行处理 ？ ！ ！     传统 的 GBM 模型 没有 正则 项 ！           Gradient   Tree   Boosting ， 目标 函数 通过 顺序 加树 进行 优化 ， 在 第 t 额颗 树 ，           $ $     L ^ t   =   \ \ sum _ { i = 1 } ^   l ( y _ i ,   \ \ hat { y } _ i   +   f _ t ( x _ i ) )   +   \ \ Omega ( f _ t )     $ $       将 损失 函数 展开 到 二阶 项 ， 丢掉 常数 项后       $ $     \ \ hat { L } ^ t   =   \ \ sum _ { i = 1 } ^   [ g _ i   f _ t ( x _ i )   +   \ \ frac { 1 } { 2 }   h _ i   f _ i ^ 2 ( x _ i ) ]   +   \ \ Omega ( f _ t )     $ $       例如 ， 损失 函数 取为       $ $     l ( y _ i ,   \ \ hat { y } _ i )   =   ( y _ i   -   \ \ hat { y } _ i ) ^ 2     $ $       那么 ， 对应 的 梯度 和 二阶 导为       $ $     g _ i   =   - 2 ( y _ i   -   \ \ hat { y } _ i )   =   - 2   e _ i     \ \ \ \     h _ i   =   2     $ $       定义 样本 集合 $ ( I _ j   =   {   i   |   q ( x _ i )   =   j   } ) $ ， 即 到达 第 j 个 叶子 结点 的 样本 集合 。     那么 损失 函数 可以 改写 为 对 第 t 颗树 的 叶子 结点 求和 ， 下面 的 权值 w 也 是 指 第 t 颗树 的       $ $     \ \ hat { L } ^ t   =   \ \ sum _ { j = 1 } ^ T   [ ( \ \ sum _ { i   \ \ in   I _ j }   g _ i )   w _ j   +   \ \ frac { 1 } { 2 }   ( \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda )   w _ j ^ 2 ]   +   \ \ gamma   T     $ $       从 上式 可以 求得 在 给定 的 q 函数 下 ， 最佳 的 权值 为       $ $     w _ j ^ *   =   -   \ \ frac { \ \ sum _ { i   \ \ in   I _ j }   g _ i } { \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda }     $ $     对应 的 最优 目标 函数 为       $ $     \ \ hat { L } ^ t ( q )   =   -   \ \ frac { 1 } { 2 }   \ \ sum _ { j = 1 } ^ T   \ \ frac { ( \ \ sum _ { i   \ \ in   I _ j }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda }   +   \ \ gamma   T     $ $     这个 值 可以 作为 q 函数 的 score 来 评估 树 的 结构 ， 作用 和 CART 的 不 纯度 gini 系数 一样 。     理论 上 来说 需要 遍历 所有 可能 的 树 ， 实际上 用 启发式 的 方法 ， 从单 叶子 节点 的 树 开始 ， 然后 添加 分支 。     设 分裂 前 的 样本 集为 I ， 分裂 后 左右 子树 的 样本 集 分别 为 $ ( I _ L ,   I _ R ) $ ， 那么 分裂 带来 的 损失 函数 减少 量 为       $ $     L _ { split }   =   \ \ frac { 1 } { 2 }   (   \ \ frac { ( \ \ sum _ { i   \ \ in   I _ L }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ L }   h _ i   +   \ \ lambda }     +     \ \ frac { ( \ \ sum _ { i   \ \ in   I _ R }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ R }   h _ i   +   \ \ lambda }   -   \ \ frac { ( \ \ sum _ { i   \ \ in   I }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I }   h _ i   +   \ \ lambda } )   -   \ \ gamma     $ $     就 像 C4 . 5   和   CART   的 信息 增益 率 和 gini 系数 增加量 那样 ， 作为 该 分裂 点 的 score ， 用来 确定 分裂 点 是否 最优 。           Shrinkage   and   Column   Subsampling     这 两种 技巧 用来 防止 过 拟合           shrinkage ：     J .   Friedman .   Stochastic   gradient   boosting .   Computational     Statistics   & amp ;   Data   Analysis ,   38 ( 4 ) : 367 – 378 ,   2002 .       shrink   将 新 加入 的 权值 乘上 一个 系数 $ ( \ \ eta ) $ ， 为 后面 的 树 提供 一定 的 学习 空间 。       列 采样 来自 随机 森林 ：     L .   Breiman .   Random   forests .   Maching   Learning ,     45 ( 1 ) : 5 – 32 ,   Oct .   2001       列 采样 之前 没 在 Boosting 里面 用过 ， 据说 比行 采样 效果 要 好 。 我 的 理解 是 ， 列 采样 导致 每个 树 学习 的 多样化 ， 行 采样 也 会 有 ， 但是 会少 很多 。     另一方面 ， 也 为 算法 并行 化 提供 了 好处 。       分裂 点 寻找 算法           Basic   Exact   Greedy   Algorithm           在 每 一次 寻找 中 ， 枚举 所有 可能 的 分裂 点 ， 然后 利用 score 确定 最佳 分裂 点 。     代表 的 实现 软件 有 ： sklearn ，   R 的 GBM ，   单机版 的 XGBoost 。     算法 首先 对 特征 进行 排序 ， 然后 依次 访问 数据 ， 并 以此 数据 该维 特征 的 值 作为 分裂 点 ， 计算 score 。           近似 方法     精确 寻找 不 适用 与 分布式 数据 ， 近似 方法 通过 特征 的 分布 ， 按照 百分比 确定 一组 候选 分裂 点 ， 通过 遍历 所有 的 候选 分裂 点来 找到 最佳 分裂 点 。     两种 策略 ： 全局 策略 和 局部 策略 。 在 全局 策略 中 ， 对 每 一个 特征 确定 一个 全局 的 候选 分裂 点 集合 ， 就 不再 改变 ； 而 在 局部 策略 中 ， 每 一次 分裂     都 要 重选一次 分裂 点 。 前者 需要 较大 的 分裂 集合 ， 后者 可以 小 一点 。 论文 中 对比 了 补充 候选 集 策略 与 分裂 点 数目 对模型 的 影响 。     全局 策略 需要 更细 的 分裂 点 才能 和 局部 策略 差不多 。               什么 意思 ：               Notably ,   it   is   also   possible     to   directly   construct   approximate   histograms   of   gradient     statistics   [ 19 ]               [ 19 ]   S .   Tyree ,   K .   Weinberger ,   K .   Agrawal ,   and   J .   Paykin .     Parallel   boosted   regression   trees   for   web   search   ranking .   In     Proceedings   of   the   20th   international   conference   on   World     wide   web ,   pages   387 – 396 .   ACM ,   2011               Weighted   Quantile   Sketch   算法     对 第 k 个 特征 ， 构造 数据 集           $ $     D _ k =   { ( x _ { 1k } ,   h _ 1 )   ,   ( x _ { 2k } , h _ 2 )   ,   ... , ( x _ { nk } , h _ n )   }     $ $     其中 $ ( h _ i ) $ 是 该 数据 点 对应 的 损失 函数 的 二阶 梯度 。 二阶 梯度 在 这里 相当于 样本 的 权值 ， 目标 函数 可以 看做 一个 带权 的 均 方 误差 ( 通过 近似 ， 将 所有 凸函数 形式 的 目标 函数 都 变成 了 和 最小 均方 误差 一样 了 ) 。     重新 改写 目标 函数 为       $ $     \ \ sum _ { i = 1 } ^ n   \ \ frac { 1 } { 2 }   h _ i ( f _ t ( x _ i )   -   g _ i   /   h _ i ) ^ 2   +   \ \ Omega ( f _ t )   +   constant     $ $     定义 序 函数 为 带权 的 序 函数       $ $     r _ k ( z )   =   \ \ frac { 1 } { \ \ sum _ { ( x , h )   \ \ in   D _ k   }   h }   \ \ sum _ { ( x , h )   \ \ in   D _ k ,   x & lt ; z }   h     $ $     它 代表 第 k 个 特征 小于 z 的 样本 比例 （ 带权 的 ） 。 候选 集 的 目标 要 使得 相邻 两个 候选 分裂 点 相差 不 超过 某个 值 $ ( \ \ epsilon ) $ 。       样本 权值 相同 的 时候 ，   quantile   sketch   算法 可以 找到 这些 分裂 点 ：           M .   Greenwald   and   S .   Khanna .   Space - efficient   online     computation   of   quantile   summaries .   In   Proceedings   of   the     2001   ACM   SIGMOD   International   Conference   on     Management   of   Data ,   pages   58 – 66 ,   2001 .       Q .   Zhang   and   W .   Wang .   A   fast   algorithm   for   approximate     quantiles   in   high   speed   data   streams .   In   Proceedings   of   the     19th   International   Conference   on   Scientific   and   Statistical     Database   Management ,   2007 .           对于 带权 的 ， 目前 都 是 通过 对 随机 抽取 的 子集 进行 排序 得到 的 。 缺点 ： 没有 理论 保证 ， 也 存在 一定 错误 概率 。       作者 提出 的 一种   分布式 带权   quantile   sketch   算法 ， 有 概率 上 的 理论 保证 。 在 附录 里面 有 详细 介绍 。           Sparsity - aware   Split   Finding     为了 发现 稀疏 数据 里面 的 模式 ， 为 每 一个 树 的 节点 提供 一个 默认 的 方向 。 如果 该 特征 缺失 ， 就 以 默认 的 方向向 树 的 底部 移动 。     （ 这 不是 相当于 为 空值 人为 地 填充 了 一个 值 补全 么 ？ ） 这个 方向 是 学习 得到 的 ！ （ 好 吧 ， 怎么 学 的 ？ ）     算法 让 所有 缺失 值 的 样本 首先 全部 走到 右子 树 ， 然后 在 非 缺失 值 样本 上 迭代 ， 依次 选取 不同 分裂 点求 出 最佳 score ， 相当于 missing   value   全部 用 最大值 填充 ，     接着 右 让 缺失 值 全部 走 左子 树 ， 然后 依次 选取 不同 分裂 点求 出 最佳 score ， 相当于 missing   value   全部 用 最小值 填充 ，     经过 两次 遍历 后 ， 选出 最佳 score ， 相当于 比 传统 的 方式 多 遍历 一次 ？ ！ （ 那 为什么 速度 还 比 传统 的 快 呢 ？ ）     算法 随非 缺失 值 样本 数目 现行 增长 ， 因为 它 只 在 非 缺失 值 样本 上 迭代 。     在 Allstate - 10K   dataset 上 ， 比 naive 的 算法 快 50 倍 ！           算法 详细 ， 请 看 论文 。       系统 设计           Column   Block   for   Parallel   Learning             最 耗时 的 地方 在于 对 样本 排序 ， 为了 减少 这部分 时间 ， 将 数据 保存 在 内存 单元 block 中 。     在 block 中 ， 数据 以 compressed   column   ( CSC )   保存 ， 每 一列 按照 该列 对应 的 特征 进行 排序 。     因此 ， 这种 数据 只 需要 计算 一次 ， 就 可以 被 反复 使用 。           此外 可以 同时 对 所有 的 叶子 结点 执行   split   finding   算法 ， 寻找 最优 分裂 点 。           什么 意思 ：         We   do   the   split     finding   of   all   leaves   collectively ,   so   one   scan   over   the   block     will   collect   the   statistics   of   the   split   candidates   in   all   leaf   branches           这种 结构 对 近似 搜索算法 也 有用 ， 可以 使用 多个 block ， 每 一个 block 对应 一个 行 的 子集 ，     不同 的 block 还 可以 在 不同 的 机器 上 ， 或者 保存 在 磁盘 上 实现 out - of - core 计算 。       对 每 一列 的 统计 可以 并行 ， 这 导致 了 split   finding   的 并行算法 。     这种 结构 也 支持 列 采样 。       这个 结构 还 没 搞懂 ， 可能 需要 看 一下 代码 。 。 。 。 。 待续           Cache - aware   Access               因为 需要 访问 每行 的 梯度 统计 ， 这种 结构 导致 内存 的 不 连续 访问 ， 这会 使得 CPU   cache 命中率 降低 ，     而 降低 算法 的 运行 速度 ！ （ 靠 ！ 这 都 考虑 到 了 ， 牛 逼 ） ， 需要 合理 选择 block 的 大小 。           216   examples   per   block   balances   the     cache   property   and   parallelization .           Blocks   for   Out - of - core   Computation                 为了 使得 核外 计算 可能 ， 将 数据 分为 多个 block ， 保存 到 磁盘 。     在 计算 的 过程 中 ， 并行 地用 另外 的 线程 将 数据 从 磁盘 预取 到 内存 缓存 中 。     但是 由于 IO 通常 会 花费 更 多 时间 ， 简单 地 预取 还是 不够 ， 我们 采用 下面 两种 技巧 来 优化 ：       Block   Compression ， block 按照 列 压缩 ， 然后 在 读取 的 时候 ， 用 另外 的 线程 解压 。 对于 行 索引 ， 保存 于 block 初始 索引 的 差值 ， 16bit 整数 保存 。       Block   Sharding ，                   个人 注解       GBDT 和 XGBOOST 的 联系       从 损失 函数 来看 ， GBDT 相当于 $ ( H = 2 ,   \ \ lambda   =   0 ,   \ \ gamma   =   0 ) $   的 特殊 情形 ，     此外 陈天奇 对 XGBOOST 并行 实现 的 优化 也 很 牛 ！       算法 细节       源码 实现         dmlc             xgboost 的 spark 版本 是 对 每 一个 分区 单独 训练 ？ ？ ？ ？ 没看 懂                   partitionedData     .     mapPartitions       {                   trainingSamples       = & gt ;                       rabitEnv     .     put     (     & quot ; DMLC _ TASK _ ID & quot ;     ,       TaskContext     .     getPartitionId     ( ) .     toString     )                       Rabit     .     init     (     rabitEnv     .     asJava     )                       var       booster     :       Booster       =       null                       if       (     trainingSamples     .     hasNext     )       {                           val       cacheFileName     :       String       =       {                               if       (     useExternalMemory       & amp ; & amp ;       trainingSamples     .     hasNext     )       {                                   s & quot ;     $ appName     - dtrain _ cache -     $ {     TaskContext     .     getPartitionId     ( )     }     & quot ;                               }       else       {                                   null                               }                           }                           val       trainingSet       =       new       DMatrix     (     new       JDMatrix     (     trainingSamples     ,       cacheFileName     ) )                           booster       =       SXGBoost     .     train     (     trainingSet     ,       xgBoostConfMap     ,       round     ,                               watches       =       new       mutable     .     HashMap     [     String   ,     DMatrix     ]       {                                   put     (     & quot ; train & quot ;     ,       trainingSet     )                               } .     toMap     ,       obj     ,       eval     )                           Rabit     .     shutdown     ( )                       }       else       {                           Rabit     .     shutdown     ( )                           throw       new       XGBoostError     (     s & quot ; detect   the   empty   partition   in   training   dataset ,   partition   ID : & quot ;       +                               s & quot ;       $ {     TaskContext     .     getPartitionId     ( ) .     toString     }     & quot ;     )                       }                       Iterator     (     booster     )               } .     cache     ( )                 TODO           了解 一下 Kaggle   2015 年 TOP3 队伍 解决方案           reference             XGBoost :   A   Scalable   Tree   Boosting   System           chen   tianqi   slide           github   dmlc   xgboost           Greedy   function   approximation :   a   gradient   boosting   machine         The   present   and   the   future   of   the   kdd   cup   competition :   an   outsider ’ s   perspective .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/xgboost.html"},
      
      
      {"title": "个性化价格推荐", "text": "    Table   of   Contents           paper           基本 思想                 paper       Personalized   Pricing   Recommender   System       基本 思想           将 用户 分为 三类 ： 标准 类型   Standard ,   折扣 类型   Discount ,   不 关注 类型   Indifferent                       价格       标准       折扣       不 关注                       标准 价格       买       买       不买               折扣 价格       买       不买       不买                       每 一类 人 的 购买 行为 对应 一个 回报 ， 对 标准 和 折扣 是 支付 金额   $ ( \ \ alpha ) $ 和 $ ( \ \ beta ) $ ， 而 对于   Indifferent   类型 ， 购买 的话 会 将 产品 转卖 ， 导致 需要 的 用户 买不到 ， 存在 一定 损失 ， 这里 将 购买 的 收益 置 0 ， 不 购买 相比 购买 存在 正 的 收益   $ ( \ \ gamma   & lt ; & lt ;   \ \ alpha ) $                       响应       标准       折扣       不 关注                       买       $ ( \ \ alpha ) $       $ ( \ \ beta ) $       0               不买       0       0       $ ( \ \ gamma ) $                       观测 歧意 ：       对于 采用 标准 价格 用户 ， 其中 观测 为 支付 的 用户 为 标准 用户 ， 但是 无法 区分 折扣 用户 和 不 关注 用户 ；       如果 采用 折扣 价格 ， 则 不 支付 的 用户 是 不 关注 用户 ， 但是 无法 区分 标准 用户 和 折扣 用户 。               解决 方法 ： 多 步 分类器 。       prescreening   预赛 选 过程 ， 过滤 掉 明显 是   Indifferent   类型 的 用户 ， 减轻 类别 不 均衡 。 即 通过 预测 用户 是否 会 下单 ， 如果 下单 率 特别 低 ， 那么 就是   Indifferent   用户       standard   stage ： 区分 标准 用户 和 其他 两类 用户 。       正 样本 （ 即 标准 用户 ） ： 展示 标准 价格 并且 以 标准 价格 下单 的 用户       负 样本 ： 展示 标准 价格 但是 并 没有 下单 的 用户 ； 展示 折扣 价格 但是 并 没有 下单 的 用户               discounted   classifier ： 在 非标准 用户 中 区分 折扣 用户 和 不 关注 用户       正 样本 ： 展示 折扣 价格 下单 的 非标准 用户       负 样本 ： 展示 折扣 价格 未 下单 的 非标准 用户                              ", "tags": "machine-learning", "url": "/wiki/machine-learning/personal-price-recommender.html"},
      
      
      {"title": "优化算法", "text": "    Table   of   Contents           梯度 下降           动量 方法           Nesterov   accelerated   gradient           Adagrad           Adadelta   ( Google   Inc ,   New   York   Univ ,   Matthew   D . Zeiler )           RMSprop   ( Hinton )           Adam ,   adaptive   moment   estimation .           Additional   strategies   for   optimizing   SGD           Batch   normalization                   Reference                 梯度 下降           批量 梯度 下降       随机 梯度 下降   SGD       mini - batch   梯度 下降           mini - batch 梯度 下降 的 问题     -   学习 率 难以 选择 ， 过 小 收敛 太慢 ， 过 大会 导致 震荡     -   自动 降低 学习 率 需要 预先指定 条件 ， 不是 自 适应 的     -   算法 对 所有 的 参数 采用 相同 的 学习 率     -   难以 跳出 鞍点 ， 这是 致命 的 问题       优化 算法       动量 方法       可以 加速 相关 方向 的 收敛 和 抑制 不 相关 方向 的 震荡 。 动量 实际上 是 对 梯度 的 指数 平滑     $ $     v _ t   =   \ \ gamma   v _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ nabla _ \ \ theta   J ( \ \ theta ) .   \ \     \ \ theta   =   \ \ theta   -   \ \ eta   v _ t .     $ $       Nesterov   accelerated   gradient       采用 预测 的 点 的 梯度 ， 而 不是 当前 梯度 。     $ $     v _ t   =   \ \ gamma   v _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ nabla _ \ \ theta   J ( \ \ theta   -   \ \ gamma   v _ { t - 1 } ) .   \ \     \ \ theta   =   \ \ theta   -   \ \ eta   v _ t .     $ $       Adagrad       解决 了 两个 问题 ， 自 适应 学习 率   和   对 不同 频次 特征 采用 不同 的 学习 率 ， 适应 于 稀疏 特征   。     更新 权值 的 方程 为     $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ t   -   \ \ frac { \ \ eta } { \ \ sqrt { G _ t   +   \ \ epsilon } }   \ \ odot   g _ t .     $ $     $ ( G _ t ) $ 是 对 过去 的 梯度 的 平方 的 累积 ， 因此 学习 率 一直 在 减少 。       Adadelta   ( Google   Inc ,   New   York   Univ ,   Matthew   D . Zeiler )       改善 Adagrad 单调 递减 的 学习 率 ， 它 不是 累计 所有 的 梯度 ， 而是 设置 了 一个 固定 的 时间 窗 $ ( w ) $ .     平滑 平方 误差 $ ( E [ g ^ 2 ] _ t ) $     $ $     E [ g ^ 2 ] _ t   =   \ \ gamma   E [ g ^ 2 ] _ { t - 1 }   +   ( 1 - \ \ gamma )   g _ t ^ 2 .       $ $     权值 更新 方程 为     $ $     \ \ Delta   \ \ theta _ t   =   -   \ \ frac { \ \ eta } { \ \ sqrt { E [ g ^ 2 ] _ t   + \ \ epsilon } }   \ \ odot   g _ t .     $ $     平滑 步长 的 平方     $ $     E [ \ \ Delta   \ \ theta ^ 2 ] _ t   =   \ \ gamma   E [ \ \ Delta   \ \ theta ^ 2 ] _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ Delta   \ \ theta ^ 2 _ t .     $ $     利用 这个 重新 设计 学习 率 使得 单位 一致     $ $     \ \ Delta   \ \ theta _ t   =   -   \ \ frac { \ \ sqrt { E [ \ \ Delta   \ \ theta ^ 2 ]   +   \ \ epsilon } } { \ \ sqrt { E [ \ \ theta ^ 2 ] _ t   +   \ \ epsilon } }   \ \ odot   g _ t .     $ $       RMSprop   ( Hinton )       第一种 Adadelta 。       Adam ,   adaptive   moment   estimation .       算法     $ $     m _ t   =   \ \ beta _ 1   m _ { t - 1 }   +   ( 1 - \ \ beta _ 1 )   g _ t .   \ \     v _ t   =   \ \ beta _ 2   v _ { t - 1 }   +   ( 1 - \ \ beta _ 2 )   g _ t ^ 2 .     $ $       $ $     \ \ hat { m } _ t   =   \ \ frac { m _ t } { 1   -   \ \ beta _ 1 ^ t } .   \ \     \ \ hat { v } _ t   =   \ \ frac { v _ t } { 1   -   \ \ beta _ 2 ^ t } .     $ $       更新 方程     $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ t   -   \ \ frac { \ \ eta } { \ \ sqrt { \ \ hat { v } _ t   +   \ \ epsilon } }   \ \ hat { m } _ t .     $ $       Additional   strategies   for   optimizing   SGD       每 一次 的 循环 都 要 打散 数据 的 顺序 。 Zaremba   and   Sutskever 训练 LSTM 的 时候 ，     发现 按照 一定 顺序 反而 能 提高 性能 ？       Batch   normalization           可以 使用 更 高 的 学习 率       移除 或 使用 较 低 的 dropout       降低 L2 权重           Reference             http : / / sebastianruder . com / optimizing - gradient - descent /           https : / / arxiv . org / pdf / 1502.03167 v3 . pdf           http : / / blog . csdn . net / happynear / article / details / 44238541        ", "tags": "machine-learning", "url": "/wiki/machine-learning/optimization.html"},
      
      
      {"title": "先验知识的编码", "text": "    Table   of   Contents           先验 知识 的 编码           跟踪 自由落体           跟踪 行人                         先验 知识 的 编码       论文 ： Label - Free   Supervision   of   Neural   Networks   with   Physics   and   Domain   Knowledge       利用 无 监督 样本 学习 ， 将 先验 信息 编码 到 损失 函数 中 ！       跟踪 自由落体           模型 不是 去 拟合 每 一帧 目标 的 高度 ， 而是 要求 模型 预测 的 结果 满足 物理 定律 。       输入 是 图像 序列 x ， 输出 一个 数值 序列 y ， 即 每 一帧 目标 的 高度       对 每 一个 图像 x ， 应用 函数 f ， 得到 预测 结果 f ( x )       用 一个 a = 9.8 的 抛物线 这些 结果 序列 ， 得到 $ ( \ \ hat { y } ) $       损失 函数 用 拟合 的 结果 和 预测 结果 的 残差 表示           $ $     g ( x ,   f ( x ) )   =   \ \ sum _ { i = 1 } ^ N   | \ \ hat { y } _ i   -   f ( x )   _   i |     $ $       利用 随机 梯度 下降 优化 损失 函数 （ 可以 使用 正常 的 正则 项 ） ， 得到 最佳 $ ( f ^   *   ) $ 。           实验 结果表明 ： 这种 方法 的 测试 集上 相关性 90 % ， 监督 学习 方法 94.5 % ， 随机 猜 只有 12.1 % ， 这个 实验 证明 ， 有效 地 将 物理 规律 编码 进 损失 函数 ， 利用 无 标注 样本 就 能够 有效 地 学习 模型 ！           跟踪 行人           水平 移动 的 行人 位置 ， 没有 了 二次 项 ， 认为 是 匀速运动 。       增加 了 方差 鼓励 ， 鼓励 输出 序列 的 方差 不为 0 ， 防止 平凡 解       增加 边界 约束 ， 防止 过大 的 reward ！       发现 无 监督 的 guide   learning 比 监督 学习效果 还好 ， 因为 监督 的 样本 太 少 ， 过 拟合 了 。      ", "tags": "machine-learning", "url": "/wiki/machine-learning/prio-knowledge.html"},
      
      
      {"title": "多目标识别", "text": "    Table   of   Contents           多 目标 检测                 多 目标 检测       论文 ： MULTIPLE   OBJECT   RECOGNITION   WITH   VISUAL   ATTENTION ， Jimmy   Lei   Ba ， Volodymyr   Mnih ， Koray   Kavukcuoglu   @ DeepMind                   每 一步 ， 产生 一个   glimpse   的 坐标   $ ( l _ n ) $   和   图像 块   $ ( x _ n ) $ ， 其中 初始 的   l   由 原始 图像 下 采样 后 ， 经过   context   network   提供 的 。      ", "tags": "machine-learning", "url": "/wiki/machine-learning/multi-object-recognition.html"},
      
      
      {"title": "小样本单样本学习算法", "text": "    Table   of   Contents                   self - train   Learning       transductive   Learning       PU   Learning           co - train     tri - train       tsvm  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pulearning.html"},
      
      
      {"title": "尚未解决的问题集", "text": "    Table   of   Contents           关于           问题 列表                 关于       记录 尚未 解决 的 问题 ， 带 解决 。       问题 列表           论文 ： Natural   Language   Processing   ( almost )   from   Scratch ， page   15 ,   求和 的 递推 公式       评估 指标 ： BLEU ？       Beam - search   decoder     ✅       2014 - QA ： Dependency   tree ， 问题 ： 词 之间 的 这个 树结构 怎么 得到 的 ？       WER 词 错误率 ， CTC      ", "tags": "machine-learning", "url": "/wiki/machine-learning/question.html"},
      
      
      {"title": "强化学习", "text": "    Table   of   Contents           关于           Markov   Decision   Process           Reinforcement   Learning           model - based   Learning                   深度 强化 学习 tutorial @ ICML2016           Value - Based   Deep   RL           Q - Learning ：           DQN ： 利用   agent   自身 经验 构建 样本 ！           Double   DQN           Prioritised   replay           Duelling   network           Deep   Policy   Networks                   相关 资料                 关于       强化 学习 是 未来 很 重要 的 方向 ！ 参考   Alpha   Go ！       Markov   Decision   Process           A   set   of   states   $ ( s   \ \ in   S ) $       A   set   of   actions   $ ( a   \ \ in   A ) $       A   transition   function   $ ( T ( s ,   a ,   s ' ) ) $       转移 概率   $ ( P ( s ' |   s ,   a ) ) $       也 叫   model   或者   dynamics               A   reward   function   $ ( R ( s ,   a ,   s ' ) ) $       通常 只是 状态 的 函数 $ ( R ( s ) ,   R ( s ' ) ) $               一个 初始状态       一个 终止 状态           MDP   不是 一个 确定性 的 搜索 问题 ， 一种 方法 是     期望值 最大 搜索     ？ ！       马尔科夫 性说 的 是 未来 的 决策 与 过去 无关 ， 至于 当前 状态 和 策略 有关 ！       （ 最优 ） 策略 ： $ ( \ \ pi ^ *   :   S   \ \ rightarrow   A ) $       状态 转移 函数   $ ( T ( s ,   a ,   s ' )   =   p ( s ' |   s ,   a ) ) $       discounting   reward :   $ ( r _ 0   +   \ \ gamma   r _ 1   +   \ \ gamma ^ 2   r _ 2   +   ... ) $       当   $ ( \ \ gamma   & lt ;   1 ) $ 时 ， discounting   reward   有 界   $ ( R _ { max } / ( 1 - \ \ gamma ) ) $ ！       或者 采用   finite   horizon ， 当 迭代 到   T   步时 ， 停止 ！               MDP   quantities :           Policy   =   对 每 一个 状态 选择 一个 action       Utility   =   sum   of   ( discounted )   rewards                   状态   value   function ：   $ ( V ^ *   ( s ) ) $           q - state   ( s ,   a ) ,   $ ( Q ^ *   ( s , a ) ) $           最优 策略   $ ( \ \ pi ^ *   ( s ) ) $               递归 定义 ：           $ ( V ^   *   ( s )   =   \ \ max _ a   Q ^   *   ( s ,   a ) ) $       $ ( Q ^   *   ( s ,   a )   =   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ *   ( s ' )   \ \ right ] ) $       $ ( V ^   *   ( s )   =   \ \ max _ a   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ *   ( s ' )   \ \ right ] ) $                   Time - limited   value :   定义 $ ( V _ k ( s ) ) $   为 状态 s 下 ， 最多 k 步下 的 最优 value           Value   iteration :       $ ( V _ 0 ( s )   =   0 ) $       $ ( V _ { k + 1 } ( s )   \ \ leftarrow   \ \ max _ a   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V _ k ^ *   ( s ' )   \ \ right ] )   ) $       重复 1 - 2 直到 收敛 ！       复杂度 ， 每次 迭代   $ ( O ( S ^ 2A ) ) $       收敛 到 唯一 最优 值 ！ 贝尔曼 算子 在   $ ( \ \ gamma & lt ; 1 ) $ 时时 压缩 算子 ， 所以 必 收敛 到 不动点 。               Policy   iteration       随机化 策略 $ ( \ \ pi ) $       Policy   evaluation ： 对 给定 的 策略   $ ( \ \ pi ) $ ， 利用 迭代 或者 线性方程 求解 的 方法 计算 该 策略 下 的 值 函数 ， 即 求解 下面 第一个 方程 。 因为 该 方程 是 一个 关于 值 函数 的 线性方程 ， 所以 对于 有限 状态 的 情况 可以 直接 求解 线性方程 ， 或者 利用 迭代 求解 。       Policy   improvement ： 对 上述 值 函数 ， 利用 贝尔曼 方程 求 出 最优 策略 。 重复 2 - 3 多次 直到 收敛 ， 收敛 条件 是 策略 不 改变 了 。           实际上 ， 它 是 在 交替 迭代 策略 和 值 函数 。 这种 方法 可以 保证 每次 迭代 值 函数 单调 不减 ， 又 因为 有 界 所以 收敛 。                   $ $     V ^ { \ \ pi _ i } ( s )   =   r ( s ,   \ \ pi _ i ( s ) )   +   \ \ gamma   \ \ sum _ { s '   \ \ in   S }   p ( s ' | s ,   \ \ pi _ i ( s ) )   V ^ { \ \ pi _ i } ( s ' )   \ \ \ \     \ \ pi _ { i + 1 } ( s )   =   \ \ arg   \ \ max _ a   r ( s ,   a )   +   \ \ gamma   \ \ sum _ { s '   \ \ in   S }   p ( s ' | s ,   a )   V ^ { \ \ pi _ i } ( s ' )     $ $       Reinforcement   Learning       仍然 是 一个 MDP 过程 ， 仍然 寻找 最优 决策 ！     但是 不 知道 状态 转移 函数 T   和   回报 函数   R ！           MDP ： Offline       RL ： Online           model - based   Learning       通过 经验 ， 学习 一个 近似 模型 ， 然后 求解 ！           Step1 ： 学习 经验 MDP 模型 ：       对 每 一个个 ( s ,   a ) 统计 s '       归一化 得到 $ ( \ \ hat { T } ( s ,   a ,   s ' ) ) $     3 .                   深度 强化 学习 tutorial @ ICML2016       AI   =   RL   +   DL           值 函数       策略       状态 转移 模型           使用 深度 学习 建模 值 函数 ， 策略 和 模型 ！       Value - Based   Deep   RL       Q - Networks       $ $     Q ( s ,   a ,   w )   \ \ approx   Q ^ *   ( s ,   a )     $ $       Q - Learning ：       最优   Q - value   应该 遵循   Bellman   方程       $ $     Q ^   *   ( s ,   a )   =   \ \ mathbb { E } _ { s ' }   \ \ left [   r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ) ^   *   | s ,   a     \ \ right ]     $ $       其中   s   表示 状态 ， a   表示 agent 对 环境 做出 的   action ！     将 方程 右边 当做 目标 ， 用 神经网络 学习 ！     损失 函数 ：       $ $     l   =   ( r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w )   -   Q ( s ,   a ,   w )   ) ^ 2     $ $       问题 ： 1 .   训练样本 不是   iid ；   2 .   目标 不 稳定 ！       DQN ： 利用   agent   自身 经验 构建 样本 ！       $ $     l   =   \ \ left ( r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w ^ - )   -   Q ( s ,   a ,   w )   \ \ right ) ^ 2     $ $       在 某 一次 replay   的 更新 中 ， $ ( w ^ - ) $ 是 固定 的 ！ replay 结束 后 ， 将线 上 的 权值 $ ( w ) $ 更新 到 $ ( w ^ - ) $       Double   DQN           当前 的   Q - network   w   用来 选择   action       老 的   Q - network   w -   用来 评估   action           $ $     a ^ *   =   \ \ arg \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w )   \ \ \ \     l   =   \ \ left (   r   +   \ \ gamma   Q (   s ' ,   a ^   *   ,   w ^ -   )   -   Q ( s ,   a ,   w )     \ \ right ) ^ 2     $ $       Prioritised   replay       按照   TD - error   对   replay   memory   中 的 样本 进行   importance   sampling 。       $ $     \ \ delta   =   \ \ left |   r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w ^ - )     -   Q ( s ,   a ,   w )   \ \ right |   \ \ \ \     P ( i )   =   \ \ frac { p _ i ^ { \ \ alpha } } { \ \ sum _ k   p _ k ^ { \ \ alpha } }   \ \ \ \     p _ i   =   \ \ delta _ i   +   \ \ epsilon     $ $       Duelling   network       将   Q   函数 分解 为 状态值 函数 与   advantage   function （ 不 知道 怎么 翻译 ）   之 和 。       $ $     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   A ( s , a ;   \ \ theta ,   \ \ alpha )     $ $       上式 V 和 A 之间 是 不定 的 ， 可以 相差 一个 任意 常数 ， 不 影响 结果 。 为此 ， 有 两种 解决方案 ， 减 最大值 和 平均值 。     平均值 方案 更加 稳定 ， 因为 V 只 需要 跟踪 平均 波动 ， 而 不是 最大 波动 。       $ $     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   \ \ left (   A ( s , a ;   \ \ theta ,   \ \ alpha )   -   \ \ max _ { a ' \ \ in   \ \ mathcal { A } }   A ( s , a ' ;   \ \ theta ,   \ \ alpha )   \ \ right )     \ \ \ \     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   \ \ left (   A ( s , a ;   \ \ theta ,   \ \ alpha )   -   \ \ frac { 1 } { | \ \ mathcal { A } | }   \ \ sum _ { a ' }   A ( s , a ' ;   \ \ theta ,   \ \ alpha )   \ \ right )     $ $               Deep   Policy   Networks       用 神经网络 建模 策略 函数       $ $     a   =   \ \ pi ( a |   s ,   \ \ mathbf { u } )   =   \ \ pi (   s ,   \ \ mathbf { u } )     $ $       目标 函数 为 total   discounted   reward       $ $     J ( u )   =   \ \ mathbf { E } ( r _ 1   +   \ \ gamma   r _ 2   +   \ \ gamma ^ 2   r _ 3   +   ...   |   \ \ pi ( . ,   u ) )     $ $       令 $ ( \ \ tao   =   ( s _ 1 ,   a _ 1 ,   ... ,   s _ t ,   a _ t ) ) $ 代表 状态 - 动作 路径 ， 用 $ ( r ( \ \ tao ) ) $ 代表 每个 路径 的 折扣 reward ， 那么 期望 回报 函数       $ $     J ( \ \ theta )   =   \ \ int   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao     $ $       对 参数 $ ( \ \ theta ) $ 求导 ， 由于 回报 函数 与 参数 无关 ， 所以 梯度 只 作用 与 策略 函数       $ $     \ \ nabla _ { \ \ theta }   J ( \ \ theta )   =   \ \ int   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao   = \ \ int     \ \ pi _ { \ \ theta } ( \ \ tao )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao   =   E _ { \ \ tao   ~   \ \ pi _ { \ \ theta } ( \ \ tao ) }   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )     $ $       利用 马尔科夫 性 ，       $ $     \ \ log   \ \ pi _ { \ \ theta } ( \ \ tao )   =   \ \ log   p ( s _ 1 )   +   \ \ sum _ { t = 1 } ^ T   \ \ left [ \ \ log   \ \ pi _ { \ \ theta } ( a _ t | s _ t )   +   \ \ log   p ( s _ { t + 1 } | s _ t ,   a _ t )   \ \ right ]     $ $       代入 上式 可得       $ $     \ \ nabla _ { \ \ theta }   J ( \ \ theta )   =   E _ { \ \ tao   ~   \ \ pi _ { \ \ theta } ( \ \ tao ) }   \ \ sum _ { t = 1 } ^ T     \ \ nabla _ { \ \ theta } \ \ log   \ \ pi _ { \ \ theta } ( a _ t | s _ t )     r ( \ \ tao )     $ $       如果 把 策略 函数 看做 在 状态 s 下 选择 动作 a 的 概率 ， 回报 是 该 样本 的 权重 ！ 即 加权 极大 似然 估计 ！       相关 资料           强化 学习 书籍 ：   https : / / webdocs . cs . ualberta . ca / ~ sutton / book / ebook / the - book . html         Tutorial :   Deep   Reinforcement   Learning ，   ICML   2016 .   David   Silver ,   Google   Deepmind .       CS234 :   Reinforcement   Learning     http : / / web . stanford . edu / class / cs234 / index . html         Berkeley   课程 ： CS   294 :   Deep   Reinforcement   Learning .     http : / / rll . berkeley . edu / deeprlcourse /           http : / / ai . berkeley . edu / course _ schedule . html        ", "tags": "machine-learning", "url": "/wiki/machine-learning/reinforcement-learning.html"},
      
      
      {"title": "文本摘要相关算法汇总", "text": "    Table   of   Contents           关于           关键 短语 提取 ： review           Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information           TextRank           KeyCluster           Topical   PageRank                 关于       汇总 文本 摘要 相关 的 模型 、 算法 即 评估 指标       关键 短语 提取 ： review       论文 ： Automatic   Keyphrase   Extraction :   A   Survey   of   the   State   of   the   Art ， Kazi   Saidul   Hasan   and   Vincent   Ng               定义 ： 自动 选择 出 文档 中 的 重要 的 、 表达 主题 的 短语 ！ 确定 文档 中 最 有 表达能力 的 少量 关键词 ！           Peter   Turney .   2000 .   Learning   algorithms   for   keyphrase   extraction .   Information   Retrieval ,   2 : 303 – 336 .       Takashi   Tomokiyo   and   Matthew   Hurst .   2003 .   A   lan -   guage   model   approach   to   keyphrase   extraction .   In   Proceedings   of   the   ACL   Workshop   on   Multiword   Ex -   pressions ,   pages   33 – 40 .       Zhiyuan   Liu ,   Peng   Li ,   Yabin   Zheng ,   and   Maosong   Sun .   2009b .   Clustering   to   find   exemplar   terms   for   keyphrase   extraction .   In   Proceedings   of   the   2009   Conference   on   Empirical   Methods   in   Natural   Lan -   guage   Processing ,   pages   257 – 266 .       Zhuoye   Ding ,   Qi   Zhang ,   and   Xuanjing   Huang .   2011 .   Keyphrase   extraction   from   online   news   using   binary   integer   programming .   In   Proceedings   of   the   5th   In -   ternational   Joint   Conference   on   Natural   Language   Processing ,   pages   165 – 173 .       Xin   Zhao ,   Jing   Jiang ,   Jing   He ,   Yang   Song ,   Palakorn   Achanauparp ,   Ee - Peng   Lim ,   and   Xiaoming   Li .   2011 .   Topical   keyphrase   extraction   from   Twitter .   In   Proceedings   of   the   49th   Annual   Meeting   of   the   Association   for   Computational   Linguistics :   Human   Language   Technologies ,   pages   379 – 388 .       Zhiyuan   Liu ,   Wenyi   Huang ,   Yabin   Zheng ,   and   Maosong   Sun .   2010 .   Automatic   keyphrase   extrac -   tion   via   topic   decomposition .   In   Proceedings   of   the   2010   Conference   on   Empirical   Methods   in   Natural   Language   Processing ,   pages   366 – 376 .                   应用 ： 文档 快速 、 高精度 的 检索 ； 提升 自然语言 处理 的 其他 任务 ： 文本 摘要 ， 文本 分类 ， 观点 挖掘 ， 文档 索引 ；           Yongzheng   Zhang ,   Nur   Zincir - Heywood ,   and   Evangelos   Milios .   2004 .   World   Wide   Web   site   summariza -   tion .   Web   Intelligence   and   Agent   Systems ,   2 : 39 – 53 .       Ga   ́ bor   Berend .   2011 .   Opinion   expression   mining   by   exploiting   keyphrase   extraction .   In   Proceedings   of   the   5th   International   Joint   Conference   on   Natural   Language   Processing ,   pages   1162 – 1170 .                   影响 关键词 提取 的 几个 主要 因素 ：           长度 ： 长 文档 的 候选词 更 多       结构 一致性 ： 科技 文档 的 结构 非常 一致 ， 可以 利用 abstract 提取 关键词 ！       主题 的 变化 ： 科技 文档 的 主题 在 同一个 文档 中 基本 不变 ， 但是 对话 则 经常 随 时间 变化 ！       主题 想 关心 ： 非正式 文档 的 多个 主题 可能 并 不 相关 。                   关键词 提取 方法 ：           利用 一些 启发式 方法 提取 一个 关键词 列表       利用 监督 或者 无 监督 学习 确定 一个 关键词 是否是 正确 的 关键词                   候选词 选择 ：           启发式 的 规则 ， 减少 候选词 数目       停止 词 列表 ： 移除 停止 词       保留 特定 词性 的 词 ： 名词 、 形容词 、 动词 etc       利用 其它 信息 ： 允许 维基百科 词条 的   n - gram       保留 满足 特定 词法 模式 的   n - gram       其它 减枝 技术               监督 学习 方法 ： Task   Reformulation ， feature   design       Task   Reformulation ：       二 分类 标注 ： 给定 一个 关键词 和 文档 ， 预测 该 关键词 是否是 该 文档 的 关键词 ； 缺点 是 不能 确定 哪些 词 更 有 表达能力 ！       Peter   Turney .   1999 .   Learning   to   extract   keyphrases   from   text .   National   Research   Council   Canada ,   In -   stitute   for   Information   Technology ,   Technical   Report   ERB - 1057 .       Peter   Turney .   2000 .   Learning   algorithms   for   keyphrase   extraction .   Information   Retrieval ,   2 : 303 – 336 .       Ian   H .   Witten ,   Gordon   W .   Paynter ,   Eibe   Frank ,   Carl   Gutwin ,   and   Craig   G .   Nevill - Manning .   1999 .     KEA   :   Practical   automatic   keyphrase   extraction .   In   Pro -   ceedings   of   the   4th   ACM   Conference   on   Digital   Li -   braries ,   pages   254 – 255 .               排序 方法 ： pairwise 的 排序 方法 效果 明显 优于 二 分类 的 方法     KEA   ！       Xin   Jiang ,   Yunhua   Hu ,   and   Hang   Li .   2009 .   A   ranking   approach   to   keyphrase   extraction .   In   Proceed -   ings   of   the   32nd   International   ACM   SIGIR   Confer -   ence   on   Research   and   Development   in   Information   Retrieval ,   pages   756 – 757 .                           特征 设计 ： 1 .   文档 内 特征 ； 2 .   文档 外 特征           统计 特征 ：       tf * idf , 不 解释       第一次 出现 的 位置 距 文档 开头 的 归一化 距离 ； 通常 关键词 出现 在 文档 的 头部       短语 在 训练 集中 作为 关键词 的 次数       其他 统计 信息 ： 短语 长度 ， 短语 跨度 （ 第一次 出现 和 最后 一次 出现 的 距离 ）               结构特征 ： 短语 出现 在 科技 文档 不同 章节 的 频率 ， 出现 在 网页 metadata 的 频率 etc       句法 特征 ： 当有 其他 特征 时 ， 这 类 特征 没 啥 用       将 短语 编码 为   POS   序列 ， 例如 编码 为   动词 - 名词 ； 形容词 - 名词   etc       词法 后缀 序列 ， 貌似 只有 拉丁语系 才 有 ，   full - tion ,   less - tion   etc               维基百科 类 特征 ： 是否 作为 维基百科 词条 ？ etc       是否 作为 搜索 关键词 ？       两个 候选词 的 语义 相关性 特征 ！                   无 监督 学习 方法 ：           Graph - Based   Ranking :       一个 词是 重要 的 ： 1 ， 与 大量 其他 候选词 是 相关 的 ； 2 ， 候选词 是 重要 的 ！       词 的 关系 通过 共生 矩阵 来 描述 （ 实际上 现在 可用 过词 向量 来 描述 啦 ）       一个 文档 的 词用 一个 图来 描述 ， 图 的 节点 是 词 ， 边 的 权重 是 词 的 关系 。 一个 节点 的 score 由 他 的 邻居 的 score 决定 ！ 选出 TOP 个 节点 即可 ！       TextRank ： Rada   Mihalcea   and   Paul   Tarau .   2004 .   TextRank :   Bringing   order   into   texts .   In   Proceedings   of   the   2004   Conference   on   Empirical   Methods   in   Natural   Language   Processing ,   pages   404 – 411 .       缺点 在于 选取 的 词 无法 覆盖 文档 的 全部 主要 信息 ！               Topic - Based   Clustering       将 候选词 按 主题 聚类       KeyCluster :   利用 维基百科 和 共生 矩阵 聚类 相似 的 词 ， 对 每 一个 类 （ 主题 ） 选出 最靠近 中心 的 词 ！ 缺点 在于 并 不是 所有 的 主题 都 重要 ！ 这种 方法 给 每 一个 主题 相同 的 权重 ！       Topical   PageRank ： 利用 textrank 对 每个 主题 内 的 词 排序 ， 词 的 最终 score 是 在 各个 主题 中 的 score 的 加权 和 ， 权重 是 该 主题 在 文档 中 的 概率 ！       CommunityCluster ： 保留 重要 主题 下 的 所有 候选词 ！               Simultaneous   Learning       Language   Modeling               评估       典型 方法 ：       to   create   a   mapping   between   the   keyphrases   in   the   gold   standard   and   those   in   the   system   output   using   exact   match       score   the   output   using   evaluation   metrics   such   as   precision   ( P ) ,   recall   ( R ) ,   and   F - score   ( F ) .               BLEU ， METEOR ,   NIST ,   and   ROUGE   解决 精确 匹配 的 问题       R - precision                   Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information       论文 ： Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information ， Y .   MATSUO ， M .   Ishizuka ， 2003       基本 思想 ： 只 需要 单个 文档 （ 长 文档 ） ， 首先 提取 高频词 ， 如果 一个 词 与 高频词 的 共现 关系 通过 卡方 检验 ， 就 认为 是 可能 的 关键词 。       TFIDF ： 在 该 文档 经常出现 ， 但是 在 整个 语料 中 出现 得 不 那么 频繁 的 词 ！           automatic   term   recognition ， automatic   indexing ， automatic   keyword   extraction       本文 的 方法 只 需要 一篇 文档 ， 不 需要 语料       选出 高频词 ， 统计 高频词 出现 的 频率       统计 词 与 高频词 的 共现 矩阵               如果 一个 词 经常 与 一个 高频词 子集 共现 ， 那么 这个 词 就 有 高 的 概率 是 关键词 ！ 这种 偏差 用卡方 统计 量 来 度量       如果 一个 词 w 跟 高频词 的 任何 子集 都 没有 特殊 的 共现 关系 ， 那么 共现 矩阵 中 w 的 分布 期望值 应该 就是 高频词 本身 的 分布 。     反之 ， 则 实际 分布 与 这种 期望 分布 存在 较大 偏差 ， 可以 用卡方 统计 量 度量 这种 偏差 。           $ $     \ \ chi ^ 2 ( w )   =   \ \ sum _ { g   \ \ in   G }   \ \ frac { ( freq ( w ,   g )   -   n _ w   p _ g ) ^ 2 } { n _ w   p _ g }     $ $       这里 w 是 某个 待 检验 的 词 ， $ ( g   \ \ in   G ) $   是 高频词 ， G 是 高频词 组成 的 集合 。 $ ( n _ w ) $ 是 w 在 共现 矩阵 中 出现 的 总数 ，     $ ( p _ g ) $ 是 高频词 g 在 高频词 中 的 归一化 频率 。           优化 ：       针对 长短句 不同 带来 的 共现 偏差 ， 重新 定义 $ ( p _ g ) $ 为   g 出现 的 句子 中词 的 数目 / 文档 的 总词 数目 ;   $ ( n _ w ) $ 定义 为 w 出现 过 的 句子 中 的 总词 数目 ！       增加 鲁棒性 ， 防止 某个 词 只 跟 某 一个 特定 的 高频词 高度 共现 ， 方法 是 减去 这个 高度 共现 的 部分 ， 即 最大值 。                   $ $     \ \ chi ' ^ 2 ( w )   =   \ \ chi ^ 2 ( w )   -   \ \ max _ { g   \ \ in   G }   \ \ frac { ( freq ( w ,   g )   -   n _ w   p _ g ) ^ 2 } { n _ w   p _ g }     $ $           词聚类 ： 将 高频词 相似 的 词聚类 ！ 词 的 相似 度 基于     Jensen - Shannon   divergence     度量 ！ 然后 采用 pairwise 聚类 ， 利用 交互 信息量 度量 。           TextRank       论文 ： TextRank :   Bringing   Order   into   Texts ， Rada   Mihalcea   and   Paul   Tarau ， 2004       带权   PageRank       $ $     WS ( V _ i )   =   ( 1 - d )   +   d   \ \ sum _ { V _ j   \ \ in   IN ( V _ i ) }   \ \ frac { w _ { ji } } { \ \ sum _ { V _ k   \ \ in   OUT ( V _ j ) }   w _ { jk } }   WS ( V _ j )     $ $       WS   是 定点 的   PageRank   score 。 随机 初始化 ， 然后 迭代 收敛 ！           将 文本 作为 一个 图 ： 每 一个 词 做 顶点 ， 如果 两个 词 出现 在 同一个 上下文 窗 （ 大小 认为 设定 ， 试验 中窗 大小 为 2 比较 好 ） ， 那么 就 有 一条 边 。 只 使用 形容词 和 名词 ！       多个 词 通过 后处理 得到 ， 例如 两个 词 A ， B 都 在 TOPN 中 ， 并且 这 两个 词 相邻 ， 那么 AB 就是 一个 新 的 关键词       关键词 评估 指标 ， P ， R ， F1       句子 的 相似性 通过 公共 词 数目 定义 ， 评估 指标   ROUGE           $ $     similarity ( S _ i ,   S _ j )   =   \ \ frac { | \ \ { w _ k |   w _ k   \ \ in   S _ i   ,   w _ k   \ \ in   S _ j   \ \ } | } { \ \ log { | S _ i | }   +   \ \ log { | S _ j | } }     $ $       KeyCluster       论文 ： Clustering   to   Find   Exemplar   Terms   for   Keyphrase   Extraction ， Zhiyuan   Liu ,   Wenyi   Huang ,   Yabin   Zheng   and   Maosong   Sun           关键词 的 几个 目标 ： 可 解释性 ； 相关性 ； 对 主题 的 覆盖率       无 监督 聚类 方法 ：       首先 将 文档 的   term   按照 语义 聚类 ， 每 一类 用 一个 代表   term   表达 ， 每 一个 类 的 中心   term               算法 流程 ：       候选词 选择 ： 过滤 停止 词 等 无 意义 词       计算   term   间 的 语义 相关性 度量 ： Wikipedia 的 tfidf ， pmi ， ngd       基于 相关性 将 term 聚类 ： Hierarchical   Clustering ， Spectral   Clustering ， Affinity   Propagation       使用 每个 类 的 代表 词 提取 关键词 ： 选出   代表 词   的 组合 短语 作为 关键词                   Topical   PageRank       论文 ： Automatic   Keyphrase   Extraction   via   Topic   Decomposition       基本 思想 ， pagerank 的 时候 ， 只 关注 某 一个 主题 ， 求 出 每个 term 在 该 主题 先 的 rank 后 ， 然后 按照 文档 的 主题 分布 加权 得到 最终 的 rank 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/text-sum.html"},
      
      
      {"title": "智能运营相关资料整理", "text": "    Table   of   Contents           关于           论文           案例                 关于       智能 运营 是 指用 人工智能 相关 算法 优化 在 电子商务 和 传统 公司 运营 相关 的 业务 指标 ， 在 当下 的 运营 中 ， 主流 还是 以 人工 策略 为主 ， 相信 在 未来 一定 会 成为 技术 主导 的 事情 ， 这里 总结 一下 相关 论文 和 案例 。       论文           价格 敏感度 建模 ： Modeling   Consumer   Preferences   and   Price   Sensitivities   from   Large - Scale   Grocery   Shopping   Transaction   Logs           案例  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-fo-operator.html"},
      
      
      {"title": "最大熵模型", "text": "    Table   of   Contents           PPT   记录           最大 熵 模型   -   多元 逻辑 回归           最大 熵 模型           reference                 PPT   记录           conditional   or   discriminative   probabilistic   models   in   NLP   ,   IR ,   and   Speech       联合 概率   or   条件 概率 ： 生成 模型   or   判别 模型           最大 熵 模型   -   多元 逻辑 回归       可以 证明 ， 多元 逻辑 回归 是 在 存在 约束 的 条件 下 ， 最大化 输出 的 分布   $ ( \ \ hat { y }   =   p ( y | x ) ) $   的 熵 。       LogisticRegressionMaxEnt . pdf   。       对于 链接 中 的 证明 ， 有个 问题 是 ， 那个 关键 的 约束 关系 是 通过 逻辑 回归 的 式子 ， 利用 极大 似然 导出 的 ，     后面 又 用来 证明 从 最大 熵 原理 推导 出 具体 表达式 ， 这 不是 循环论证 么 ？ ！       结论 ： 最大 熵 模型 就是 多元 逻辑 回归 ， 他 也 是 建模 条件 概率 ， 而 不是 后验 概率 或者 联合 概率 ， 是 判别 模型 ；     通常 我们 说 建模 后验 概率 （ 贝叶斯 的 方法 ） 实际上 就是 建模 联合 概率 ， 是 生成 模型 。       最大 熵 模型       给定 数据 集   $ ( ( x ,   y ) ) $ ， 其 经验 分布 为 $ ( \ \ tilde { p } ( x ,   y ) ) $ 。 $ ( f ( x ,   y ) ) $   是 样本空间 到 实数 集合 的 映射 ， 成为 特征 ！     目标 是 学习 得到 条件 分布 $ ( p ( y | x ) ) $ ！ 为此 ， 可以 让 该 条件 分布 满足 最大 熵 原理 即       $ $     \ \ max   H ( p )   =   -   \ \ sum _ { x , y }   \ \ tilde { p } ( x )   p ( y | x )   \ \ log   p ( y | x )   \ \ \ \     s . t .   p ( f _ i )   =   \ \ tilde { p } ( f _ i ) ,   i = 1 , ... , n     \ \ \ \     \ \ sum _ { y }   p ( y | x )   =   1           \ \ \ \     $ $     这里 ：     $ $     p ( f _ i )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x ) p ( y | x )   f _ i ( x , y )   \ \ \ \     \ \ tilde { p } ( f _ i )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x ,   y )   f _ i ( x , y )     $ $       上面 $ ( p ( f _ i ) ,   \ \ tilde { p } ( f _ i ) ) $   分别 表示 特征 $ ( f _ i ) $ 在 样本 中 的 期望值 和 在 条件 分布 $ ( p ) $ 下 的 期望值 ，     约束条件 要求 这 两者 相同 。 如果 没有 这个 约束 ， 那么 最大 熵 的 分布 就是 均匀分布 （ 闭 区间 ） 、 指数分布 （ 半 无限 区间 ， 均值 约束 ） 和 正态分布 （ 无限 区间 ， 均值 和 方差 约束 ） 了 ！       利用 拉格朗 日 对偶原理 ， 可 得 拉格朗 日 函数 ：       $ $     L ( p ,   \ \ lambda ,   \ \ gamma )   =   -   \ \ sum _ { x , y }   \ \ tilde { p } ( x )   p ( y | x )   \ \ log   p ( y | x )   +   \ \ sum _ i   \ \ lambda _ i   ( p ( f _ i )   -   \ \ tilde { p } ( f _ i ) )   + \ \ gamma   ( \ \ sum _ { y }   p ( y | x )   -   1 )     $ $       由   KKT   条件 ， $ ( \ \ partial   L   /   \ \ partial   p ( y | x )   =   0 ,   \ \ partial   L   /   \ \ partial   \ \ gamma   =   0 ) $ 可 得 ：       $ $     p ^ *   ( y | x )   =   \ \ frac { 1 } { Z _ { \ \ lambda } ( x ) }   \ \ exp ( \ \ sum _ i   \ \ lambda _ i   f _ i ( x , y ) )     \ \ \ \     Z _ { \ \ lambda } ( x )   =   \ \ sum _ y   \ \ exp ( \ \ sum _ i   \ \ lambda _ i   f _ i ( x , y ) )     $ $       而 其 对偶 问题 优化 的 对象 变为       $ $     \ \ Phi ( \ \ lambda )   =   - \ \ sum _ x   \ \ tilde { p } ( x ) \ \ log   Z _ { \ \ lambda } ( x )   +   \ \ sum _ i   \ \ lambda _ i   \ \ tilde { p } ( f _ i )     $ $       这个 目标 函数 可以 看做 似然 对数 ！ 其中 条件 概率 采取 上述 指数 模型 建模 ！ （ 和 逻辑 回归 模型 一致 ）       $ $     L _ { \ \ tilde { p } } ( p )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x , y )   \ \ log ( y | x )     $ $       和 逻辑 回归 的 关系 ： 取 特征 映射 为 $ ( f _ i ( x ,   y )   =   f _ i ( x ) ) $ 即 样本 的 第 $ ( i ) $ 个 特征 分量 ！       reference           stanford   NLP   ppt     http : / / nlp . stanford . edu / pubs / maxent - tutorial - slides - 6 . pdf        ", "tags": "machine-learning", "url": "/wiki/machine-learning/maxent.html"},
      
      
      {"title": "机器学习 - 周志华", "text": "    Table   of   Contents           关于           绪论           线性 模型           类别 不 平衡 问题           集成 学习           集成 学习 基本 思想           Hoeffding   不等式 的 推导           Boosting                         关于       南 大 周志华 老师 写得 机器 学习 是 国内 少有 的 中文 机器 学习 教材 ， 很多 人 推荐 ，     所以 看看 。       绪论           NFL 定理           线性 模型           最大 熵 模型 与 多元 逻辑 回归 的 关系 ？ 没 啥 区别                     https : / / www . quora . com / What - is - the - relationship - between - Log - Linear - model - MaxEnt - model - and - Logistic - Regression           https : / / stackoverflow . com / questions / 21241602 / maximum - entropy - model - and - logistic - regression                     类别 不 平衡 问题       三种 策略 ： （ 假设 负 样本 比正 样本 多 很多 ， 实际 遇到 的 问题 基本上 都 是 这种 问题 ）           -   对负 样本 欠 采样 ， 简单 的 欠 采样 可能 会 丢失 关键 信息 ， 代表性 算法   EasyEnsemble   [ Liu   ,   2009 ]   利用 集成 学习 的 机制 ， 将 反例 划分 为 若干个 集合 ， 供 不同 的 学习 使用 ， 这样 在 每 一个 学习 奇 看来 ， 都 是 欠 采样 ， 但 全局 来看 却 不会 丢失 信息 。     -   对 正 样本 过 采样 ， 简单 的 重复 样本 会 导致 严重 的 过 拟合 ， 代表性 算法   SMOTE   [ Chawla ,   2002 ]   通过 对 训练 集正 样本 进行 插值 来 产生 额外 的 正例 。     -   resale ， 直接 基于 原始 训练 机 进行 学习 ， 在 预测 的 时候 采用 阈值 移动 的 策略 ， 将 正负 样本 的 比例 因素 考虑 进去 。       集成 学习       集成 学习 基本 思想       利用 很多 个 独立 的 （ 或者 不同 的 ） 弱 分类器 ， 进行 投票 得到 一个 强 的 分类器 。 该 理论 可以 由 下面 的 推导 得到       设 每个 弱 分类器 的 性能 为 $ ( \ \ epsilon ) $ ， 即     $ $     P ( h _ i ( x )   \ \ neq   f ( x ) )   =   \ \ epsilon     $ $     这里 $ ( h _ i ) $ 是 弱 分类器 的 判决 函数 ， $ ( f ) $ 是 要 学习 的 未知 函数 。     那么 $ ( T ) $ 个 这样 的 分离器 采用 简单 投票 策略 的 分类器 $ ( H ) $ 判决 错误 ， 要求 一半 以上 的 判错 ， 概率 为     $ $     P ( H ( x )   \ \ neq   f ( x ) )   =   \ \ sum _ { k = 0 } ^ { T / 2 }   \ \ binom { T } { k }   ( 1 - \ \ epsilon ) ^ k   \ \ epsilon ^ { T - k }     \ \ \ \                                         \ \ le   \ \ exp ( - \ \ frac { 1 } { 2 } T ( 1 - \ \ epsilon ) ^ 2 )     $ $     后面 这个 式子 基于   Hoeffding   不等式         可以 看出 ， 只要 T 充分 大 ， 就 可以 使得 误差 足够 小 ！ ！     个体 学习 器 的 准确性 和 多样性 确实 一个 矛盾体 ， 准确性 高 了 之后 ， 要 增加 多样性 就 会 牺牲 准确性 。       两种 类型 ：     -   个体 学习 期 之前 强 依赖 ， 需要 串行 ， Boosting     -   不 强 依赖 ， 可 并行 ， Bagging   和   随机 森林       Hoeffding   不等式 的 推导       Hoeffding 不等式 是 ， 如果 随机变量 $ ( X _ i ) $ 独立 ， 切 都 在 0 到 1 之间 ， 那么 有     $ $     P ( \ \ bar { X }   -   E   \ \ bar { X }   & gt ;   t )   \ \ le   e ^ { - 2   n   t ^ 2 }   \ \     P ( \ \ bar { X }   -   E   \ \ bar { X }   & lt ;   - t )   \ \ le   e ^ { - 2   n   t ^ 2 }     $ $     $ ( \ \ bar { X } ) $   是 这 n 个 随机变量 的 均值 。         Hoeffding   引理   ： 随机变量 的 指数函数 的 期望       如果 随机变量 $ ( X ) $ 满足 均值 为 0 ， 且 $ ( a   \ \ le   X   \ \ le   b ) ， 那么 其 指数函数 的 期望 的 上界 为     $ $     E   e ^ { \ \ lambda   X }   \ \ le   \ \ exp ( \ \ frac { 1 } { 8 }   \ \ lambda ^ 2 ( b - a ) ^ 2 )     $ $     该 引理 证明 可以 参考 维基百科 ， 它 的 基本思路 是 将 闭 区间 上 的 指数函数 扩大 为 链接 两端 点 的 线性 函数 ，     然后 对 两边 求 期望 ， 这样 讲 期望 去掉 了 。 剩下 的 就是 求 去掉 期望 后 的 函数 的 上界 （ 指数 界 ） 的 问题 了 。         Markov   不等式   ： 随机变量 概率 和 期望 的 不等式       随机变量 $ ( X ) $ 大于 0 ， 那么 对 正数 $ ( a ) $ 有     $ $     P ( X   \ \ ge   a )   \ \ le   E ( X ) / a     $ $     证明 很 简单 ， 将 概率 转换 为 示性 函数 的 期望 表示 即可 。 因为     $ $     I ( X   \ \ ge   a )   \ \ le   X   /   a     $ $     两边 求 期望 就是 了 。       利用 这个 引理 和   Markov   不等式 可以 证明 前面 的 不等式 ，     $ $     P ( \ \ bar { X }   -   E   \ \ bar { X }   & gt ;   t )   =   P ( e ^ { s ( \ \ bar { X }   -   E   \ \ bar { X } ) }   & gt ;   e ^ { st } )   \ \ \ \                             \ \ le   e ^ { - st }   E ( e ^ { s ( \ \ bar { X }   -   E   \ \ bar { X } ) } )       \ \ \ \                             \ \ le   e ^ { - st }   e { \ \ frac { 1 } { 8   n }   s ^ 2   }     $ $     注意 最后 一个 不等式 是 将 前 一个 式子 展开 成 n 个 乘积 后 ， 再 缩放 的 。     因为 上式 对 所有 的 s 都 成立 ， 所以 可以 取 一个 最小 的 作为 它 的 上界 ，     利用 二次 函数 的 性质 可得 上界 为 Hoeffding   不等式 的 右边 。       利用 Hoeffding   不等式 ， 令 其中 的 独立 随机变量 为 $ ( y _ i   =   I ( h _ i ( x )   =   f ( x ) ) ) $ ， 即 每个 分类器     是否 判决 正确 。 设 $ ( \ \ bar { y } } ) $ 是 这 T 个 变量 的 均值 ， 那么 有     $ $     P ( H ( x )   \ \ neq   f ( x ) )   =   P ( \ \ bar ( y )   & lt ;   0.5 )   =   P ( \ \ bar { y }   -   E   \ \ bar { y }   & lt ;   0.5   -   \ \ epsilon )   \ \ \ \             \ \ le   \ \ exp ( - 2   T   ( 0.5 - \ \ epsilon ) ^ 2 )     $ $     这 就是 前面 集成 学习 基本 理论 里面 的 那个 不等式 。       Boosting  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-zzh.html"},
      
      
      {"title": "机器学习中的基本概念", "text": "    Table   of   Contents           关于           基础 统计 分布           几种 常见 的 分布           正态分布 的 和           卡方 分布           student - t   分布           F 分布           卡方 分布 、 t 分布 、 F 分布 的 联系           参考                   统计 检验           方差 统计           卡方 统计           F   classif           F   regression                   特征 变换           特征 Hash                         关于       记录 机器 学习 中 的 基本 问题 和 概念 。       基础 统计 分布       几种 常见 的 分布       包括 正态分布 、 泊松 分布 、 指数分布 等 。 略 ， 后面 可能 会 写 。       正态分布 的 和       两个 服从 正太 分布 的 随机变量   $ ( X _ 1 ,   X _ 2 ) $ ，   只要 其 联合 分布 为 正态分布 ， 那么 和 也 为 正态分布 。       卡方 分布       设 $ ( X _ 1 , ... , X _ n ) $   iid ， 服从 标准 正态分布 ， 那么 平方和 $ ( X _ 1 ^ 2 +...+ X _ n ^ 2 ) $ 服从 自由度 为 n 的 卡方 分布 。       卡方 分布 的 和 ：   $ ( X _ 1 , X _ 2 ) $ 独立 ， $ ( X _ 1   \ \ sim   \ \ chi _ m ^ 2 ,   X _ 2   \ \ sim   \ \ chi _ n ^ 2 ) $ ， 那么   $ ( X _ 1 + X _ 2   \ \ sim   \ \ chi _ { n + m } ^ 2 ) $       student - t   分布       $ ( X _ 1 , X _ 2 ) $ 独立 ， 且 $ ( X _ 1 \ \ sim \ \ chi _ n ^ 2 ,   X _ 2   \ \ sim   N ( 0 , 1 ) ) $ ， 那么 $ ( X _ 2   /   \ \ sqrt { X _ 1 / n } ) $ 服从 自由度 为 n 的 t 分布 。     一个 例子 ， 从 正态分布 总体 采样 的 n + 1 个 样本均值 对 样本 标准差 归一化 后 的 值 服从 自由度 为 n 的 t 分布 。       F 分布       $ ( X _ 1 \ \ sim \ \ chi _ n ^ 2 ,   X _ 2 \ \ sim \ \ chi _ m ^ 2 ) $ 且 统计 独立 ， 那么 $ ( m ^ { - 1 }   X _ 2   /   ( n ^ { - 1 }   X _ 1 ) ) $ 服从 自由度 为 $ ( ( m , n ) ) $ 的 F 分布       由于 F 分布 是 两个 卡方 分布 的 比值 ， 而卡方 分布 是 正态分布 的 平方和 ， 常 出现 在 方差 之中 ， 所以 F 分布 在 方差分析 之中 被 大量 使用 。       卡方 分布 、 t 分布 、 F 分布 的 联系       设   $ ( X _ 1 , ... , X _ n , Y _ 1 , ... , Y _ m ) $ 独立 同 分布 ( iid ) ， 服从 标准 正态分布 ， 记   $ ( \ \ bar { X }   =   ( X _ 1 +...+ X _ n ) / n ) $ ，     $ ( S ^ 2 = \ \ sum _ i   ( X _ i   -   \ \ bar { X } ) ^ 2 / ( n - 1 ) ) $ ， 则 ：                         $ ( ( n - 1 ) S ^ 2 ) $ 服从 自由度 为 n - 1 的 卡方 分布       $ ( \ \ sqrt { n } \ \ bar { X } / S ) $ 服从 自由度 为 n - 1 的 t 分布       $ ( [ S _ Y ^ 2 / ( m - 1 ) ] / [ S _ X ^ 2 / ( n - 1 ) ] ) $ 服从 自由度 为 ( m - 1 , n - 1 ) 的 F 分布           参考           陈希孺 ， 概率论 与 数理统计 ， 中国 科学技术 大学 出版社         Cochran   theaream             统计 检验       方差 统计       去掉 方差 太小 的 特征 。       卡方 统计       卡方 检验 ， 也 称 独立性 检验 ， 拟合 优度 检验 。 使用 要求 ， 自变量 为 正值 。       sklearn   中 用来 检验 正值 特征 与 目标 是否 独立 ， 从而 进行 特征选择 。       例如 变量   X   为 性别 （ 男 0 ， 女 1 ） ， 变量   Y   为 是否 为 左撇子 （ 否   0 ,   是   1 )   。 对于 某个 样本 ，     有列 联表 ：                         男       女       总计                       否       43       44       87               是       9       4       13               总计       52       48       100                   从 数据 中 ， 可以 看到 几个 边缘 分布 ：       $ $     P ( 男 )   =   0.52 ,   P ( 女 )   =   0.48   \ \ \ \     P ( 否 )   =   0.87 ,   P ( 是 )   =   0.13     $ $       如果 两个 变量 是 独立 的 ， 那么 列联表 里面 的 分布 应该 由式       $ $     P ( X ,   Y )   =   P ( X )   P ( Y )     $ $       得到 ， 我们 将 这个 值 作为 期望值 ， 记 作 $ ( E _ { i , j } ) $ ， 而 将 实际 值 记作 $ ( O _ { i , j } ) $ ， 例如 ， 男性 不是 左撇子 的 期望值 为     $ ( E _ { 1 , 1 }   =   100   *   0.52 * 0.87   =   45 ) $ ， 而 观测 值为 $ ( O _ { 1 , 1 }   =   43 ) $ 。       利用 上述 符号 ， 定义 统计 量               $ $     \ \ chi ^ 2   =   \ \ sum _ { i = 1 } ^ r   \ \ sum _ { j = 1 } ^ c   \ \ frac { ( O _ { i , j }   -   E _ { i , j } ) ^ 2 } { E _ { i , j } }     $ $       则 它 近似 服从 自由度 为 $ ( ( r - 1 ) ( c - 1 ) ) $ 的 卡方 分布 （ 理论 呢 ？ ） 。 该 统计 量 越 小 ， 说明 越 符合 独立 分布 ， 因此 ， 变量 间 越 独立 。       如果 其中 一个 是 连续 值 ，   sklearn   中是 将 连续 值 求和 ， 然后 用类 的 分布 概率 乘以 该值 作为 期望值 ， 而 实际 不同 类 求和 的 值 作为 观测 值 ， 然后 求卡方值 。               ##   Y 是 类别 的 one - hot 编码 ， X 是 特征       observed       =       safe _ sparse _ dot     (     Y     .     T     ,       X     )                         #   n _ classes   *   n _ features       feature _ count       =       X     .     sum     (     axis     =     0     )     .     reshape     (     1     ,       -     1     )       class _ prob       =       Y     .     mean     (     axis     =     0     )     .     reshape     (     1     ,       -     1     )       expected       =       np     .     dot     (     class _ prob     .     T     ,       feature _ count     )                 F   classif       ANOVA   F - value       连续变量 与 类别 变量 之间 的 独立性 检验 ， 只 要求 为 正态分布 。       统计 检验 量 是 样本 在 各个 分组 之间 的 差异 ( between - group   mean   square   value ) 与 组内 的 差异 之 和 ( within - group   mean   of   squares ) 的 比值 。     组间 差异 是 指 每个 样本 用 它 所在 组 的 均值 替换 ， 然后 汇总 每个 样本 与 样本均值 的 差 的 平方 ， 最后 除以 自由度 。     而 组内 差异 是 指 直接 计算 每个 样本 与 该组 样本均值 的 差 的 平方 ， 最后 除以 自由度 。       如果 自变量 对 因变量 没有 显著 影响 ， 那么 这个 比值 应该 接近 于 1 ， 反之 将 远大于 1 .       $ $     MSB   =   \ \ frac { 1 } { K - 1 } \ \ sum _ { i , j }   ( \ \ bar { Y _ i }   -   \ \ bar { Y } ) ^ 2     \ \ \ \     MSW   =   \ \ frac { 1 } { K ( N - 1 ) } \ \ sum _ { i , j }   ( Y _ { i , j }   -   \ \ bar { Y _ i } ) ^ 2   \ \ \ \     F   =   MSB   /   MSW     $ $       这里 $ ( K , N ) $ 分别 是 分组 数目 （ 或者 分类 类别 数目 ） 和 每组 样本 数目 ， 分母 其实 是 自由度 。     最后 得到 的 统计 检验 量 服从 自由度 为 ( K - 1 , K ( N - 1 ) ) 的 F 分布 。       参考   wikipedia         F   regression       Univariate   linear   regression   tests       构建 很多 个单 因素 的 线性 回归 检验 .           SST   总 的 平方差 之 和   $ ( \ \ sum   ( Y   -   \ \ bar { Y } ) ^ 2 ) $       SSM   模型 的 平方差 之 和   $ ( \ \ sum   ( Y _ { pred }   -   \ \ bar { Y } ) ^ 2 ) $       SSR   残余 的 平方差 之 和   SST   -   SSM .       MSM   模型 均 方差   SSM   /   SSM 的 自由度       MSR   残余 均 方差   SSR   /   SSR 的 自由度           $ $     R ^ 2   =   SSM   /   SST     \ \ \ \     F   =     MSM   /   MSR     $ $       如果 自变量 与 因变量 没有 明显 的 关系 ， 那么 F 值 应该 很小 。       参考     http : / / homepages . inf . ed . ac . uk / bwebb / statistics / Univariate _ Regression . pdf         特征 变换       特征 Hash       一种 快速 的 空间 效率高 的 向 量化 特征 的 方法 。 Yahoo   研究院 的 Weinberger 与 ICML2009 提出 。       Feature   Hashing   for   Large   Scale   Multitask   Learning         Hash   算法 是 一种 有效 的 特征 降维 的 方法 ， 非 参数估计 。 可以 用来 做   multitask   learning   with   hundreds   of     thousands   of   tasks ！ ！ ！       核 方法   kernel   trick ： $ ( x _ 1 , ... , x _ n   \ \ in   X ) $ ， 半 正定 核       $ $     k ( x _ 1 ,   x _ 2 )   =   ( \ \ phi ( x _ 1 ) ,   \ \ phi ( x _ 2 ) )     $ $       这种 方法 可以 将 原 空间 非线性 决策 边界 变成 变换 后 空间 的 线性 可分 的 界 。 （ SVM ）     相反 的 问题 是 ， 如果 问题 在 原 空间 是 现行 可分 的 （ 通常 是 通过 人工 的 非线性 特征 工程 实现 ） ，     但是 特征 的 维度 很 高 。 作者 提出 一种 和   kernel   trick   互补 的 方法   hash   trick ，     将 高 维空间 $ ( R ^ d ) $ 特征 映射 到 低 维空间 $ ( R ^ m ) $ ， $ ( \ \ phi ( x )   \ \ to   R ^ m ) $ （ Langford   et   al . ,   2007 ;   Shi   et   al . ,   2009 ） 。     并且 有 $ ( m   & lt ; & lt ;   d ) $ 。     不同于     随机 投影 ？     ，   hash   trick   解决 了 稀疏 性 ， 也 不 需要 额外 的 存储空间 存储 核 矩阵 。       hash   内积       hash   trick   在   multi - task   learning   场景 下 很 有用 ， 这里 原始 特征 是 数据 集 的 交叉 积   cross   product 。     每 一个 任务   U   可以 采用 不同 的 hash   函数 $ ( \ \ phi _ 1 ( x ) , ... , \ \ phi _ { | U | } ( x ) ) $ ， 另外 共享 一个 通用 的 hash 函数 $ ( \ \ phi _ 0 ( x ) ) $ 。       以 垃圾邮件 分类 为例 ， 每 一个 用户 需要 有 他 自己 的 个性化 偏好 ， 这里 的 任务 集   U   是 针对 所有 用户 （ 对于   Yahoo   mail   和   Gmail 数目 很大 ） 。     特征 空间 是 大量 的 语言 的 词汇 集合 。       论文 的 主要 贡献 ：           专门 实现 的 任意 内积 hash 函数 ， 可以 应用 到 许多   kernel   method       指数 界 解释       独立 hash 子 空间 使得 大规模   multi - task   learning   空间 使用 很小       实际 的 协同 垃圾邮件 过滤 系统           Hash   函数 ：       hash   函数   $ ( h :   \ \ mathbb { N }   \ \ to   \ \ { 1 , ... , m \ \ } ) $ ， 另 一个 hash 函数 $ ( \ \ xi   :   \ \ mathbb { N }   \ \ to   \ \ { \ \ pm   1 \ \ } ) $ .     对于 向量   $ ( x , x '   \ \ in   l _ 2 ) $ ， 定义 hash   特征 映射       $ $     \ \ phi _ i ^ { ( h ,   \ \ xi ) }   =   \ \ sum _ { j :   h ( j ) = i }   \ \ xi ( j )   x _ j   \ \ \ \     \ \ langle   x ,   x '   \ \ rangle _ { \ \ phi }   =   \ \ langle     \ \ phi ^ { ( h ,   \ \ xi ) } ( x ) ,   \ \ phi ^ { ( h ,   \ \ xi ) } ( x ' )   \ \ rangle     $ $       这里 的 hash 函数 定义 在 自然数 集上 ， 实际上 对 字符串 也 成立 ， 因为 有限 长 字符串 都 可以 表示 为 一个 自然数 。       个人 理解 ： 举例 ， 假设 h 是 一个 求 余函数 ， 那么 特征 映射 后 的 低 维空间 的 每 一个 分量 相当于 将 向量 等 间隔 的 分量 交替 求和 ， 从而 实现 降维 。       引理 ：   hash   kernel   是 无偏 的 ， 如果   $ ( E _ { \ \ phi }   [ \ \ langle   x ,   x '   \ \ rangle _ { \ \ phi } ]   =   \ \ langle   x ,   x '   \ \ rangle ) $ 。     并且 方差   $ ( \ \ sigma _ { x , x ' } ^ 2   =   \ \ frac { 1 } { m }   \ \ sum _ { i \ \ neq   j }   x _ i ^ 2   x _ j ^ { ' 2 }   +   x _ i   x _ i '   x _ j   x _ j ' ) $ ， 如果 原始 向量     都 是 标准化 的 ， 即 二 范数 为 1 ， 那么 方差 为 $ ( O ( { \ \ frac { 1 } { m } } ) ) $ 。       略去 若干 理论 和 证明 ， 后面 有 需要 再 来看 。       Multiple   Hashing       近似 正交 性 ： 对于 multi - task   learning ， 需要 学习 不同 的 权值 ， 当 映射 到 同一个 空间 时 ， 需要 参数 空间 交叉 部分 尽可能少 。     设 $ ( w   \ \ in   R ^ m ) $ 是 $ (   U   \ \   { u } ) $ 中 的 某个 任务 的 参数 向量 ， 对 任意 特征 $ ( x   \ \ in   u ) $ ， $ ( w ) $   和   $ ( \ \ phi _ u ( x ) ) $ 的 内积 很小 。       应用 ： 减少 存储 ， 避免 矩阵 向量 乘法 运算   Locality   Sensitive   Hashing   ？       个性化 ： 每 一个 task 更新 局部 ( local ) 权值 和 全局 ( global ) 权值 。 正交 性 让 我们 可以 将 这些 特征 hash 到 同一个 空间 ， 而 没有 太多 的 重叠 部分 。       邮件 过滤 问题 ： 每个 用户 有 一些 标记 数据 ， 但是 很少 ， 如果 对 每 一个 用户 单独 训练 一个 模型 $ ( w _ u ) $ （ 模型 参数 ？ ） ， 将 缺乏 数据 而 使 模型 不可 用 。     可以 通过 一个 全局 模型 $ ( w _ 0 ) $ 使得 数据 能够 在 多个 分类器 中 共享 。 存储 所有 分类器 需要 $ ( O ( d   ( | U |   +   1 ) ) ) $ 的 空间 复杂度 。     简单 的 方法 是 排除 低频 的 token 。 但 这回 导致 恶意 错误 拼写 的 单词 被 丢弃 ， 而 hash 到 一个 低 维空间 ， 使得 这些 低频词 也 能 对模型 有所 贡献 。     而且 大规模 的 邮件 过滤器 对 内存 和 时间 有 严格要求 ， 因为 用户量 太 大 。 为此 ， 我们 将 权值 向量 $ ( w _ 0 ,   ... ,   w _ { | U } ) $   通过 不同 的 映射 $ ( \ \ phi _ 0 , ... , \ \ phi _ { | U | } ) $ ，     映射 到 低维 特征 空间 $ ( R ^ m ) $ 。 最终 的 权值 为 ：       $ $     w _ h   =   \ \ phi _ 0 ( w _ 0 )   +   \ \ sum _ { u   \ \ in   U }   \ \ phi _ u ( w _ u )     $ $       实际上 ， $ ( w _ h ) $ 可以 在 低 维空间 直接 学习 ， 重来 不 需要 去 计算 高维 向量 。 对于 用户 u 新 的 文档 或者 邮件 x ， 预测 任务 变成 计算 内积 $ ( \ \ langle   \ \ phi _ 0 ( x )   +   \ \ phi _ u ( x ) ,   w _ h   \ \ rangle ) $ 。 hash 操作 将会 带来 内积 的 计算 的 失真 $ ( \ \ epsilon _ d ) $ ， 其他 hash 函数 带来 的 重叠 $ ( \ \ epsilon _ i ) $ 。       $ $     \ \ langle   \ \ phi _ 0 ( x )   +   \ \ phi _ u ( x ) ,   w _ h   \ \ rangle   =   \ \ langle   x ,   w _ 0   +   w _ u   \ \ rangle   +   \ \ epsilon _ d   +   \ \ epsilon _ i     $ $       大规模 协同 过滤 ： $ ( M   =   U ^ T   W ) $ ， U 和 W   $ ( \ \ in   R ^ { nd } ) $ ， 存储 需要 大量 空间 ， 利用 hash 只 需要 两个 m 维 向量 $ ( u ,   w ) $       $ $     u _ i   =   \ \ sum _ { j , k : h ( j , k ) = i }   \ \ xi ( j , k )   U _ { jk } , \ \ \ \     w _ i   =   \ \ sum _ { j , k : h ' ( j , k ) = i }   \ \ xi ' ( j , k )   W _ { jk }     $ $       这里 $ ( ( h , \ \ xi ) ,   ( h ' , \ \ xi ' ) ) $ 是 两组 独立 的 hash   函数 。 近似计算 $ ( M _ { ij } ) $ 的 方法 是 ：       $ $     M _ { ij } ^ { \ \ phi }   =   \ \ sum _ k   \ \ xi ( k , i ) \ \ xi ' ( k , j )   u _ { h ( k , i ) }   w _ { h ' ( k , j ) }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/general-machine-learning.html"},
      
      
      {"title": "机器学习平台调研", "text": "    Table   of   Contents           关于           阿里巴巴           机器 学习                   微软   Azure           亚马逊           Google           Twitter                 关于       调研 现阶段 的 机器 学习 平台       阿里巴巴       目前 公开 的 平台 是 数加 ：   https : / / data . aliyun . com / product / learn       特点 ：           构建 于 阿里 云 MaxCompute 、 GPU 等 计算 集群 之上       分布式 算法 ， 包括 数据处理 、 特征 工程 、 机器 学习 算法 、 文本 算法 等       基于 MaxCompute 、 GPU 集群 ， 支持 MR 、 MPI 、 SQL 、 BSP 、 SPARK 等 计算 类型 。       机器 学习 支持 的 功能 如下           机器 学习           输入输出 源 ： ODPS 数据表       特征 预处理 ：       采样 ： 随机 采样 ； 加权 采样 （ 某 一列 作为 权值 ） ； 分层 采样       清洗 ： SQL 条件 过滤       缺失 值 填充 ：       归一化 ： log ， MinMax ， ZScore               特征 工程 ：       降维去 噪 ： PCA       规范化       离散 化 ： 等距 等频       异常 值 平滑 ： Zscore 平滑 （ 3sigma ） ， 百分位 平滑 ， 阈值       特征选择 ： 随机 森林 、 GBDT 特征 重要性 ； 线性 模型 特征 重要性 ， 重要性 定义 为   weight   *   std 。 即 学到 的 系数 与 特征 的 标准差 乘积 ？ ！ ( 支持 可视化 )       偏好 计算 ：               统计分析 ：       百分位 统计       相关系数       直方图 统计       特征分析 ： 离散 值 和 连续 值       统计 检验 ： t 检验 ， 卡方 检验               机器 学习 ：       分类 和 回归 算法   +   XGBOOST               文本 分析 和 特征 ：       word2vec       文本 相似性       分词                   微软   Azure           可视化 交互 ， 和 阿里 差不多       支持 脚本 ！ ！   Python   R       更 多 的 数据处理 功能 ， 滤波       OpenCV ？           亚马逊       不够 开放       Google           语音 ， 图像 ， 文本 等 都 有 很 好 的 支持       基于 API 接口 调用           Twitter  ", "tags": "machine-learning", "url": "/wiki/machine-learning/machine-learning-parform.html"},
      
      
      {"title": "机器学习相关资料", "text": "    Table   of   Contents           会议           研究组           课程           Stanford   CS231n           Stanford   CS224d           MIT 深度 学习 导论           MIT 自动 驾驶           MIT 通用 人工智能                   书籍                 会议           NIPS     https : / / nips . cc /         ICML       CVPR       KDD       AAAI       ICLR       IJCAI           研究组           南京大学 - 周志华     https : / / cs . nju . edu . cn / zhouzh /         DeepMind     https : / / deepmind . com / research / publications /         Hinton     http : / / www . cs . toronto . edu / ~ hinton /         Yoshua   Bengio     http : / / www . iro . umontreal . ca / ~ bengioy / yoshua _ en /             课程       Stanford   CS231n       课程 地址 ：   http : / / cs231n . stanford . edu / syllabus . html         李飞飞 主讲 的 深度 学习 在 机器 视觉 的 应用 ， 即使 不 做 CV 也 非常 值得一看 。       Stanford   CS224d       课程 地址 ：   http : / / cs224d . stanford . edu / syllabus . html         经典 课程 ， 主讲 深度 学习 在 NLP 中 的 应用 ， 即使 不 搞 NLP 也 值得一看 。       MIT 深度 学习 导论       课程 地址 ：     http : / / introtodeeplearning . com /         MIT 自动 驾驶       课程 地址 ：     https : / / selfdrivingcars . mit . edu /         MIT 通用 人工智能       课程 地址 ：     https : / / agi . mit . edu /         书籍  ", "tags": "machine-learning", "url": "/wiki/machine-learning/conference-list.html"},
      
      
      {"title": "机器翻译", "text": "    Table   of   Contents           SCORE           BLEU           WER                   机器翻译 ： 联合 训练   alignment   and   translater           低频词           低频词 ： 混合 模型                 SCORE       BLEU       WER       机器翻译 ： 联合 训练   alignment   and   translater       Bahdanau   D ,   Cho   K ,     Bengio   Y   .   Neural   machine   translation   by   jointly   learning   to   align   and   translate [ J ] .   arXiv   preprint   arXiv : 1409.0473 ,   2014 .       Encoder   -   Decoder   当 句子 很长 的 时候 ， 难以 学习 ， 因为 需要 把 一个 很长 的 序列 压缩 为 一个 固定 长度 的 向量 。       attention   向量 是 变长 的 ， 这个 问题 怎么 解决 ？       低频词       论文 ： Luong   M ,   Sutskever   I ,   Le   Q   V ,   et   al .   Addressing   the   Rare   Word   Problem   in   Neural   Machine   Translation [ C ] .   meeting   of   the   association   for   computational   linguistics ,   2014 :   11 - 19 .       低频词 ： 混合 模型       论文 ： Luong   M ,   Manning   C   D .   Achieving   Open   Vocabulary   Neural   Machine   Translation   with   Hybrid   Word - Character   Models [ C ] .   meeting   of   the   association   for   computational   linguistics ,   2016 :   1054 - 1063 .       解决 的 问题 ： encoder   +   decoder   +   attention   模型 ， 对 低频词 效果 不好 ，     常将 低频词 全部 映射 为 一个   的 特殊 词 ， 然后 通过 后续 规则 解决 。       方法 ： 在   encoder   和   decoder   测加 了 一个 对   的   Character   Model ， 这个 模型 和 翻译 模型 联合 优化 ！          ", "tags": "machine-learning", "url": "/wiki/machine-learning/mt.html"},
      
      
      {"title": "树模型", "text": "    Table   of   Contents           关于           树 模型 简史           随机 森林           GBDT           xgboost           一些 待 整理 的 记录           参考                 关于       树 模型 是 一个 很 经典 的 模型 ， 而且 不断 有 新 的 东西 产生 ， 比如 randomforest   和   xgboost 。     因此 有 必要 单独 为 它 做 一个 专题 。       树 模型 简史       随机 森林       GBDT       xgboost       一些 待 整理 的 记录       Leo   Breiman ,   1928   -   2005         -   1954 :   PhD   Berkeley   ( mathematics )         -   1960   - 1967 :   UCLA   ( mathematics )           -   1969   - 1982 :   Consultant           -   1982   -   1993   Berkeley   ( statistics )         -   1984   “ Classification   & amp ;   Regression   Trees ”     ( with   Friedman ,   Olshen ,   Stone )     -   1996   “ Bagging ”     -   2001   “ Random   Forests ”       Linear   discriminant   analysis   ( 1930 ’ s )       -   Logistic   regression   ( 1944 )     -   Nearest   neighbors   classifiers   ( 1951 )       参考           Leo   Breiman ,   RANDOM   FORESTS ,   2011 ,     https : / / www . stat . berkeley . edu / ~ breiman / randomforest2001 . pdf        ", "tags": "machine-learning", "url": "/wiki/machine-learning/tree-model.html"},
      
      
      {"title": "残差网络", "text": "    Table   of   Contents           关于           残差 网络 2015 年 的 论文 导读           摘要           导言           相关 的 研究 工作           Residual   Representations .           Shortcut   Connection                   Deep   Residual   Learning                   何凯明 PPT @ ICML2016           深度 的 演化           深度 频谱           初始化 技巧           Batch   Normalize           Deep   Residual   Network   10 - 100 层           单位 映射 的 重要性           未来 的 方向                   参考                 关于       Residual   Networks   残差 网络 ， 何凯明 ， 孙剑   @ MSRA 。       残差 网络 2015 年 的 论文 导读       摘要           152 层 残差 网络 ， 是   VGG   net 的 8 倍 ， 但是 复杂度 更 低 ， 效果 更好 。       ImageNet   测试 集 错误率 为   3.57 %       COCO   object   detection   dataset   28 %   相对 提升       ILSVRC   & amp ;   COCO   2015   competitions   第一名 ， on   the   tasks   of   ImageNet   detection ,   ImageNet   localization ,     COCO   detection ,   and   COCO   segmentation           导言           深度 卷积 网络 （ CNN ） 是 图像 分类 问题 的 重大突破 ，     它 可以 自动 学习 底层 / 中层 / 高层 特征 。     特征 的 层级 可以 通过 stack 的 方式 （ 增加 深度 ） 得到 提升 。           CNN 的 重要 论文 ：           Y .   LeCun ,   B .   Boser ,   J .   S .   Denker ,   D .   Henderson ,   R .   E .   Howard ,     W .   Hubbard ,   and   L .   D .   Jackel .   Backpropagation   applied   to   handwritten     zip   code   recognition .   Neural   computation ,   1989       A .   Krizhevsky ,   I .   Sutskever ,   and   G .   Hinton .   Imagenet   classification     with   deep   convolutional   neural   networks .   In   NIPS ,   2012 .       P .   Sermanet ,   D .   Eigen ,   X .   Zhang ,   M .   Mathieu ,   R .   Fergus ,   and   Y .   LeCun .     Overfeat :   Integrated   recognition ,   localization   and   detection     using   convolutional   networks .   In   ICLR ,   2014       M .   D .   Zeiler   and   R .   Fergus .   Visualizing   and   understanding   convolutional     neural   networks .   In   ECCV ,   2014 .           近期 研究 表明 ， 堆叠 的 深度 是 至关重要 的 因素 。           K .   Simonyan   and   A .   Zisserman .   Very   deep   convolutional   networks     for   large - scale   image   recognition .   In   ICLR ,   2015       C .   Szegedy ,   W .   Liu ,   Y .   Jia ,   P .   Sermanet ,   S .   Reed ,   D .   Anguelov ,   D .   Erhan ,     V .   Vanhoucke ,   and   A .   Rabinovich .   Going   deeper   with   convolutions .     In   CVPR ,   2015           ImageNet   的 最佳 结果 都 是 很 深 的 模型 ， 从 13 层到 30 层 。 深度 模型 对 其他 的 图像 任务 也 有 帮助 。           K .   He ,   X .   Zhang ,   S .   Ren ,   and   J .   Sun .   Delving   deep   into   rectifiers :     Surpassing   human - level   performance   on   imagenet   classification .   In     ICCV ,   2015 .       S .   Ioffe   and   C .   Szegedy .     Batch   normalization   :   Accelerating   deep     network   training   by   reducing   internal   covariate   shift .   In   ICML ,   2015       O .   Russakovsky ,   J .   Deng ,   H .   Su ,   J .   Krause ,   S .   Satheesh ,   S .   Ma ,     Z .   Huang ,   A .   Karpathy ,   A .   Khosla ,   M .   Bernstein ,   et   al .   Imagenet     large   scale   visual   recognition   challenge .   arXiv : 1409.0575 ,   2014 .           学习 深度 模型 最大 的 问题 在于   vanishing   gradient ， 梯度 消减 ！ 导致 模型 无法 收敛 。     利用 这些 技巧 ， 几十层 深度 的 模型 也 可以 通过 BP 算法 + SGD 进行 训练 。           Y .   Bengio ,   P .   Simard ,   and   P .   Frasconi .   Learning   long - term   dependencies     with   gradient   descent   is   difficult .   IEEE   Transactions   on   Neural     Networks ,   5 ( 2 ) : 157 – 166 ,   1994 .       X .   Glorot   and   Y .   Bengio .   Understanding   the   difficulty   of   training     deep   feedforward   neural   networks .   In   AISTATS ,   2010 .           梯度 消减 的 问题 被 很大 程度 上 通过   normalized   initialization   和   intermediate   normalization   layers   解决 了           Y .   LeCun ,   L .   Bottou ,   G .   B .   Orr ,   and   K . - R .   Muller .   Efficient   backprop .   ¨     In   Neural   Networks :   Tricks   of   the   Trade ,   pages   9 – 50 .   Springer ,   1998 .       A .   M .   Saxe ,   J .   L .   McClelland ,   and   S .   Ganguli .   Exact   solutions   to     the   nonlinear   dynamics   of   learning   in   deep   linear   neural   networks .     arXiv : 1312.6120 ,   2013 .       K .   He ,   X .   Zhang ,   S .   Ren ,   and   J .   Sun .   Delving   deep   into   rectifiers :     Surpassing   human - level   performance   on   imagenet   classification .   In     ICCV ,   2015 .       S .   Ioffe   and   C .   Szegedy .   Batch   normalization :   Accelerating   deep     network   training   by   reducing   internal   covariate   shift .   In   ICML ,   2015 .           随着 层数 的 加深 ， 模型 的 性能 逐渐 饱和 ， 然后 迅速 恶化 。 这个 问题 并 不是 由于 过 拟合 ， 更深 的 模型 导致 更差 的 性能 ！     理论 上 来讲 ， 深层 模型 应该 可以 做到 比 浅层 模型 更好 的 性能 ， 可以 设想 多余 的 层 是 恒等 变换 ， 那么 深层 模型 结果 和 浅层 一样 。     但是 实际上 的 结果 并非如此 。       残差 网络 并 不 直接 拟合 目标 ， 而是 拟合 残差 。 假设 潜在 的 目标 映射 为 $ ( \ \ mathcal { H } ( x ) ) $ ， 我们 让 非线性 层 学习 残差     $ ( \ \ mathcal { F } ( x ) : = \ \ mathcal { H } ( x )   -   x ) $ ， 并 提供 一条 短路 （ 或直 连 ） 通道 ， 使得 输出 为 $ ( \ \ mathcal { F } ( x ) + x ) $ 。     我们 假设 优化 残差 比 原始 映射 要 简单 ！ （ 假设 ！ ！ ！ ！ ）     在 极端 情况 下 ， 可以 让 非线性 层置 0 ， 使得 直接 输出 输入 值 。 （ 我 的 思考 ： 存在 正则 项 的 时候 ， 这个 确实 更优 ， 那 是不是 就 证明 残差 网络 不会 比 浅层 网络 更差 了 呢 ？ ！ ）     短路 连接 在 这里 可以 跳过 一层 或者 多层 。 单位 短路 通道 （ 即 短路 通道 直接 输出 输入 的 值 ） 不 增加 计算 复杂度 也 不 增加 额外 的 参数 。     整个 网络 可以 采用   end - to - end   使用 SGD + BP 算法 ， 可以 采用 现有 的 求解 器 就 能 实现 。           C .   M .   Bishop .   Neural   networks   for   pattern   recognition .   Oxford     university   press ,   1995 .       B .   D .   Ripley .   Pattern   recognition   and   neural   networks .   Cambridge     university   press ,   1996 .           W .   Venables   and   B .   Ripley .   Modern   applied   statistics   with   s - plus .     1999               ImageNet   论文 ：   O .   Russakovsky ,   J .   Deng ,   H .   Su ,   J .   Krause ,   S .   Satheesh ,   S .   Ma ,     Z .   Huang ,   A .   Karpathy ,   A .   Khosla ,   M .   Bernstein ,   et   al .   Imagenet     large   scale   visual   recognition   challenge .   arXiv : 1409.0575 ,   2014 .           CIFAR - 10   论文 ： A .   Krizhevsky .   Learning   multiple   layers   of   features   from   tiny   images .     Tech   Report ,   2009 .               We   present   successfully   trained   models   on   this   dataset   ( CIFAR - 10 )   with     over   100   layers ,   and   explore   models   with   over   1000   layers .     Our   ensemble   has   3.57 %   top - 5   error   on   the   ImageNet   test   set ,   and   won   the   1st   place   in   the   ILSVRC     2015   classification   competition .     The   extremely   deep   representations   also   have   excellent   generalization   performance     on   other   recognition   tasks ,   and   lead   us   to   further   win   the   1st   places   on :   ImageNet   detection ,   ImageNet   localization ,     COCO   detection ,   and   COCO   segmentation   in   ILSVRC   & amp ;   COCO   2015   competitions .           相关 的 研究 工作       Residual   Representations .           VLAD ： H .   Jegou ,   F .   Perronnin ,   M .   Douze ,   J .   Sanchez ,   P .   Perez ,   and     C .   Schmid .   Aggregating   local   image   descriptors   into   compact   codes .     TPAMI ,   2012       Fisher   Vector ： F .   Perronnin   and   C .   Dance .   Fisher   kernels   on   visual   vocabularies   for     image   categorization .   In   CVPR ,   2007 .           这 两种 表达 被 应用 在 图像 检索 和 分类 中 ：               K .   Chatfield ,   V .   Lempitsky ,   A .   Vedaldi ,   and   A .   Zisserman .   The   devil     is   in   the   details :   an   evaluation   of   recent   feature   encoding   methods .     In   BMVC ,   2011               A .   Vedaldi   and   B .   Fulkerson .   VLFeat :   An   open   and   portable   library     of   computer   vision   algorithms ,   2008               在 矢量 量化 中 ， 编码 残差 比 编码 原始 矢量 更加 有效 。               H .   Jegou ,   M .   Douze ,   and   C .   Schmid .   Product   quantization   for   nearest     neighbor   search .   TPAMI ,   33 ,   2011 .               低级 视觉 和 计算机 图形学 中 ， 为了 解决 PDE ， 采用 Multigrid 方法 。 。 。 。 不 懂 ， 所以 略 。               Shortcut   Connection       已 被 研究 多日 了 （ 哈哈哈哈 ） ， 早起 的 多层 感知器 研究 在 输入输出 间 单独 加 了 一个 线性 层 。     在 另外 两篇 论文 中 ， 一些 中间层 直接 连接 到 一个 辅助 的 分类器 ， 通过 这种 方式 减少 梯度 消减 。           C . - Y .   Lee ,   S .   Xie ,   P .   Gallagher ,   Z .   Zhang ,   and   Z .   Tu .   Deeply   supervised     nets .   arXiv : 1409.5185 ,   2014 .       R .   K .   Srivastava ,   K .   Greff ,   and   J .   Schmidhuber .   Highway   networks .     arXiv : 1505.00387 ,   2015           等等 其他 ， 略 。       highway   networks   在 短路 链接 采用 了 门 函数 ， 该门 函数 有 参数 需要 通过 数据 学习 。     当门 关掉 （ 为 0 值 ） 时 ， 网络 就是 传统 的 神经网络 ， 而 不是 残差 网络 。       Deep   Residual   Learning           假设 ： （ 还是 一个 open   question ） 多层 非线性 可以 逼近 复杂 函数 。       当 输入输出 是 相同 的 维度 ， 可以 假设 它 逼近 残差 $ ( \ \ mathcal { H } ( x )   -   x ) $ 。 虽然 逼近 原始 函数 和 逼近 残差 ， 这 两个 函数 都 很 复杂 ，     但是 后者 更 容易 ！ 前面 说 过 ， 如果 这些 加入 的 非线性 层 是 单位 映射 ， 那么 多层 不会 比 浅层 差 。 但是 由于 梯度 消减 ， 多层 非线性 难以 逼近 单位 函数 ，     但是 残差 网络 可以 很 容易 ， 让 非线性 层置 0 即可 。 实际上 ， 单位 映射 往往 不是 最优 的 。 实验 结果表明 ， 残差 部分 学 出来 的 结果 都 比较 小 ，     这 表明 单位 映射 是 一个 很 好 的 先验 条件 。       残差 网络 基本 模块 是 ：           $ $     y   =   \ \ mathcal { F } ( x ,   { W _ i } )   +   x       \ \ \ \     \ \ mathcal { F }   =   W _ 2   \ \ sigma ( W _ 1   x )       \ \ \ \     \ \ sigma   =   ReLU     $ $       如果 输入输出 维度 不同 ， 可以 通过 投影 的 方法 解决 。 $ ( W _ s ) $ 仅仅 用来 解决 维度 匹配 的 问题 ， 如果 维度 相同 ， 单位 映射 就 好 了 。       $ $     y   =   \ \ mathcal { F } ( x ,   { W _ i } )   +   W _ s   x     $ $               在 论文 里面 ， 在 ImageNet 上 最好 的 结果 是 110 层 ， 作者 也 试过 1202 层 ， 发现 训练 集 误差 相近 ， 但是 测试 集 效果 变差 了 ， 作者 认为 是     过 拟合 的 原因 ， 因为 没有 用到 MaxOut [ 1 ] 和 Dropout [ 2 ] 强 正则 化 的 做法 。               I .   J .   Goodfellow ,   D .   Warde - Farley ,   M .   Mirza ,   A .   Courville ,   and     Y .   Bengio .   Maxout   networks .   arXiv : 1302.4389 ,   2013 .           G .   E .   Hinton ,   N .   Srivastava ,   A .   Krizhevsky ,   I .   Sutskever ,   and     R .   R .   Salakhutdinov .   Improving   neural   networks   by   preventing   coadaptation     of   feature   detectors .   arXiv : 1207.0580 ,   2012           何凯明 PPT @ ICML2016       此时 他 已经 来到 Facebook   AI 团队 了 ！       深度 的 演化           AlexNet ,   8   layers   ( ILSVRC   2012 )       VGG ,   19   layers   ( ILSVRC   2014 )       GoogleNet ,   22   layers   ( ILSVRC     2014 )       ResNet ,   152   layers   ( ILSVRC   2015 )               >         200   citations       in     6       months     after       posted     on     arXiv   ( Dec .   2015 )           深度 频谱           5   layers :   easy               10       layers :   initialization ,   Batch       Normalization                       30       layers :   skip         connections                       100     layers :   identity         skip         connections                   初始化 技巧       总结 ， 好 的 初始化 很 重要 ， 当 层数 较 深 （ 20 - 30 ） 时 ， 可能 收敛 更 快 ， 初始化 不好 可能 不 收敛 。           LeCun   et   al     1998         “ Efficient     Backprop ”       Glorot & amp ;     Bengio   2010   “ Understanding     the   difficulty     of     training         deep         feedforward   neural     networks ”           Batch   Normalize           输入 标准化       标准化 每 一层   for   each   mini - batch       极大 地 加速 训练       减少 初值 敏感       增强 正则 化           $ $     \ \ hat { x }   =   \ \ frac { x   -   \ \ mu } { \ \ sigma }   \ \ \ \     y   =   \ \ gamma   \ \ hat { x }   +   \ \ beta     $ $           $ ( \ \ mu ,   \ \ sigma ) $   分别 是   mini - batch   的 均值 和 标准差 ， 是 由 数据 计算出来 的       $ ( \ \ gamma ,   \ \ beta ) $   是 缩放 因子 和 位移 量 ， 需要 模型 学 出来 。       注意 ， 训练 集 的 均值 方差 是从 数据 中 计算 ， 但是 测试 集是 采用 训练 集 计算 的 结果 （ 平均 ） 。           Deep   Residual   Network   10 - 100 层           简单 叠加 会 变差 ！           单位 映射 的 重要性       单位 映射 下 ：       $ $     x _ L   =   x _ l   +   \ \ sum _ { i = l } ^ { L - 1 }   \ \ mathcal { F } _ i ( x _ i )     \ \ \ \     \ \ frac { \ \ partial   E } { \ \ partial   x _ l }   =   \ \ frac { \ \ partial   E } { \ \ partial   x _ L } ( 1   +   \ \ frac { \ \ partial   E } { \ \ partial   x _ l }   \ \ sum _ { i = l } ^ { L - 1 }   \ \ mathcal { F } _ i ( x _ i ) )     $ $       在 单位 映射 下 ， 梯度 可以 以 恒定 比例 传递 过来 ，     如果 不是 ， 一旦 深度 变深 了 ， 要么 衰减 ， 要么 爆炸 ！       加总 之后 ， 还是 单位 映射 好 ， （ 我 觉得 还是 梯度 传递 的 问题 ， 需要 单位 范数 的 映射 才能 不 使得 梯度 消失 和 爆炸 ！ ） pre - active           Kaiming     He ,   Xiangyu   Zhang ,     Shaoqing           Ren ,       & amp ;       Jian         Sun .         “ Identity       Mappings         in     Deep         Residual         Networks ” .     arXiv       2016 .           未来 的 方向           Representation       skipping     1       layer       vs .   multiple         layers ?       Flat     vs .   Bottleneck ?       Inception - ResNet [ Szegedy   et       al     2016 ]       ResNetin     ResNet [ Targ   et     al     2016 ]       Width   vs .   Depth       [ Zagoruyko   & amp ;         Komodakis   2016 ]               Generalization       DropOut ,     MaxOut ,   DropConnect ,         …       Drop     Layer       ( Stochastic   Depth )     [ Huang     et     al     2016 ]               Optimization       Without       residual / shortcut ?                   参考             Deep   Residual   Learning   for   Image   Recognition           Deep         Residual         Networks        ", "tags": "machine-learning", "url": "/wiki/machine-learning/residual-network.html"},
      
      
      {"title": "深度学习", "text": "    Table   of   Contents           关于           DNN   方向           CNN   方向           RNN   方向           递归 神经网络 语言 模型                   DQN   方向                 关于       深度 学习 的 调研 ， 相当于 review   。       DNN   方向       CNN   方向       RNN   方向       递归 神经网络 语言 模型       论文 ：           Mikolov ,   Recurrent   neural   network   based   language   model ,   interspeech   2010 ,   Johns   Hopkins   University           人物 ：           Toma ´ s   Mikolov ,   \" Johns   Hopkins   University \" ,   \" Speech @ FIT ,   Brno   University   of   Technology ,   Czech   Republi \"           DQN   方向  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-learning.html"},
      
      
      {"title": "用神经网络做风格迁移", "text": "    Table   of   Contents           关于           神经网络 风格 迁移           实时 风格 迁移                 关于       风格 迁移 就是 将 图片 变成 某种 风格 的 图片 ， 例如 为 你 的 照片 加上 梵高 的 画 的 风格 。       神经网络 风格 迁移       用 神经网络 做 风格 迁移 的 方法 最早 由   Leon   A .   Gatys   提出 ：   Image   Style   Transfer   Using   Convolutional   Neural   Networks ， 2016               基本 思想 是 用 一个 神经网络 ， 对于 原始 照片   p   ， 用 神经网络 中 的 某 一层 特征   $ ( F _ { ij } ^ l ) $   ( 这里 第一个 下标 表示 特征 通道 ， 第二个 下标 表示 空间 维度 ， 将 二维 空间 压缩 为 一维 便于 表述 ， 下同 ) 作为 内容 的 表达 ，     对于 生成 的 照片   x ， 用 神经网络 中 的 同 一层 特征 表达 生成 的 图片 的 内容   $ ( P _ { ij } ^ l ) $ ， 要求 新 生成 的 照片 内容 和 原始 照片 内容 接近 ，     即 损失 函数       $ $     L _ { content } ( p ,   x ,   l )   =   \ \ frac { 1 } { 2 }   \ \ sum _ { ij } ( F _ { ij } ^ l   P _ { ij } ^ l ) ^ 2     $ $       较 小 。       另一方面 ， 需要 生成 的 照片 的 风格 和 图片   a   相似 ， 风格 可以 通过 特征 空间 的   Gram   矩阵 来 表达 。       $ $     G _ { ij } ^ l   =   \ \ sum _ { k }   F _ { ik } ^ l   F _ { jk } ^ l     $ $       通过 图片 x 不同 层 的 Gram 矩阵 和 图片   a   相似 ， 实现 风格 的 相似 。       $ $     E _ l   =   \ \ frac { 1 } { 4N _ l ^ 2M _ l ^ 2 }   \ \ sum _ { ij }   ( G _ { ij } ^ l   -   A _ { ij } ^ l ) ^ 2   \ \ \ \     L _ { style } ( a ,   x )   =   \ \ sum _ { l = 1 } ^ L   w _ l   E _ l .     $ $       $ ( N _ l ,   M _ l ) $ 分别 是 第 l 层 特征 数目 和 空间 维度 ！       通过 内容 和 风格 损失 函数 最小化 ， 实现 内容 和 风格 的   tradeoff 。       $ $     L   =   \ \ alpha   L _ { content } ( p ,   x )   +   \ \ beta   L _ { style } ( a ,   x )     $ $       注意 上述 两个 损失 函数 的 特征 都 是 做 了 max - pooling 后 的 特征 。           一些 讨论 ：       内容 匹配 如果 选用 约 高层 的 表征 ， 保留 的 细节 越 少 ， 感受 约 平滑       太慢 了 ： 用   K40   GPU ， 一张 512x512 图片 的 风格 迁移 也 需要 1 小时 ！                           实时 风格 迁移       论文 ： Perceptual   Losses   for   Real - Time   Style   Transfer   and   Super - Resolution ， Justin   Johnson ,   Alexandre   Alahi ,   and   Li   Fei - Fei ， 2016 .       特点 ：   perceptual   loss   functions ，   three   orders   of   magnitude   faster ， real - time           超 分辨   pixel   loss   function ： 不能 很 好 的 刻画 语义 的 不同 （ 例如 两个 只是 平移 了 一点点 的 图片 ， 像素 的 差异 很大 ， 但是 语义 差异 较 小 ）       解决之道 ： 利用 训练 好 的 深度 神经网络 的 高层 语义 空间 的 特征 表达 ！                       解决 风格 迁移 test 速度慢 的 方法 是 ， 用 一个 神经网络 建模 这种 变换 关系 ， 目标 用 前者 的 损失 函数 ！       包含 两个 网络 ：   image   trans -   formation   network   $ ( f _ W ) $   ，   loss   network   $ ( \ \ phi ) $ ； 前者 是 一个 深度 残差 网络 ， 后者 是 一个 预 训练 的   VGG   网络 。           $ $     W ^ *   =   \ \ arg   \ \ min   _   W   \ \ mathbf { E }   _   { x ,   {   y _ i   } }   \ \ sum _ { i }   \ \ lambda   _   i   l   _   i   ( f _ W   ( x ) ,   y _ i )     $ $       损失 函数 和   Gatys   一样 ！   TV   范数 正则 化 ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/style-transform.html"},
      
      
      {"title": "知识图谱", "text": "    Table   of   Contents           AAAI   tutorial           知识 图谱 简介           NLP 基础                   跨 语言 知识 图谱 构建 ： 李 涓 子                 AAAI   tutorial       知识 图谱 简介           知识 图谱 ： 将 知识 表达 为 图 的 形式       获取 实体 、 属性 、 关系           图 的 顶点 代表 实体               应用 ： QA ， 决策               工业界 产品 ：           Google   Knowledge   Vault       Amazon   Product   Graph                   数据 来源 ： 结构 数据 和 非 结构化 数据 ， 图片 、 视频 ？           知识 表达 ：       RDF ：     :   r ( s , p , o )       ABox   ( assertions )   versus   TBox   ( terminology )       Common   ontological   primitives       rdfs : domain ,   rdfs : range ,   rdf : type ,   rdfs : subClassOf ,   rdfs : subPropertyOf ,   ...       owl : inverseOf ,   owl : TransitiveProperty ,   owl : FunctionalProperty ,   ...                       语义 网           从 文本 中 获取 知识 的 方法           chunking       polysemy / word   sense   disambiguation   消 歧义       entity   coreference   实体 共止 ： 通过 判定 两个 实体 是否 存在 共指 关系 ？ 如 “ IBM ” 和 “ IBM   Inc . ”       relational   extraction ： 关系 抽取                   基本 问题           实体 识别       实体 的 属性 和 标签       实体 关系                   NLP 基础           Entity   resolution       Entity   linking       Relation   extraction       Coreference   Resolution       Dependency   Parsing       Part   of   speech   tagging           Named   entity   recognition               抽取 方法 ：           规则 ： 高精度 低 召回       监督 学习                   定义   domain               论文 ： Toward   an   Architecture   for   Never - Ending   Language   Learning ，       跨 语言 知识 图谱 构建 ： 李 涓 子           语义 搜索   Semantic   Search       RDF   ：   资源 描述 框架   W3C   标准       要素 ： 资源 、 属性 和 属性 值       RDF 陈述 ： 主体 、 谓语 和 客体               语义 网 ： 数据 用 一个   directed   labeled   graph   描述 ， 每 一个 顶点 对应 一个 资源 ， 每 一个 边 标注 了 一个 属性 类型 。       语义 网 描述 ： RDF ， RFDS       语义 网 查询 ： SOAP            ", "tags": "machine-learning", "url": "/wiki/machine-learning/knowledge-graph.html"},
      
      
      {"title": "知识图谱：实体关系挖掘", "text": "    Table   of   Contents           关于           TransE                 关于       实体 关系 挖掘 相关 的 论文 阅读 笔记 ， 因为 用户 画像 中要 做 用户 、 poi 等 实体 间 的 关系 挖掘 ， 所以 看 了 一些 关系 挖掘 的 论文 。       TransE       论文 ： Bordes   A ,   Usunier   N ,   Garcia - Duran   A ,   et   al .   Translating   embeddings   for   modeling   multi - relational   data [ C ] / / Advances   in   neural   information   processing   systems .   2013 :   2787 - 2795 .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/relation-model.html"},
      
      
      {"title": "神经程序员", "text": "    Table   of   Contents           关于           摘要           模型                 关于       用 神经网络 做 编程 ！ ！           Neural   Programmer :   Inducing   Latent   Programs   with   Gradient   Descent ,   Arvind   Neelakantan ,   Quoc   V   Le ,   Ilya   Sutskever ,   ICLR   2016           摘要       神经网络 虽然 在 很多 领域 如 语音 识别 ， 图像识别 etc 等 领域 取得 了 巨大 的 成功 ， 但是 在 基本 的 算术 和 逻辑 代数 运算 上 ，     神经网络 的 精确 学习 却 很 困难 ！   Neural   Programmer   增加 了 一部分 基本 的 算术 和 逻辑 操作 ， 解决 了 这 一点 。           A   major   limitation   of   these   models   is   in   their   inability   to   learn   even   simple   arithmetic   and   logic   operations .     recurrent   neural   networks   ( RNNs )   fail   at   the   task   of   adding   two   binary   numbers   even   when   the   result   has   less   than   10   bits           往 梯度 中 增加 高斯 噪声 ， 可以 提升 训练 效果 ， 增加 泛化 能力 。       模型       Neural   Programmer   由 3 个 部分 构成 ：           question   Recurrent   Neural   Network   ( RNN )   处理 用 自然语言 输入 的 问题       selector   生成 两个 概率分布 ， 用于 （ soft   select ） 选择 数据 分片 和 操作       history   RNN   记住 历史 选择 的 数据 分片 和 操作                   除了 操作 列表 ， 其他 的 都 可以 通过 梯度 下降 ， 由 数据   ( question ,   data   source ,   answer )   三元组 样本 训练 得到 ！       data   source   以 表格 形式 存在   $ ( table   \ \ in   \ \ mathbb { R } ^ { M   \ \ times   C } ) $       QUESTION   MODULE   是 一个 简单 的   RNN   模块 ， 将 输入 的 词 序列 （ 分布 是 表达 ） 编码 成 一个   d   维 的 向量 q 。     如果 问题 包含 长 句子 ， 采用 一个 双向   RNN 。       预处理 将 数字 单独 拿 出来 ， 放到 一个 列表 中 。       SELECTOR   生成 两个 分布 ， 一个 是 operator 的 概率分布 ， 一个 是 数据 列 的 概率分布 （ 问题 ： 数据 列是 变动 的 ， 怎么办 ？ ）     输入 是 问题 的 编码 向量 q （ d 维 ) 和 输入 历史 的 向量 h [ t ] （ d 维 ） 。       每 一个   operator   编码 为 一个 d 维 向量 ！ 所有 的   operator   构成 一个 矩阵   U 。   operator   选择 表达式 为 ：       $ $     \ \ alpha _ t ^ { op }   =   softmax ( U   tanh ( W ^ { op }   [ q ;   h _ t ] ) )     $ $       数据 列名 采用 问题 编码 RNN 中 的 词 向量 表达 ！ 或者   RNN   phrase   embedding 。     所有 的 列名 构成 一个 矩阵   P ！ 列 选择 表达 为       $ $     \ \ alpha _ t ^ { col }   =   softmax ( P   tanh ( W ^ { col }   [ q ;   h _ t ] ) )     $ $       将 出现 的 数字 单独 拿 出来 ， 对于 比较 操作 ， 需要 知道 比较 的 列 ， 即   pivot ，       $ $     \ \ beta _ { op }   =   softmax ( Z   U ( op ) )     \ \ \ \     pivot _ { op }   =   \ \ sum _ { i = 1 } ^ N   \ \ beta ( i )   qn _ i     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/neural-programmer.html"},
      
      
      {"title": "统计检验", "text": "    Table   of   Contents           关于           独立性 检验           KS （ Kolmogorov – Smirnov ） 检验                 关于       汇总 统计 检验 相关 知识点 。       独立性 检验           卡方 检验       F 检验           KS （ Kolmogorov – Smirnov ） 检验       检验 数据 拟合 优度 。   KS   test         检验 统计 量       $ $     F _ n ( x )   =   \ \ frac { 1 } { n }   \ \ sum _ { i = 1 } ^ n   I _ { [ - \ \ infty ,   x ] } ( X _ i )   \ \ \ \     D _ n   =   \ \ sup _ { x }   | F _ n ( x )   -   F ( x ) |     $ $           Wiener 过程   $ ( W _ t ) $       $ ( W _ 0   =   0 ) $       独立 增量 过程   $ ( W _ { t + u }   -   W _ t ,   u   \ \ ge   0 ) $   独立 于   $ ( W _ s :   s   \ \ le   t ) $       高斯 增量   $ ( W _ { t + u }   -   W _ t   \ \ sim   \ \ mathcal { N } ( 0 ,   u ) ) $       连续 路径 ， 以 概率 1 在 t 空间 连续                   Wiener 过程 作为   random   walk   的 极限 ！ 设   $ ( \ \ xi _ 1 ,   \ \ xi _ 2 ,   ... ) $   iid ， 均值 为 0 ， 方差 为 1 的 随机变量 。 对 任意 正整数 n ， 定义 连续 时间 随机 过程       $ $     W _ n ( t )   =   \ \ frac { 1 } { n }   \ \ sum _ { 1   \ \ le   k   \ \ le   nt }   \ \ xi _ k ,   n   \ \ rightarrow   \ \ infty     $ $       由 中心 极限 定理 可知 ， 对于 充分 大 的 n ， $ ( W _ n ( t )   -   W _ n ( s )   \ \ rightarrow   \ \ mathcal { N } ( 0 ,   t - s ) ) $ 。           Brownian   bridge ， 定义 随机 过程   $ ( B _ t ) $   为           $ $     B _ t   : =   ( W _ t   |   W _ T   =   0 ) ,   t   \ \ in   [ 0 ,   T ]     $ $     其中   $ ( W _ t ) $ 是   Wiener   过程 。           Kolmogorov   分布 ， 定义 随机变量           $ $     K   =   \ \ sup _ { t   \ \ in   [ 0 ,   1 ] } | B ( t ) |     $ $     其中   $ ( B ( t ) ) $   是   Brownian   bridge .       KS 检验 的 检验 统计 量   $ ( \ \ sqrt { n }   D _ n   \ \ rightarrow   \ \ sup _ t   | B ( F ( t ) ) | ) $ ， 即 近似 服从   Kolmogorov – Smirnov   分布 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/statistic-test.html"},
      
      
      {"title": "自动微分", "text": "    Table   of   Contents           论文           要点                 论文       自动 微分 综述 :   Automatic   differentiation   in   machine   learning -   a   survey . pdf       要点           前向 模式       令   dx = r ,   在 计算 图中 向前 冒泡 , 可以 方便 计算 函数 f 在 x 处沿 r 方向 的 方向 导数 。       令 r 为 单位向量 , 就 可以 计算 f 对 任意 自变量 的 偏导 , 如果 有 n 个 自变量 , 那么 要 计算 n 次       适合 从 低维到 高维 的 函数 计算 偏 导数       对 偶数               反向 模式       适合 从 高维 到 低维 的 函数 计算 偏 导数               前向 + 反向   可以 直接 计算 海森 矩阵 与 向量 乘积 ? ? ? ?   [ & gt ;     _         _     ____ ]      ", "tags": "machine-learning", "url": "/wiki/machine-learning/autodiff.html"},
      
      
      {"title": "自编码模型", "text": "    Table   of   Contents           关于           历史           神经网络 预 训练 方案           好 的 特征 表达 ：   交互 信息   到   自 编码           独立 成分 分析 （ ICA ） ： Bell   and   Sejnowski   ( 1995 )           传统 自 编码           Denoise   准则                   Variational   autoencoders   论文 导读           变 分界   variational   bound           The   SGVB   estimator   and   AEVB   algorithm           例子 ： 变分 自 编码                   Stacked   What - Where   Auto - encoders           软 最大值 max 和 argmax           where 的 重要性           结论                   Reference                 关于       自 编码 模型 常用 做 深度 神经网络 预 训练 。       历史       神经网络 预 训练 方案       多层 神经网络 直接 训练 会 因为 局部 最优 问题 ， 导致 初值 敏感 ， 所以 随机 初始化 效果 不好 。     可以 采用 局部 无 监督 准则 进行 逐层 初始化 预 训练 ， 使得 后续 训练 更大 可能 跳出 局部 最优 。           RBM ： Hinton   et   al . ,   2006 ;   Hinton   and   Salakhutdinov ,   2006 ;   Lee   et   al . ,   2008       auto - encoder ： Bengio   et   al . ,   2007 ;   Ranzato   et   al . ,   2007       semi - supervised   embedding ： Weston   et   al . , 2008       kernel   PCA ： Cho   and   Saul ,   2010           RBM   和   自 编码   模型 函数 形式 很 像 ， 但是 训练 和 解释 都 不同 。 一个 很大 不同 是 ， 确定性 的 自 编码 可以 用实 数值 作为 隐层 的 表达 ，     而 随机 的 RBM 采用 二进制 表达 隐层 。 但是 在 实际上 ， 应用 在 深度 网络 中 的 RBM 还是 用 实数 均值 作为 表达 。     autoencoder   的 重构 误差 可以 看做   RBM   的   log - likelihood   gradient   一种 近似 。     RBM   中 的   Contrastive   Divergence   更新 。       如何 构造 一个 好 的 表达 ！ ？ 采用 无 监督 学习 ， 学出 输入 中 的 重要 模式 ！       好 的 特征 表达 ：   交互 信息   到   自 编码       定义 好 的 表达 ： 对 最终 我们 感兴趣 的 任务 是 有用 的 ， 相比 不 采用 这种 表达 ， 它 能够 帮助 系统 更快 地 达到 更 高 的 性能 ！           A   good   representation   is   one   that   will   yield   a   better   performing   classifier           实验 表明 ， 一个 无 监督 准则 的 初始化 ， 可以 帮助 分类 任务 得到 明显 地 提升 。     人类 能够 快速 地 学习 新 的 东西 一个 重要 的 原因 是 已经 获得 了 这个 任务 的 一些 先验 知识 。       学习 一个 输入 $ ( X ) $   的 表达 $ ( Y ) $ ， 实际上 是 学习 条件 概率 $ ( q ( Y | X )   =   q ( Y | X ;   \ \ theta ) ) $ 。 $ ( \ \ theta ) $ 是 要 学习 的 参数 。       一个 基本 要求 是 要 尽可能 保留 输入 的 信息 ， 在 信息论 里面 可以 表达 为 最大化 交互 信息   $ ( \ \ mathbb { I } ( X ;   Y ) ) $ ：   Linsker   ( 1989 )       独立 成分 分析 （ ICA ） ： Bell   and   Sejnowski   ( 1995 )       $ $     \ \ arg   \ \ max _ { \ \ theta }   \ \ mathbb { I } ( X ;   Y )   =   \ \ arg   \ \ max _ { \ \ theta }   -   \ \ mathbb { H } ( X | Y )   \ \ \ \             =   \ \ arg   \ \ max _ { \ \ theta }   \ \ mathbb { E } _ { q ( X ,   Y ) }   [ \ \ log   q ( X | Y ) ]     $ $       对于 任意 分布   $ ( p ( X ,   Y ) ) $ ， 利用   KL   距离 的 性质 可知 ：       $ $     \ \ mathbb { E } _ { q ( X ,   Y ) }   [ \ \ log   p ( X | Y ) ]   \ \ le   -   \ \ mathbb { H } ( X | Y )     $ $       设 这个 分布 通过 参数 $ ( \ \ theta ' ) $ 刻画 ， 那么 优化 下面 这个 式子 相当于 优化 条件 熵 的 下界 ：       $ $     \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ( X ,   Y ;   \ \ theta ) }   [ \ \ log   p ( X | Y ;   \ \ theta ' ) ]     $ $       当 两个 分布 相同 的 时候 ， 可以 得到 精确 的 交互 信息 。 infomax   ICA   中 ， 特征 映射 为   $ ( Y   =   f _ { \ \ theta } ( X ) ) $ 。     那么 $ ( q ( X ,   Y ; \ \ theta )   =   q ( X )   \ \ approx   q ^ 0 ( X ) ) $ ， 即用 样本 集 的 分布 代替 总体 分布 。 优化 问题 变为 ：       $ $     \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   [ \ \ log   p ( X | Y = f _ { \ \ theta } ( X ) ;   \ \ theta ' ) ]     $ $       UFLDL 里面 的   独立 成分 分析   ：     找到 一组 基 向量 使得 变换 后 的 特征 是 稀疏 的 。     数据 必须 ZCA 白化 ， 标准 正交 基维 数 小于 输入 维度 ， 是 一组 不 完备 基 。       $ $     \ \ min   | |   W   x   | | _ 1     \ \ \ \     s .   t .   WW ^ T   =   I     $ $       优化 方法 ： 梯度 下降   +   每 一步 增加 投影 。       $ $     W   =   W   -   \ \ alpha   \ \ nabla _ W   | |   W   x   | | _ 1   \ \ \ \     W   =   ( WW ^ T ) ^ { - 1 / 2 }   W     $ $       传统 自 编码       在 传统   autoencoder ( AE )   中 ， 特征 变换 函数 （ Encoder ） 用 sigmoid 函数 来 近似 ：       $ $     y   =   f _ { \ \ theta } ( x )   =   s ( Wx + b )       \ \ \ \     \ \ theta   =   \ \ {   W ,   b   \ \ }     $ $       特征 重构 （ Decoder ） 变换 也 用 sigmoid 函数       $ $     z   =   g _ { \ \ theta ' } ( y )   =   s ( W '   y   +   b ' )   \ \ \ \     \ \ theta '     =   \ \ {   W ' ,   b '   \ \ }     $ $       损失 函数 为 重构 误差 ：       $ $     L ( x ,   z )   =   \ \ varpropto   -   \ \ log   p ( x   |   z )     $ $           对于 实 数值 $ ( x   \ \ in   \ \ mathbb { R } ^ d ,   X | z   \ \ sim   \ \ mathcal { N } ( z ,   \ \ sigma ^ 2   I ) $ ， 那么 重构 误差 对应 于均方 误差 $ ( L ( x ,   z )   =   C ( \ \ sigma ^ 2 ) | | x - z | | ^ 2 ) $ 。       对于 二进制 变量   $ ( x   \ \ in   \ \ {   0 ,   1   \ \ } ,   X | z   \ \ sim   \ \ mathcal { B } ( z )   ) $ ， 那么 重构 误差 对应 于 交叉 熵 损失 函数 。           常用 的 两种 形式 ： 纺射 + sigmoid   Encoder ； 纺射   Decoder   +   均方 误差 ，   纺射 + sigmoid   Decoder   +   交叉 熵 损失 函数       autoenoder   的 训练 最小化 重构 误差 ， 即 优化 下列 问题 ：       $ $     \ \ arg   \ \ min _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   L ( X ,   Z = g _ { \ \ theta ' } ( f _ { \ \ theta } ( X ) ) )     $ $       等价 于       $ $     \ \ arg   \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   \ \ log ( p ( X |   Y = f _ { \ \ theta } ( X ) ;   \ \ theta ' ) )     $ $       这 表明 ， 我们 是 在 最大化 X 和 Y 的 交互 信息量 的 下界 ！           training   an   autoencoder   to   minimize   reconstruction   error   amounts     to   maximizing   a   lower   bound   on   the   mutual   information   between   input   X   and   learnt   representation   Y           但是 ， 简单 地 保留 原有 信息 是 不够 的 ！ 比如 简单 地 将 Y 设置 为 X ， 但是 这个 并 没什么 卵用 ！     如果 Y 的 维数 不少 于 X ， 那么 学 一个 单位 映射 就 可以 最大 限度 地 保留 X 中 的 信息 ！     传统 的 autoencoder 方法 采用 不 完备 的 表达 $ ( d '   & lt ;   d ) $ 。     降维后 的 Y 相当于 X 的 有损压缩 表达 。     当 采用 纺射 变换 做 编码 和 解码 ， 而 没有 非线性 变化 ， 那么 就是 PCA ！     但是 引入 非线性 变化 后 ， 将 能够 学到 不 一样 的 特征 表达 ！           The   use   of   “ tied   weights ”   can   also   change   the   solution :   forcing   encoder   and   decoder   matrices   to     be   symmetric   and   thus   have   the   same   scale   can   make   it   harder   for   the   encoder   to   stay   in   the   linear     regime   of   its   nonlinearity   without   paying   a   high   price   in   reconstruction   error .           另外 ， 也 可以 添加 其他 约束 ， 而 不是 更 低 的 维度 。     例如 通过 添加 稀疏 约束 ， 可以 采用 过 完备 的 维度 ， 即 比 输入 更大 的 维度 。       稀疏 表达 ， 稀疏 编码 ： Olshausen   and   Field   ( 1996 )   on   sparse   coding .     稀疏 自 编码 （ A   sparse   over - complete   representations ） ： sparse   representations   ( Ranzato   et   al . ,   2007 ,   2008 ) .       Denoise   准则       目标 ： 还原 部分 腐蚀 的 输入 ， 即 降噪 ！ Denoising       一个 好 的 表达 应该 是 能够 鲁棒 地 表达 腐蚀 后 的 输入 ， 可以 帮助 恢复 任务 ！       输入 加入 噪声 ： 高斯 噪声 （ 连续变量 ） ， 椒盐 噪声 ， 马赛克 噪声 。       几何 解释 ：     流形 学习 。       Variational   autoencoders   论文 导读       论文 ： Auto - Encoding   Variational   Bayes ,   Diederik   P .   Kingma ,   Max   Welling ,   2014               N 个 iid 样本 $ ( x ^ { ( i ) } ) $ ， 连续 或 离散 值 。 假定 这些 数据 从 一些 随机 过程 产生 ！           从 先验 分布 $ ( p _ { \ \ theta ^ *   } ( z ) ) $ 产生 隐 变量 $ ( z ) $ 。       从 条件 概率 $ ( p _ { \ \ theta ^ *   } ( x | z ) ) $ 产生 $ ( x ) $ 。           假定 先验 分布 和 条件 分布 无限 可微 ！     难点 ：           边际 分布 $ ( p ( x ) ) $ 需要 计算 一个 积分 ， 对于 神经网络 等 复杂 模型 ， 难以 求导 ！ EM   算法 失效 ；       大 数据 集 ， batch 优化 没有 效率 ， 需要 随机 梯度 之类 的 优化           工作 ：           有效 地 近似 地 实现   ML   和   MAP 估计 参数 $ ( \ \ theta ) $       给定 x ， 推导 z ， 有效 的 近似算法       有效 的 估计 x 的 边际 分布           建立 识别 模型 $ ( q _ { \ \ phi } ( z | x ) ) $ ， 近似 $ ( p _ { \ \ theta } ( z | x ) ) $ ， 编码 角度 来看 ， 就是 一个 编码器 ！     而   $ ( p _ { \ \ theta } ( x | z ) ) $   作为 解码器 ！       变 分界   variational   bound       边际 分布 的 对数 似然 函数 为       $ $     \ \ log   p ( x ^ { ( 1 ) } ,   ... ,   x ^ { ( N ) } )   =   \ \ sum _ i   \ \ log   p _ { \ \ theta }   ( x ^ { ( i ) } )     $ $       而 其中 每一项 可以 改写 为       $ $     \ \ log   p _ { \ \ theta }   ( x ^ { ( i ) } )   =   D _ { KL } ( q _ { \ \ phi } ( z | x ^ { ( i ) } )   | |   p _ { \ \ theta }   ( x ^ { ( i ) } ) )   \ \ \ \             +   \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )             \ \ \ \     \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )     =   \ \ mathbb { E } _ { q _ { \ \ phi ( z | x ) } } [ - \ \ log   q _ { \ \ phi } ( z | x )   +   \ \ log   p _ { \ \ theta } ( x ,   z ) ]   \ \ \ \             =   -   D _ { KL } ( q _ { \ \ phi } ( z | x ^ { ( i ) } )   | |   p _ { \ \ theta } ( z ) )   +   \ \ mathbb { E } _ { q _ { \ \ phi ( z | x ^ { ( i ) } } }   \ \ left [   \ \ log   p _ { \ \ theta } ( x ^ { ( i ) }   |   z )   \ \ right ] }     $ $       对数 似然 函数 第一项 是 近似 误差 ， 第二项 是 近似 之后 的 似然 函数 ， 或者 数据 i 的 边际 对数 似然 函数 下界 ！     第二项 可以 写为 一个 KL 距离 和 一个 期望 ， 前者 可以 通过 解析 积分 计算 ， 后者 要 采用 近似 估计 ！     而 通常 的   Monte   Carlo   梯度 估计 在 这个 问题 上 方差 太 大 ， 不 适用 与 这里 ！       The   SGVB   estimator   and   AEVB   algorithm       分布 $ ( \ \ tilde { z }   \ \ sim   q _ { \ \ phi ( z | x ^ { ( i ) } } ) $ 通过 一个 可微 的 变换 $ ( g _ { \ \ phi } ( \ \ epsilon ,   x ) ) $ ，     从 一个 noise 变量 $ ( \ \ epsilon   \ \ sim   p ( \ \ epsilon ) ) $ 采样 得到 。       $ $     \ \ mathbb { E } _ { q _ { \ \ phi } ( z | x ^ { ( i ) } ) } [ f ( z ) ]   \ \ approx   \ \ frac { 1 } { L }   \ \ sum _ { l = 1 } ^ L   f ( g _ { \ \ phi } ( \ \ epsilon ^ { ( l ) } ,   x ^ { ( i ) } ) ) ,   \ \ \ \     \ \ epsilon ^ { ( l ) }   \ \ sim   p ( \ \ epsilon )     $ $       mini - batch 方法 ：       $ $     \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   X )   \ \ approx   \ \ tilde { \ \ mathcal { L } } ^ M ( \ \ theta ,   \ \ phi ;   x ^ M )   \ \ \ \             =   \ \ frac { N } { M }   \ \ sum _ { i = 1 } ^ M   \ \ tilde { \ \ mathcal { L } } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )     $ $       $ ( { X ^ M } ) $   是 随机 从 全部 数据 集 采样 的 M 个 数据 。       auto - encoder 角度 ： 似然 函数 的 第一项 相当于 正则 ， 第二项 是 重构 误差       例子 ： 变分 自 编码       用 多层 感知器 从 输入 学习 到 两个 参数 向量 $ ( \ \ mu ,   \ \ sigma ) $ ，     隐 变量 通过 随机 采样 得到 $ ( z   \ \ sim   \ \ mathcal { N } ( z ;   \ \ mu ,   \ \ sigma ) ) $ 。       解码器 和 编码器 一样 ， 从应 变量 z 通过 多层 感知器 学习 到 两个 参数 $ ( \ \ mu ' ,   \ \ sigma ' ) $ ，     重构 变量 $ ( x '   \ \ sim   \ \ mathcal { N } ( x ' ;   \ \ mu ' ,   \ \ sigma ' ) ) $ 得到 。     对于 贝努利 分布 ， 参数 只有 一个 均值 。       Stacked   What - Where   Auto - encoders       论文 导读 ： Stacked   What - Where   Auto - encoders ，   Junbo   Zhao ,   Michael   Mathieu ,   Ross   Goroshin ,     Yann   LeCun   ,   ICLR   2016 .       What :   就是 polling 后 得到 的 max 值 ， 而 where 是 最大值 所在 的 位置 信息 ， 用来 帮助 解 卷积 器 重构 ！       文章 的 方法 将 编码器 学习 和 监督 学习 进行 联合 训练 学习 ， 作者 认为 （ 深度 比较 深时 ？ ） 用 自 编码 初始化 的 参数 所 携带 的 信息 ，     会 在 调优 的 时候 丢失 ， 导致 预 训练 没 啥 乱 用 ！ 而 解决 的 方法 就是 进行 联合 训练 。     此时 ， 重构 误差 相当于 一种 正则 ！     目标 函数 为 ：       $ $     L   =   L _ { NLL }   +   \ \ lambda _ { L2rec }   L _ { L2rec }   +   \ \ lambda _ { L2M }   L _ { L2M }     $ $       其中 NLL 代表 监督 学习 的 损失 函数 ， 负 对数 似然 函数 ， 对 回归 问题 是 L2 损失 ， 分类 问题 是 交叉 熵 。     重构 损失 函数 为 L2 损失 函数 。 L2rec   代表 输入 和 输出 的 重构 样本 的 重构 误差 ， L2M   代表 中间 编码器 输入 特征 和 解码器 输出 特征 的 重构 误差 。               实现 监督 学习 ， 无 监督 学习 ， 半 监督 学习 的 统一 框架 ！       软 最大值 max 和 argmax       Ross   Goroshin ,   Michael   Mathieu ,   and   Yann   LeCun .   Learning   to   linearize   under   uncertainty .   arXiv   preprint   arXiv : 1506.03011 ,   2015 .               文章 中 ， 取 $ ( \ \ beta = 100 ) $ ！       where 的 重要性               对比 采用 where 信息 做 unpooling 的 重构 结果 和 直接 copy 的 upsampling 的 结果 ， 可以 看出 where 信息 对 重构 输入 至关重要 ！     想想 也 能 想到 啊 ！ 肯定 重要 啊 ！       what 学习 到 的 具有 平移 不变性 ！       结论               从 论文 中 的 结果 来看 ， 添加 重构 误差 项 还是 挺 重要 的 ！     如果 没有 重构 误差 项 ， 错误率 会 增加 ！     训练 中 也 可以 加入   droupout   正则 化 方法 ！       Reference           Hinton ,   G .   E .   and   Salakhutdinov ,   R .   R .   Reducing   the   dimensionality   of   data   with   neural   networks .   Science   2006 .       2010 ,   Pascal   Vincent ,   Yoshua   Bengio ,   Stacked   Denoising   Autoencoders :   Learning   Useful   Representations   in   a   Deep   Network   with   a   Local   Denoising   Criterion .           近期 进展           Richard   Socher ,   Jeffrey   Pennington ,   Eric   H .   Huang ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .     Semi - supervised   recursive   autoencoders   for   predicting   sentiment   distributions .   In     EMNLP ,   2011 .       Variational   autoencoders      ", "tags": "machine-learning", "url": "/wiki/machine-learning/auto-encoder.html"},
      
      
      {"title": "语义分割", "text": "    Table   of   Contents           关于           Learning   Hierarchical   Features   for   Scene   Labeling                 关于       用 卷积 网络 做 图像 的 语义 分割 ： 将 图像 中 的 每 一个 像素 标注 到 其 所属 对象 。       Learning   Hierarchical   Features   for   Scene   Labeling  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn-segment.html"},
      
      
      {"title": "语音识别技术", "text": "    Table   of   Contents           基于 HMM 的 语音 识别 技术           参考                 基于 HMM 的 语音 识别 技术       主要 参考文献 [ 1 ]               首先 ， 语音 信号 被 分帧 ( 一般 10ms ) 提取 特征 ， 比如   MFCC   特征 ， 得到 一系列 的 特征向量 序列   $ ( Y _ { 1 : T }   =   y _ 1 , ... , y _ T ) $ , 这里 $ ( y _ i ) $ 都 代表 一帧 的 语音 特征 。 语音 识别器 寻找 一个 最佳 词 序列   $ ( w _ { 1 : L }   =   w _ 1 , ... , w _ L ) $ 进行 解码 ， 即 最大化 后验 概率   $ ( P ( w | Y ) ) $ 。     后验 概率 建模 通常 比较 困难 （ 现在 可以 直接 用 深度 学习 建模 啦 ） ， 所以   HMM   的 语音 识别 通过 条件 概率 建模 ，       参考           The   Application   of   Hidden   Markov   Models   in   Speech   Recognition ,   Mark   Gales ,   Steve   Young ,   Cambridge   University ,   2008      ", "tags": "machine-learning", "url": "/wiki/machine-learning/speech-recognition.html"},
      
      
      {"title": "近似最近邻搜索", "text": "    Table   of   Contents           综述           Metric   Tree ，   Spill   Tree                   大规模 最近 邻 搜索           partitioning   trees                         综述       论文 ： An   Investigation   of   Practical   Approximate   Nearest   Neighbor   Algorithms ， Ting   Liu ,   Andrew   W .   Moore ,   Alexander   Gray   and   Ke   Yang ， 2004           最近 邻 搜索 方案 ：       Voronoi   diagrams ： 1 - 2 维 ， F .   P .   Preparata   and   M .   Shamos .   Computational   Geometry .   Springer - Verlag ,       kd 树 、 metric   trees 、 ball - trees ： 中等 维度 10s       J .   H .   Friedman ,   J .   L .   Bentley ,   and   R .   A .   Finkel .   An   algorithm   for   finding   best   matches   in   loga -   rithmic   expected   time .   ACM   Transactions   on   Mathematical   Software ,   3 ( 3 ) : 209 – 226 ,   September   1977 .       J .   K .   Uhlmann .   Satisfying   general   proximity / similarity   queries   with   metric   trees .   Information   Processing   Letters ,   40 : 175 – 179 ,   1991 .       S .   M .   Omohundro .   Efficient   Algorithms   with   Neural   Network   Behaviour .   Journal   of   Complex   Systems ,   1 ( 2 ) : 273 – 347 ,   1987 .                       $ ( 1 + \ \ epsilon ) $ - k 最近 邻 搜索 ： 返回 的 点 最大 距离 不 超过 第 k 个 近邻 距离 的   $ ( 1 + \ \ epsilon ) $   倍 ！           Metric   Tree ，   Spill   Tree           metric   tree   将 样本 按照 二叉树 结构 保存 ， 根 节点 代表 所有 的 样本 ， 它 的 两个 子 节点 将 样本 分割 成 不想 交 的 两 部分 ；     用   v   代表 节点 ， N ( v ) 代表 v 节点 的 样本 ， 左右 子 节点 用   v . lc   v . rc   表示       分割 ： 对于 节点 v ， 找到 两个   pivot   点   v . lpv   v . rpv ， 使得 这 两个 节点 的 距离 是 集合 的 距离 ， 即   $ ( | | v . lpv   -   v . rpv | |   =   \ \ max _ { p1 , p2   \ \ in   N ( v ) }   | | p1   -   p2 | | ) $ ， 最优   pivot   寻找 是 二次 复杂度 ， 可以 近似 用 线性 复杂度 找   pivot ：       先 随机 找 一个点 p       找到   N ( v )   中距 p 最远 的 点 作为   v . lpv       再 找 距 v . lpv   最远 的 点 作为   v . rpv               找到 两个   pivot   点后 ， 然后 将 所有 点点 投影 到 这 两个 点 的 连线 上   $ ( u   =   v . rpv   -   v . lpv ) $ ， 找到 投影 的 中值   A   作为 分割 点 ， 投影 小于 A 的 分到 左子 树 ， 大于 A 的 分到 右子 树 ； 为 计算 效率 计 ， 可以 直接 用 中点 代替 ， 即   $ ( 1 / 2 ( v . rpv   -   v . lpv ) ) $       每 一个 节点   v   保留   N ( v )   的 覆盖 超球 信息 ， 球心   v . center ,   半径   v . r .           搜索 （ MT - DFS ） ： 深度 优先 搜索 ， 如果 待 查找 的 q 投影 在 A 左边 ， 先找 左子 树 ， 反之 先找 右子 树 ； 保留 k 个 已 找到 的 最近 邻 ， 设 这些 点 的 距 q 最大 距离 为 r ； 如果 节点   v   的 所有 点 距离 q 都 大于 r 就 可以 减枝 ， 不再 找 v 和 v 的 子树 。 可以 通过 条件   $ ( | | v . center   -   q | |   -   v . r   \ \ ge   r ) $   判断 ！               metric   tree   的 计算 通过 减枝 减少 寻找 数目 ， 但是 如果 无法 减枝 的 节点 ， 需要 不断 回溯 ， 使得 查找 性能 不高 。           spill   tree   则 放弃 精确 查找 ， 不 回溯 提高 性能 。 但是 精度 将 难以 接受 。       提高 精度 ： 分裂 的 时候 两个 子树 可以 有 交集 ， 相交 部分 为   $ ( \ \ tao ) $ ,   overlapping   buffer .   通过 这种 方法 提高 精度       混合 搜索 ： $ ( \ \ tao ) $   的 引入 带来 新 的 问题 ， 可能 某些 节点 重叠 部分 太 多 ， 导致 左右 子树 包含 了 全部 数据 ！ 为此 ， 可以 设定 一个 阈值   $ ( \ \ rho & lt ; 1 ) $ （ 典型值 70 % ） ， 如果 任何 一个 子 节点 包含 超过 这个 比率 的 样本 ， 则 对 这个 节点 不 进行   spill 分割 ， 而 进行 常规 的 不 交叠 分割 ， 并 标记 为   nonoverlapping   节点 ， 其他 的 节点 标记 为 overlapping 节点 。 搜索 的 时候 ， 只 对   nonoverlapping   节点 回溯 ！       维度 超过 30 后 ， 速度 就 会 变得 很 慢 ！ 通过 随机 投影 到 一个 低 维空间 后 ， 再 利用 混合 搜索 ！ 随机 投影 带来 的 精度 损失 ， 可以 通过 多次 投影 找回 ！         Johnson - Lindenstrauss   定理   ： W .   Johnson   and   J .   Lindenstrauss .   Extensions   of   lipschitz   maps   into   a   hilbert   space .   Contemp .   Math . ,   26 : 189 – 206 ,   1984 .       LSH ： A .   Gionis ,   P .   Indyk ,   and   R .   Motwani .   Similarity   Search   in   High   Dimensions   via   Hashing .   In   Proc   25th   VLDB   Conference ,   1999 .           大规模 最近 邻 搜索       Muja   M ,   Lowe   D   G .   Scalable   nearest   neighbor   algorithms   for   high   dimensional   data [ J ] .   IEEE   Transactions   on   Pattern   Analysis   and   Machine   Intelligence ,   2014 ,   36 ( 11 ) :   2227 - 2240 .           三类 最近 邻 搜索 方法 ：       partitioning   trees       hashing       neighboring   graph                   partitioning   trees           kd - tree ： 低 维空间 高效 ， 但是 维度 高 了 效果 急剧下降 ！       多个   randomized   k - d   trees      ", "tags": "machine-learning", "url": "/wiki/machine-learning/ann.html"},
      
      
      
      
      
        
      
      {"title": "随机微分方程", "text": "    Table   of   Contents           关于           数学 基础           概率 空间 、 随机变量 、 随机 过程                         关于       随机 微分方程 的 读书笔记 .   An   Introduction   with   Applications .   6ed .       数学 基础       概率 空间 、 随机变量 、 随机 过程           随机变量 是 联系 事件 域 与 欧式 空间 的 桥梁 。   $ $ X :   \ \ Omega   \ \ rightarrow   R ^ n   $ $       对 欧时 空间 的 积分 定义 在 集合 测度 上 ， 对 随机变量 的 积分 定义 在 概率 测度 上 。           $ $     \ \ int _ { R ^ n }   f ( x )   dx   \ \ \ \     \ \ int _ { w   \ \ in   \ \ Omega }   f ( X ( w ) )   dP ( w )   =   \ \ int _ { R ^ n }   f ( x ) d \ \ mu _ X ( x )     $ $       $ $ dx $ $ 是 几何 测度 ， $ $ d \ \ mu _ X ( x ) $ $ 是 概率 测度 。           利用 对 随机变量 的 积分 定义 ， 可以 构造 出 Lp 空间 、 巴拿赫 空间 、 希尔伯特 空间 。      ", "tags": "math", "url": "/wiki/math/sde.html"},
      
      
      
      
      
        
      
      {"title": "毛笔字相关知识", "text": "    Table   of   Contents           关于           笔法                 关于       学习 些 毛笔字 的 一些 知识点 记录 。       笔法           起笔 — — 也 叫 落笔 、 发笔 。 即 毛笔 的 笔尖 接触 纸面 的 霎间 。       行笔 — — 指 毛笔 在 纸 上 的 运行 。 行笔 有 中锋 、 偏锋 、 侧锋 三种 情况 。       收笔 — — 指 毛笔 的 笔尖 离开 纸面 的 霎间 。           起笔 和 收笔 均 有 两种 情况 ， 一种 叫藏锋 ， 一种 叫 露锋 。     -   藏锋 — — 指 起笔 和 收笔 的 笔锋 不 显露出来 ， 书写 时 将 笔锋 隐藏 在 笔画 里面 。 藏锋 起笔 ， 一般 用逆锋 的 方法 ， 也 就是 向 运行 相反 的 方向 逆入 起笔 ， 使 笔锋 藏 在 笔画 内 。 藏锋 收笔 ， 一般 用 回锋 的 方法 使 笔锋 藏于 笔画 内 。 这种 藏 蜂起 笔 和 收笔 的 方法 ， 不露锋芒 ， 比较 含蓄 。 给人以 绵 和 遒润 、 沉着 涵蕴 的 感觉 。     -   露锋 — — 抬起 笔 和 收笔 的 笔锋 显露 在 笔画 之外 ， 直 下 起笔 或 直接 出锋 。 这种 起笔 、 收笔 的 方法 ， 锋芒毕露 ， 给人以 精神 外耀 。 爽利 挺拔 的 感觉 。     -   提笔 — — 毛笔 在 行进 过程 中 轻轻 向上 提 ， 使 笔画 变细 ， 叫 提笔 ( 见图 9 ) 。 但 提笔 的 程度 有个 限制 ， 那 就是 笔锋 不能 离开 纸面 ， 因为 离开 纸面 就 成 收笔 了 。     -   按笔 — — 毛笔 在 行进 过程 中 轻轻 向下 按 ， 使 笔画 变粗 ， 叫 按笔 ( 见图 9 ) 。     -   疾笔 和 涩 笔 — — 疾笔 是 指 运行 速度 较 快 。 涩 笔 是 指 运行 速度 较慢 。 一般来说 ， 疾笔 所 书写 的 笔画 容易 光洁 秀劲 ， 涩 笔 所 书写 的 笔画 易于 毛涩 凝滞 。 书写 时 快慢 要 得当 。 过快 ， 笔画 浮滑 、 乏力 ; 过 慢 ， 笔致 和 形体 易于 呆 痴 。     -   转 与 折 — — 折 ， 是 指 毛笔 在 运行 过程 中 的 某 一点 上 突然 改变方向 ( 见图 10 ) 。 转 与 折 的 区别 在于 转是 圆弧形 的 ， 没有 折点 ; 折是 有 棱角 ， 有 折点 。 如 “ 礼 ” 字 ， 右边 坚折 横 钩 转弯 的 地方 就是 “ 转 ” ， 因是 慢慢 地弯 过来 的 圆弧形 ， 叫 圆转 。 再 如 “ 口 ” 字 ， 横折 竖 的 转弯处 是 方 的     -   方笔 和 圆笔 — — 方笔 指 笔画 起笔 的 地方 呈 方形 或 菱形 ; 圆笔 指 笔画 起笔 的 地方 呈圆形 。 图 11 中 的 “ 二 ” 字 ， 上面 一根 起笔 的 地方 是 方形 的 ， 叫方笔 ; 下面 一横 起笔 的 地方 是 圆形 的 ， 叫 圆笔 。 方笔 有 锋芒 、 有 棱角 ， 容易 见 骨力 ， 显得 刚健 果断 ; 圆笔 没有 锋芒 ， 不露圭角 ， 骨力 内涵 ， 显得 含蓄 柔韧 。 一般来说 ， 楷书 笔画 的 起笔 要 有 方有圆 ， 方圆 结合 ， 这样 才能 显得 丰富 多变 。       源自 网络     http : / / www . ixueyi . com / shuhua / 238589 . html    ", "tags": "other", "url": "/wiki/other/maobi.html"},
      
      
      
      
      
        
      
      {"title": "BB84协议", "text": "    Table   of   Contents           BB84 协议                 BB84 协议       维基百科 链接     https : / / en . wikipedia . org / wiki / BB84         指 1984 年 ，   Charles   Bennett   and   Gilles   Brassard   提出 的 量子 秘钥 分发 协议 ！  ", "tags": "phys", "url": "/wiki/phys/bb84.html"},
      
      
      
      
      
        
      
      {"title": "PMML 预测模型教程", "text": "    Table   of   Contents           什么 是 PMML           使用 PMML 发布 预测 模型           PMML 文件 分析           基本 框架 分析           模型 分析           MiningField           Segmentation           Output   & amp ;   Target                           参考                 什么 是 PMML       PMML   是 一种 基于 XML 的 标准 语言 ， 用于 表达 数据挖掘 模型 ， 可以 用来 在 不同 的 应用程序 中 交换 模型 。     一种 非常 有用 的 应用 场景 是 在 生产 环境 中 部署 用 各种 建模 工具 训练 出来 的 模型 。     目前 最新 的 标准 是 4.3     http : / / dmg . org / pmml / pmml - v4 - 3 . html   。       PMML   文件 的 结构 遵从 了 用于 构建 预测 解决方案 的 常用 步骤 ， 包括 ：           数据 词典 ， 这是 一种 数据分析 阶段 的 产品 ， 可以 识别 和 定义 哪些 输入 数据字 段 对于 解决 眼前 的 问题 是 最 有用 的 。 这 可以 包括 数值 、 顺序 和 分类 字 段 。       挖掘 架构 ， 定义 了 处理 缺少 值 和 离群 值 的 策略 。 这 非常 有用 ， 因为 通常 情况 ， 当 将 模型 应用 于 实践 时 ， 所 需 的 输入 数据字 段 可能 为空 或者 被误 呈现 。       数据 转换 ， 定义 了 将 原始 输入 数据 预处理 至 派生 字段 所 需 的 计算 。 派生 字 段 （ 有时 也 称为 特征 检测器 ） 对 输入 字 段 进行 合并 或 修改 ， 以 获取 更 多 相关 信息 。 例如 ， 为了 预测 停车 所 需 的 制动 压力 ， 一个 预测 模型 可能 将 室外 温度 和 水 的 存在 （ 是否 在 下雨 ？ ） 作为 原始数据 。 派生 字 段 可能 会 将 这 两个 字 段 结合 起来 ， 以 探测 路上 是否 结冰 。 然后 结冰 字段 被 作为 模型 的 直接 输入 来 预测 停车 所 需 的 制动 压力 。       模型 定义 ， 定义 了 用于 构建 模型 的 结构 和 参数 。 PMML   涵盖 了 多种 统计 技术 。 例如 ， 为了 呈现 一个 神经网络 ， 它 定义 了 所有 的 神经 层 和 神经元 之间 的 连接 权重 。 对于 一个 决策树 来说 ， 它 定义 了 所有 树 节点 及 简单 和 复合 谓语 。       输出 ， 定义 了 预期 模型 输出 。 对于 一个 分类 任务 来说 ， 输出 可以 包括 预测 类及 与 所有 可能 类 相关 的 概率 。       目标 ， 定义 了 应用 于 模型 输出 的 后处理 步骤 。 对于 一个 回归 任务 来说 ， 此 步骤 支持 将 输出 转变 为 人们 很 容易 就 可以 理解 的 分数 （ 预测 结果 ） 。       模型 解释 ， 定义 了 将 测试数据 传递 至 模型 时 获得 的 性能 度量 标准 （ 与 训练 数据 相对 ） 。 这些 度量 标准 包括 字 段 相关性 、 混淆 矩阵 、 增益 图及 接收者 操作 特征 （ ROC ） 曲线图 。       模型 验证 ， 定义 了 一个 包含 输入 数据 记录 和 预期 模型 输出 的 示例 集 。 这是 非常 重要 的 一个 步骤 ， 因为 在 应用程序 之间 移动 模型 时 ， 该 模型 需要 通过 匹配 测试 。 这样 就 可以 确保 ， 在 呈现 相同 的 输入 时 ， 新 系统 可以 生成 与 旧 系统 同样 的 输出 。 如果 实际 情况 是 这样的话 ， 一个 模型 将 被 认为 经过 了 验证 ， 且 随时 可 用于 实践 。           一个 通用 的 PMML 文件 结构 如下 ( 参考   http : / / dmg . org / pmml / v4 - 3 / GeneralStructure . html   ) ：               & lt ; ? xml   version = & quot ; 1.0 & quot ; ? & gt ;       & lt ; PMML       version =     & quot ; 4.3 & quot ;           xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 3 & quot ;           xmlns : xsi =     & quot ; http : / / www . w3 . org / 2001 / XMLSchema - instance & quot ;     & gt ;             & lt ; Header       copyright =     & quot ; Example . com & quot ;     / & gt ;           & lt ; DataDictionary & gt ;     ...     & lt ; / DataDictionary & gt ;           ...   a   model   ...       & lt ; / PMML & gt ;                 使用 PMML 发布 预测 模型       下面 以 目前 应用 广泛 的   XGBoost   模型 为例 ， 介绍 使用 PMML 发布 预测 模型 。       首先 ， 我们 需要 有 一个 XGBoost 模型 ， 为此 ， 可以 以 Iris 数据 集 训练 一个 简单 的 二 分类 模型 （ 只用 其中 的 两类 ） 。   然后 利用   XGBoost   训练 得到 模型 文件 。               import       xgboost       as       xgb       from       sklearn . datasets       import       load _ iris         iris       =       load _ iris     ( )       mask       =       iris     .     target       & lt ;       2       X       =       iris     .     data     [     mask     , : ]       y       =       iris     .     target     [     mask     ]         params       =       {               &# 39 ; objective &# 39 ;       :       &# 39 ; reg : logistic &# 39 ;     ,               &# 39 ; num _ round &# 39 ;       :       10     ,               &# 39 ; max _ depth &# 39 ;       :       3       }       dtrain       =       xgb     .     DMatrix     (     X     ,       label     =     y     )       evallist       =       [ (     dtrain     ,       &# 39 ; train &# 39 ;     ) ]       bst       =       xgb     .     train     (     params     ,       dtrain     ,       evals     =     evallist     )       bst     .     save _ model     (     &# 39 ; xgb . bin &# 39 ;     )                       [ 0 ]   train - rmse : 0.364576   [ 1 ]   train - rmse : 0.27088   [ 2 ]   train - rmse : 0.203626   [ 3 ]   train - rmse : 0.154579   [ 4 ]   train - rmse : 0.118482   [ 5 ]   train - rmse : 0.091745   [ 6 ]   train - rmse : 0.071832   [ 7 ]   train - rmse : 0.056919   [ 8 ]   train - rmse : 0.045683   [ 9 ]   train - rmse : 0.037156               然后 生成 一个 特征 映射 文件 ， 因为 模型 文件 中 没有 特征 名 ， 只有 特征 id 。               f       =       open     (     &# 39 ; fmap . txt &# 39 ;     ,       &# 39 ; w &# 39 ;     )       for       i     ,       fn       in       enumerate     (     iris     .     feature _ names     ) :               f     .     write     (     &# 39 ;     % d     \ \ t     % s     \ \ t     % s     \ \ n     &# 39 ;       %       (     i     ,       fn     ,       &# 39 ; q &# 39 ;     ) )       f     .     close     ( )                 利用   jpmml - xgboost   项目   https : / / github . com / jpmml / jpmml - xgboost   提供 的 工具 ， 进行 转换 。     你 也 可以 直接 下载 我 已经   编译 好 的 jar 包   。     然后 执行 下述 命令 ， 即可 得到 转换 后 的 PMML 文件   xgb . pmml . xml 。             ! java   - jar   converter - executable - 1.2 - SNAPSHOT . jar   - - model - input   xgb . bin     - - fmap - input   fmap . txt     - - pmml - output   xgb . pmml . xml               得到 PMML 文件 xgb . pmml . xml 后 ， 我们 就 可以 在 生产 环境 部署 了 。     PMML 模型 的 部署 可以 使用     https : / / github . com / jpmml / jpmml - evaluator     进行 部署 ，     可以 很 容易 应用 到 分布式 环境 ！       下面 是 一段 预测 的 代码 ：               InputStream       is       =       new       FileInputStream     (     & quot ; path - to - pmml - file & quot ;     ) ;       PMML       pmml       =       PMMLUtil     .     unmarshal     (     is     ) ;         / /   这里 的 LocatorTransformer 是 为了 将 模型 转换 为 可 序列化 的 对象 ， 如果 不 需要 在 分布式 环境 ( 如 Spark ) 使用 模型 ， 就 可以 不用 转换       LocatorTransformer       lt       =       new       LocatorTransformer     ( ) ;       lt     .     applyTo     (     pmml     ) ;         Evaluator       evaluator       =       ModelEvaluatorFactory     .     newInstance     ( ) .     newModelEvaluator     (     pmml     ) ;         / /   预测       List     & lt ;     InputField     & gt ;       fields       =       evaluator     .     getActiveFields     ( ) ;         Map     & lt ;     FieldName     ,       Double     & gt ;       input       =       new       HashMap     & lt ; & gt ; ( ) ;       for     (     InputField       field       :       fields     ) {               input     .     put     (     field     .     getName     ( ) ,       1.2     ) ;       / / 对 每 一个 特征 指定 对应 的 值       }       Map     & lt ;     FieldName     ,       ? & gt ;       results       =       evaluator     .     evaluate     (     input     ) ;       List     & lt ;     TargetField     & gt ;       output       =       evaluator     .     getTargetFields     ( ) ;       Object       value       =       results     .     get     (     output     .     get     (     0     ) .     getName     ( ) ) ;                 PMML 文件 分析       基本 框架 分析       打开   xgb . pmml . xml   文件 ， 我们 可以 看到 一个 实际 可用 的 PMML 文件 结构 。               & lt ; ? xml   version = & quot ; 1.0 & quot ;   encoding = & quot ; UTF - 8 & quot ;   standalone = & quot ; yes & quot ; ? & gt ;       & lt ; PMML       xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 3 & quot ;       version =     & quot ; 4.3 & quot ;     & gt ;               & lt ; Header & gt ;                       & lt ; Application       name =     & quot ; JPMML - XGBoost & quot ;       version =     & quot ; 1.2 - SNAPSHOT & quot ;     / & gt ;                       & lt ; Timestamp & gt ;   2017 - 10 - 17T03 : 41 : 51Z   & lt ; / Timestamp & gt ;               & lt ; / Header & gt ;               & lt ; DataDictionary & gt ;                       & lt ; DataField       name =     & quot ; _ target & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; sepal   width   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; petal   length   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; petal   width   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;               & lt ; / DataDictionary & gt ;               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                     ...             & lt ; / MiningModel & gt ;       & lt ; / PMML & gt ;                 PMML 文件 是 基于 XML 格式 的 文本文件 ， 有且 只有 一个 根 节点   PMML   。       PMML     根 节点 除了   xmlns   属性 外 ， 有且 只有 一个 属性     version   ， 它 的 值 表明 PMML 标准 的 版本 。         PMML     子 元素 有 两个 是 必须 的 ，   Header     和     DataDictionary   。         Header       头部 信息 ， 只 包含 说明 信息 ， 对 预测 逻辑 没有 影响 ， 通常 包括 ：             -   包含 的 属性                     -   copyright   版权                     -   description   描述 文本                     -   modelVersion   模型 版本             -   包含 的 子 元素                     -   Application   描述 生成 PMML 文件 的 软件 相关 信息 ， 本 例子 说明 这个 PMML 是 由     JPMML - XGBoost     软件 生成 的 ， 该软件 版本 是     1.2 - SNAPSHOT   。                     -   Timestamp   生成 的 时间 戳                     -   Annotation   可 选 ， 描述 模型 版本 更新 信息         DataDictionary     数据 字典 ， 描述 字 段 信息 ， 包括 模型 的 输入 字段 和 输出 字 段 ， 这里   _ target   是 输出 字 段 ， 其他 三个 是 输入 字 段 。 每 一个 字段 用     DataField     元素 描述   ref   。   DataField     有 三个 必须 的 属性 ：   name     字段 或 特征 名字 ，   optype     操作 类型 ，   dataType     数据类型 。     DataDictionary     只 负责 字段 的 定义 ， 对字 段 的 处理 比如 缺失 值 处理 应该 在 模型 的   MiningField     区域 定义 。         optype     是 操作 类型 ， 有 三个 可选值 ：   categorical     类别 变量 ， 只能 进行 相等 的 判断 ;     ordinal     序数 变量 还 可以 进行 顺序 比较 ;     continuous     只有 这个 操作 类型 才能 进行 算术 运算 ， 这种 类型 的 变量 应用 比较 多 。         dataType     是 数据类型 ， 大约 有 十几种 类型   [ ref ]   。         PMML     可选 的 元素 有 4 个 ， 分别 是 ：   ref   。 包括     string   ,     interger   ,     float   ,     double   ,     date     等 常见 的 数据类型 。             MiningBuildTask     可以 包含 任意 XML 值 ， 用于 表达 模型 训练 时 的 相关 信息 ， 对模型 预测 没有 影响 。         TransformationDictionary     变换 字典 ， 用于 定义 各种 变换 。         MODEL - ELEMENT     这 是 个 模型 元素 集合 ， 用来 表达 模型 的 参数 和 预测 逻辑 。 具体 使用 时 ， 可以 是 这个 集合 里面 任意 一种 元素 ， 在 这个 例子 里面 ， 用 得 就是     MiningModel     这个 元素 ， 还 可以 是   GeneralRegressionModel   等 18 个 元素 中 的 任意 一个 ， 可以 参看   链接   。         Extension     扩展 信息             MODEL - ELEMENT     大约 包括 18 个 不同 的 模型 ， 每 一个 模型 都 有 几个 相同 的 属性 。   functionName     用于 定义 该 模型 是 回归 、 分类 还是 聚类 等等 ， 是 必须 的 属性 。 PMML 里面 一共 定义 了 7 种 类型   ref   ， 常用 的 有 回归   regression   、 分类   classification   、 聚类   clustering   。 另外 两个 可选 的 属性 是 ：   modelName     和     algorithmName   ， 只 用于 提示 ， 对模型 预测 没有 实质性 影响 。       MODEL - ELEMENT     都 包含 了 这样 一些 元素 ：   MiningSchema   （ 必须 ） 、     Output   、     Targets   等 ， 这些 在 后面 的 章节 将会 详细 介绍 。       模型 分析       找到     MiningModel     区块 ， 可以 看到 这个 元素 的 主要 结构 如下 ( 非 主要 结构 已 被 省略 ) ：               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;               & lt ; MiningSchema & gt ;                       & lt ; MiningField       name =     & quot ; _ target & quot ;       usageType =     & quot ; target & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; sepal   width   ( cm ) & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; petal   width   ( cm ) & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; petal   length   ( cm ) & quot ;     / & gt ;               & lt ; / MiningSchema & gt ;               & lt ; Segmentation       multipleModelMethod =     & quot ; modelChain & quot ;     & gt ;                       & lt ; Segment       id =     & quot ; 1 & quot ;     & gt ;                               & lt ; True / & gt ;                               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                                     ...                             & lt ; / MiningModel & gt ;                       & lt ; / Segment & gt ;                       & lt ; Segment       id =     & quot ; 2 & quot ;     & gt ;                               & lt ; True / & gt ;                               & lt ; RegressionModel       functionName =     & quot ; regression & quot ;       normalizationMethod =     & quot ; logit & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                                     ...                             & lt ; / RegressionModel & gt ;                       & lt ; / Segment & gt ;               & lt ; / Segmentation & gt ;       & lt ; / MiningModel & gt ;                   MiningModel     实际上 是 一种 通用 的 模型 ， 通常 用于 模型 的 融合 。 它 包含 了 一个 特有 子 的 元素     Segmentation   ， 用于 融合 多个 模型 。 本文 的 例子 里面 顶层 的 模型 （ 即   MiningModel   ） 包含 了 两个 模型 。 多个 模型 的 融合 方式 是   modelChain   ， 即前 一个 模型 的 输出 作为 后 一个 模型 的 输入 。 融合 方式 由   Segmentation   的 属性   multipleModelMethod   指定 。 实际上 ， 因为 XGBoost 用 的 是 回归 树 ， 然后 将 所有 树 的 输出 结果 相加 得到 第一个 模型 的 输出 ； 第二个 模型 只是 对 第一个 模型 的 输出 做 了 一个 简单 的   logit   变换 ， 将 原始 值 转换 为 概率 值 。         MiningSchema     是 所有 模型 都 必须 有 的 子 元素 ， 包括 模型 内 的 子 模型 也 都 有 。 对于 所有 输入 这个 模型 的 数据 ， 都 必须 经过     MiningSchema   ， 这个 元素 里面 包含 了 这个 模型 用到 的 所有 字 段 ， 相比     DataDictionary   ，   MiningSchema   可以 为 每个 模型 定义 特有 的 一些 信息 ， 还 包括 缺失 值 异常 值 处理 等 特征 预处理 操作 。   ref         MiningField       每 一个 字段 由   MiningField   定义 ， 它 包含 以下 属性 ：             name     必须 ， 字段名 字         usageType     字 段 用途 ， 默认 是   active   即 用作 输入 特征 ， 值   target     表示 该字 段 是 监督 学习 模型 的 目标 ， 也 是 模型 预测 结果 。 其他 取值 参考   ref           optype     操作 类型 ， 一般 在 数据 字典 中 定义 ， 这里 可以 重载 这个 属性         outliers     异常 值 处理 方式 ：   asIs       asMissingValues       asExtremeValues   （ 极端 值 通过 属性   lowValue   和   highValue   定义 ）         missingValueReplacement     缺失 值 替换 值 ， 如果 有 这个 属性 ， 在 进入 模型 前先用 这个 值 替换         missingValueTreatment     只是 提示 替换 的 值 的 来源 ， 对 PMML 预测 没有 影响                   & lt ; MiningField       name =     & quot ; foo & quot ;       missingValueReplacement =     & quot ; 3.14 & quot ;       missingValueTreatment =     & quot ; asMean & quot ;     / & gt ;                 Segmentation       多个 模型 用     Segmentation     来 组织 ， 每 一个 模型 都 被 包括 在子 元素     Segment     中 。     Segmentation     只有 一个 属性     multipleModelMethod     用来 表明 多个 模型 的 组合 方式 ， 可以 取得 值 如下   ref               modelChain     模型 链 ， Predicates 的 值 为 TRUE 的 模型 按照 顺序 打分 。 模型 的 输出 字 段     OutputFields     里面 的 字 段 ， 可以 作为 后续 模型 的 输入 。         sum     求和 ， 将 多个 模型 的 预测值 求和 。         average     平均 ， 将 多个 模型 的 预测值 平均 。         majorityVote     投票         weightedMajorityVote   ,     weightedAverage   ,     max   ,     median             每 一个   Segment   包含 属性   id   和   weight   ，   weight   可 选 属性 ， 在 加权 融合 的 情况 下才 有用 。 每 一个   Segment   包含 的 子 元素 有       PREDICATE       和       MODEL - ELEMENT     。 这个 例子 中 的   PREDICATE   是   & lt ; True / & gt ;   ， 表明 使用 这个 模型 计算 预测值 ， 如果 为   & lt ; False / & gt ;   则 不 使用 。 模型 元素 有 两个 ， 一个 是   MiningModel   ， 另 一个 是   TreeModel   。   TreeModel   我们 在 后面 介绍 。       Output   & amp ;   Target           Output       和       Target       都 可以 用于 定义 模型 的 输出 。         Target     可以 对 输出 结果 做 简单 的 线性变换 ： $ ( f ( x )   =   10   +   3.14   x ) $               & lt ; Targets & gt ;           & lt ; Target       field =     & quot ; amount & quot ;       rescaleConstant =     & quot ; 10 & quot ;       rescaleFactor =     & quot ; 3.14 & quot ;       min =     & quot ; - 10 & quot ;       max =     & quot ; 10.5 & quot ;       castInteger =     & quot ; round & quot ;     / & gt ;       & lt ; / Targets & gt ;                   Output     则 可以 应用 更 复杂 的 变换 ，   OutputField   的   feature   属性 ， 可以 输出 很多 有用 的 信息   ref   ， 例如 预测 原始 值 ， 决策 树叶子 结点 的 ID 值 等等 。     下面 是 一个 对模型 输出 结果 做 变换 $ ( f ( x )   =   10 ^ { x   +   0.5 }   -   1 ) $ 的 例子 ， 这个 例子 来自 于用 XGBoost 对 log1p 后 的 值 做 回归 ， 因为 多个 决策树 的 结果 加和后 ， 要 乘以 0.5 ， 所以 反 变换 就是 上面 这个 表达式 了 。               & lt ; Output & gt ;               & lt ; OutputField       name =     & quot ; rawResult & quot ;       dataType =     & quot ; double & quot ;       feature =     & quot ; predictedValue & quot ;       / & gt ;               & lt ; OutputField       name =     & quot ; _ target & quot ;       dataType =     & quot ; double & quot ;       feature =     & quot ; transformedValue & quot ;       & gt ;                       & lt ; ! - -     pow ( 10 ,   s + 0.5 )   -   1   - - & gt ;                       & lt ; Apply       function =     & quot ; round & quot ;     & gt ;                               & lt ; Apply       function =     & quot ; - & quot ;     & gt ;                                       & lt ; Apply       function =     & quot ; pow & quot ;     & gt ;                                               & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   10.0   & lt ; / Constant & gt ;                                               & lt ; Apply       function =     & quot ; +& quot ;     & gt ;                                                       & lt ; FieldRef       field =     & quot ; rawResult & quot ;     & gt ; & lt ; / FieldRef & gt ;                                                       & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   0.5   & lt ; / Constant & gt ;                                               & lt ; / Apply & gt ;                                       & lt ; / Apply & gt ;                                       & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   1.0   & lt ; / Constant & gt ;                               & lt ; / Apply & gt ;                       & lt ; / Apply & gt ;               & lt ; / OutputField & gt ;       & lt ; / Output & gt ;                 PMML 内置 了 常用 的   数学 函数   ， 函数 的 应用 非常简单 ， 直接 创建 一个   Apply   元素 ， 并 指定 属性   function   为 函数 名 即可 ， 然后 将 参数 依次 作为 子 元素 。 参数 可以 是 常数   Constant   和 其他 字 段   FieldRef   ， 甚至 一个 新 的 函数 应用 结果   Apply   。 用户 也 可以 创建 自定义 函数 ， 将 函数 的 定义 放到     TransformationDictionary     中 ， 然后 就 可以 直接 引用 了 。       参考           https : / / www . ibm . com / developerworks / cn / opensource / ind - PMML1 / index . html       PMML - 4 _ 3 标准 文档     http : / / dmg . org / pmml / pmml - v4 - 3 . html        ", "tags": "pmml", "url": "/wiki/pmml/intro.html"},
      
      
      
      
      
        
      
      {"title": "ggplot绘图工具", "text": "    Table   of   Contents           关于                 关于       ggplot 本是 一个 R 绘图 工具包 ， 现在 在 python 中 也 可以 用 了 。  ", "tags": "python", "url": "/wiki/python/ggplot.html"},
      
      
      {"title": "IPython", "text": "    Table   of   Contents           关于           Display 模块                 关于       使用 IPython 多年 ， 但是 一直 没 系统地 学习 过 ， 现在 系统地 学 一下 ， 提升 效率 。       Display 模块  ", "tags": "python", "url": "/wiki/python/ipython.html"},
      
      
      {"title": "MAC上安装light gbm库", "text": "    Table   of   Contents           步骤                 步骤       参考   https : / / stackoverflow . com / questions / 44937698 / lightgbm - oserror - library - not - loaded               brew   install   cmake           brew   install   gcc   - - without - multilib           git   clone   - - recursive   https : / / github . com / Microsoft / LightGBM   ;   cd   LightGBM           mkdir   build   ;   cd   build           cmake   ..   - DCMAKE _ C _ COMPILER = / usr / local / Cellar / gcc / 6.2 . 0 / bin / gcc - 6     - DCMAKE _ CXX _ COMPILER = / usr / local / Cellar / gcc / 6.2 . 0 / bin / g ++ - 6     - DOpenMP _ C _ FLAGS = - fopemmp       注意 把 gcc 的 两个 路径 改成 你 机器 上 的 路径 ， DOpenMP _ C _ FLAGS 标记 是因为 cmake 脚本 没有 自动 找到 opemmp 的 路径 ， 手动 指定         make   - j           cd   python - packages           sudo   python   setup . py   install        ", "tags": "python", "url": "/wiki/python/light-gbm-install.html"},
      
      
      {"title": "Matplotlib 中文字体问题", "text": "    Table   of   Contents           问题 背景           问题 原因           解决 方法                 问题 背景       用   matplotlib   画图 时 ， 经常 遇到 中文 ( 非 ASCII ) 字体 乱码 问题 ， 导致 这些 字符 全部 变成   □ □ ！       问题 原因       找 不到 中文字体       解决 方法       首先 下载   msyh . ttf   即 微软 雅黑 字体 文件 ， 放到   matplotlib   的 字体 目录 里面 ， 或者 系统 字体 中 。     字体 目录 通过 如下 命令 获取 ， 不同 系统 不 一样 。               import       matplotlib       print     (     matplotlib     .     matplotlib _ fname     ( ) )                       / Library / Python / 2.7 / site - packages / matplotlib / mpl - data / matplotlibrc               / Library / Python / 2.7 / site - packages / matplotlib / mpl - data /   为 配置 目录 ， 在 该 目录 下   fonts / ttf /   目录 就是 存放 字体 文件 的 目录 。       最 简单 的 方法 是 在 运行 程序 前 ， 动态 设置               import       matplotlib       matplotlib     .     rcParams     [     u     &# 39 ; font . sans - serif &# 39 ;     ]       =       [     &# 39 ; Microsoft   YaHei &# 39 ;     ]       +       matplotlib     .     rcParams     [     u     &# 39 ; font . sans - serif &# 39 ;     ]         import       numpy       as       np       import       matplotlib . pyplot       as       plt       plt     .     plot     (     np     .     random     .     randn     (     100     ) )       plt     .     title     (     u     &# 39 ; 测试 雅黑 字体 &# 39 ;     )                 也 可以 通过 设置   matplotlibrc   文件 ， 编辑 该 文件 ， 编辑 下述 两项 ， 去掉 注释 并 将 值 设为 下述 值             font . family                   :   sans - serif     font . sans - serif             :   Microsoft   YaHei               可以 通过 下述 代码 查看 系统 支持 的 字体 ， MAC 默认 中文字体 是     STHeiti                 fm       =       matplotlib     .     font _ manager     .     FontManager     ( )       for       f       in       fm     .     ttflist     :               print       f     .     name     .     decode     (     &# 39 ; utf - 8 &# 39 ;     )                 注意 ， 要 使得 配置 生效 ， 还 需要 删除 字体 缓存 ！ MAC   中 字体 缓存 在     ~ / . matplotlib / fontList . cache     ， 删除 该 文件 即可 ！  ", "tags": "python", "url": "/wiki/python/matplotlib-chinese-font.html"},
      
      
      {"title": "Numba 加速", "text": "    Table   of   Contents               Numba 是 一个 用于 python 计算 加速 的 软件包 ， 通过 增加 Annotation 注释 的 方式 ， 不 需要 修改 原始 代码 ， 使用方便 。     下面 通过 一个 例子 展示 使用 方法 。               import       numpy       as       np       from       numba       import       jit         def       sum2d     (     arr     ) :               M     ,       N       =       arr     .     shape               result       =       0.0               for       i       in       range     (     M     ) :                       for       j       in       range     (     N     ) :                               result       + =       arr     [     i     ,     j     ]               return       result         @ jit       def       sum2d _ jit     (     arr     ) :               M     ,       N       =       arr     .     shape               result       =       0.0               for       i       in       range     (     M     ) :                       for       j       in       range     (     N     ) :                               result       + =       arr     [     i     ,     j     ]               return       result         arr       =       np     .     random     .     randn     (     10000     ,     10000     )                         %     time         sum2d     (     arr     )                       CPU   times :   user   20.4   s ,   sys :   34.1   ms ,   total :   20.4   s   Wall   time :   20.4   s                       %     time       sum2d _ jit     (     arr     )                       CPU   times :   user   122   ms ,   sys :   513   µ s ,   total :   122   ms   Wall   time :   123   ms                       %     time       np     .     sum     (     arr     )                       CPU   times :   user   77   ms ,   sys :   1.12   ms ,   total :   78.1   ms   Wall   time :   77.2   ms               通过 numba 优化 的 代码 可以 比 原始 python 代码 快 将近 两个 数量级 ， 和 numpy 的 性能 接近 ！  ", "tags": "python", "url": "/wiki/python/numba.html"},
      
      
      {"title": "Pandas 中的坑", "text": "    Table   of   Contents           Series   操作           DataFrame   的 index 操作           遍历 操作           函数 式 编程 方式 ， apply 方式 （ 个人 比较 推荐 这种 方式 ）           iteration   迭代           向 量化 string 操作                   DataFrame 的 修改           DataFrame   API           category   类型 数据处理           IO 操作           read _ csv                   可视化 plot           TIPS                 Series   操作           他 像 一个 数组 ， 你 可以 像 数组 那样 索引 ， 他 也 想 一个 字典 ， 你 可以 像 字典 那样 索引         . map ( )   ， 参数 可以 是 一个 函数 ， 也 可以 是 一个 字典 （ 或 Series ） ， 此时 就是 一个 转换 表 。           DataFrame   的   index   操作       索引 操作 多而杂 ， 现 总结 如下 。     参考     Indexing   and   Selecting   Data   。             . loc   基于 label ( 如果 只有 一个 索引 则 为行 的 index ， 行 优先 ) 的 索引 。       单个 标签 ， 如   a   ， 多个 标签 列表   [ ' a ' ,   ' b ' ,   ' c ' ]   。 注意 ， 如果 提供 两个 索引 ， 即 行 索引 加列 索引 ， 在 python 中 处理 为 一个 tuple 。       slice   object   with   label ，   ' a '   :   ' f '   .   这种 形式 的 索引 叫做   slice   object   。 例如     df . loc [ ' a ' : ' d ' ]         一个 boolean 数组 .     df . loc [ df . A   & gt ;   0.5 ]         一个 返回 上述 索引 的 单 参数 （ 该 参数 是   df   本身 ） 函数 .     df . loc [ lambda   x :   x . A   & gt ;   0.5   ]                             df1       =       pd     .     DataFrame     (     np     .     random     .     randn     (     6     ,       4     ) ,                                               index     =     list     (     &# 39 ; abcdef &# 39 ;     ) ,                                               columns     =     list     (     &# 39 ; ABCD &# 39 ;     ) )                 A           B           C           D       a           0.333368             0.953575             0.189191             0.186499       b           0.344776             0.940556             0.624198             0.278640       c           0.269827             0.449311             0.679678             0.769818       d           0.910729             0.024516             0.745065             0.399805       e           0.868005             0.822731             0.908870             0.376258       f           0.141232             0.983130             0.730339             0.782900         df     .     loc     [     &# 39 ; a &# 39 ;     ]       df     .     loc     [     &# 39 ; a &# 39 ;     ,       &# 39 ; B &# 39 ;     ]       df     .     loc     [ [     &# 39 ; a &# 39 ;     ,     &# 39 ; b &# 39 ;     ] ]       df     .     loc     [ [     &# 39 ; a &# 39 ;     ,     &# 39 ; b &# 39 ;     ] ,       [     &# 39 ; B &# 39 ;     ,     &# 39 ; C &# 39 ;     ] ]                           . iloc     则 是 基于 序号 的 索引 ( 还是 行 优先 ) ， 从 0 到   length   -   1   。           一个 整数 ，   df . iloc [ 5 ]   ,     df . iloc [ 5 , 3 ]         整数 列表 ，   df . iloc [ [ 2 , 5 , 3 ] ]   ,     df . iloc [ [ 2 , 5 , 3 ] ,   [ 3 , 2 ] ]         slice   object   with   int ，     df . iloc [ 1 : 3 ]   ,     df . iloc [ 1 : 3 ,   : 2 ]         一个 boolean 数组 ，     df . iloc [ list ( df . A & gt ; 0.5 ) ]   ， 注意 这里 如果 写成   df . iloc [ df . A   & gt ;   0.5 ]   是 有 问题 的 。 不 清楚 为啥 ， 可能 与   df . A   & gt ;   0.5   是 一个   Series   有关 ， 而   Series   在 迭代 的 时候 更 像   dict   。       一个 返回 上述 索引 的 单 参数 函数 。 与   . loc   类似 。                     . ix     则 相当于 上述 两个 之 和 ， 两种 index 都 能 处理 。             [ ]   或者   __ getitem __ ( )   和 上面 的 行 优先 相反 ， 他 是 列 优先 ， 而且 不能 实现 多维 索引 。 此外 ， 还 可以 传入 slice   object （ label 和 int 都 可以 ） 和   boolean 数组 来 筛选 特定 的 行       列名 label ，   df [ ' A ' ]         列名 列表 ，   df [ [ ' A ' , ' B ' ] ]         slice   object ，   df [ : 3 ]   ，   df [ ' a ' : ' c ' ]         boolean   array ，   df [ df . A & gt ; 0.5 ]                   . head   和   . tail   函数 访问 前 （ 后 ） 几行       以 属性 方式 访问 ， 对 DataFrame 是 列名 ， 对 Series 是 键 。 例如   df . A   ,     df . A . a           . at   ,   . iat   和   . get _ value   ， 获取 单个 值 ， 注意 两者 的 区别 。   df . at [ ' a ' , ' A ' ] ,     df . get _ value ( ' a ' , ' A ' )   。 他们 等价 于   df . loc [ ' a ' , ' A ' ]   。 这个 比 采用   [ ]   速度 要 快 ， 遍历 的 时候 推荐 用 这个 。           遍历 操作       函数 式 编程 方式 ， apply 方式 （ 个人 比较 推荐 这种 方式 ）           tablewise   函数     . pipe ( )   ，   . pipe ( f ,   args   ... )   将 DataFrame 作为 函数   f   的 第一个 参数 传 过去 ， 其他 参数 也 原样 传递 。                   def       f     (     x     ,       a     ) :               return       np     .     log     (     x     )       /       np     .     log     (     a     )       df     .     pipe     (     f     ,       3     )         #   等价 于   f ( df ,   3 )                     一行 一列 方式 应用 函数     . apply ( )   ， 用 参数   axis   指定 行 ( 取值 0 ) 或列 ( 取值 1 )                   df     .     apply     (     np     .     mean     )           #   对 每 一列 求 均值       df     .     apply     (     np     .     mean     ,       axis     =     1     )         #   对 每 一行 求 均值                     elementwise     . applymap ( )   ， 求 每 一个 元素 的 ID ，   df . applymap ( id )             iteration   迭代                 for   i   in   obj     方式 ， 对 不同 数据结构 不同             Series     :   代表 值         DataFrame     :   代表 列 label ， 即 列名         Panel     :   item   label                     . iteriems ( )   ， 对 DataFrame 相当于 对列 迭代 。             Series   :   ( index ,   value )         DataFrame     :   ( column ,   Series )         Panel     :   ( item ,   DataFrame )                     df . iterrow ( )   ， 对 DataFrame 的 每 一行 进行 迭代 ， 返回 一个 Tuple     ( index ,   Series )               df . itertuples ( )   ， 也 是 一行 一行 地 迭代 ， 返回 的 是 一个 namedtuple ， 通常 比   iterrow   快 ， 因为 不 需要 做 转换                   for       idx     ,       row       in       df     .     iterrows     ( ) :               print       idx     ,       row         for       row       in       df     .     itertuples     ( ) :               print       row         for       c     ,       col       in       df     .     iteritems     ( ) :               print       c     ,       col                       . dt   ，   对 Datetime 类型 的 Series 可以 通过 这个 对象 访问   hour ,   second ,   day   等值 。         还 可以 通过   strftime   格式化 时间                   s       =       pd     .     Series     (     pd     .     date _ range     (     &# 39 ; 20130101   09 : 10 : 12 &# 39 ;     ,       periods     =     4     ) )       s     .     dt     .     hour                     迭代 方式 通常 性能 较差 ， 可以 通过 几种 方式 进行 优化 。       将 运算 编程 向 量化 操作       采用   . apply   等 函数 式 编程 方式       将 操作 的 内 循环 等 耗费 时间 的 操作 用 cython 编写                   向 量化 string 操作       通过 访问 Series 的   . str   属性 ， 例如   s . str . lower ( )   可以 将 每 一个 元素 变成 小写 。 可以 在   . str   属性 上 使用     所有 的 字符串 函数 ， 就 相当于 每 一个 元素 使用 那样 。       DataFrame 的 修改           删除 列 或行 操作   DataFrame . drop ( labels ,   axis = 0 ,   level = None ,   inplace = False ,   errors = ' raise ' )   ， 举例                   ##   删除 一列 ， 返回 删除 后 的 DataFrame ， 对原 DataFrame 没有 影响       df     .     drop     (     &# 39 ; colname &# 39 ;     ,       axis     =     1     )         ##   在 原 DataFrame 上 删除 一列       df     .     drop     (     &# 39 ; colname &# 39 ;     ,       axis     =     1     ,       inplace     =     True     )                 DataFrame   API       category   类型 数据处理           转换 为 category ,     Series . astype ( ' category ' )           Series . cat     是 category 类型 的 cat 对象 ， 可以 通过     cat . rename _ categories ( [ obj ] )   重命名 类型 ， 也 可以 直接         赋值   cat . categories   改变 他们 的 名字 。           IO 操作       read _ csv       最 常用 的 文本文件 读取 函数 。 一些 重要 的 参数 如下 ：                     -     sep     字 段 分隔符 ， 默认 为   ,   ， 有些 文件 用   \ \ t       -     header     头部 用来 做 index 的 行 序号 ， None 表示 没有     -     names     用来 指定 行 的 label ， 一个 list     -     index _ col     是否 使用 index 列 ， 默认 false     -     nrows     读取 的 行数     -     true _ values ,   false _ values     真假 值 字符串 替换     -     na _ values     NAN 值 列表 ， 默认 作为 NAN 的 值 为   ' - 1 .# IND ' ,   ' 1 .# QNAN ' ,   ' 1 .# IND ' ,   ' - 1 .# QNAN ' ,   ' # N / A   N / A ' ,   ' # N / A ' ,   ' N / A ' ,   ' NA ' ,   ' # NA ' ,   ' NULL ' ,   ' NaN ' ,   ' - NaN ' ,   ' nan ' ,   ' - nan ' ,   ' '       -     iterator     默认 false ， 指定 为 true 用来 迭代 地读 大 文件 ， 可以 用   chunksize   指定 每次 读取 的 行数 。       可视化 plot       TIPS             df . loc [ ' a ' , ' A ' ]   =   5   是 有效 的 ， 但是   df . loc [ ' a ' ] [ ' A ' ]   =   5   对   df   是 没有 影响 的 。      ", "tags": "python", "url": "/wiki/python/pandas.html"},
      
      
      {"title": "Python 多线程和多进程编程总结", "text": "    Table   of   Contents           简介           线程 与 进程           多线程 编程           线程 的 状态           线程 的 类型           python 的 GIL           创建 线程           线程 合并 （ join 方法 ）           线程 同步 与 互斥 锁           可 重入 锁           条件 变量           队列           线程 通信           后台 线程                   进程           类 Process           不加 daemon 属性           加上 daemon 属性           设置 daemon 执行 完 结束 的 方法                   Lock           Semaphore           Event           Queue           Pipe                   Pool                   资料 来源                 简介       早已 进入 多核 时代 的 计算机 ， 怎能不 用 多线程 和 多 进程 进行 加速 。     我 在 使用 python 的 过程 中 ， 用到 过 几次 多线程 和 多 进程 加速 ， 觉得     充分利用 CPU 节省时间 是 一种 很 有 “ 延长 生命 ” 的 感觉 。 现将 网络 上 看到 的 python 的     多线程 和 多 进程 编程 常用 的 知识点 汇总 在 这里 。       线程 与 进程       线程 与 进程 是 操作系统 里面 的 术语 ， 简单 来讲 ， 每 一个 应用程序 都 有 一个 自己 的 进程 。     操作系统 会为 这些 进程 分配 一些 执行 资源 ， 例如 内存空间 等 。     在 进程 中 ， 又 可以 创建 一些 线程 ， 他们 共享 这些 内存空间 ， 并 由 操作系统 调用 ，     以便 并行计算 。       32 位 系统 受限于 总线 宽度 ， 单个 进程 最 多 能够 访问 的 地址 空间     只有 4G ， 利用   物理地址 扩展 ( PAE )       技术 ， 可以 让 CPU 访问 超过 4G 内存 。 但是 在 单个 进程 还是 只能 访问 4G     空间 ， PAE 的 优势 是 可以 让 不同 进程 累计 使用 的 内存 超过 4G 。     在 个人电脑 上 ， 还是 建议 使用 64 位 系统 ， 便于 使用 大 内存     提升 程序 的 运行 性能 。       多线程 编程       线程 的 状态       创建 线程 之后 ， 线程 并 不是 始终保持 一个 状态 。 其 状态 大概 如下 ：           New   创建 。       Runnable   就绪 。 等待 调度       Running   运行 。       Blocked   阻塞 。 阻塞 可能 在   Wait   Locked   Sleeping       Dead   消亡           线程 的 类型       线程 有着 不同 的 状态 ， 也 有 不同 的 类型 。 大致 可 分为 ：           主线 程       子 线程       守护 线程 （ 后台 线程 ）       前台 线程           python 的 GIL       GIL 即 全局 解释器 锁 ， 它 使得 python 的 多线程 无法 充分利用     多核 的 优势 ， 但是 对于 I / O 操作 频繁 的 爬虫 之类 的 程序 ，     利用 多线程 带来 的 优势 还是 很 明显 的 。     如果 要 利用 多 核优势 ， 还是 用 多 进程 吧 。       创建 线程       Python 提供 两个 模块 进行 多线程 的 操作 ， 分别 是   thread   和   threading   ，     前者 是 比较 低级 的 模块 ， 用于 更 底层 的 操作 ， 一般 应用 级别 的 开发 不 常用 。       第一种 方法 是 创建   threading . Thread   的 子类 ， 重写   run   方法 。               import       time       import       threading         class       MyThread     (     threading     .     Thread     ) :               def       run     (     self     ) :                       for       i       in       range     (     5     ) :                               print       &# 39 ; thread   { } ,   @ number :   { } &# 39 ;     .     format     (     self     .     name     ,       i     )                               time     .     sleep     (     1     )         def       main     ( ) :               print       & quot ; Start   main   threading & quot ;               #   创建 三个 线程               threads       =       [     MyThread     ( )       for       i       in       range     (     3     ) ]               #   启动 三个 线程               for       t       in       threads     :                       t     .     start     ( )                 print       & quot ; End   Main   threading & quot ;           if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               main     ( )                 输入 如下 ： （ 不同 的 环境 不 一样 ）             Start   main   threading   thread   Thread - 1 ,   @ number :   0   thread   Thread - 2 ,   @ number :   0   thread   Thread - 3 ,   @ number :   0   End   Main   threading   thread   Thread - 1 ,   @ number :   1   thread   Thread - 3 ,   @ number :   1   thread   Thread - 2 ,   @ number :   1   thread   Thread - 3 ,   @ number :   2   thread   Thread - 1 ,   @ number :   2     thread   Thread - 2 ,   @ number :   2   thread   Thread - 2 ,   @ number :   3   thread   Thread - 1 ,   @ number :   3   thread   Thread - 3 ,   @ number :   3               线程 合并 （ join 方法 ）       主线 程 结束 后 ， 子 线程 还 在 运行 ，   join   方法 使得 主线 程 等到 子 线程 结束     时才 退出 。               def       main     ( ) :               print       & quot ; Start   main   threading & quot ;                 threads       =       [     MyThread     ( )       for       i       in       range     (     3     ) ]                 for       t       in       threads     :                       t     .     start     ( )                 #   一次 让 新创建 的 线程 执行   join               for       t       in       threads     :                       t     .     join     ( )                 print       & quot ; End   Main   threading & quot ;                 线程 同步 与 互斥 锁       为了 避免 线程 不 同步 造成 是 数据 不 同步 ， 可以 对 资源 进行 加锁 。     也 就是 访问 资源 的 线程 需要 获得 锁 ， 才能 访问 。       threading   模块 正好 提供 了 一个 Lock 功能               mutex       =       threading     .     Lock     ( )                 在线 程中 获取 锁               mutex     .     acquire     ( )                 使用 完后 ， 释放 锁               mutex     .     release     ( )                 可 重入 锁       为了 支持 在 同一 线程 中 多次 请求 同一 资源 ，     python 提供 了 可 重入 锁 （ RLock ） 。     RLock 内部 维护 着 一个   Lock   和 一个   counter   变量 ，       counter   记录 了 acquire 的 次数 ， 从而 使得 资源 可以 被 多次 require 。     直到 一个 线程 所有 的 acquire 都 被 release ， 其他 的 线程 才能 获得 资源 。       创建 RLock               mutex       =       threading     .     RLock     ( )                 线程 内 多次 进入 锁 和 释放 锁               class       MyThread     (     threading     .     Thread     ) :                 def       run     (     self     ) :                       if       mutex     .     acquire     (     1     ) :                               print       & quot ; thread   { }   get   mutex & quot ;     .     format     (     self     .     name     )                               time     .     sleep     (     1     )                               mutex     .     acquire     ( )                               mutex     .     release     ( )                               mutex     .     release     ( )                 条件 变量       实用 锁 可以 达到 线程 同步 ， 前面 的 互斥 锁 就是 这种 机制 。 更 复杂 的 环境 ， 需要 针对 锁 进行 一些 条件 判断 。 Python 提供 了 Condition 对象 。 它 除了 具有 acquire 和 release 方法 之外 ， 还 提供 了 wait 和 notify 方法 。 线程 首先 acquire 一个 条件 变量 锁 。 如果 条件 不足 ， 则 该 线程 wait ， 如果 满足 就 执行 线程 ， 甚至 可以 notify 其他 线程 。 其他 处于 wait 状态 的 线程 接到 通知 后 会 重新 判断 条件 。       条件 变量 可以 看成 不同 的 线程 先后 acquire 获得 锁 ， 如果 不 满足条件 ， 可以 理解 为 被 扔 到 一个 （ Lock 或 RLock ） 的 waiting 池 。 直达 其他 线程 notify 之后 再 重新 判断 条件 。 该 模式 常用 于 生成 消费者 模式 ：               queue       =       [ ]         con       =       threading     .     Condition     ( )         class       Producer     (     threading     .     Thread     ) :               def       run     (     self     ) :                       while       True     :                               if       con     .     acquire     ( ) :                                       if       len     (     queue     )       & gt ;       100     :                                               con     .     wait     ( )                                       else     :                                               elem       =       random     .     randrange     (     100     )                                               queue     .     append     (     elem     )                                               print       & quot ; Producer   a   elem   { } ,   Now   size   is   { } & quot ;     .     format     (     elem     ,       len     (     queue     ) )                                               time     .     sleep     (     random     .     random     ( ) )                                               con     .     notify     ( )                                       con     .     release     ( )         class       Consumer     (     threading     .     Thread     ) :               def       run     (     self     ) :                       while       True     :                               if       con     .     acquire     ( ) :                                       if       len     (     queue     )       & lt ;       0     :                                               con     .     wait     ( )                                       else     :                                               elem       =       queue     .     pop     ( )                                               print       & quot ; Consumer   a   elem   { } .   Now   size   is   { } & quot ;     .     format     (     elem     ,       len     (     queue     ) )                                               time     .     sleep     (     random     .     random     ( ) )                                               con     .     notify     ( )                                       con     .     release     ( )         def       main     ( ) :               for       i       in       range     (     3     ) :                       Producer     ( )     .     start     ( )                 for       i       in       range     (     2     ) :                       Consumer     ( )     .     start     ( )                 队列       带锁 的 队列   Queue   。     创建 10 个 元素 的 队列               queue       =       Queue     .     Queue     (     10     )                 队列 通过   put   加入 元素 ， 通过   get   方法 获取 元素 。       线程 通信       线程 可以 读取 共享 的 内存 ， 通过 内存 做 一些 数据处理 。     这 就是 线程 通信 的 一种 ， python 还 提供 了 更加 高级 的 线程 通信接口 。     Event 对象 可以 用来 进行 线程 通信 ， 调用 event 对象 的 wait 方法 ，     线程 则 会 阻塞 等待 ， 直到 别的 线程 set 之后 ， 才 会 被 唤醒 。               class       MyThread     (     threading     .     Thread     ) :               def       __ init __     (     self     ,       event     ) :                       super     (     MyThread     ,       self     )     .     __ init __     ( )                       self     .     event       =       event                 def       run     (     self     ) :                       print       & quot ; thread   { }   is   ready   & quot ;     .     format     (     self     .     name     )                       self     .     event     .     wait     ( )                       print       & quot ; thread   { }   run & quot ;     .     format     (     self     .     name     )         signal       =       threading     .     Event     ( )         def       main     ( ) :               start       =       time     .     time     ( )               for       i       in       range     (     3     ) :                       t       =       MyThread     (     signal     )                       t     .     start     ( )               time     .     sleep     (     3     )               print       & quot ; after   { } s & quot ;     .     format     (     time     .     time     ( )       -       start     )               signal     .     set     ( )                 后台 线程       默认 情况 下 ， 主线 程 退出 之后 ， 即使 子 线程 没有 join 。 那么 主线 程 结束 后 ，     子 线程 也 依然 会 继续执行 。 如果 希望 主线 程 退出 后 ，     其子 线程 也 退出 而 不再 执行 ， 则 需要 设置 子 线程 为 后台 线程 。 python 提供 了   setDeamon   方法 。       进程       python 中 的 多线程 其实 并 不是 真正 的 多线程 ， 如果 想要 充分 地 使用 多核 CPU 的 资源 ， 在 python 中 大部分 情况 需要 使用 多 进程 。 Python 提供 了 非常 好用 的 多 进程 包 multiprocessing ， 只 需要 定义 一个 函数 ， Python 会 完成 其他 所有 事情 。 借助 这个 包 ， 可以 轻松 完成 从单 进程 到 并发 执行 的 转换 。 multiprocessing 支持 子 进程 、 通信 和 共享 数据 、 执行 不同 形式 的 同步 ， 提供 了 Process 、 Queue 、 Pipe 、 Lock 等 组件 。       类 Process               创建 进程 的 类 ：   Process ( [ group   [ ,   target   [ ,   name   [ ,   args   [ ,   kwargs ] ] ] ] ] )   ，     target 表示 调用 对象 ， args 表示 调用 对象 的 位置 参数 元组 。     kwargs 表示 调用 对象 的 字典 。 name 为 别名 。 group 实质 上 不 使用 。               方法 ： is _ alive ( ) 、 join ( [ timeout ] ) 、 run ( ) 、 start ( ) 、 terminate ( ) 。 其中 ， Process 以 start ( ) 启动 某个 进程 。               属性 ： authkey 、 daemon （ 要 通过 start ( ) 设置 ） 、 exitcode ( 进程 在 运行 时为 None 、 如果 为 – N ， 表示 被 信号 N 结束 ） 、 name 、 pid 。 其中 daemon 是 父 进程 终止 后 自动 终止 ， 且 自己 不能 产生 新 进程 ， 必须 在 start ( ) 之前 设置 。               例 ： 创建 函数 并 将 其 作为 单个 进程               import       multiprocessing       import       time         def       worker     (     interval     ) :               n       =       5               while       n       & gt ;       0     :                       print     (     & quot ; The   time   is   { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) )                       time     .     sleep     (     interval     )                       n       - =       1         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     start     ( )               print       & quot ; p . pid : & quot ;     ,       p     .     pid               print       & quot ; p . name : & quot ;     ,       p     .     name               print       & quot ; p . is _ alive : & quot ;     ,       p     .     is _ alive     ( )                 结果             p . pid :   8736   p . name :   Process - 1   p . is _ alive :   True   The   time   is   Tue   Apr   21   20 : 55 : 12   2015   The   time   is   Tue   Apr   21   20 : 55 : 15   2015   The   time   is   Tue   Apr   21   20 : 55 : 18   2015   The   time   is   Tue   Apr   21   20 : 55 : 21   2015   The   time   is   Tue   Apr   21   20 : 55 : 24   2015               例 ： 创建 函数 并 将 其 作为 多个 进程               import       multiprocessing       import       time         def       worker _ 1     (     interval     ) :               print       & quot ; worker _ 1 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 1 & quot ;         def       worker _ 2     (     interval     ) :               print       & quot ; worker _ 2 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 2 & quot ;         def       worker _ 3     (     interval     ) :               print       & quot ; worker _ 3 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 3 & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p1       =       multiprocessing     .     Process     (     target       =       worker _ 1     ,       args       =       (     2     , ) )               p2       =       multiprocessing     .     Process     (     target       =       worker _ 2     ,       args       =       (     3     , ) )               p3       =       multiprocessing     .     Process     (     target       =       worker _ 3     ,       args       =       (     4     , ) )                 p1     .     start     ( )               p2     .     start     ( )               p3     .     start     ( )                 print     (     & quot ; The   number   of   CPU   is : & quot ;       +       str     (     multiprocessing     .     cpu _ count     ( ) ) )               for       p       in       multiprocessing     .     active _ children     ( ) :                       print     (     & quot ; child       p . name : & quot ;       +       p     .     name       +       & quot ;     \ \ t     p . id & quot ;       +       str     (     p     .     pid     ) )               print       & quot ; END ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! & quot ;                 结果             The   number   of   CPU   is : 4   child       p . name : Process - 3         p . id7992   child       p . name : Process - 2         p . id4204   child       p . name : Process - 1         p . id6380   END ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !   worker _ 1   worker _ 3   worker _ 2   end   worker _ 1   end   worker _ 2   end   worker _ 3               例 ： 将 进程 定义 为类               import       multiprocessing       import       time         class       ClockProcess     (     multiprocessing     .     Process     ) :               def       __ init __     (     self     ,       interval     ) :                       multiprocessing     .     Process     .     __ init __     (     self     )                       self     .     interval       =       interval                 def       run     (     self     ) :                       n       =       5                       while       n       & gt ;       0     :                               print     (     & quot ; the   time   is   { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) )                               time     .     sleep     (     self     .     interval     )                               n       - =       1         if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               p       =       ClockProcess     (     3     )               p     .     start     ( )                 注 ： 进程 p 调用 start ( ) 时 ， 自动 调用 run ( )       结果             the   time   is   Tue   Apr   21   20 : 31 : 30   2015   the   time   is   Tue   Apr   21   20 : 31 : 33   2015   the   time   is   Tue   Apr   21   20 : 31 : 36   2015   the   time   is   Tue   Apr   21   20 : 31 : 39   2015   the   time   is   Tue   Apr   21   20 : 31 : 42   2015               例 ： daemon 程序 对比 结果       不加 daemon 属性               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     start     ( )               print       & quot ; end ! & quot ;                 结果             end !   work   start : Tue   Apr   21   21 : 29 : 10   2015   work   end : Tue   Apr   21   21 : 29 : 13   2015               加上 daemon 属性               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     daemon       =       True               p     .     start     ( )               print       & quot ; end ! & quot ;                 结果             end !               注 ： 因子 进程 设置 了 daemon 属性 ， 主 进程 结束 ， 它们 就 随着 结束 了 。       设置 daemon 执行 完 结束 的 方法               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     daemon       =       True               p     .     start     ( )               p     .     join     ( )               print       & quot ; end ! & quot ;                 结果             work   start : Tue   Apr   21   22 : 16 : 32   2015   work   end : Tue   Apr   21   22 : 16 : 35   2015   end !               Lock       当 多个 进程 需要 访问共享 资源 的 时候 ， Lock 可以 用来 避免 访问 的 冲突 。               import       multiprocessing       import       sys         def       worker _ with     (     lock     ,       f     ) :               with       lock     :                       fs       =       open     (     f     ,       &# 39 ; a +&# 39 ;     )                       n       =       10                       while       n       & gt ;       1     :                               fs     .     write     (     & quot ; Lockd   acquired   via   with     \ \ n     & quot ;     )                               n       - =       1                       fs     .     close     ( )         def       worker _ no _ with     (     lock     ,       f     ) :               lock     .     acquire     ( )               try     :                       fs       =       open     (     f     ,       &# 39 ; a +&# 39 ;     )                       n       =       10                       while       n       & gt ;       1     :                               fs     .     write     (     & quot ; Lock   acquired   directly     \ \ n     & quot ;     )                               n       - =       1                       fs     .     close     ( )               finally     :                       lock     .     release     ( )         if       __ name __       = =       & quot ; __ main __& quot ;     :               lock       =       multiprocessing     .     Lock     ( )               f       =       & quot ; file . txt & quot ;               w       =       multiprocessing     .     Process     (     target       =       worker _ with     ,       args     =     (     lock     ,       f     ) )               nw       =       multiprocessing     .     Process     (     target       =       worker _ no _ with     ,       args     =     (     lock     ,       f     ) )               w     .     start     ( )               nw     .     start     ( )               print       & quot ; end & quot ;                 结果 （ 输出 文件 ）             Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly               Semaphore       Semaphore 用来 控制 对 共享资源 的 访问 数量 ， 例如 池 的 最大 连接数 。               import       multiprocessing       import       time         def       worker     (     s     ,       i     ) :               s     .     acquire     ( )               print     (     multiprocessing     .     current _ process     ( )     .     name       +       & quot ; acquire & quot ;     ) ;               time     .     sleep     (     i     )               print     (     multiprocessing     .     current _ process     ( )     .     name       +       & quot ; release     \ \ n     & quot ;     ) ;               s     .     release     ( )         if       __ name __       = =       & quot ; __ main __& quot ;     :               s       =       multiprocessing     .     Semaphore     (     2     )               for       i       in       range     (     5     ) :                       p       =       multiprocessing     .     Process     (     target       =       worker     ,       args     =     (     s     ,       i     *     2     ) )                       p     .     start     ( )                 结果             Process - 1acquire   Process - 1release     Process - 2acquire   Process - 3acquire   Process - 2release     Process - 5acquire   Process - 3release     Process - 4acquire   Process - 5release     Process - 4release               Event       Event 用来 实现 进程 间 同步 通信 。               import       multiprocessing       import       time         def       wait _ for _ event     (     e     ) :               print     (     & quot ; wait _ for _ event :   starting & quot ;     )               e     .     wait     ( )               print     (     & quot ; wairt _ for _ event :   e . is _ set ( ) - & gt ; & quot ;       +       str     (     e     .     is _ set     ( ) ) )         def       wait _ for _ event _ timeout     (     e     ,       t     ) :               print     (     & quot ; wait _ for _ event _ timeout : starting & quot ;     )               e     .     wait     (     t     )               print     (     & quot ; wait _ for _ event _ timeout : e . is _ set - & gt ; & quot ;       +       str     (     e     .     is _ set     ( ) ) )         if       __ name __       = =       & quot ; __ main __& quot ;     :               e       =       multiprocessing     .     Event     ( )               w1       =       multiprocessing     .     Process     (     name       =       & quot ; block & quot ;     ,                               target       =       wait _ for _ event     ,                               args       =       (     e     , ) )                 w2       =       multiprocessing     .     Process     (     name       =       & quot ; non - block & quot ;     ,                               target       =       wait _ for _ event _ timeout     ,                               args       =       (     e     ,       2     ) )               w1     .     start     ( )               w2     .     start     ( )                 time     .     sleep     (     3     )                 e     .     set     ( )               print     (     & quot ; main :   event   is   set & quot ;     )                 结果               wait _ for _ event     :       starting       wait _ for _ event _ timeout     :     starting       wait _ for _ event _ timeout     :     e     .     is _ set     - & gt ;     False       main     :       event       is       set       wairt _ for _ event     :       e     .     is _ set     ( ) - & gt ;     True                 Queue       Queue 是 多 进程 安全 的 队列 ， 可以 使用 Queue 实现 多 进程 之间 的 数据 传递 。 put 方法 用以 插入 数据 到 队列 中 ， put 方法 还有 两个 可 选 参数 ： blocked 和 timeout 。 如果 blocked 为 True （ 默认值 ） ， 并且 timeout 为 正值 ， 该 方法 会 阻塞 timeout 指定 的 时间 ， 直到 该 队列 有 剩余 的 空间 。 如果 超时 ， 会 抛出 Queue . Full 异常 。 如果 blocked 为 False ， 但 该 Queue 已满 ， 会 立即 抛出 Queue . Full 异常 。       get 方法 可以 从 队列 读取 并且 删除 一个 元素 。 同样 ， get 方法 有 两个 可 选 参数 ： blocked 和 timeout 。 如果 blocked 为 True （ 默认值 ） ， 并且 timeout 为 正值 ， 那么 在 等待时间 内 没有 取 到 任何 元素 ， 会 抛出 Queue . Empty 异常 。 如果 blocked 为 False ， 有 两种 情况 存在 ， 如果 Queue 有 一个 值 可用 ， 则 立即 返回 该值 ， 否则 ， 如果 队 列为 空 ， 则 立即 抛出 Queue . Empty 异常 。 Queue 的 一段 示例 代码 ：                 import       multiprocessing         def       writer _ proc     (     q     ) :                           try     :                                         q     .     put     (     1     ,       block       =       False     )                 except     :                                         pass               def       reader _ proc     (     q     ) :                           try     :                                         print       q     .     get     (     block       =       False     )                 except     :                                         pass         if       __ name __       = =       & quot ; __ main __& quot ;     :               q       =       multiprocessing     .     Queue     ( )               writer       =       multiprocessing     .     Process     (     target     =     writer _ proc     ,       args     =     (     q     , ) )                   writer     .     start     ( )                       reader       =       multiprocessing     .     Process     (     target     =     reader _ proc     ,       args     =     (     q     , ) )                   reader     .     start     ( )                     reader     .     join     ( )                   writer     .     join     ( )                 结果             1               Pipe       Pipe 方法 返回 ( conn1 ,   conn2 ) 代表 一个 管道 的 两个 端 。 Pipe 方法 有 duplex 参数 ， 如果 duplex 参数 为 True ( 默认值 ) ， 那么 这个 管道 是 全双工 模式 ， 也就是说 conn1 和 conn2 均 可 收发 。 duplex 为 False ， conn1 只 负责 接受 消息 ， conn2 只 负责 发送 消息 。       send 和 recv 方法 分别 是 发送 和 接受 消息 的 方法 。 例如 ， 在 全双工 模式 下 ， 可以 调用 conn1 . send 发送 消息 ， conn1 . recv 接收 消息 。 如果 没有 消息 可 接收 ， recv 方法 会 一直 阻塞 。 如果 管道 已经 被 关闭 ， 那么 recv 方法 会 抛出 EOFError 。                 import       multiprocessing       import       time         def       proc1     (     pipe     ) :               while       True     :                       for       i       in       xrange     (     10000     ) :                               print       & quot ; send :       % s     & quot ;       %     (     i     )                               pipe     .     send     (     i     )                               time     .     sleep     (     1     )         def       proc2     (     pipe     ) :               while       True     :                       print       & quot ; proc2   rev : & quot ;     ,       pipe     .     recv     ( )                       time     .     sleep     (     1     )         def       proc3     (     pipe     ) :               while       True     :                       print       & quot ; PROC3   rev : & quot ;     ,       pipe     .     recv     ( )                       time     .     sleep     (     1     )         if       __ name __       = =       & quot ; __ main __& quot ;     :               pipe       =       multiprocessing     .     Pipe     ( )               p1       =       multiprocessing     .     Process     (     target     =     proc1     ,       args     =     (     pipe     [     0     ] , ) )               p2       =       multiprocessing     .     Process     (     target     =     proc2     ,       args     =     (     pipe     [     1     ] , ) )               # p3   =   multiprocessing . Process ( target = proc3 ,   args = ( pipe [ 1 ] , ) )                 p1     .     start     ( )               p2     .     start     ( )               # p3 . start ( )                 p1     .     join     ( )               p2     .     join     ( )               # p3 . join ( )                 结果                           Pool       在 利用 Python 进行 系统管理 的 时候 ， 特别 是 同时 操作 多个 文件目录 ， 或者 远程 控制 多台 主机 ， 并行操作 可以 节约 大量 的 时间 。 当 被 操作 对象 数目 不大时 ， 可以 直接 利用 multiprocessing 中 的 Process 动态 成生 多个 进程 ， 十几个 还好 ， 但 如果 是 上 百个 ， 上 千个 目标 ， 手动 的 去 限制 进程 数量 却 又 太过 繁琐 ， 此时 可以 发挥 进程 池 的 功效 。     Pool 可以 提供 指定 数量 的 进程 ， 供 用户 调用 ， 当有 新 的 请求 提交 到 pool 中时 ， 如果 池 还 没有 满 ， 那么 就 会 创建 一个 新 的 进程 用来 执行 该 请求 ； 但 如果 池中 的 进程 数 已经 达到 规定 最大值 ， 那么 该 请求 就 会 等待 ， 直到 池中 有 进程 结束 ， 才 会 创建 新 的 进程 来 它 。       例 ： 使用 进程 池               # coding :   utf - 8       import       multiprocessing       import       time         def       func     (     msg     ) :               print       & quot ; msg : & quot ;     ,       msg               time     .     sleep     (     3     )               print       & quot ; end & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               pool       =       multiprocessing     .     Pool     (     processes       =       3     )               for       i       in       xrange     (     4     ) :                       msg       =       & quot ; hello       % d     & quot ;       %     (     i     )                       pool     .     apply _ async     (     func     ,       (     msg     ,       ) )           # 维持 执行 的 进程 总数 为 processes ， 当 一个 进程 执行 完毕 后 会 添加 新 的 进程 进去                 print       & quot ; Mark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ & quot ;               pool     .     close     ( )               pool     .     join     ( )           # 调用 join 之前 ， 先 调用 close 函数 ， 否则 会 出错 。 执行 完 close 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 所有 子 进程 结束               print       & quot ; Sub - process ( es )   done .& quot ;                 一次 执行 结果               mMsg     :       hark     ~       Mark     ~       Mark     ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~     ello       0         msg     :       hello       1       msg     :       hello       2       end       msg     :       hello       3       end       end       end       Sub     -     process     (     es     )       done     .                 函数 解释 ：             apply _ async ( func [ ,   args [ ,   kwds [ ,   callback ] ] ] )   它 是非 阻塞 ， apply ( func [ ,   args [ ,   kwds ] ] ) 是 阻塞 的 （ 理解 区别 ， 看例 1 例 2 结果 区别 ）   close ( )         关闭 pool ， 使 其 不 在 接受 新 的 任务 。   terminate ( )         结束 工作 进程 ， 不 在 处理 未 完成 的 任务 。   join ( )         主 进程 阻塞 ， 等待 子 进程 的 退出 ，   join 方法 要 在 close 或 terminate 之后 使用 。               执行 说明 ： 创建 一个 进程 池 pool ， 并 设定 进程 的 数量 为 3 ， xrange ( 4 ) 会 相继 产生 四个 对象 [ 0 ,   1 ,   2 ,   4 ] ， 四个 对象 被 提交 到 pool 中 ， 因 pool 指定 进程 数为 3 ， 所以 0 、 1 、 2 会 直接 送到 进程 中 执行 ， 当 其中 一个 执行 完 事后 才 空出 一个 进程 处理 对象 3 ， 所以 会 出现 输出 “ msg :   hello   3 ” 出现 在 ” end ” 后 。 因为 为 非 阻塞 ， 主 函数 会 自己 执行 自个 的 ， 不 搭理 进程 的 执行 ， 所以 运行 完 for 循环 后 直接 输出 “ mMsg :   hark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ” ， 主程序 在 pool . join （ ） 处 等待 各个 进程 的 结束 。       例 ： 使用 进程 池 （ 阻塞 ）               # coding :   utf - 8       import       multiprocessing       import       time         def       func     (     msg     ) :               print       & quot ; msg : & quot ;     ,       msg               time     .     sleep     (     3     )               print       & quot ; end & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               pool       =       multiprocessing     .     Pool     (     processes       =       3     )               for       i       in       xrange     (     4     ) :                       msg       =       & quot ; hello       % d     & quot ;       %     (     i     )                       pool     .     apply     (     func     ,       (     msg     ,       ) )           # 维持 执行 的 进程 总数 为 processes ， 当 一个 进程 执行 完毕 后 会 添加 新 的 进程 进去                 print       & quot ; Mark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ & quot ;               pool     .     close     ( )               pool     .     join     ( )           # 调用 join 之前 ， 先 调用 close 函数 ， 否则 会 出错 。 执行 完 close 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 所有 子 进程 结束               print       & quot ; Sub - process ( es )   done .& quot ;                 一次 执行 的 结果               msg     :       hello       0       end       msg     :       hello       1       end       msg     :       hello       2       end       msg     :       hello       3       end       Mark     ~       Mark     ~       Mark     ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~       Sub     -     process     (     es     )       done     .                 例 ： 使用 进程 池 ， 并 关注 结果               msg     :       hello       0       msg     :       hello       1       msg     :       hello       2       end       end       end       : : :       donehello       0       : : :       donehello       1       : : :       donehello       2       Sub     -     process     (     es     )       done     .                 例 ： 使用 多个 进程 池               # coding :   utf - 8       import       multiprocessing       import       os     ,       time     ,       random         def       Lee     ( ) :               print       & quot ;     \ \ n     Run   task   Lee -     % s     & quot ;       %     (     os     .     getpid     ( ) )       # os . getpid ( ) 获取 当前 的 进程 的 ID               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       10     )       # random . random ( ) 随机 生成 0 - 1 之间 的 小数               end       =       time     .     time     ( )               print       &# 39 ; Task   Lee ,   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Marlon     ( ) :               print       & quot ;     \ \ n     Run   task   Marlon -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       40     )               end     =     time     .     time     ( )               print       &# 39 ; Task   Marlon   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Allen     ( ) :               print       & quot ;     \ \ n     Run   task   Allen -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       30     )               end       =       time     .     time     ( )               print       &# 39 ; Task   Allen   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Frank     ( ) :               print       & quot ;     \ \ n     Run   task   Frank -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       20     )               end       =       time     .     time     ( )               print       &# 39 ; Task   Frank   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         if       __ name __     = =     &# 39 ; __ main __&# 39 ;     :               function _ list     =         [     Lee     ,       Marlon     ,       Allen     ,       Frank     ]                 print       & quot ; parent   process       % s     & quot ;       %     (     os     .     getpid     ( ) )                 pool     =     multiprocessing     .     Pool     (     4     )               for       func       in       function _ list     :                       pool     .     apply _ async     (     func     )               # Pool 执行 函数 ， apply 执行 函数 , 当有 一个 进程 执行 完毕 后 ， 会 添加 一个 新 的 进程 到 pool 中                 print       &# 39 ; Waiting   for   all   subprocesses   done ...&# 39 ;               pool     .     close     ( )               pool     .     join     ( )             # 调用 join 之前 ， 一定 要 先 调用 close ( )   函数 ， 否则 会 出错 ,   close ( ) 执行 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 素有 子 进程 结束               print       &# 39 ; All   subprocesses   done .&# 39 ;                 一次 执行 结果             parent   process   7704     Waiting   for   all   subprocesses   done ...   Run   task   Lee - 6948     Run   task   Marlon - 2896     Run   task   Allen - 7304     Run   task   Frank - 3052   Task   Lee ,   runs   1.59   seconds .   Task   Marlon   runs   8.48   seconds .   Task   Frank   runs   15.68   seconds .   Task   Allen   runs   18.08   seconds .   All   subprocesses   done .               资料 来源             http : / / www . cnblogs . com / kaituorensheng / p / 4445418 . html           http : / / python . jobbole . com / 85177 /        ", "tags": "python", "url": "/wiki/python/python-multiprocessing-tutorial.html"},
      
      
      {"title": "python 拾遗", "text": "    Table   of   Contents           基本 语法 相关           函数           类           装饰 器           上下文 管理器   with           迭代 器 相关 函数           各种 坑           工具 函数                 基本 语法 相关             * args   ， 将 列表   args   展开 为 参数 列表 ，   * * kwargs   ， 将 字典   kwargs   展开 成 kv 形式 的 参数 列表 。           参考     https : / / stackoverflow . com / questions / 2921847 / what - does - the - star - operator - mean - in - python       和     https : / / docs . python . org / 3 / tutorial / controlflow . html # unpacking - argument - lists             整数 除法 ， 在 python2 . x 中   /   代表 整数 除法 （ 除非 左右 操作数 有 一个 是 浮点数 ） ， 而 在 python3 . x 中 ， 代表 浮点数 除法 ， python3 . x 用   / /   代表 整数 除法 。       反射 函数 ，   type ( )   ，   isinstance ( )   ，   hasattr ( )   ，   setattr ( )     以及 属性   __ class __         Mixin ： python 通过 多重 继承 实现 组合 模式 。 sklearn 中 很多 基础 类 使用 这种 模式 实现 的 。   https : / / github . com / scikit - learn / scikit - learn / blob / 51a765a / sklearn / base . py # L281             函数       参考 ：   https : / / stackoverflow . com / documentation / python / 228 / functions             任意 参数 ：   * args ,   * * kwargs                     def       func     (     *     args     ) :               #   args   will   be   a   tuple   containing   all   values   that   are   passed   in               for       i       in       args     :                       print     (     i     )                         字符串 函数 拾遗 ： 除了   upper ,   lower     之外 还有 ：   https : / / stackoverflow . com / documentation / python / 278 / string - methods               capitalize     将 第一个 变为 大写 ， 其他 的 变为 小写 ！         title     将 每个 单词 第一个 字母 变为 大写 ， 其他 的 为 小写 ！             casefold     @ since ( 3.3 )   小写 转换 的 unicode 字符 版 ！ 希腊字母 也 可以 转                 translate     按照 替换 表 进行 字母 替换             format     格式化         split ,   rsplit     当 指定     maxsplit     参数 的 时候 ， 有 区别       string   模块 常量 ， 要 使用   import   string   导入 ：   string . ascii _ letters     字母表 ,     string . ascii _ lowercase ,   string . ascii _ uppercase ,   string . digits ,   string . hexdigits ,   string . octaldigits ,   string . punctuation ,   string . whitespace ,   string . printable         unicodedata ,   一个 有趣 的 模块         str . count ( sub [ ,   start [ ,   end ] ] )     统计 出现 的 次数         str . replace ( old ,   new [ ,   count ] )     简单 替换 ，   re . sub   基于 正则 式 替换         str . isupper ( ) ,   str . islower ( )   and   str . istitle ( )           str . ljust   and   str . rjust     字符串 对齐         ' hello ' [ : : - 1 ]     字符串 反转         str . strip ( [ chars ] ) ,   str . rstrip ( [ chars ] )   and   str . lstrip ( [ chars ] )     去掉 空白 字符         \" foo \"   in   my _ str     字符串 包含 检查         str . startswith ( )   and   str . endswith ( )     字符串 首尾         encode ,   decode     字符串 和 unicode 字符串 之间 的 转换       共同 前缀 ！                           import       os       mylist       =       [     & quot ; & amp ; abcd & quot ;     ,       & quot ; & amp ; abbe & quot ;     ,       & quot ; & amp ; ab & quot ;     ]       print       (     os     .     path     .     commonprefix     (     mylist     ) )                 类       参考     https : / / stackoverflow . com / documentation / python / 419 / classes             Bound ,   unbound ,   and   static   methods           Bound   方法 就是 通过 类 的 实例 访问 的 方法 （ instancemethod ） ，   unbound   方法 就是 通过 类 本身 访问 的 动态 方法 （ Python3 . x   已 废弃 )               a       =       A     ( )       a     .     f       #   & lt ; bound   method   A . f   of   & lt ; __ main __. A   object   at   ...& gt ; & gt ;       a     .     f     (     2     )       #   4         #   Note :   the   bound   method   object   a . f   is   recreated   * every   time *   you   call   it :       a     .     f       is       a     .     f         #   False       #   As   a   performance   optimization   you   can   store   the   bound   method   in   the   object &# 39 ; s       #   __ dict __ ,   in   which   case   the   method   object   will   remain   fixed :       a     .     f       =       a     .     f       a     .     f       is       a     .     f         #   True                     @ classmethod   装饰 器 ， 第一个 参数 代表 类 ， 类 方法 ， 既 可以 通过 类 调用 ， 也 可以 通过 类 的 实例 调用       @ staticmethod   ， 只能 通过 类 调用           无 装饰 器 ，   只能 通过 实例 调用               类 的 继承 ： 从 3 . x 开始 ，   super     不再 需要 传入 参数 了 ！                       class       Square     (     Rectangle     ) :               def       __ init __     (     self     ,       s     ) :                       #   call   parent   constructor ,   w   and   h   are   both   s                       super     (     Rectangle     ,       self     )     .     __ init __     (     s     ,       s     )                         self     .     s       =       s                     抽象类 ：   abc   模块 ， 需要 制定   __ metaclass __   变量 为   ABCMeta   ， 子类 需要 注册 ！                   from       abc       import       ABCMeta         class       AbstractClass     (     object     ) :               #   the   metaclass   attribute   must   always   be   set   as   a   class   variable               __ metaclass __       =       ABCMeta               #   the   abstractmethod   decorator   registers   this   method   as   undefined             @ abstractmethod             def       virtual _ method _ subclasses _ must _ define     (     self     ) :                     #   Can   be   left   completely   blank ,   or   a   base   implementation   can   be   provided                     #   Note   that   ordinarily   a   blank   interpretation   implicitly   returns   ` None ` ,                     #   but   by   registering ,   this   behaviour   is   no   longer   enforced .         class       Subclass     :             def       virtual _ method _ subclasses _ must _ define     (     self     ) :                     return         #   registration   is   mandatory   to   truly   create   an   abstract   class       AbstractClass     .     register     (     SubClass     )                     多重 继承 ：   class   FooBar ( Foo ,   Bar )         私有 变量 ：   _ non _ public     通过 添加 一个 下划线 实现 ！ （ 哈哈 ！ ）       属性 方法 ： 可以 将 属性 设置 为 只读 ， 不 提供   setter   方法                   class       MyClass     :                 def       __ init __     (     self     ) :                     self     .     _ my _ string       =       & quot ; & quot ;                 @ property               def       my _ string     (     self     ) :                       return       self     .     _ my _ string                 @ my _ string . setter               def       my _ string     (     self     ,       new _ value     ) :                       self     .     _ my _ string       =       new _ value                 @ my _ string . deleter               def       x     (     self     ) :                       del       self     .     _ my _ string                 装饰 器       任何 传入 一个 函数 ， 返回 一个 函数 的 函数 都 可以 作为 装饰 器 ！ ( 貌似 无效 ！ py2 . 7 )               def       super _ secret _ function     (     f     ) :               return       f         @ super _ secret _ function       def       my _ function     ( ) :               print     (     & quot ; This   is   my   secret   function .& quot ;     )                 这里   @     相当于 实现     my _ function   =   super _ secret _ function ( my _ function )         装饰 器类 ：   __ call __     方法 进行 装饰 。 如果 需要 装饰 成员 方法 ， 需要 指定   __ get __   方法 ！     装饰 器 可以 使用 参数 ， 只要 在 函数 内部 定义 一个 类 ， 然后 返回 即可 。               class       Decorator     (     object     ) :               & quot ; & quot ; & quot ; Simple   decorator   class .& quot ; & quot ; & quot ;                 def       __ init __     (     self     ,       func     ) :                       self     .     func       =       func                 def       __ call __     (     self     ,       *     args     ,       * *     kwargs     ) :                       print     (     &# 39 ; Before   the   function   call .&# 39 ;     )                       res       =       self     .     func     (     *     args     ,       * *     kwargs     )                       print     (     &# 39 ; After   the   function   call .&# 39 ;     )                       return       res         @ Decorator       def       testfunc     ( ) :               print     (     &# 39 ; Inside   the   function .&# 39 ;     )         testfunc     ( )       #   Before   the   function   call .       #   Inside   the   function .       #   After   the   function   call .                 上下文 管理器     with             把 文件 当做 上下文 ， 上下文 结束 的 时候 会 自动 关闭 文件 ！       自己 的 上下文 管理器 需要 实现 两个 方法     __ enter __ ( )   and   __ exit __ ( )             迭代 器 相关 函数             enumerate ( iter )     返回   ( idx ,   value )   的 迭代 器 ， 可以 应用 在 需要 使用 index 的 时候 。         ( x   for   x   ...   )     圆括号 生成 迭代 器 的 一个 语法 糖 。           各种 坑           不要 用 可变 对象 作为 函数 默认值 。 字典 , 集合 , 列表 等等 对象 是 不 适合 作为 函数 默认值 的 .         因为 这个 默认值 是 在 函数 建立 的 时候 就 生成 了 ,   每次 调用 都 是 用 了 这个 对象 的 ” 缓存 ” .                   In       [     1     ] :       def       append _ to _ list     (     value     ,       def _ list     =     [ ] ) :             ...     :                       def _ list     .     append     (     value     )             ...     :                       return       def _ list             ...     :         In       [     2     ] :       my _ list       =       append _ to _ list     (     1     )         In       [     3     ] :       my _ list       Out     [     3     ] :       [     1     ]         In       [     4     ] :       my _ other _ list       =       append _ to _ list     (     2     )         In       [     5     ] :       my _ other _ list       Out     [     5     ] :       [     1     ,       2     ]       #   看到 了 吧 ， 其实 我们 本来 只想 生成 [ 2 ]   但是 却 把 第一次 运行 的 效果 页 带 了 进来                     生成器 不 保留 迭代 过后 的 结果                   In       [     12     ] :       gen       =       (     i       for       i       in       range     (     5     ) )         In       [     13     ] :       2       in       gen       Out     [     13     ] :       True         In       [     14     ] :       3       in       gen       Out     [     14     ] :       True         In       [     15     ] :       1       in       gen       Out     [     15     ] :       False       #   1 为什么 不 在 gen 里面 了 ?   因为 调用 1 - & gt ; 2 , 这个 时候 1 已经 不 在 迭代 器 里面 了 , 被 按 需 生成 过 了                     在 循环 中 修改 列表 项 ， 例如 在 循环 中 删除 某个 元素 导致 index 紊乱                   In       [     44     ] :       a       =       [     1     ,       2     ,       3     ,       4     ,       5     ]         In       [     45     ] :       for       i       in       a     :             ....     :               if       not       i       %       2     :             ....     :                       a     .     remove     (     i     )             ....     :         In       [     46     ] :       a       Out     [     46     ] :       [     1     ,       3     ,       5     ]       #   没有 问题         In       [     50     ] :       b       =       [     2     ,       4     ,       5     ,       6     ]         In       [     51     ] :       for       i       in       b     :             ....     :                 if       not       i       %       2     :             ....     :                         b     .     remove     (     i     )             ....     :         In       [     52     ] :       b       Out     [     52     ] :       [     4     ,       5     ]       #   本来 我 想要 的 结果 应该 是 去除 偶数 的 列表                     IndexError ，                   In       [     55     ] :       my _ list       =       [     1     ,       2     ,       3     ,       4     ,       5     ]         In       [     56     ] :       my _ list     [     5     ]       #   根本 没有 这个 元素         In       [     57     ] :       my _ list     [     5     : ]       #   这个 是 可以 的                     全局变量 和 局部变量 重名 问题                   In       [     58     ] :       def       my _ func     ( ) :             ....     :                       print     (     var     )       #   我 可以 先 调用 一个 未定义 的 变量             ....     :         In       [     59     ] :       var       =       &# 39 ; global &# 39 ;       #   后 赋值         In       [     60     ] :       my _ func     ( )       #   反正 只要 调用函数 时候 变量 被 定义 了 就 可以 了       global         In       [     61     ] :       def       my _ func     ( ) :             ....     :               var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     62     ] :       var       =       &# 39 ; global &# 39 ;         In       [     63     ] :       my _ func     ( )         In       [     64     ] :       print     (     var     )         global       #   局部变量 没有 影响 到 全局变量         In       [     65     ] :       def       my _ func     ( ) :             ....     :                       print     (     var     )       #   虽然 你 全局 设置 这个 变量 ,   但是 局部变量 有 同名 的 ,   python 以为 你 忘 了 定义 本地 变量 了             ....     :                       var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     66     ] :       var       =       &# 39 ; global &# 39 ;         In       [     67     ] :       my _ func     ( )       - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -       UnboundLocalError                                                       Traceback       (     most       recent       call       last     )       & lt ;     ipython     -     input     -     67     -     d82eda95de40     & gt ;       in       & lt ;     module     & gt ;     ( )       - - - - & gt ;       1       my _ func     ( )         & lt ;     ipython     -     input     -     65     -     0     ad11d690936     & gt ;       in       my _ func     ( )                   1       def       my _ func     ( ) :       - - - - & gt ;       2                       print     (     var     )                   3                       var       =       &# 39 ; locally   changed &# 39 ;                   4         UnboundLocalError     :       local       variable       &# 39 ; var &# 39 ;       referenced       before       assignment         In       [     68     ] :       def       my _ func     ( ) :             ....     :                       global       var       #   这个 时候 得加 全局 了             ....     :                       print     (     var     )       #   这样 就 能 正常 使用             ....     :                       var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     69     ] :       var       =       &# 39 ; global &# 39 ;         In       [     70     ] :         In       [     70     ] :       my _ func     ( )       global         In       [     71     ] :       print     (     var     )       locally       changed       #   但是 使用 了 global 就 改变 了 全局变量                       is     和     = =     前者 比较 是否是 同一个 对象 ， 后者 比较 值 是否 相等           工具 函数           获取 对象 成员 ，   getmembers   ,     dir   等 函数                   from       inspect       import       getmembers       for       k     ,     v       in       (     getmembers     (     object     ) ) :               print       k     ,     &# 39 ; : &# 39 ;     ,       v        ", "tags": "python", "url": "/wiki/python/python.html"},
      
      
      {"title": "Python 教程", "text": "    Table   of   Contents           关于           安装           数据结构           List                         关于       深感 需要 一门 语言 作为 个人 核心 语言 ， python   很 不错 ， 所以 打算 深入 学习 这门 语言 。     于是 ， 刷 一遍 python 官方 文档 很 有 必要 ！       官方 文档 地址         安装       WINDOW 平台 直接 安装 编译 好 的 二进制码 就行 ， 按照 正常 软件 安装 流程 安装 。       MAC   可以 先 安装   brew ,   然后     brew   install   python     一般 系统 自带   python       Linux   安装 ， 一般 自带 python 。 这里 介绍 一个 local 模式 。       首先 下载   python   对应 版本 的 源代码 ， 然后 编译 。 编译 时 指定     - - prefix = 你 要 安装 的 目录       通过 将 编译 后 的 bin 目录 加 到 环境变量   PATH   最 前面 的 方式 ， 覆盖 系统 路径 中 的 python 。     在 . bashrc 中 加入 代码     export   PATH = / path - to - local - python / bin / : $ PATH         数据结构       List           可以 用来 做 stack ， 使用     append   ,     pop     方法 ！       也 可以 用来 做 队列 ， 但是 不够 高效 ！ 可以 使用     collections . deque   ， 它 被 设计 为 在 双向   append , pop   都 很 高效 ！      ", "tags": "python", "url": "/wiki/python/python-tutorial.html"},
      
      
      {"title": "Python 日期时间相关模块总结", "text": "    Table   of   Contents           简介           time   模块           基本 用法                   datetime   模块           calendar   模块           总结                 简介       Python 标准 库 提供 了 对 日期 、 时间 、 日历 进行 操作 的 模块   time ,   datetime ,   calendar   。     其中   time   模块 基本上 是 对 Unix 时间 戳 的 操作 和 处理 ， 以及 涉及 到 操作系统     相关 的 操作 ，   datetime   模块 则 是 对 日期 和 时间 进行 的 处理 封装 ， 支持 时间     之间 的 运算 ， 在 对 日期 和 时间 的 处理 上 比   time   模块 要 方便 。       time   模块         time   模块 在 python 标准 库中 ， 被 放在 了 通用 操作系统 服务 目录 下 ，     由此可见 这个 模块 跟 操作系统 有着 很大 的 关系 。     因为 这个 原因 ， 某些 函数 是 与 操作系统 平台 有关 的 。     由于 是 基于 Unix 时间 戳 ， 导致 时间 表示 的 范围     被 限定 在 1970 - 2038 年 之间 。     这个 模块 中 的 基本 数据 结果 是   struct _ time   ， 实际上 是 一个 有 名字 的 元组 。       这个 模块 提供 的 时间 操作 函数 主要 是 时间 戳 ， 时间 字符串 和   struct _ time   三种 数据 中 的 相互 转换 。     还有 一些 操作系统 跟 时间 有关 的 系统 调用 。       基本 用法       生成 Unix 时间 戳 ， 单位 是 秒               & gt ; & gt ; & gt ;       ts       =       time     .     time     ( )       & gt ; & gt ; & gt ;       ts       1445495655.495                 时间 戳 是 一个 浮点数 ， 可以 通过 内置 函数 转换 为 其他 格式 。       时间 戳 转换 为   struct _ time                 & gt ; & gt ; & gt ;       time     .     gmtime     (     ts     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     6     ,       tm _ min     =     34     ,       tm _ sec     =     15     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     =     0     )         & gt ; & gt ; & gt ;       time     .     localtime     (     ts     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     14     ,       tm _ min     =     34     ,       tm _ sec     =     15     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     =     0     )                 时间 戳 转换 为 方便 阅读 的 字符串                     & gt ; & gt ; & gt ;       time     .     ctime     (     ts     )       &# 39 ; Thu   Oct   22   14 : 34 : 15   2015 &# 39 ;                   struct _ time   转换 为 字符串                     & gt ; & gt ; & gt ;       st       =       time     .     localtime     (     ts     )       & gt ; & gt ; & gt ;       time     .     asctime     (     st     )       &# 39 ; Thu   Oct   22   14 : 34 : 15   2015 &# 39 ;         & gt ; & gt ; & gt ;       time     .     strftime     (     &# 39 ; % Y - % m -     % d     &# 39 ;     ,       st     )       &# 39 ; 2015 - 10 - 22 &# 39 ;                   struct _ time   转换 为 时间 戳                   & gt ; & gt ; & gt ;       time     .     mktime     (     st     )       1445495655.0                 时间 字符串 转   struct _ time                         & gt ; & gt ; & gt ;       time     .     strptime     (     &# 39 ; 2015 - 10 - 22 &# 39 ;     ,     &# 39 ; % Y - % m -     % d     &# 39 ;     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     0     ,       tm _ min     =     0     ,       tm _ sec     =     0     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     = -     1     )                 与 时间 有关 的 系统 调用 ， 如   time . sleep   。       datetime   模块       datetime 模块 提供 对 时间 和 日期 的 封装 ， 并 提供 他们 之间 的 数学 运算 。     该 模块 包含 4 个类 ， 用 得 多 的 是   datetime . timedelta   和   datetime . datetime   。                       object               timedelta               #   主要 用于 计算 时间跨度               tzinfo                     #   时 区 相关               time                         #   只 关注 时间               date                         #   只 关注 日期                       datetime         #   同时 有 时间 和 日期                 date 只 包含 年月日 3 个 属性 ， datetime 还 包含 时 、 分 、 秒 、 毫秒 。       获取 现在 时间                     & gt ; & gt ; & gt ;       datetime     .     today     ( )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       15     ,       8     ,       54     ,       88000     )       & gt ; & gt ; & gt ;       datetime     .     now     ( )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       15     ,       8     ,       41     ,       304000     )                 与 时间 戳 的 转换                       & gt ; & gt ; & gt ;       datetime     .     fromtimestamp     (     ts     )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       14     ,       34     ,       15     ,       495000     )                 利用   datetime . combine ( date , time )   可以 将 date 和 time 组合 为 datetime 。     利用   datetime . strptime   和   datetime . strftime       可以 在 时间 字符串 和 datetime 对象 间 相互 转换 。                     & gt ; & gt ; & gt ;       dt       =       datetime     .     strptime     (     &# 39 ; 2015 - 10 - 12 &# 39 ;     ,     &# 39 ; % Y - % m -     % d     &# 39 ;     )       & gt ; & gt ; & gt ;       dt       datetime     .     datetime     (     2015     ,       10     ,       12     ,       0     ,       0     )       & gt ; & gt ; & gt ;       dt     .     strftime     (     &# 39 ;     % d     / % m / % Y &# 39 ;     )       &# 39 ; 12 / 10 / 2015 &# 39 ;                 对 datetime 对象 做 部分 修改                       & gt ; & gt ; & gt ;       dt     .     replace     (     year     =     2016     )       datetime     .     datetime     (     2016     ,       10     ,       12     ,       0     ,       0     )                 转换 为 timetuple 也 就是 time . struct _ time                     & gt ; & gt ; & gt ;       dt     .     timetuple       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     12     ,       tm _ hour     =     0     ,       tm _ min     =     0     ,       tm _ sec     =     0     ,       tm _ wday     =     0     ,       tm _ yday     =     285     ,       tm _ isdst     = -     1     )                   datetime . timedelta   对象 和   datetime . time   对象 的 属性 是 类似 的 ， 只不过 前者 是 时间差 。       total _ seconds   方法 返回 总 的 秒数 。         timedelta   和   datetime   之间 的 数学 运算 可以 归纳 为                       timedelta       =       datetime       -       datetime       datetime       =       datetime       +       timedelta                 例如                     & gt ; & gt ; & gt ;       delta       =       timedelta     (     hours     =     1     )       & gt ; & gt ; & gt ;       delta       datetime     .     timedelta     (     0     ,       3600     )       & gt ; & gt ; & gt ;         & gt ; & gt ; & gt ;       dt       +       delta       datetime     .     datetime     (     2015     ,       10     ,       12     ,       1     ,       0     )                 calendar   模块       这个 模块 主要 提供 日历 的 一些 操作 ， 可以 很 方便 生成 一个 文本 日历 。                       & gt ; & gt ; & gt ;       print       calendar     .     month     (     2015     ,     10     )                 October       2015       Mo       Tu       We       Th       Fr       Sa       Su                           1         2         3         4         5         6         7         8         9       10       11       12       13       14       15       16       17       18       19       20       21       22       23       24       25       26       27       28       29       30       31                 calendar 包含 一个   Calendar   对象 ， 描述 了 日历 数据 的 结构 和 一些 设置 操作 。     而 格式化 任务 交给 了 两个 子类   TextCalendar   和   HTMLCalendar   。       总结       时间 模块 的 比较 ：     如果 对 时间 操作 是 在 字符串 和 时间 戳间 转换 的话 ， 用 time 模块 。     如果 需要 对 时间 进行 比较复杂 的 数学 运算 的话 ， 用 datetime 模块 。  ", "tags": "python", "url": "/wiki/python/python-date-time-module.html"},
      
      
      {"title": "requests包", "text": "    Table   of   Contents           关于           快速 入门           基本操作                         关于       如果 你 还 在 为 urllib 包而 烦恼 ， 不妨 试试 Python   Requests 包 。     这个 包 的 特点 是 ， 简单明了 ！       快速 入门       参考   官方 文档   。       基本操作           GET :     requests . get ( \" https : / / github . com / timeline . json \" )         POST :       PUT       HEAD       DELETE           OPTIONS               传递 URL 参数 ：                       payload       =       {     &# 39 ; key1 &# 39 ;     :       &# 39 ; value1 &# 39 ;     ,       &# 39 ; key2 &# 39 ;     :       &# 39 ; value2 &# 39 ;     }       r       =       requests     .     get     (     & quot ; http : / / httpbin . org / get & quot ;     ,       params     =     payload     )       print       r     .     url                     响应 内容     r . text   （ 文本 ） ,     r . content   （ 二进制 ） ， 编码   r . encoding   。 requests 会 自动 帮 你 解码     gzip   ！       JSON 响应     r . json ( )         原始 套 接字                   r       =       requests     .     get     (     url     ,       stream     =     True     )       r     .     raw       r     .     iter _ content     (     chunk _ size     )                     定制 HTTP 头     requests . get ( url ,   headers = headers )         POST   multipart - encoded                   files       =       {     &# 39 ; file &# 39 ;     :       open     (     &# 39 ; report . xls &# 39 ;     ,       &# 39 ; rb &# 39 ;     ) }       r       =       requests     .     post     (     url     ,       files     =     files     )                     Cookie     r . cookie     是 一个 字典      ", "tags": "python", "url": "/wiki/python/requests.html"},
      
      
      {"title": "seaborn 绘图工具", "text": "    Table   of   Contents           关于           配置           主题 风格           移除 上边 和 右边 的 边框           设置 局部 式样           线性 渐变 颜色 画板                   可视化 数据 集           绘制 分布图   distplot           两个 变量 分布图   jointplot           线性 回归 图           category   变量                   Grid                 关于       kaggle   上 很多 人用 这个 工具 做图 ， 图片 还 不错 ， 比   ggplot   貌似 要 更加 符合 matlab 风格 。     网站 地址     https : / / stanford . edu / ~ mwaskom / software / seaborn         配置       主题 风格       默认 风格 已经 不错 ， 如果 要 配置 风格 可以 使用 包 的   axes _ style ( )   和   set _ style ( )   命令 。     五中 默认 主题 是   darkgrid ,   whitegrid ,   dark ,   white ,   ticks   ， 默认 是   darkgrid   。     关键词 汇总 有   grid   代表 有 网格 ， 没有 的 代表 没有 网格 。               import       seaborn       as       sns       sns     .     set _ style     (     & quot ; whitegrid & quot ;     )       data       =       np     .     random     .     normal     (     size     =     (     20     ,     6     ) )       +       np     .     arange     (     6     )       /       2       sns     .     boxplot     (     data     =     data     )                 移除 上边 和 右边 的 边框       对于   white ,   ticks   主题 ， 通常 可以 移除 上边 和 右边 的 边框 ， 通过   sns . despine ( )   命令 就 可以 了 。       设置 局部 式样       使用     sns . axes _ style ( )     函数 和     with     环境 。                               with       sns     .     axes _ style     (     & quot ; darkgrid & quot ;     ) :               plt     .     subplot     (     211     )               sinplot     ( )       plt     .     subplot     (     212     )       sinplot     (     -     1     )                 重写 其他 配置 属性 ：                           sns     .     set _ style     (     & quot ; darkgrid & quot ;     ,       {     & quot ; axes . facecolor & quot ;     :       & quot ; . 9 & quot ;     } )                 所有 的 配置 属性 可以 通过   . axes _ style ( )   ， 传入 空 参数 得到 ：                         sns     .     axes _ style     ( )         {     &# 39 ; axes . axisbelow &# 39 ;     :       True     ,         &# 39 ; axes . edgecolor &# 39 ;     :       &# 39 ; . 8 &# 39 ;     ,         &# 39 ; axes . facecolor &# 39 ;     :       &# 39 ; white &# 39 ;     ,         &# 39 ; axes . grid &# 39 ;     :       True     ,         &# 39 ; axes . labelcolor &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; axes . linewidth &# 39 ;     :       1.0     ,         &# 39 ; figure . facecolor &# 39 ;     :       &# 39 ; white &# 39 ;     ,         &# 39 ; font . family &# 39 ;     :       [     u     &# 39 ; sans - serif &# 39 ;     ] ,         &# 39 ; font . sans - serif &# 39 ;     :       [     u     &# 39 ; Arial &# 39 ;     ,           u     &# 39 ; Liberation   Sans &# 39 ;     ,           u     &# 39 ; Bitstream   Vera   Sans &# 39 ;     ,           u     &# 39 ; sans - serif &# 39 ;     ] ,         &# 39 ; grid . color &# 39 ;     :       &# 39 ; . 8 &# 39 ;     ,         &# 39 ; grid . linestyle &# 39 ;     :       u     &# 39 ; - &# 39 ;     ,         &# 39 ; image . cmap &# 39 ;     :       u     &# 39 ; Greys &# 39 ;     ,         &# 39 ; legend . frameon &# 39 ;     :       False     ,         &# 39 ; legend . numpoints &# 39 ;     :       1     ,         &# 39 ; legend . scatterpoints &# 39 ;     :       1     ,         &# 39 ; lines . solid _ capstyle &# 39 ;     :       u     &# 39 ; round &# 39 ;     ,         &# 39 ; text . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; xtick . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; xtick . direction &# 39 ;     :       u     &# 39 ; out &# 39 ;     ,         &# 39 ; xtick . major . size &# 39 ;     :       0.0     ,         &# 39 ; xtick . minor . size &# 39 ;     :       0.0     ,         &# 39 ; ytick . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; ytick . direction &# 39 ;     :       u     &# 39 ; out &# 39 ;     ,         &# 39 ; ytick . major . size &# 39 ;     :       0.0     ,         &# 39 ; ytick . minor . size &# 39 ;     :       0.0     }         ` `     `         ###   设置 绘图 上下文       内置 四中 上下文 ：     ` paper ,   notebook ,   talk ,   poster `     ， 默认 是     ` notebook `     。       上下文 可以 通过     ` sns . set _ context ( &# 39 ; paper &# 39 ; ) `     这种 形式 进行 设置 。           ` set _ context `     函数 还 可以 指定 两个 参数 ，     ` font _ scale `     和     ` rc `     参数 ， 用来 指定 字体大小 和 其他 运行 参数 。 其他 运行 参数 ？       ` sns . set _ context ( & quot ; notebook & quot ; ,   font _ scale = 1.5 ,   rc = { & quot ; lines . linewidth & quot ; :   2.5 } ) `         函数     ` sns . set ( ) `     将 所有 配置 设置 为 默认 情况     (     传入 空 参数     )     ， 或者 更 多 其他 参数 ， 包括     ` rc `     参数 。         ##   Color   palettes       ` sns . color _ palette ( ) `     获取 和 设置     color       palette       使用     hls     循环 颜色 系统 ， 可以 指定 第一个 参数 为     ` hls `     ， 或者 使用     ` hls _ palette ( ) `     函数 。       绘制 颜色 画板 的 函数 是       ` sns . palplot ( pal ) `     。       一种 更亮 的 版本 是       husl       系统 。         ###   Color   Brewer       类似 于     color       map       ？       好多 颜色 ， 先不看 了 。 看不下去 了 。         ###   使用 命名 好 的 颜色       xkcd       颜色 命名 有     954     中 颜色 ， 可以 通过 字典       ` sns . xkcd _ rgb `       查看 。       使用 方式 也 是 通过 这个 字典 。       利用 这个 字典 ， 可以 将 颜色 名字 列表 转换 为 颜色 画板 。         ` `     `     python       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       1     ] ,       sns     .     xkcd _ rgb     [     & quot ; pale   red & quot ;     ] ,       lw     =     3     )       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       2     ] ,       sns     .     xkcd _ rgb     [     & quot ; medium   green & quot ;     ] ,       lw     =     3     )       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       3     ] ,       sns     .     xkcd _ rgb     [     & quot ; denim   blue & quot ;     ] ,       lw     =     3     ) ;         colors       =       [     & quot ; windows   blue & quot ;     ,       & quot ; amber & quot ;     ,       & quot ; greyish & quot ;     ,       & quot ; faded   green & quot ;     ,       & quot ; dusty   purple & quot ;     ]       sns     .     palplot     (     sns     .     xkcd _ palette     (     colors     ) )                 交互式 颜色 选择 工具     http : / / www . luminoso . com / colors /         线性 渐变 颜色 画板               sns     .     palplot     (     sns     .     color _ palette     (     & quot ; Blues & quot ;     ) )       sns     .     palplot     (     sns     .     color _ palette     (     & quot ; BuGn _ r & quot ;     ) )       sns     .     palplot     (     sns     .     color _ palette     (     & quot ; GnBu _ d & quot ;     ) )                 更 多 画板 工具 后面 再 详细 地 添加 吧 。       可视化 数据 集       绘制 分布图     distplot               distplot ( )   ， 它会 绘制 histogram   直方图 ， 并且 会 拟合 一个     kernel   density   estimate   ( KDE )   。                                           x       =       np     .     random     .     normal     (     size     =     100     )       sns     .     distplot     (     x     ) ;                         Histograms   直方图 ：   将     distplot     的 参数设置 为     kde = False     就 可以 去掉 KDE ，   rugplot ( )       也 可以 通过 制定 参数     rug = True     来 实现 。 rug 图是 像 条形码 一样 的 图 ， 用 竖线 的 疏密 来 表达 数据 的 多少 。               KDE 图 ， 将   distplot     的 参数设置 为     hist = False   即可 ， 也 可以 通过     . kdeplot ( )     来 实现 。               fit ， 制定 fit 参数               两个 变量 分布图     jointplot             散点图 ，     . jointplot ( x =   ' x ' ,   y = ' y ' ,   data = data )         两 变量 直方图 ， hexbins   plot ， 添加 参数     kind = ' hex '         两 变量 KDE 图 ，   kind = \" kde \"   ， 也 可以 通过   kdeplot         高 level   KDE 图 ，     sns . kdeplot ( df . x , df . y ,   shade = True ,   n _ levels = 60 )               jointplot     返回 一个     JoinGrid                   . pairplot     观察 两 两 变量 之间 的 关系 。 可以 指定 第三个 变量 为     hue   ， 使得 用 颜色 来 区分 第三个 变量 。               线性 回归 图             regplot ( )   and   lmplot ( )   ， 线性 回归 图 ，   lmplot ( )   功能 比较 强大 。         lmplot     的     order   参数 可以 指定 回归 的 阶数 。         robust = True     可以 指定 去掉 异常 值 。         scatter _ kws = { \" s \" :   80 }     这个 参数 可以 指定 scatter   的 属性         hue     指定 第三个 变量 作为 颜色 画图         col     指定 第三个 变量 作为 列 分隔             row     指定 第三个 变量 作为 行 分隔               在 其他 作图 上下文 做 回归 图 ，   指定 作图 类型 为 ：   kind = \" reg \"                 category   变量             sns . stripplot     其中 一个 变量 是 category 类型       因为 tripplot 作图 时 ， 数据 点会 重叠 ， 所以 可以 指定 参数   jitter = True   ， 增加 一点 随机 偏移 ， 让 数据 点 不 重叠       另 一种 防止 重叠 的 方法 使用 函数     sns . swarmplot   ， 会 采用 防止 重叠 算法 ， 可以 指定 第三个 变量 为   hue           . boxplot ( )     会 将 另 一个 连续变量 的 分布 也 显示 出来         sns . violinplot     会 将 另 一个 变量 的 KDE 也 显示 出来       用     scale     指定 第三个 变量       用     hue     指定 第三个 变量 ， 同时 指定     split = Ture         统计 估计     barplot         对 category 进行 统计     sns . countplot   ， 类似 于   df . count _ values ( ) . plot           sns . pointplot     不 知道 干 啥 的           Grid           FaceGrid       PairGrid      ", "tags": "python", "url": "/wiki/python/seaborn.html"},
      
      
      {"title": "编译tensorflow", "text": "    Table   of   Contents           关于           环境           安装           安装 JDK8           安装 bazel                   参考资料                 关于       记录 从 源码 编译 tensorflow 的 过程       环境           CentOS   6     rpm   - - query   centos - release         GLIBC   版本 2.12 ；     strings   / lib64 / libc . so . 6   | grep   GLIBC _             安装       安装 JDK8       因为 bazel 需要 JDK8 。 你 可以 从 Oracle 官网 下载 ， 也 可以 下载 我 的 备份     https : / / pan . baidu . com / s / 1jJoMjuY         下载 后 解压 到 某个 目录 ， 例如 在 我 的 机器 上 是     ~ / jdk1 . 8.0 _ 161   ， 确保 这个 目录 下 有     / bin     这个 子目录 。       配置 环境变量             export   JAVA _ HOME = ~ / jdk1 . 8.0 _ 161   export   PATH = $ JAVA _ HOME / bin : $ PATH               执行   java   - version   命令 ， 查看 是否 配置 正确             java   version   & quot ; 1.8 . 0 _ 161 & quot ;   Java ( TM )   SE   Runtime   Environment   ( build   1.8 . 0 _ 11 - b12 )   Java   HotSpot ( TM )   64 - Bit   Server   VM   ( build   25.11 - b03 ,   mixed   mode )               安装 bazel       可以 直接 通过 软件包 安装 ， 参考 官网 文档   https : / / docs . bazel . build / versions / master / install . html         下面 介绍 从 源码 安装 步骤 ， 首先 下载 源文件   https : / / github . com / bazelbuild / bazel / releases   ， 并 解压 到   bazel /   目录 。     然后 运行 脚本   . / compile . sh         参考资料           https : / / www . tensorflow . org / install / install _ sources      ", "tags": "python", "url": "/wiki/python/compile-tf.html"},
      
      
      
      
      
        
      
      {"title": "MS-Word使用技巧汇总", "text": "  关于       微软 的 word 文档 是 一个 很 好 的 文字处理 工具 ， 但是 有时候 也 需要 一些 技巧 。     比如 在 撰写 大 论文 的 时候 ， 高效 的 排版 技巧 是 非常 必要 的 ， 往往 能够 减少     很多 不必要 的 时间 浪费 。 本文 汇总 了 我 在 使用 word 文档 时 的 一些 技巧 ，     大多数 是从 网上 总结 别人 的 经验 得到 的 ， 也 有 一些 自己 摸索 出来 的 技巧 。     如果 你 有 其他 需求 ， 最好 放狗 搜 ， 一般 都 能 搜索 到 。       格式 篇       长 文档 格式 的 一致性       通过 对 段落 添加 式样 标签 ， 如 “ 标题 1 ” ， “ 正文 ” 等 ， 而 不是 对 每 一个 段落         手动 添加 格式 。 这样 的 好处 是 ， 当 需要 更改 所有 相同 段落 的 时候 ， 比如         需要 将 所有 正文 段落 改成 四号 字体 ， 只 需要 更改 式样 标签 的 式样 即可 。         如果 你 熟悉 HTML 标签 就 会 知道 ， 这 相当于 给 一个 内容 块 添加 了 一个 标签 ，         如果 要 修改 这个 标签 的 式样 ， 只 需要 为 这个 标签 添加 格式 即可 ， 而 不用         在 每 一个 段落 上 申明 它 的 格式 。 总之 一个 原则   一定 不要 手动 为 一个 文本         单独 添加 格式 ！                 公式 排版       公式 对齐       制表 位 与 对齐 。 如果 你 需要 排版 数学公式 ， 需要 将 公式 居中 ， 而 将 公式 的     编号 放在 行末 并 右 对齐 。 显然 用 空格 来 控制 格式 不是 一个 好 主意 。 可以 采用     制表 位 实现 ， 创建 新 式样 公式 ， 并 修改 格式 ， 在 制表 位中 添加 两个 制表 位 ，     20 居中 和 40 右 对齐 ， 然后 在 公式 前 和 编号 前 分别 添加 制表符 即可 。 效果 如下 图 。     总之 一个 原则   尽量 不要 通过 空格 来 调整 格式 ！                 公式 自动 编号       “ 插入 ” - & gt ; “ 引用 ” - & gt ; “ 题注 ” 插入 公式 编号 ；     在 需要 引用 公式 的 地方 用 “ 插入 ” - & gt ; “ 引用 ” - & gt ; “ 交叉 引用 ” 引用 公式 编号 ，     可以 选择 “ 整项 题注 ” 引用 公式 编号 行 的 所有 内容 。       页眉 自动 根据 章节 标题 自动 插入 ：       可以 采用 域 的 方式 在 文中 的 页眉 中 自动 插入 标题 的 编号 或 名称 ：     （ 1 ） 双击 页眉 位置 ， 页眉 处于 可 编辑 状态 ；       （ 2 ） 选择 插入 - 文档 部件 - 域       （ 3 ） 选择 “ 类别 - 链接 和 引用 ” 下 的 styleref ， 选择 章节 使用 的 “ 样式 名 ” （ 要 使用 样式 哦 ） ， “ 域 选项 ” 里 不选 任何 项即 插入 章节 标题 ； 若 “ 域 选项 ” 框中 选择 了 “ 插入 段落 编号 ” ， 则 插入 章节 编号 。       文档 分节       为什么 要 分节 ？           分节 可以 对 每 一节 应用 不同 的 页眉 、 页脚 和 页码 。       分节 文档 转换 为 PDF 时 ， 可以 自动 保证 每 一节 的 第一页 在 奇数 页 。           分节 编制 页码       首先 我们 需要 把 封面 页 和 正文 页 进行 分节 ， 首先 将 封面 页 和 正文 的 第一页 放在 一起 ， 中间 不要 间隔 ， 然后 在 正文 的 第一个 字 之前 点击 光标 ， 插入 分节 符 。       然后 取消 链接 到 前 一条 页眉 ， 最后 设置 页码 格式 中 起始页 码为 1 .       参考     http : / / jingyan . baidu . com / article / 4f7d57129d9c501a201927ea . html    ", "tags": "software", "url": "/wiki/software/ms-word.html"},
      
      
      
      
      
        
      
      {"title": "Spark reducebykey", "text": "    Table   of   Contents               spark   reduceByKey   是 默认 包含 了 map 端 combine 操作 的 ,   不过 如果 key 是 array 除外 , 这种 无法 使用 map 段 合并  ", "tags": "spark", "url": "/wiki/spark/spark-reducebykey.html"},
      
      
      {"title": "Spark Streaming", "text": "    Table   of   Contents           简介           StreamingContext           Input   DStreams   and   Receivers           基本 源                   DStreams   的 变换 操作           UpdateStateByKey   操作           Transform   操作           Window   操作           join   操作                         简介       Spark   Streaming   将流 数据 按照 时间 离散 化 ， 每 单位 时间 一个 batch ！ 这 是 和 其他 流 处理 系统 不同 的 地方 。     好处 是 效率 更高 ， 缺点 是 牺牲 了 一定 的 实时性 。               StreamingContext               import       org . apache . spark ._       import       org . apache . spark . streaming ._         val       conf       =       new       SparkConf     ( ) .     setAppName     (     appName     ) .     setMaster     (     master     )       val       ssc       =       new       StreamingContext     (     conf     ,       Seconds     (     1     ) )                 每 一个   Batch   是 一个 DStream 对象 ， 一个   DStream   对象 实际上 是 一些 列 的 RDD 。               Input   DStreams   and   Receivers           输入 源 ：       基本 数据源 ： 文件系统 和 套 接字 连接       高级 源 ： Kafka ,   Flume ,   Kinesis ,   etc               每 一个   DStream   都 有 至少 一个   Receiver ， 本地 模式 运行 时 ， 线程 数目 应该 多余 源 的 数目 。           基本 源           文件系统 ：   streamingContext . fileStream [ KeyClass ,   ValueClass ,   InputFormatClass ] ( dataDirectory )         RDD 队列 ：   streamingContext . queueStream ( queueOfRDDs )         Custom   Receivers           DStreams   的 变换 操作           支持 RDD 上 类似 的 变换 操作 。           UpdateStateByKey   操作       用于 维护 一个 持续 的 状态 ， 状态 可以 是 任意 数据类型 。               def       updateFunction     (     newValues     :       Seq     [     Int     ] ,       runningCount     :       Option     [     Int     ] )     :       Option     [     Int     ]       =       {               val       newCount       =       ...         / /   add   the   new   values   with   the   previous   running   count   to   get   the   new   count               Some     (     newCount     )       }       val       runningCounts       =       pairs     .     updateStateByKey     [     Int     ] (     updateFunction       _     )                 Transform   操作       直接 对 rdd 进行 操作               val       spamInfoRDD       =       ssc     .     sparkContext     .     newAPIHadoopRDD     ( ... )       / /   RDD   containing   spam   information         val       cleanedDStream       =       wordCounts     .     transform       {       rdd       = & gt ;           rdd     .     join     (     spamInfoRDD     ) .     filter     ( ... )       / /   join   data   stream   with   spam   information   to   do   data   cleaning           ...       }                 Window   操作           window   大小       sliding   大小                           / /   Reduce   last   30   seconds   of   data ,   every   10   seconds       val       windowedWordCounts       =       pairs     .     reduceByKeyAndWindow     ( (     a     :     Int     ,     b     :     Int     )       = & gt ;       (     a       +       b     ) ,       Seconds     (     30     ) ,       Seconds     (     10     ) )                 join   操作           Stream - stream   joins   将 两个   DStream   join 到 一起 ， 实际上 是 在 每个 时 间隙 ， 将 两个   DStream   的   RDD   join 到 一起 。      ", "tags": "spark", "url": "/wiki/spark/spark-streaming.html"},
      
      
      {"title": "Spark动态加载JAR包的问题", "text": "    Table   of   Contents           问题                 问题       在 开发 Spark 程序 时 ， 有时候 需要 动态 加载 jar 包到 系统 的   classpath   。     例如 ， file : / / / xxx . jar   包中 存在 一个 类 A 的 子类 AA ， A 类 在 当前 Spark     程序 中 存在 ， 但是 没有 子类 AA ，               val       jarPath       =       lines     (     1     )       val       myJar       =       Array     (     new       Path     (     jarPath     ) .     toUri     .     toURL     )         log     .     info     (     s & quot ; Load   jar       $ jarPath     & quot ;     )       val       parentLoader       =       Thread     .     currentThread     ( ) .     getContextClassLoader       val       classLoader       =       new       URLClassLoader     (     myJar     ,       parentLoader     )         val       a       =       classLoader     .     loadClass     (     & quot ; AA & quot ;     ) .     newInstance     ( ) .     asInstanceOf     [     A     ]                 如果 AA 操作 要 用到 SparkContext ， 那么 将会 报错     java . lang . ClassNotFoundException   。     可以 通过 将 jar 包 增加 到 上下文 中 ， 解决 此 问题 ！               sc     .     addJar     (     jarPath     )        ", "tags": "spark", "url": "/wiki/spark/dynamic-add-jar.html"},
      
      
      {"title": "Storm", "text": "    Table   of   Contents           安装 本地 环境           基本概念                 安装 本地 环境       这里 只 介绍 本地 环境 的 安装 ， 集群 的 安装 可以 参考 本地 环境 安装 和 网上 资料 。           安装   zookeeper ， zookeeper 在 storm 中 用于 管理 和 协调 集群 。           ` ` ` bash     brew   install   zookeeper     ` ` ` `           安装                   基本概念               拓扑   Topologies                 http : / / storm . apache . org / releases / 1.0 . 6 / Concepts . html            ", "tags": "spark", "url": "/wiki/spark/storm.html"},
      
      
      
      
      
        
      
      {"title": "Tensorflow 数据读取", "text": "    Table   of   Contents           TFRecordDataset           TFRecordDataset 的 生成                         TensorFlow   导入 数据 的 基本 机制 有 两种 , 一种 是 利用     placeholder   , 一种 是 将 数据 读取 也 作为 一个 OP 。     如果 在 GPU 上 执行 计算 图 的 计算 , 那么 前 一种 方式 会 因为 从 CPU 拷贝 数据 到 GPU 而 堵塞 , 后 一种 方式 可以 利用 多线程 提升 性能 。       TFRecordDataset         TFRecordDataset     是 Tensorflow 中 标准 格式 ,       TFRecordDataset 的 生成       如果 利用 Spark 可以 利用     spark - tensorflow - connector   ,     可以 直接 下载 编译 好 的 jar 包 , 通过     - - jars   path - to - spark - tensorflow - connector . jar     导入 这个 包 , 然后 就 可以 直接 将   DataFrame   转换成 分布式 TFRecord 文件 。     不过 这个 包要   Spark2 . 0   以上 才 支持 。               import       org . apache . commons . io . FileUtils       import       org . apache . spark . sql .     {       DataFrame     ,       Row       }       import       org . apache . spark . sql . catalyst . expressions . GenericRow       import       org . apache . spark . sql . types ._         val       path       =       & quot ; test - output . tfrecord & quot ;       val       testRows     :       Array     [     Row     ]       =       Array     (       new       GenericRow     (     Array     [     Any     ] (     11     ,       1     ,       23L     ,       10.0 F     ,       14.0     ,       List     (     1.0     ,       2.0     ) ,       & quot ; r1 & quot ;     ) ) ,       new       GenericRow     (     Array     [     Any     ] (     21     ,       2     ,       24L     ,       12.0 F     ,       15.0     ,       List     (     2.0     ,       2.0     ) ,       & quot ; r2 & quot ;     ) ) )       val       schema       =       StructType     (     List     (     StructField     (     & quot ; id & quot ;     ,       IntegerType     ) ,                                                                   StructField     (     & quot ; IntegerCol & quot ;     ,       IntegerType     ) ,                                                                 StructField     (     & quot ; LongCol & quot ;     ,       LongType     ) ,                                                                 StructField     (     & quot ; FloatCol & quot ;     ,       FloatType     ) ,                                                                 StructField     (     & quot ; DoubleCol & quot ;     ,       DoubleType     ) ,                                                                 StructField     (     & quot ; VectorCol & quot ;     ,       ArrayType     (     DoubleType     ,       true     ) ) ,                                                                 StructField     (     & quot ; StringCol & quot ;     ,       StringType     ) ) )         val       rdd       =       spark     .     sparkContext     .     parallelize     (     testRows     )         / / Save   DataFrame   as   TFRecords       val       df     :       DataFrame       =       spark     .     createDataFrame     (     rdd     ,       schema     )       df     .     write     .     format     (     & quot ; tfrecords & quot ;     ) .     option     (     & quot ; recordType & quot ;     ,       & quot ; Example & quot ;     ) .     save     (     path     )         / / Read   TFRecords   into   DataFrame .       / / The   DataFrame   schema   is   inferred   from   the   TFRecords   if   no   custom   schema   is   provided .       val       importedDf1     :       DataFrame       =       spark     .     read     .     format     (     & quot ; tfrecords & quot ;     ) .     option     (     & quot ; recordType & quot ;     ,       & quot ; Example & quot ;     ) .     load     (     path     )       importedDf1     .     show     ( )         / / Read   TFRecords   into   DataFrame   using   custom   schema       val       importedDf2     :       DataFrame       =       spark     .     read     .     format     (     & quot ; tfrecords & quot ;     ) .     schema     (     schema     ) .     load     (     path     )       importedDf2     .     show     ( )                 提交 命令               ##   jar 包在 本地       $ SPARK _ HOME   / bin / spark - shell   - - jars   target / spark - tensorflow - connector _ 2.11 - 1.10 . 0 . jar       ##   jar 包在 HDFS 上       $ SPARK _ HOME   / bin / spark - shell   - - jars   viewfs : / / xxxx / path / to / spark / tensorflow / connector . jar      ", "tags": "tensorflow", "url": "/wiki/tensorflow/dataset.html"},
      
      
      {"title": "Tensorflow 简介", "text": "    Table   of   Contents           关于           安装           学习 资料                 关于       从 TensorFlow 问世 开始 , 就 开始 关注 TensorFlow 了 。 但是 一直 没有 系统地 学习 , 这里 记录 一下 TensorFlow 的 笔记 。       安装       现在 直接 用 pip 安装 即可 , 在 国内 可以 使用   USTC   或者   THU   的 镜像 。               #   不 使用 镜像     pip   install   tensorflow       #   使用 USTC 的 镜像     pip   install   - i   https : / / mirrors . ustc . edu . cn / pypi / web / simple     tensorflow               学习 资料           官方 教程     https : / / www . tensorflow . org / tutorials /         中文 教程     http : / / www . tensorfly . cn /         TensorFlow   开发 模板     https : / / github . com / tobegit3hub / tensorflow _ template _ application        ", "tags": "tensorflow", "url": "/wiki/tensorflow/intro.html"},
      
      
      
      
      
        
      
      {"title": "atom使用技巧", "text": "    Table   of   Contents          ", "tags": "tools", "url": "/wiki/tools/atom.html"},
      
      
      {"title": "bash使用技巧", "text": "    Table   of   Contents           例子           基本 语法                 例子       提示符 修改               export       PS1     =     &# 39 ; \ \ u @ \ \ h   \ \ w   \ \ $   &# 39 ;       (   普通 示例   )       export       PS1     =     &# 39 ; \ \ [ \ \ e [ 0 ; 32m \ \ ] [ \ \ u @ \ \ h   \ \ w   \ \ $ ] \ \ [ \ \ e [ m \ \ ] &# 39 ;       (   颜色 示例   )       export       PS1     =     &# 39 ; \ \ t :   &# 39 ;       (   时间 示例   )       export       PS1     =     &# 39 ; \ \ u @ \ \ h   [ \ \ $ ( ls   |   wc   - l ) ] : \ \ $   &# 39 ;       (   显示 当前目录 行下 文件 数量   )                 基本 语法           流控制   if   ...   elif   ...   else                   if     condition     then             statements     [     elif     condition             then     statements .   ..   ]       [     else             statements     ]       fi                 其中 condition 可以 是 一个 命令 ， 也 可以 是 一个 条件 表达式 ， 其中 条件 判断 有 很多 细节 。           for   循环 。                   ##   use   list       for     i   in     1       2       3       5       6       do               echo       $ i       done         for     i   in     {     1   , 3 , 6 , 7   }       do               echo       $ i       done         ##   use   seq   command       for     i   in     $ (   seq     1       2       20     )       do               echo       $ i       done         ##   C - style       for       ( (     c     =     1     ;     c & lt ;   =     5     ;     c++   ) )       do               echo       $ i       done         ##   use   file       for     f   in   / etc / *     do               echo     f     done        ", "tags": "tools", "url": "/wiki/tools/bash.html"},
      
      
      {"title": "CentOS 上搭建PPTP VPN", "text": "    Table   of   Contents           关于           过程                 关于       记录 PPTP   VPN   搭建 流程 ， 亲测 成功 。     我 是 参考 这个 地方   http : / / www . dabu . info / centos6 - 4 - structures - pptp - vpn . html         过程           安装 ppp 和 iptables                 yum   install   - y   perl   ppp   iptables                   安装   pptpd           首先 查看 ppp 版本 ， 不同 版本 对应 的 pptpd 版本 也 不 一样             yum   list   installed   ppp               找到 对应 的 ppp 版本 :   http : / / poptop . sourceforge . net / yum / stable / packages /               ppp   2.4 . 4 — — — — — — & gt ; pptpd   1.3 . 4   ppp   2.4 . 5 — — — — — — & gt ; pptpd   1.4 . 0               我 的 电脑 是 i686 ,   2.4 . 5 版本 ， 所以 找到 对应 的 pptpd 版本 链接 为     http : / / poptop . sourceforge . net / yum / stable / packages / pptpd - 1.4 . 0 - 1 . fc12 . i686 . rpm         推荐 用   rpm   安装             rpm   - Uvh   http : / / poptop . sourceforge . net / yum / stable / packages / pptpd - 1.4 . 0 - 1 . fc12 . i686 . rpm   yum   install   pptpd               手动 安装 参考 原始 链接 。           修改 配置 pptpd                 cp   / etc / ppp / options . pptpd   / etc / ppp / options . pptpd . bak   vi   / etc / ppp / options . pptpd               添加 DNS 解析             ms - dns   8.8 . 8.8   ms - dns   8.8 . 4.4                   配置 用户 与 密码 ：                 cp   / etc / ppp / chap - secrets       / etc / ppp / chap - secrets . bak   vi   / etc / ppp / chap - secrets               加入 一行 ， 空格 隔开 ， 注意 后面 的 * 号             你 的 登陆 用户名   pptpd   你 的 登陆密码   *                   配置 pptpd   ip 转发                 cp   / etc / pptpd . conf           / etc / pptpd . conf . bak   vi   / etc / pptpd . conf               加入 下面 两行 ， 并 以 空行 结尾 这个 文件 。 下面 的 代码 不 需要 更改 其中 的 IP             localip   192.168 . 9.1   remoteip   192.168 . 9.11 - 30   / / 表示 vpn 客户端 获得 ip 的 范围                   配置 流量 转发                 vi   / etc / sysctl . conf               将   net . ipv4 . ip _ forward   =   0     改成     net . ipv4 . ip _ forward   =   1         保存 修改     / sbin / sysctl   - p             启动 pptpd 服务                 / sbin / service   pptpd   start     #   或者   service   pptpd   start                 到 此 ， 可以 测试 pptp 拨号 了 ， 应该 可以 成功 拨号 ， 用户名 和 密码 填 上面 配置 的 用户名 和 密码 ！     但是 还 没有 网络 访问 权限 。           配置 网络流量 转发                 iptables   - t   nat   - A   POSTROUTING         - s   192.168 . 9.0 / 24   - j   SNAT   - - to - source     你 的 服务器 公网 IP               保存 转发 规则 ， 并 重启 服务             / etc / init . d / iptables   save   / sbin / service   iptables   restart   service   pptpd   restart               至此 ， 就 可以 使用   VPN   科学 上网 了 ！           设置 开机 启动                 chkconfig   pptpd   on   chkconfig   iptables   on                   如果 你 嫌 安装 麻烦 ， 可以 试试   shadesocks ,   openvpn 等 易用 的 VPN 方案 。      ", "tags": "tools", "url": "/wiki/tools/vpn.html"},
      
      
      {"title": "Geohash 原理", "text": "    Table   of   Contents          ", "tags": "tools", "url": "/wiki/tools/geohash.html"},
      
      
      {"title": "Git常用命令汇总", "text": "    Table   of   Contents           关于           提交           打 tag           分支           . gitignore 文件           强大 的 分支           创建 分支           分支 合并           删除 分支           分支 管理           远程 分支                   Hook           常见 案例 汇总                 关于       这里 汇集 了 git 常用命令 和 用法 ， 便于 速查 ， 不定期 更新 。       提交           给   git   commit   加上   - a   选项 ， Git   就 会 自动 把 所有 已经 跟踪 过 的 文件 暂存 起来 一并 提交 ， 从而 跳过   git   add   步骤 ：           打 tag       将 当前 版本 添加 tag   & lt ; tagname & gt ;               git   tag   & lt ; tagname & gt ;               列出 标签             $   git   tag   v0 . 1   v1 . 3     $   git   tag   - l     &# 39 ; v1 . 4.2 . * &# 39 ;     v1 . 4.2 . 1   v1 . 4.2 . 2   v1 . 4.2 . 3   v1 . 4.2 . 4               将 tag 推送 到 远程 服务器 ， 参见 [ 1 - 2 ] ， 推送 所有 的 tags             git   push   - - tags               只 推送 单个 tag             git   push   origin   & lt ; tagname & gt ;                     http : / / stackoverflow . com / a / 5195913 / 4349983           https : / / git - scm . com / book / zh / v1 / Git - % E5 % 9F % BA % E7 % A1 % 80 - % E6 % 89 % 93 % E6 % A0 % 87 % E7 % AD % BE             分支               ##   创建 dev 分支     git   branch   dev     ##   切换 到 dev 分支     git   checkout   dev     ##   上述 两条 命令 也 可以 合并 为 一条     git   checkout   - b   dev       ##   在 dev 分支 作 修改 后 ， 再 切换 回 master 分支     git   checkout   master     ##   还 可以 创建 多个 其他 分支     git   checkout   - b   issu35   git   checkout   - b   issu37     ##   在 master 分支 中 ， 合并 dev 分支       ##   可 与 选择 只 合并 某 一个 分支     git   merge   issu35         ##   合并 后 ， 可以 删除 issu35 分支     git   branch   - d   issu35     ##   如果 合并 分支 时 ， 发生冲突 ， 解决 冲突 后 再 合并       ##   可以 通过 命令 查看 冲突 的 地方     git   status                 . gitignore   文件       文件   . gitignore   的 格式 规范 如下 ：           所有 空行 或者 以 注释 符号     ＃     开头 的 行 都 会 被   Git   忽略 。       可以 使用 标准 的     glob     模式匹配 。       匹配 模式 最后 跟 反 斜杠 （   /   ） 说明 要 忽略 的 是 目录 。       要 忽略 指定 模式 以外 的 文件 或 目录 ， 可以 在 模式 前 加上 惊叹号 （ ! ） 取反 。           所谓 的   glob   模式 是 指   shell   所 使用 的 简化 了 的 正则表达式 。           星号 （   *   ） 匹配 零个 或 多个 任意 字符 ；       [ abc ]   匹配 任何 一个 列 在 方括号 中 的 字符 （ 这个 例子 要么 匹配 一个   a ， 要么 匹配 一个   b ， 要么 匹配 一个   c ） ；       问号 （ ? ） 只 匹配 一个 任意 字符 ； 如果 在 方括号 中 使用 短 划线 分隔 两个 字符 ， 表示 所有 在 这 两个 字符 范围 内 的 都 可以 匹配 （ 比如   [ 0 - 9 ]   表示 匹配 所有   0   到   9   的 数字 ） 。           我们 再 看 一个   . gitignore   文件 的 例子 ：             #   此 为 注释   –   将 被   Git   忽略   #   忽略 所有   . a   结尾 的 文件   * . a   #   但   lib . a   除外   ! lib . a   #   仅仅 忽略 项目 根目录 下 的   TODO   文件 ， 不 包括   subdir / TODO   / TODO   #   忽略   build /   目录 下 的 所有 文件   build /   #   会 忽略   doc / notes . txt   但 不 包括   doc / server / arch . txt   doc / * . txt   #   ignore   all   . txt   files   in   the   doc /   directory   doc / * * / * . txt               A     * * /     pattern   is   available   in   Git   since   version   1.8 . 2 .       如果 不 小心 把 文件 加 到 缓存 区 ， 可以 先 通过 如下 命令 删除 ， 然后 再 将 模式 加 到   . gitignore   文件 中 。     使用   git   rm   的   - - cached   参数 。 后面 可以 列出 文件 或者 目录 的 名字 ， 也 可以 使用   glob   模式 。             git   rm   - - cached   readme . txt   git   rm   log /   \ \ *   . log               注意 到 星号   *   之前 的 反 斜杠     \ \   ， 因为   Git   有 它 自己 的 文件 模式 扩展 匹配 方式 ， 所以 我们 不用   shell   来 帮忙 展开 （ 译注 ： 实际上 不加 反 斜杠 也 可以 运行 ， 只不过 按照   shell   扩展 的话 ， 仅仅 删除 指定 目录 下 的 文件 而 不会 递归 匹配 。 上面 的 例子 本来 就 指定 了 目录 ， 所以 效果 等同 ， 但 下面 的 例子 就会用 递归 方式 匹配 ， 所以 必须 加反 斜杠 。 ） 。 此 命令 删除 所有   log /   目录 下 扩展 名为   . log   的 文件 。 类似 的 比如 ：             $   git   rm     \ \ *   ~               会 递归 删除 当前目录 及其 子目录 中 所有   ~   结尾 的 文件 。       强大 的 分支       创建 分支       创建   dev   分支 ：             git   branch   dev               git 的 分支 实际上 是 一个 指针 ， 指向 一个 提交 的 对象 ！     git 有 一个   HEAD   指针 ， 指向 当前 分支 指针 的 指针 ！     可以 通过   git   checkout   dev   命令 将   HEAD   指针 指向   dev   分支 。       可以 合并 创建 和 切换 分支 为 一条 指令   git   checkout   - b   dev   .       由于 git 创建 分支 非常 快 ， 只是 写入 42 字节 到 文件 ！ 所以 可以 频繁 地 使用 分支 ！       分支 合并       使用   git   merge   other - branch     合并 分支 :       fast - forward 模式 ： 如果 其中 当前 分支 是 另 一个 分支 的 上游 ， 那么 git 会 简单 的 将 当前 分支 对应 的 指针 移动 到 另 一个 分支 所 指向 的 commit 对象 ！     非常 快 ！       正常 合并 ： 当前 分支 不是 另 一个 分支 的 祖先 ， 那么 git 会 将 当前 分支 快照 和 另 一个 分支 的 快照 ， 以及 两者 最近 共同 祖先 ， 进行 三方 合并 ！     最后 创建 一个 新 的 快照 ， 将 当前 分支 指针 指向 它 ！       合并 冲突 ： 手动 修复 冲突 后 ， 重新 提交 ！       分 之 合并 的 图形化 工具 ：   git   mergetool         删除 分支       分支 的 删除 ：   git   branch   - d   one - branch         分支 管理           查看 当前 所有 分支 命令 ：   git   branch         查看 所有 分支 最后 一次 提交 ：   git   branch   - v   ,   - - merged , - - no - merged   参数 可以 指定 显示 合并 分支 和 未 合并 分支 ！           远程 分支       Hook       在     . git / hooks /     目录 下 的 可 执行 脚本 ， 支持   bash ,   ruby ,   python   等 。       常见 案例 汇总           已经 在 本地 有个 仓库 了 , 怎么 关联 到 远程 仓库 地址     &# 103 ; &# 105 ; &# 116 ; &# 64 ; &# 103 ; &# 105 ; &# 116 ; &# 104 ; &# 117 ; &# 98 ; &# 46 ; &# 99 ; &# 111 ; &# 109 ; &# 58 ; &# 120 ; &# 120 ; &# 120 ; &# 120 ; &# 47 ; &# 120 ; &# 120 ; &# 46 ; &# 103 ; &# 105 ; &# 116 ;                   git   remote   add   origin     git @ github . com : xxxx / xx . git               这里   origin   代表 远程 仓库 的 名字           如何 合并 远程 分支   branch1   到 本地 分支   branch2                   $ git     fetch   origin   branch1   From   github . com : tracholar / wiki     *   branch                         master           - & gt ;   FETCH _ HEAD       $ git     merge   FETCH _ HEAD               FETCH _ HEAD   是 一个 特殊 的 临时 分支 , 这 两个 操作 可以 合并 到 一个 操作             git   pull   origin   branch1               注意 , 以上 操作 都 是 在 本地 分支   branch2   上 操作 !           远程 仓库 太 大 , 只想 获取 master 分支 最后 一次 提交 的 代码 , 怎么 操作 ?                 git   clone   git @ github . com : xxxx / xx . git     - - depth   =     1     - - branch   =   master   - - single - branch                 - - depth = 1     表示 克隆 深度 为 1 , 即 只 克隆 最后 一次 提交 的 结果 ,     - - branch = master     表示 克隆 主 分支 ,   还要 加上     - - single - branch     参数 , 才 会 只 克隆 单一 分支 !  ", "tags": "tools", "url": "/wiki/tools/git.html"},
      
      
      {"title": "go语言", "text": "    Table   of   Contents           安装 环境           基础           基础 语法                         安装 环境       只试 过 MAC   OS 环境 ， 所以 就 说 这个 ， 不同 环境 应该 差不多 。     安装 GO 运行 环境 ， 然后 配置 两个 环境变量 ｀ GOPATH ｀ 和 ｀ GOBIN ｀       基础       基础 语法           程序 入口 包是   main         导出 名 的 首字母 必须 为 大写 ！       批量 import 方式                   import       (               & quot ; fmt & quot ;               & quot ; math & quot ;       )                     函数 签名 方式 ， 可以 将 相同 类型 的 形参 类型 申明 合并                   func       add     (     x       int     ,       y       int     )       int       {               #     TODO               return       x     +     y       }                     函数 可以 返回 多值 。                   func       swap     (     x     ,       y       string     )       (     string     ,       string     )       {               return       y     ,       x       }                     赋值 语法   a   : =   \" hello   world . \"         命名 返回值 ： Go   的 返回值 可以 被 命名 ， 并且 就 像 在 函数 体 开头 声明 的 变量 那样 使用 。         返回值 的 名称 应当 具有 一定 的 意义 ， 可以 作为 文档 使用 。         没有 参数 的   return   语句 返回 各个 返回 变量 的 当前 值 。 这种 用法 被称作 “ 裸 ” 返回 。         直接 返回 语句 仅 应当 用 在 像 下面 这样 的 短 函数 中 。 在 长 的 函数 中 它们 会 影响 代码 的 可读性 。                   func       getsum     (     sum       int     )       (     x     ,       y       int     ) {               sum       =       x     +     y               return       }                     变量 声明 关键字   var   ， 类型 放在 最后 。   =   用于 初始化 变量 ， 初始化 使用         表达式 可以 省略 类型 。                   var       c     ,       python     ,       java       bool       var       i     ,       j       int       =       1     ,       2       var       ruby     ,       Go     ,       haskell       =       true     ,       false     ,       & quot ; helloword & quot ;                     短 声明 变量 ， 在 函数 中 ，     : =     简洁 赋值 语句 在 明确 类型 的 地方 ， 可以 用于 替代   var   定义 。       基本 数据类型                   bool         string         int         int8         int16         int32         int64       uint       uint8       uint16       uint32       uint64       uintptr         byte       / /   uint8   的 别名         rune       / /   int32   的 别名                 / /   代表 一个 Unicode 码         float32       float64         complex64       complex128                     零值     变量 在 定义 时 没有 明确 的 初始化 时会 赋值 为   零值   。     零值 是 ：       数值 类型 为   0   ，       布尔 类型 为   false   ，       字符串 为   \" \"   （ 空 字符串 ） 。               表达式   T ( v )   将值   v   转换 为 类型   T   。         与   C   不同 的 是   Go   的 在 不同 类型 之间 的 项目 赋值 时 需要 显式 转换 .       GO 类型 推导 ， 变量 的 类型 由右值 推导 得出 。                   func       main     ( )       {               v       : =       & quot ; HaHa & quot ;       / /   change   me !               fmt     .     Printf     (     & quot ; v   is   of   type   % T \ \ n & quot ;     ,       v     )       }                     常量 的 定义   const   Pi   =   3.14   ， 常量 不能 使用   : =   语法 定义 。   =   表示 定义 ， 而   : =   表示 赋值 ？       Go   只有 一种 循环 结构 — —   for   循环 。 循环 初始化 语句 和 后置 语句 都 是 可选 的 。                   sum       : =       0       for       i       : =       0     ;       i       & lt ;       10     ;       i     ++       {               sum       + =       i       }                     for   是   Go   的   “ while ”                   for       sum       & lt ;       1000       {               sum       + =       sum       }                     死循环                   for       {       }                     if   条件 判断                   if       x       & lt ;       0       {               return       sqrt     (     -     x     )       +       & quot ; i & quot ;       }         if       x       & gt ;     10       {               ...       }       else       {               ...       }                     GO   的 循环 条件 和 判断 条件 都 不 需要 用 小括号 括 起来 ， 但是 括 起来 也 是 可以 的 。     亲测 通过 ， 另外 for 和 if 条件 表达式 中 定义 的 变量 的 作用域 仅 在 该 语句 块 中 有效 。               switch 分支 语句 ,   switch   的 条件 从上到下 的 执行 ， 当 匹配 成功 的 时候 停止 。 不像 C 需要 break 语句       没有 条件 的   switch         没有 条件 的   switch   同   switch   true   一样 。         这一 构造 使得 可以 用 更 清晰 的 形式 来 编写 长 的   if - then - else   链 。                           switch       {               case       t     .     Hour     ( )       & lt ;       12     :                       fmt     .     Println     (     & quot ; Good   morning ! & quot ;     )               case       t     .     Hour     ( )       & lt ;       17     :                       fmt     .     Println     (     & quot ; Good   afternoon .& quot ;     )               default     :                       fmt     .     Println     (     & quot ; Good   evening .& quot ;     )               }                     defer   语句 会 延迟 函数 的 执行 直到 上层 函数 返回 。         延迟 调用 的 参数 会 立刻 生成 ， 但是 在 上层 函数 返回 前 函数 都 不会 被 调用 。       defer 栈 ： 延迟 的 函数调用 被 压入 一个 栈中 。 当 函数 返回 时 ，   会 按照 后进先出 的 顺序调用 被 延迟 的 函数调用 。       Go   具有 指针 。   指针 保存 了 变量 的 内存地址 。         类型     * T     是 指向 类型     T     的 值 的 指针 。 其零值 是     nil     。         Go 指针 和 C 指针 的 区别 在于 Go 没有 指针 运算 。       结构 体 ， 和 C 差不多 ， 利用 关键字   type   定义 声明 。                   type       Vertex       struct       {               X       int               Y       int       }                 结构 体 也 可以 通过 指针 访问 ， 还是 用 符号   .   访问 成员           结构 体 文法 。   结构 体 文法 表示 通过 结构 体字 段 的 值 作为 列表 来 新 分配 一个 结构 体 。         使用   Name :   语法 可以 仅 列出 部分 字 段 。 （ 字段名 的 顺序 无关 。 ）         特殊 的 前缀   & amp ;   返回 一个 指向 结构 体 的 指针 。                   var       (               v1       =       Vertex     {     1     ,       2     }         / /   类型 为   Vertex               v2       =       Vertex     {     X     :       1     }         / /   Y : 0   被 省略               v3       =       Vertex     { }                 / /   X : 0   和   Y : 0               p         =       & amp ;     Vertex     {     1     ,       2     }       / /   类型 为   * Vertex       )                     数组 ， 类型   [ n ] T   是 一个 有   n   个 类型 为   T   的 值 的 数组 。                   var       a       [     10     ]     int                     slice ， 一个 slice 会 指向 一个 序列 的 值 ， 并且 包含 长度 信息 ，   len ( s )   返回 序列   s   的 长度 。       slice 的 slice ， 类似 于 二维 数组 ， 初始化 代码                   game       : =       [ ] [ ]     string     {                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,               }                     slice 切片 操作 ， 类似 于 python 代码                   s     [     lo     :     hi     ]       s     [ :     hi     ]       s     [     lo     : ]                     make 函数 构造 slice ,     make ( type ,   value   [ ,   capacity ] )   .   函数   cap ( )   获取 slice 的 容量 。                   a       : =       make     ( [ ]     int     ,       5     )                     slice 的 零值 是   nil   。           slice 添加 元素 ，   append ( slice ,   value1 ,   ... ,   valueN )   方法 ， 添加 元素 的 时候 ， slice 的 容量 会 自动 增加 。               slice 用法 ：   https : / / blog . go - zh . org / go - slices - usage - and - internals               range     迭代 ， 第一个 为 下标 ， 第二个 为值 的 拷贝 。 可以 用   _   来 忽略 下标                   var       pow       =       [ ]     int     {     1     ,       2     ,       4     ,       8     ,       16     ,       32     ,       64     ,       128     }       for       i     ,       v       : =       range       pow       {                       fmt     .     Printf     (     & quot ; 2 * * % d   =   % d \ \ n & quot ;     ,       i     ,       v     )       }       for       _     ,       v       : =       range       pow       {       }       for       idx       : =       range       pow       {       }                     map   映射 键到 值 。   类似 于 python 的 字典     map   在 使用 之前 必须 用   make   来 创建 ； 值为   nil   的   map   是 空 的 ， 并且 不能 对 其 赋值 。                   m       =       make     (     map     [     string     ]     int     )       m     [     & quot ; Bell   Labs & quot ;     ]       =       8                         map   的 操作     在   map   m   中 插入 或 修改 一个 元素 ：       m [ key ]   =   elem       获得 元素 ：       elem   =   m [ key ]       删除 元素 ：       delete ( m ,   key )       通过 双 赋值 检测 某个 键 存在 ：       elem ,   ok   =   m [ key ]       如果   key   在   m   中 ，   ok   为   true 。 否则 ，   ok   为   false ， 并且   elem   是   map   的 元素 类型 的 零值 。     同样 的 ， 当 从   map   中 读取 某个 不 存在 的 键 时 ， 结果 是   map   的 元素 类型 的 零值 。               函数 可以 作为 值 传递           函数 闭包                   / /   fibonacci   函数 会 返回 一个 返回   int   的 函数 。       func       fibonacci     ( )       func     ( )       int       {               var       a0     ,       a1       =       0     ,       1               return       func     ( )       int     {                       var       tmp       =       a0       +       a1                       a0       =       a1                       a1       =       tmp                       return       a0               }       }                     go 没有 类 ！ ！ ！ 然而 ， 仍然 可以 在 结构 体 类型 上 定义方法 。     方法 接收者   出现 在   func   关键字 和 方法 名 之间 的 参数 中 。                   func       (     v       *     Vertex     )       Abs     ( )       float64       {               return       math     .     Sqrt     (     v     .     X     *     v     .     X       +       v     .     Y     *     v     .     Y     )       }       v     .     Abs     ( )                     你 可以 对 包中 的   任意   类型定义 任意 方法 ， 而 不仅仅 是 针对 结构 体 。     但是 ， 不能 对 来自 其他 包 的 类型 或 基础 类型定义 方法 。                   type       MyFloat       float64         func       (     f       MyFloat     )       Abs     ( )       float64       {               if       f       & lt ;       0       {                       return       float64     (     -     f     )               }               return       float64     (     f     )       }         func       main     ( )       {               f       : =       MyFloat     (     -     math     .     Sqrt2     )               fmt     .     Println     (     f     .     Abs     ( ) )       }                     指针 作为 函数 接收者 ， 有 两个 原因 需要 使用 指针 接收者 。 首先 避免 在 每个 方法 调用 中 拷贝 值 （ 如果 值 类型 是 大 的 结构 体 的话 会 更 有效率 ） 。         其次 ， 方法 可以 修改 接收者 指向 的 值 。       接口 是 由 一组 方法 定义 的 集合 。 接口 区分 类型 本身 的 方法 和 类型 指针 绑定 的 方法 。                   type       Abser       interface       {               Abs     ( )       float64       }                     隐式 接口 ？ 什么 鬼 东西       Stringers     一个 普遍存在 的 接口 是   fmt   包中 定义 的   Stringer 。                   type       Stringer       interface       {               String     ( )       string       }                 Stringer   是 一个 可以 用 字符串 描述 自己 的 类型 。   fmt   包   （ 还有 许多 其他 包 ） 使用 这个 来 进行 输出 。     -   错误 接口               type       error       interface       {               Error     ( )       string       }                     Reader 接口           [ 1 ]   Go 学习 网页   https : / / tour . go - zh . org /    ", "tags": "tools", "url": "/wiki/tools/go.html"},
      
      
      {"title": "Hadoop", "text": "    Table   of   Contents           关于           Hadoop   shell                 关于       作为 大 数据 工作者 ， 怎能不 懂 hadoop 。       Hadoop   shell       利用 hadoop   shell   访问 HDFS 文件系统 ， 官方 文档 见   https : / / hadoop . apache . org / docs / r1 . 0.4 / cn / hdfs _ shell . html   。     这里 记录 一些 常用命令 和 技巧 。       调用 文件系统 ( FS ) Shell 命令 应 使用     bin / hadoop   fs   & lt ; args & gt ;   的 形式 。   所有 的 的 FS   shell 命令 使用 URI 路径 作为 参数 。 URI 格式 是 scheme : / / authority / path 。 对 HDFS 文件系统 ， scheme 是 hdfs ， 对 本地 文件系统 ， scheme 是 file 。 其中 scheme 和 authority 参数 都 是 可选 的 ， 如果 未 加 指定 ， 就 会 使用 配置 中 指定 的 默认 scheme 。 一个 HDFS 文件 或 目录 比如 / parent / child 可以 表示 成 hdfs : / / namenode : namenodeport / parent / child ， 或者 更 简单 的 / parent / child （ 假设 你 配置文件 中 的 默认值 是 namenode : namenodeport ） 。 大多数 FS   Shell 命令 的 行为 和 对应 的 Unix   Shell 命令 类似 ， 不同之处 会 在 下面 介绍 各 命令 使用 详情 时 指出 。 出错 信息 会 输出 到 stderr ， 其他 信息 输出 到 stdout 。               需要 添加   -   号 ， 例如   cat   命令 调用 形式 为   hadoop   fs   - cat   some - path         与 bash 相同 的 命令 有 ，   cat   ,     chmod   ,     chown   ,     du   ( 显示 文件大小 ) ,     ls   ,     lsr   ( ls   - R ) ,     mkdir     ( mkdir   - p   创建 所有 不 存在 的 父 目录 ) ,       mv   ,     rm   ,     rmr     ( rm   - R ) ,     stat   ,     tail   ,     test   - [ ezd ]   ，       特有 的 命令         dus   ，   显示 文件大小         touchz   ， 创建 一个 0 字节 的 空 文件         text   ，   将 源文件 输出 为 文本格式 。 允许 的 格式 是 zip 和 TextRecordInputStream 。               与 本地 文件系统 交互 的 命令         copyFromLocal         将 本地 路径 拷贝到 HDFS 中         copyToLocal             将 HDFS 中 的 路径 原样 拷贝到 本地         put   ， 可以 用   -   代表 标准 输入 ， 例如   hadoop   fs   - put   -   hdfs : / / host : port / hadoop / hadoopfile           get   ，   例子     hadoop   fs   - get   / user / hadoop / file   localfile           getmerge   ,   获取 分布式 文件目录 中 所有 的 文件 并 合并 到 一个 本地 文件 中 ， 使用 方法 ：   hadoop   fs   - getmerge   & lt ; src & gt ;   & lt ; localdst & gt ;   [ addnl ]   ， addnl 是 可选 的 ， 用于 指定 在 每个 文件 结尾 添加 一个 换行符 。              ", "tags": "tools", "url": "/wiki/tools/hadoop.html"},
      
      
      {"title": "Hive", "text": "    Table   of   Contents           关于           配置           Hive   基本概念           Hive   的 定位           数据 单元                   类型 系统           复杂 类型           操作           内置 函数                   Hive   SQL           优化 排序                   HIVE   命令           文件格式           textfile 格式           Avro   格式           ORC   格式           Parquet   格式           压缩 文件格式           LZO                   UDF           JOIN           编写 自己 的 UDF           UNION           Lateral   View           子 查询           采样           虚拟 列           窗 函数 和 分析 函数           窗 函数 ( 没 搞懂 )           OVER           分析 函数           其他 细节                   Enhanced   Aggregation ,   Cube ,   Grouping   and   Rollup           EXPLAIN   命令           HIVE   权限 管理           MORE           UDF   开发           UDF                   MAC   切换 不同 的 JDK           ERROR   汇总           问题           TIPS                 关于       学习 Hive 时 的 笔记       配置       环境 要求             -   Hive   1.2   需要   java   1.7 + ，   0.14 - 1.1   可以 工作 在   java1 . 6   版本 上 。             -   Hadoop   2 . x             -   可以 运行 在   Linux   和   Windows   环境 ，   Mac   通常 用来 做 开发 的 ！       添加     $ HIVE _ HOME     环境变量 ， 并 将   $ bin   目录 添加 到   $ PATH   变量 中 。         hive . metastore . warehouse . dir     配置 指明 数据仓库 的 目录 ， 默认 是     / user / hive / warehouse     和     / tmp   （ 临时 文件目录 ）       Hive   基本概念       Hive   的 定位       Hive   用来 做 数据仓库 ， 非 实时 数据处理 。       数据 单元           Database ：   用来 做 名字 空间 ， 防止 表 名字 冲突 ， 也 可以 用于 用户 权限 管理 。       Tables ： 表 ， 就是 传统 数据库 意义 上 的 表       Partitions ： 分区 ， 一个 表 通常 由 多个 分区 组成 。 获取 某个 特定 分区 的 数据 比 全表 扫描 快 ！ 每个 分区 一个 目录 ！       Buckets   ( or   Clusters ) ： 分桶 或 分簇 ， 在 一个 分区 里面 ， 可以 按照 某些 字段 的 hash 值 进行 分桶 ， 便于 采样 。 例如 PV 表 按照 userid 分桶       clustered   by   ( userid )   into   100   buckets   。           类型 系统           Integers       TINYINT — 1   byte   integer       SMALLINT — 2   byte   integer       INT — 4   byte   integer       BIGINT — 8   byte   integer               Boolean   type       BOOLEAN — TRUE / FALSE               Floating   point   numbers       FLOAT — single   precision       DOUBLE — Double   precision               Fixed   point   numbers       DECIMAL — a   fixed   point   value   of   user   defined   scale   and   precision               String   types       STRING — sequence   of   characters   in   a   specified   character   set       VARCHAR — sequence   of   characters   in   a   specified   character   set   with   a   maximum   length       CHAR — sequence   of   characters   in   a   specified   character   set   with   a   defined   length               Date   and   time   types       TIMESTAMP —   a   specific   point   in   time ,   up   to   nanosecond   precision       DATE — a   date               Binary   types       BINARY — a   sequence   of   bytes                   隐式 类型转换 ， 只能 从 低 精度 到 高精度 。 也 允许 从   STRING   到   DOUBLE 。     显示 类型 转化 可以 用 内置 函数 实现 。       复杂 类型           Structs   结构 体       Maps   key - value       Arrays   索引 list           结构 体 类型 的 操作 ！ 怎么 用 ？ ！       操作       除了 常规 的 比较 操作 ， 还 支持 正则 式 比较 ：         A   RLIKE   B ,   A   REGEXP   B   ， 字符串 A 是否 匹配 Java 正则 式 B 。 注意 有个 坑 ， 正则 式 B 中 的   \ \   需要 转义字符 ！ ！     例子 ：   lat   rlike   ' \ \ d + \ \ . \ \ d + '   是 错误 的 ， 应该 是     lat   rlike   ' \ \ \ \ d + \ \ \ \ . \ \ \ \ d + '         内置 函数           数值 类型 函数 ：   round ,   floor ,   ceil ,   rand         字符串 函数 ：     caoncat ,   substr ,   upper ,   ucase ,   lower ,   lcase ,   trim ,   ltrim ,   rtrim ,   regexp _ replace         时间 函数 ：     from _ unixtime ,   to _ date ,   year ,   month ,   day         复杂 类型 函数 ：   size ,   get _ json _ object   ,     reflect , java _ method   可以 用来 调用 所有 java 内置 的 函数 ！ ！       其他 ：   cast         内置 聚合 函数 ：   count ,   sum ,   avg ,   min ,   max             count   会 自动 去掉 NULL 值 ， 这 在 条件 count 的 时候 很 有用 ， 例如 分别 统计 在 a & gt ; 1 的 情况 下 和 a & lt ; 0 情况 下 的 uid ， 可以 用 一个 查询 搞定 ， 不用 join               select       count     (     distinct       if     (     a     & gt ;     1     ,       uid     ,       null     ) )       as       cnt1     ,                       count     (     distinct       if     (     a     & lt ;     0     ,       uid     ,       null     ) )       as       cnt0       from       some _ table                 Hive   SQL             row _ number ( )     函数 用法 ，   partition   用来 将 数据 分区 编号 ，   order   by   描述 编号 顺序                   select       uid     ,       row _ number     ( )       over       (     partition       by       uid       order       by       uid     )       from                       datediff ( d1 ,   d2 )   ，   其中 时间 字符串 要是 这种 格式   yyyy - MM - dd   ， 如果 不是 ， 需要 先 转换             from _ unixtime ( t ,   ' yyyyMMdd ' )   ,     unix _ timestamp ( str ,   ' yyyy - MM - dd ' )   这 两个 函数 可以 实现 时间 字符串 格式 转换               JOIN ： 支持 常规 的 内 连接   inner   join ， 外 链接   outer   join 。 还 支持   left   semi   join ， 用来 从 左边 表 过滤 出 满足 join 条件 的 记录 ，     相当于   where   exists   subquery   方式 ， 也 可以 替代 in ， 效率 比   inner   join 高 。 （ 此时 要 不要 将 大表放 右边 ？ ！ 哈哈 ）               join 优化 ： 将 大表放 右边 ！           Also   it   is   best   to   put   the   largest   table   on   the   rightmost   side   of   the   join   to   get   the   best   performance .                   聚合 操作 不能   distinct   两个 不同 的 列               HAVING   @ since ( 0.7 . 0 )   可以 将 聚合 函数 放在   WHERE   中 当做 条件 使用 ，           多表 / 文件 插入 操作 ： 还是 看 代码 吧 ！                   FROM       pv _ users       INSERT       OVERWRITE       TABLE       pv _ gender _ sum               SELECT       pv _ users     .     gender     ,       count _ distinct     (     pv _ users     .     userid     )               GROUP       BY       pv _ users     .     gender         INSERT       OVERWRITE       DIRECTORY       &# 39 ; / user / data / tmp / pv _ age _ sum &# 39 ;               SELECT       pv _ users     .     age     ,       count _ distinct     (     pv _ users     .     userid     )               GROUP       BY       pv _ users     .     age     ;                     动态 分区 插入 ： @ since ( 0.6 . 0 ) ， 能够 减少 调度 时间 ， 显著 提升 性能 ！ 但是 注意 几个 问题 。                   insert       overwrite       table       pv       partition       (     dt     =     &# 39 ; 2010 - 01 - 01 &# 39 ;     ,       country     )         - - - - -   动态 决定 country 分区 ， dt 分区 值 固定       insert       overwrite       table       pv       partition       (     dt     ,       country     =     &# 39 ; US &# 39 ;     )                         - - - - -   将 所有 dt 分区 下   country = &# 39 ; US &# 39 ;   子 分区 都 覆盖 ， 一般 不要 能 用 这种 写法 ！                 如果 分区 字段 为 NULL ， 会 写入 到 默认 分区 HIVE _ DEFAULT _ PARTITION 中 。       影响 动态 分区 的 一些 配置 ：           hive . exec . max . dynamic . partitions . pernode   ( default   value   being   100 )   每个   mapper   或者   reducer   能够 创建 的 最大 分区 数目 。       hive . exec . max . dynamic . partitions   ( default   value   being   1000 )   一个 表 能够 创建 的 最大 分区 数目 。       hive . exec . max . created . files   ( default   value   being   100000 )   所有 的   mapper   和   reducer   能够 创建 的 全部 文件 数目 最大值       hive . exec . dynamic . partition . mode = strict   禁止 动态 分区   nostric   使用 动态 分区 。 只能 使用 静态 分区 。           hive . exec . dynamic . partition = true / false   彻底 禁止 动态 分区 。               写入 本地 文件 ：   INSERT   OVERWRITE   LOCAL   DIRECTORY   ' / tmp / pv _ gender _ sum '             快速 采样 ：   TABLESAMPLE ( BUCKET   x   OUT   OF   y )   ， 需要 在建 表 的 时候   CLUSTERED   BY   支持 。 例如 选出 第 3 个 bucket 。                   TABLESAMPLE     (     BUCKET       3       OUT       OF       64       ON       userid     )                     UNION   ALL ：       Array   操作 ：                   CREATE       TABLE       array _ table       (     int _ array _ column       ARRAY     & lt ;     INT     & gt ;     ) ;         SELECT       pv     .     friends     [     2     ]       FROM       page _ views       pv     ;                 相关 UDAF 函数   percentile _ approx ,   histogram _ numeric ,   collect _ set ,   collect _ list       -   Map   操作 ：           Custom   Map / Reduce   Scripts ：     MAP ,   REDUCE   （ 是   TRANSFORM   的 语法 糖 而已 ） ， 或者   TRANSFORM     函数 （ 是否 只能 实现 UDF 的 功能 ， UDAF 和 UDTF 呢 ？ ）                   FROM       (                 FROM       pv _ users                 MAP       pv _ users     .     userid     ,       pv _ users     .     date                 USING       &# 39 ; map _ script &# 39 ;                 AS       dt     ,       uid                 CLUSTER       BY       dt     )       map _ output         INSERT       OVERWRITE       TABLE       pv _ users _ reduced                 REDUCE       map _ output     .     dt     ,       map _ output     .     uid                 USING       &# 39 ; reduce _ script &# 39 ;                 AS       date     ,       count     ;                 脚本 ：               import       sys       import       datetime         for       line       in       sys     .     stdin     :           line       =       line     .     strip     ( )           userid     ,       unixtime       =       line     .     split     (     &# 39 ;     \ \ t     &# 39 ;     )           weekday       =       datetime     .     datetime     .     fromtimestamp     (     float     (     unixtime     ) )     .     isoweekday     ( )           print       &# 39 ; , &# 39 ;     .     join     ( [     userid     ,       str     (     weekday     ) ] )                           CLUSTER   BY ,   DISTRIBUTE   BY ,   SORT   BY                 分组 ：   CLUSTER   BY     相当于 先 按列   DISTRIBUTE   BY   ， 然后   SORT   BY                 优化 排序       不要 使用   order   by       https : / / stackoverflow . com / questions / 13715044 / hive - cluster - by - vs - order - by - vs - sort - by                     -     ORDER   BY     全局 排序 ， 但是 只能 使用 一个 reducer     -     DISTRIBUTE   BY     采用 Hash 算法 将 map 处理 后 的 数据 分 发给 reduce ， 它 保证 了 相同 的 key 是 在 同一个 reducer     -     SORT   BY     不是 全局 排序 ， 而是 在 数据 进入 reduce 之前 完成 排序 ， 只能 保证 每个 reducer 的 输出 是 有序 的 ， 不能 保证 全局 有序 。     -     CLUSTER   BY     相当于 先   DISTRIBUTE   然后   sort 。 也 不能 保证 全局 有序 。       HIVE   命令             set   & lt ; key & gt ; = & lt ; value & gt ;     设置 参数         add   FILE [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *   ,     add   JAR [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *   ,     add   ARCHIVE [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *     添加 文件           文件格式       textfile 格式       文本格式               ROW       FORMAT       DELIMITED             FIELDS       TERMINATED       BY       &# 39 ; \ \ 001 &# 39 ;             LINES       TERMINATED       BY       &# 39 ; \ \ n &# 39 ;             COLLECTION       ITEMS       TERMINATED       BY       &# 39 ; \ \ 002 &# 39 ;             MAP       KEYS       TERMINATED       BY       &# 39 ; \ \ 003 &# 39 ;         STORED       AS       TEXTFILE     ;                 Avro   格式           要求 ： Hive   0.9 . 1 +       不同 版本 要求 的 语法 还 不同 ， 具体 参看   https : / / cwiki . apache . org / confluence / display / Hive / AvroSerDe   。     0.14   以后 的 版本 可以 在 创建 表 的 时候 这样 写 ：   STORED   AS   AVRO     即可 。           ORC   格式       Optimized   Row   Columnar   格式 ， 采用 这种 格式 可以 提升 HIVE 读写 性能 。           输出 是 单个 文件 ， 减少   NameNode   的 负载       支持 Hive 所有 格式 ， 包括 复杂 格式       文件 中 存储 了 轻量级 的 索引 ， 便于 以 行为 单位 移动 指针       压缩 ： 游程 编码 （ Int ） 字典 码 （ String ）       同时 读 同一个 文件       split   文件 ， 而 不 需要 scanning   for   markers       限制 了 读写 内存           文件 结构 ： 以 Strip 为 单位 （ 默认 250MB ） 。       cli 读取 命令     hive   - - orcfiledump         创建 表 的 时候 这样 写 ：   STORED   AS   ORC     即可 。       Parquet   格式       Hadoop   生态 中 的 一个 ！       压缩 文件格式       直接 从   gzip   等 格式 中 存取 为 text 格式 表格 。               CREATE       TABLE       raw       (     line       STRING     )             ROW       FORMAT       DELIMITED       FIELDS       TERMINATED       BY       &# 39 ; \ \ t &# 39 ;       LINES       TERMINATED       BY       &# 39 ; \ \ n &# 39 ;     ;         LOAD       DATA       LOCAL       INPATH       &# 39 ; / tmp / weblogs / 20090603 - access . log . gz &# 39 ;       INTO       TABLE       raw     ;                 LZO       略       UDF       UDF ， UDAF ， UDTF       HIVE 数据类型 与   java   数据类型 对应 关系 ：               hive           java       map             HashMap       array         ArrayList     & lt ; ?     & gt ;                 JOIN           多表   JOIN   的 时候 ， 当 JOIN 条件 都 包含 同一个 Key 的 时候 ， 会 用 同一个   Map / Reduce   处理 ， 例如                   SELECT       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key1     )                 只有 一个   Map / Reduce   任务 。 而 下面 这个 会 有 2 个   Map / Reduce   任务 ( JOIN   a , b ;   JOIN   *   , c ) 。               SELECT       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key2     )                     In   every   map / reduce   stage   of   the   join ,   the   last   table   in   the   sequence   is   streamed   through   the   reducers   where   as   the   others   are   buffered .   Therefore ,   it   helps   to   reduce   the   memory   needed   in   the   reducer   for   buffering   the   rows   for   a   particular   value   of   the   join   key   by   organizing   the   tables   such   that   the   largest   tables   appear   last   in   the   sequence .           将 大表放 后面 ， 大表会 以 streaming 的 方式 进入 reducer ， 而 其他 的 一 buffer 的 方式 存在 （ 内存 ？ ） ， 可以 减少 内存 的 需求 。     默认 让 最后 的 表以 streaming 方式 进入 reducer ， 也 可以 手动 指定 。               SELECT       / * +   STREAMTABLE ( a )   * /       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key1     )                 当 存在 这个 hint 的 时候 ， 会 将表 b ， c 缓存 ， 而 让 a 以流 的 方式 进入 reducer 。 不 存在 的 时候 ， 则 会 将 最后 的 表以 流 的 方式 进入 reducer 。           JOIN   逻辑 发生 在   WHERE   之前 ！ 对于   inner   join ， 条件 放在   ON   还是   WHERE   都 是 一样 的 ， 但是 如果 是 其他   JOIN ，   则 会 有 区别 。       https : / / stackoverflow . com / questions / 354070 / sql - join - where - clause - vs - on - clause             但是 在 实现 的 时候 ， 会 先用 WHERE 里面 的 条件 过滤 吧 ？ ！ 否则 性能 不是 很差 ！ ？               JOIN   是 不可 交换 的 ！               左半 连接 更 有效 ！   要求 右表 的 字 段 只 在   ON   条件 中 出现 ！ 在 map 端 过滤 掉 不会 参加 join 操作 的 数据 ， 则 可以 大大 节省 网络 IO 。                   LEFT   SEMI   JOIN   implements   the   uncorrelated   IN / EXISTS   subquery   semantics   in   an   efficient   way .               MAPJOIN ， JOIN   小表 的 时候 ， 可以 减少   reducer ， 提升 性能 ！ 但是 右 链接 和 全 连接 中 不能 用 ！                   SELECT       / * +   MAPJOIN ( b )   * /       a     .     key     ,       a     .     value       FROM       a       JOIN       b       ON       a     .     key       =       b     .     key                 上述 代码 不 需要   reducer ！           The   restriction   is   that   a     FULL / RIGHT   OUTER   JOIN     b   cannot   be   performed .               如果 在   JOIN   列 上 ， 进行 分桶 了 ， 并且 其中 一个 表 的 桶 数目 是 另 一个 的 倍数 ， 那么 就 可以 采用   MAPJOIN   优化 了 。     在   MAP   的 时候 ， 左表 的 第一个 桶 只会 去取 右表 的 第一个 桶 ， 而 不是 所有 的 数据 ！ 这个 行为 不是 默认 的 ， 需要 设置 参数 ：                   set       hive     .     optimize     .     bucketmapjoin       =       true                     如果 两个 表   JOIN   的 字 段 分桶 且 排序 的 ， 并且 分桶 数目 相同 ， 那么 可以 采用   sort - merge 。 例如 满足 上述 条件 的 两个 表 的 join 可以     只 需要   Map   阶段 ！                   SELECT       / * +   MAPJOIN ( b )   * /       a     .     key     ,       a     .     value       FROM       A       a       JOIN       B       b       ON       a     .     key       =       b     .     key                 同时 需要 设置 参数 ：               set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     BucketizedHiveInputFormat     ;       set       hive     .     optimize     .     bucketmapjoin       =       true     ;       set       hive     .     optimize     .     bucketmapjoin     .     sortedmerge       =       true     ;                     MAPJOIN   可以 用来   JOIN   小表 ， 实现 优化 ， 但是 下列 情况 是 不行 的 ！ ！ ！ 我晕 ！       Union   Followed   by   a   MapJoin       Lateral   View   Followed   by   a   MapJoin       Reduce   Sink   ( Group   By / Join / Sort   By / Cluster   By / Distribute   By )   Followed   by   MapJoin       MapJoin   Followed   by   Union       MapJoin   Followed   by   Join       MapJoin   Followed   by   MapJoin                   可以 设置     hive . auto . convert . join = true     让 hive 自动 帮 你 转为   MAPJOIN 。 从   Hive   0.11 . 0     开始 ， 默认值 就是 true 。     MAPJOIN   将 小表 放到 内存 ， 保存 为 一个   HASH   MAP 。 工作 流程 是 ：   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + JoinOptimization               Local   work :   read   records   via   standard   table   scan   ( including   filters   and   projections )   from   source   on   local   machine   build   hashtable   in   memory   write   hashtable   to   local   disk   upload   hashtable   to   dfs   add   hashtable   to   distributed   cache     Map   task   read   hashtable   from   local   disk   ( distributed   cache )   into   memory   match   records &# 39 ;   keys   against   hashtable   combine   matches   and   write   to   output               MAPJOIN   的 限制 ：           一次 只能 一个 KEY       chain   of   MAPJOINs   是 不 可以 的 ， 除非 写成 子 查询 形式 。   mapjoin ( table ,   subquery ( mapjoin ( table ,   subquery .... )             每 一次   MAPJOIN   都 需要 重新 建立   HASH   表 ， 包括 上传 和 下载               优化 链式   MAPJOIN                       select       / * +   MAPJOIN ( time _ dim ,   date _ dim )   * /       count     (     *     )       from       store _ sales       join       time _ dim       on       (     ss _ sold _ time _ sk       =       t _ time _ sk     )       join       date _ dim       on       (     ss _ sold _ date _ sk       =       d _ date _ sk     )       where       t _ hour       =       8       and       d _ year       =       2002                 通过 两个 值 设置   MAPJOIN               set       hive     .     auto     .     convert     .     join     .     noconditionaltask       =       true     ;       set       hive     .     auto     .     convert     .     join     .     noconditionaltask     .     size       =       10000000     ;                 SMB   Map   Join :   Sort - Merge - Bucket   ( SMB )   joins       表 已经 是 分桶 并且 排序 好 的 ，   JOIN   过程 通过 顺序   merge   已经 排序 好 的 表 即可 。 （ 效率 比 普通   JOIN   高 ）           However ,   if   the   tables   are   partitioned ,   there   could   be   a   slow   down   as   each   mapper   would   need   to   get   a   very   small   chunk   of   a   partition   which   has   a   single   key .                   set       hive     .     auto     .     convert     .     sortmerge     .     join     =     true     ;       set       hive     .     optimize     .     bucketmapjoin       =       true     ;       set       hive     .     optimize     .     bucketmapjoin     .     sortedmerge       =       true     ;         - - - - - -   大表 自动 转化 设置       set       hive     .     auto     .     convert     .     sortmerge     .     join     .     bigtable     .     selection     .     policy               =       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     TableSizeBasedBigTableSelectorForAutoSMJ     ;                 大表 选择 策略 会 自动 决定 哪个 表 被   streaming ， 而 不是   hash   并且   streaming 。 可 选 策略 有               org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     AvgPartitionSizeBasedBigTableSelectorForAutoSMJ       (     default     )       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     LeftmostBigTableSelectorForAutoSMJ       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     TableSizeBasedBigTableSelectorForAutoSMJ                 如果 表有 不同 数量 的 keys （ SORT   列 ） ， 会 发生 异常 ！       SMB   存在 的 目的 主要 是 为了 解决 大表 与 大表间 的   Join   问题 ， 分桶 其实 就是 把 大表 化成 了 “ 小表 ” ， 然后   Map - Side   Join   解决 之 ， 这是 典型 的 分而治之 的 思想 。       这个 Blog 写 的 不错 ，   https : / / my . oschina . net / leejun2005 / blog / 178631         编写 自己 的 UDF       TRANSFORM   貌似 不能 实现   UDAF 。 可以 用 java 写 UDF 或 UDAF ， UDTF 等 。 需要 jdk1 . 7 版本 。               ADD       JAR       hdfs     :     / / /     user     /     hadoop     -     data     /     user _ upload     /     hive     -     kv     -     udaf _ 2     .     10     -     0     .     0     .     1     .     jar     ;       CREATE       TEMPORARY       FUNCTION       kv       as       &# 39 ; KV &# 39 ;     ;                 UNION           UNION   ALL ： 不去 重 融合   1.2 . 0   以前 只 支持 这个       UNION   DISTINCT ： 去 重 ， 1.2 . 0   以后 默认 （ UNION ） 是 这个 融合       UNION   常 需要 对 列名 重命名 ， 使得 UNION 的 时候 ， 列名 是 相同 的           Lateral   View       用 在   UDTF   中 。 对于 输入 的 一行 ， 输出 是 多行 。 0.12 . 0   版本 开始 ， 列名 可以 不用 写 ， 会 自动 采用 UDTF 输出 的 StructObjectInspector 对象 自动 得到 列名 。 参考   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + LateralView         一个 例子 ， adid _ list 是 一个 Array ，   explode ( )   函数 会 将 这个 list 输出 为 多行 。               SELECT       pageid     ,       adid       FROM       pageAds       LATERAL       VIEW       explode     (     adid _ list     )       adTable       AS       adid     ;         SELECT       adid     ,       count     (     1     )       FROM       pageAds       LATERAL       VIEW       explode     (     adid _ list     )       adTable       AS       adid       GROUP       BY       adid     ;         SELECT       *       FROM       src       LATERAL       VIEW       OUTER       explode     (     array     ( ) )       C       AS       a       limit       10     ;                 FROM   语句 里面 可以 包含 多个   Lateral   View 。 通过     OUTER     关键字 可以 让     explode     输出 为 NULL 的 时候 ，     记录 至少 存在 一行 ！ （ 没有 这个 关键字 ， 结果 中将 不会 出现 记录 ）       子 查询       子 查询 放在   FROM   里面 ， 在   0.13   版本 后 ， 可以 放在   IN   和   EXISTS   之中 ， 但是 存在 一些 限制 。           只能 放在 表达式 右边       IN / NOT   IN   子 查询 只 支持 单列       EXISTS / NOT   EXISTS   必须 有 一个 或者 多个   correlated   predicates   （ where   条件 ？ ）       对父 查询 字段 的 引用 只 支持 在   WHERE   中 。           采样               TABLESAMPLE       (     BUCKET       x       OUT       OF       y       [     ON       colname     ] )                 colname   可以 是非 分区 字 段 以外 的 字 段   或者     RAND ( )   。     表 采样 是 很 慢 的 ， 如果 建表 的 时候 采用   CLUSTERED   BY   创建 ，     那么 可以 加快 采样 速度 ， 因为 只要 简单 地 取出 对应 的 BUCKET 就 可以 了 ， 而 不用 全表 扫描 。       更 多 信息 参考 ：   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + Sampling         block   sampling   @ since ( 0.8 )               TABLESAMPLE       (     n       PERCENT     )       TABLESAMPLE       (     ByteLengthLiteral     )           - - - -   例如   100M       TABLESAMPLE       (     n       ROWS     )         - - - - -   例子       SELECT       *       FROM       source       TABLESAMPLE     (     100     M     )       s     ;                 这个 是 在   HDFS   block   level   上 进行 的 采样 ，     所以 一些 压缩 格式 表 数据 不 支持 这个 特性 。 复现 可以 通过 设置 种子 来 实现   set   hive . sample . seednumber = & lt ; INTEGER & gt ; ;   。       虚拟 列         INPUT __ FILE __ NAME ,   BLOCK __ OFFSET __ INSIDE __ FILE     在   Mapper   里面 分别 指 输入 文件名   和   全局 文件 位置       简单 例子               select       INPUT __ FILE __ NAME     ,       key     ,       BLOCK __ OFFSET __ INSIDE __ FILE       from       src     ;       select       key     ,       count     (     INPUT __ FILE __ NAME     )       from       src       group       by       key       order       by       key     ;       select       *       from       src       where       BLOCK __ OFFSET __ INSIDE __ FILE       & gt ;       12000       order       by       key     ;                 窗 函数 和 分析 函数       @ since ( 0.11 )       窗 函数 ( 没 搞懂 )           LEAD ，   LEAD   开窗 函数 返回 位于 分区 中 当前 行 的 下方 （ 之后 ） 的 某个 给定 偏移量 位置 的 行 的 值 。 如果 超过 窗 的 末尾 了 ， 返回 NULL 。 例子     https : / / docs . aws . amazon . com / zh _ cn / redshift / latest / dg / r _ Examples _ of _ LEAD _ WF . html         LAG ， LAG   开窗 函数 返回 位于 分区 中 当前 行 的 上方 （ 之前 ） 的 某个 给定 偏移量 位置 的 行 的 值 。 如果 超过 窗 的 开头 ， 返回 NULL 。 例子   https : / / docs . aws . amazon . com / zh _ cn / redshift / latest / dg / r _ Examples _ of _ LAG _ WF . html         FIRST _ VALUE ， 返回 分区 第一个 值       LAST _ VALUE ， 返回 分区 最后 一个 值           OVER       可以 将 聚合 函数 的 返回值 应用 到 每 一列 （ 窗 函数 的 功能 ） ， 就 像 分析 函数 那样 ！ ！           标准 聚合 函数     COUNT ,   SUM ,   AVG ,   MAX ,   MIN           PARTITION   BY     一个 或 多个 分区 列 ，   ORDER   BY     一个 或 多个 字 段       可以 在   OVER   里面 指定 窗     ROWS   ( ( CURRENT   ROW )   |   ( UNBOUNDED   |   [ num ] )   PRECEDING )   AND   ( UNBOUNDED   |   [ num ] )   FOLLOWING                     SELECT       a     ,       SUM     (     b     )       OVER       (     PARTITION       BY       c     ,       d       ORDER       BY       e     ,       f     )       FROM       T     ;         SELECT       a     ,       SUM     (     b     )       OVER       (     PARTITION       BY       c       ORDER       BY       d       ROWS       BETWEEN       UNBOUNDED       PRECEDING       AND       CURRENT       ROW     )       FROM       T     ;                 分析 函数           RANK ， 返回 序数 ， 可能 存在 相同 的 序号 ， 因为 同时 排 第一 之类 的       ROW _ NUMBER ， 返回 行号       DENSE _ RANK ， 返回 序号 ， 不 存在 相同 的 序号 ， 即使 相同 ， 也 会 让 后面 一个 序号 + 1       CUME _ DIST ， 返回 累积 分布   0 - 1   之间 的 值 。       PERCENT _ RANK ， 百分比 排名   0 - 1   之间 的 值 。 计算公式   ( row _ number   -   1 )   /   ( count   -   1 )       NTILE ， 对 排名 进行 分组 ， 尽可能 保证 每组 数目 均匀 ， 返回 分组 编号 。           其他 细节             DISTINCT     关键词 使用 在 这些 函数 中要 到   2.1 . 0   版本 之后 。       在   OVER   中 使用 聚合 函数 也 要 到   2.1 . 0   版本 之后                   COUNT     (     DISTINCT       a     )       OVER       (     PARTITION       BY       c     )         SELECT       rank     ( )       OVER       (     ORDER       BY       sum     (     b     ) )       FROM       T       GROUP       BY       a     ;                 Enhanced   Aggregation ,   Cube ,   Grouping   and   Rollup           GROUPING   SETS ： 等价 于 多个   GROUP   BY   然后   UNION                   SELECT       a     ,       b     ,       SUM     (     c     )       FROM       tab1       GROUP       BY       a     ,       b       GROUPING       SETS       (       (     a     ,     b     )       ,       a     )         - - - -   等价 于       SELECT       a     ,       b     ,       SUM     (       c       )       FROM       tab1       GROUP       BY       a     ,       b       UNION       SELECT       a     ,       null     ,       SUM     (       c       )       FROM       tab1       GROUP       BY       a                         Grouping __ ID               Cubes   and   Rollups       WITH   CUBE / ROLLUP     关键字 ， 只能 用 在     GROUP   BY     之中 。       GROUP   BY   a ,   b ,   c   WITH   CUBE     会 组合 所有 的 可能     ( a ,   b ,   c ) ,   ( a ,   b ) ,   ( b ,   c ) ,   ( a ,   c ) ,   ( a ) ,   ( b ) ,   ( c ) ,   (   )   。     而     GROUP   BY   a ,   b ,   c ,   WITH   ROLLUP     等价 于       GROUP   BY   a ,   b ,   c   GROUPING   SETS   (   ( a ,   b ,   c ) ,   ( a ,   b ) ,   ( a ) ,   (   ) )   .               设置     hive . new . job . grouping . set . cardinality     的 值 ， 当 候选 分组 数目 （ 上面 分别 是 8 和 4 ） 超过 这个 值时 ， 将 开启 额外 的   Mapper   Reducer   任务 来 处理 。       EXPLAIN   命令       用来 显示   query     的 执行 计划 的 ， 例如 对于 这个 query 有 几个 stage 等 。         EXPLAIN   [ EXTENDED | DEPENDENCY | AUTHORIZATION ]   query         HIVE   权限 管理       略       MORE       HIVE   on   spark !       UDF   开发       支持   JAVA （ 或 Scala ）   写 UDF   UDAF   UDTF ！ 依赖 ：           Java   UDF ：   https : / / github . com / apache / hive / tree / master / ql / src / java / org / apache / hadoop / hive / ql / udf / generic         Scala   UDF 及 单元测试 的 例子 ：   https : / / github . com / sharethrough / hive - udfs             UDF       实现 普通 函数   ( v1 ,   ... )   - & gt ;   ( w1 ,   ... ) 。     需要 继承   org . apache . hadoop . hive . ql . exec . UDF   这个 类 ， 并 实现     evaluate     方法 。         UDFTrim   模板         MAC   切换 不同 的 JDK       参考 ：   https : / / stackoverflow . com / questions / 20974607 / can - java - 7 - and - java - 8 - co - exist - on - osx               use - java     ( )       {               export       JAVA _ HOME     =     `   / usr / libexec / java _ home   - v     1   .   $ 1     `       }                 然后 使用     use - java   7     就 可以 切换 到     jdk   1.7     了 。       ERROR   汇总           metainfo   超大 ：                     org . apache . hadoop . yarn . exceptions . YarnRuntimeException :   java . io . IOException :   Split   metadata   size   exceeded   10000000 .   Aborting   job   job _ 1469532484579 _ 647683               解决 方法 ：   set   mapreduce . jobtracker . split . metainfo . maxsize = - 1 ;         问题           HIVE   中 使用   VIEW   视图 ！           TIPS               HIVE   中 上传 本地 csv 文件 作为 表格 的 简单 方法 ： 利用 hive 建表 命令 ， 创建 一个 表格 ， 然后 将 本地 csv 文件 通过   hadoop   shell   上 传到     表 对应 的 HDF 文件夹 即可 ！ 注意 建表 的 时候 要 用 文本格式 ， 注意 分隔符 要 匹配 。               SQL   将列 重命名 不要 命名 为 已 存在 的 列 的 名字 ！ 否则 将会取 存在 的 列 的 值 ， 而 不是 你 想要 的 值 ！           group   by   和   sort   by   func ( col ) ， 可以 是 一个 函数       设置 mapreduce 阶段 的 mapper 和 reducer 数目                   #   YARN :   Hadoop   2       set     mapreduce . job . maps   =   & lt ; num & gt ;   ;       set     mapreduce . job . reduces   =   & lt ; num & gt ;   ;                     设置 每个 reducer 最大 处理 数据量     set   hive . exec . reducers . bytes . per . reducer = & lt ; number & gt ;         用     distribute   by     控制 进入 reducer 的 样本       不要 用     count ( distinct   id )     因为 只能 用 一个 reducer ！ 可以 用   sum ( 1 )   +   group   by   id   ， 多 一个 job 但是 快 很多 ， 因为   group   by   可以 用 多个 reducer 。     一个 数据 ， 约 3 亿 不同 的 id ， 第一种 用时 40 分钟 ， 其中 reducer 耗时 35 分钟 ！ 后 一种 5 分钟 ！       https : / / stackoverflow . com / questions / 19311193 / why - is - countdistinct - slower - than - group - by - in - hive             另 一种 去 重 的 方法               select       sum     (     if     (     r     =     1     ,       1     ,       0     ) )       as       distinct _ num     ,               sum     (     1     )       as       num       from       (               select                       id     ,                       row _ number     ( )       over       (     partition       by       id     )       as       r               from       tableA       )                       SET   hive . exec . parallel = true ;     让 不同 job 并发 执行 ！       skew   join   优化 ：     set   hive . optimize . skewjoin = true ;   将 一个 join 操作 变为 两个 ， 第一个 会 将 同一个 key 分散 到 不同 的 reduce       skew   groupby   优化 ：   set   hive . groupby . skewindata =   true   ;         输入 是否 融合 小 文件 ：                   set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     HiveInputFormat     ;           - - -   不 融合       set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     CombineHiveInputFormat     ;         - - -   融合        ", "tags": "tools", "url": "/wiki/tools/hive.html"},
      
      
      {"title": "Linux 基础知识", "text": "    Table   of   Contents           关于           常用命令 集合           文件系统 相关           权限 相关           操作系统 相关           字符串 工具           sort   命令           watch   命令           常用 参数 ：           例子                   wc   命令           tar   打包 命令           网络 命令                   VIM           三种 模式           移动 光标           编辑 命令           插入 模式           命令 模式           外部 工具 集成           参考                         关于       作为 一个 合格 的 工程师 ， 怎能不 懂 Linux ， 所以 学习 记录 一些 知识点 。       常用命令 集合       文件系统 相关           重定向 ：   2 & gt ; & amp ; 1   表示 将 stderr 重定向 到 stdout ，   2 & gt ; / dev / null   表示 重定向 stderr 到 空洞 ， 也 就是 不 打印 出 stderr 。         后面 这个 在 运行 hadoop 和 spark 的 时候 有用 。 也 可以 将 stdout 和 stderr 重定向 到 不同 文件 ， 例如 ：                                 some - cmd     1   & gt ; stdout . txt     2   & gt ; stderr . txt               权限 相关             sudo   :             在   sudo   出现 之前 ， 使用   su   命令 提升 权限 ， 缺点 是 必须 知道 超级 用户 的 密码 。   sudo   可以 将 用户 的 名字 、 可以 使用 的 特殊 命令 、 按照 那种 用户组 执行 等 信息 保存 在     文件 中 （ 通常 是   / etc / sudoers   ）     sudo   [ - bhHpV ] [ - s   ] [ - u   & lt ; 用户 & gt ; ] [ 指令 ]     或     sudo   [ - klv ]     参数 [ 编辑 ]     　 　 - b   　 在 后台 执行 指令 。     　 　 - h   　 显示 帮助 。     　 　 - H   　 将 HOME 环境变量 设为 新 身份 的 HOME 环境变量 。     　 　 - k   　 结束 密码 的 有效期限 ， 也 就是 下次 再 执行 sudo 时 便 需要 输入 密码 。     　 　 - l   　 列出 目前 用户 可 执行 与 无法 执行 的 指令 。     　 　 - p   　 改变 询问 密码 的 提示 符号 。     　 　 - s   　 执行 指定 的 shell 。     　 　 - u   & lt ; 用户 & gt ;   　 以 指定 的 用户 作为 新 的 身份 。 若 不 加上 此参数 ， 则 预设 以 root 作为 新 的 身份 。     　 　 - v   　 延长 密码 有效期限 5 分钟 。     　 　 - V   　 显示 版本信息 。           - S       从 标准 输入 流 替代 终端 来 获取 密码           操作系统 相关       后台 可靠 运行 的 几种 方法 ：                     -   后台 运行 加 一个   & amp ;   即可 ， 例如   sleep   100   & amp ;   ， 只能 一直 保持 会话 ， 如果 shell 会话关 了 ， 这个 子 进程 也 会 关掉 。           -   nohup   方式 ：         我们 知道 ， 当 用户 注销 （ logout ） 或者 网络 断开 时 ， 终端 会 收到   HUP （ hangup ） 信号 从而 关闭 其 所有 子 进程 。 因此 ， 我们 的 解决办法 就 有 两种 途径 ： 要么 让 进程 忽略   HUP   信号 ， 要么 让 进程 运行 在 新 的 会 话 里 从而 成为 不 属于 此 终端 的 子 进程 。     只 需 在 要 处理 的 命令 前 加上   nohup   即可 ， 标准 输出 和 标准 错误 缺 省会 被 重定向 到   nohup . out   文件 中 。 一般 我们 可 在 结尾 加上 \" & amp ; \" 来 将 命令 同时 放入 后台 运行 ， 也 可用 \" & gt ; filename   2 & gt ; & amp ; 1 \" 来 更改 缺省 的 重定向 文件名 。                       -   setsid   方式 ：         nohup   无疑 能 通过 忽略   HUP   信号 来 使 我们 的 进程 避免 中途 被 中断 ， 但 如果 我们 换个 角度 思考 ， 如果 我们 的 进程 不 属于 接受   HUP   信号 的 终端 的 子 进程 ， 那么 自然 也 就 不会 受到   HUP   信号 的 影响 了 。 setsid   就 能 帮助 我们 做到 这 一点 。                               使用 的 时候 ， 也 只 需 在 要 处理 的 命令 前 加上   setsid   即可 。     -   当 我们 将 \" & amp ; \" 也 放入 “ ( ) ” 内 之后 ， 我们 就 会 发现 所 提交 的 作业 并 不 在 作业 列表 中 ， 也就是说 ， 是 无法 通过 jobs 来 查看 的 。 让 我们 来 看看 为什么 这样 就 能 躲过   HUP   信号 的 影响 吧 。                                                       subshell   示例     [   root @ pvcent107   ~   ]     #   ( ping   www . ibm . com   & amp ; )       [   root @ pvcent107   ~   ]     #   ps   - ef   | grep   www . ibm . com     root             16270               1         0       14   : 13   pts / 4           00   : 00 : 00   ping   www . ibm . com   root             16278       15362         0       14   : 13   pts / 4           00   : 00 : 00   grep   www . ibm . com     [   root @ pvcent107   ~   ]     #                 从 上例 中 可以 看出 ， 新 提交 的 进程 的 父   ID （ PPID ） 为 1 （ init   进程 的   PID ） ， 并 不是 当前 终端 的 进程   ID 。 因此 并 不 属于 当前 终端 的 子 进程 ， 从而 也 就 不会 受到 当前 终端 的   HUP   信号 的 影响 了 。     -   如果 已经 提交 了 任务 ， 但是 没加 nohup 或者 setsid ， 怎么 补救 ？ 可以 用     disown     命令 来 补救 。             -     disown   - h   jobspec     是 某个 作业 忽略 HUP 信号             -     disown   - ah     是 所有 作业 忽略 HUP 信号             -     disown   - rh     是 正在 运行 的 作业 忽略 HUP 信号     -   更 多 技巧 ， 参考 IBM 文档     1 .   IBM 文档     https : / / www . ibm . com / developerworks / cn / linux / l - cn - nohup /         字符串 工具           grep   字符 匹配                   ##   从 log . txt 中 查找 keyword 出现 的 行     grep   keyword   log . txt     ##   查询 多个 模式     grep   - E     &# 39 ; keyword | otherword &# 39 ;     log . txt   grep   - e   keyword   - e   otherword   log . txt                 grep   - E     扩展 了 默认 的 正则 式 ， 可以 使用 一下 特性 ：             [ ]   字符 选择         +   1 个 和 多个         ?   有 或 无         { }   注意 花 括号 要加 反 斜杠   \ \   转义 ， 因为 bash 中花 括号 有 特殊 含义           sort   命令       将 文件 的 每 一行 作为 一个 单位 ， 进行 排序 ， 从首 字符 开始 ， 按照 ASCII 码 进行 比较 （ 默认 情况 ） 。                         -   u   参数 ， 去 重     -   r   参数 ， 降序     -   o   参数 ， 排序 后 写入 原文件 ， 重定向 会 将 文件 内容 清空 ， 达 不到 要求 ， 可以 用 这个 参数     -   n   参数 ， 表示 不 按照 ASCII 码 排序 ， 而是 按照 数值 大小     -   t   参数   和   k   参数 ， 用来 排序 csv 格式 ， t   指明 分隔符   k   指明 排序 的 列 序号     -   f   参数 ， 忽略 大小写     -   c   参数 ， 检查 是否 排好序 ， 输出 第一个 乱序 行 信息 ， 返回 1 ； C 参数 ， 也 是 检查 ， 但 不 输出 乱序 行 信息     -   M   参数 ， 以 月份 排序 ， 会 识别 月份 （ 只 对 英文 吧 ）     -   b   参数 ， 忽略 每 一行 前面 所有 空白       watch   命令       用来 周期性地 执行 某个 程序 ， 全屏 显示 执行 结果 。 常用 来     tail   log - file   ， 是 的 ， 我 就是 要 干 这个 事情 才 搜到 这个 命令 的 。       常用 参数 ：             - n   或   - - interval     指定 间隔 秒钟 数 ， 缺省 为 2         - d   或   - - differences     高亮 变化 的 区域 ， 如果   - d = cumulative   会 将 每次 变动 积累 并 高亮         - t   或   - no - title     关闭 顶部 时间 显示           最后 接 你 要 周期 执行 的 命令 即可 。       例子               #   监控 log     watch   - n     1     - d   tail   log - file       #   显示 网络连接 变化     watch   - n     1     - d   netstat   - ant       #   每秒 高亮 显示 http 连接数 变化     watch   - n     1     - d     &# 39 ; pstree | grep   http &# 39 ;                   wc     命令       用来 统计 文件 的 文本 行 数 、 单词 数 、 字节数 等 。       参数 ： - c ,   - - bytes     打印 字节数 （ print   the   byte   counts ）       参数 ： - m ,   - - chars     打印 字符 数 （ print   the   character   counts ）       参数 ： - l ,   - - lines     打印 行 数 （ print   the   newline   counts ）       参数 ： - L ,   - - max - line - length     打印 最长 行 的 长度 （ print   the   length   of   the   longest   line ）       参数 ： - w ,   - - words     打印 单词 数 （ print   the   word   counts ）         tar     打包 命令       常用 参数     - v     显示 处理过程 。   - c     创建 ,     - f     指定 文件 ,     - r     追加 ,     - x   解压 ，     - z     gzip 压缩 。       常用 组合 有 ：             tar   cf   files . tar   files     将 files 打包 到 tar 包中         tar   rf   file . tar   file     将 文件 添加 到 tar 包中 ， 原来 已经 有个 tar 包了         tar   tf   file . tar     测试 tar 完整性         tar   xf   file . tar     解压 tar 包         tar   zcf   file . tar . gz   files     将 files 打包 并 压缩 到   file . tar . gz         tar   zxf   file . tar . gz     解压包           网络 命令             netstat     查看 网络 状态         iptables             VIM       三种 模式           插入 模式 ： 可以 输入 文本 ， 按   i   进入       编辑 模式 ： 按   ESC   进入 ， 可以 移动 和 操纵 文本       命令 模式 ： 可以 执行 冒号 命令 ， 常用 的 有   : w   [ filename ]   写入 文件 ，   : q   退出 vim 。           移动 光标       在 编辑 模式 下 操作 。           基本 移动 ：       k       上       j       下       h       左       l       右               大 范围 移动                           ctrl   +   f   前进 ( forward ) 一页       ctrl   +   b   后退 ( back ) 一页               更大 范围 移动         *   和   #     搜索 当前 光标 所在 的     单词     ， 跳转 到 下 / 上 个 地方         (   和   )     移动 到 前 / 后 一个     句子     的 开始 ， 以 英文 句号 为准 ， 对 中文 貌似 没用 ， 相当于 段落 跳转         {   和   }     在     段落     间 移动         fa     前进 到 字符 a ， a 可 换成 其他 字符         gg     文件 起始 位置         G     最后 一行 起始 位置         ngg     n 行 起始 位置               当前页 跳转         H     当前页 的 header ，   3H   当前页 第 3 行         M     当前页 的 middle         L     当前页 最后 一行   ，   3L   当期 那 也 倒数第 3 行               行内 移动         g _     行尾 ， 空字符 不算         ^     行首 ， 空字符 不算         $     行尾         0     行首         w     下 一个 word 的 开头         e     下 一个 word 的 结尾         b     前 一个 word 的 开头               搜索 匹配         / str   ， 在 编辑 模式 下 输入   /   ， 后面 跟 要 搜索 字符串 str ， 然后 回车 搜索 。         ? str   ， 反向 搜索 字符串 str         n   ， 搜索 到 匹配 后 继续 向 后 搜索         N   ， 搜索 到 匹配 后 ， 继续 向前 搜索                   编辑 命令           替换 与 删除 ： 用   d   删除 行 ， 用   x   删除 字符 ， 用   r   替换 字符 。         rc     用 字符 c 替换 当前 字符         nrc     用 c 替换 前 n 个字符         x     删除 光标 字符         nx     删除 光标 右侧 n 个字符         dw     删除 光标 右侧 的 word         ndw     删除 n 个 word         db     删除 光标 左侧 一个 word         ndb     删除 光标 左侧 n 个 word         dd     删除 一行         ndd     删除 n 行         d $     删除 字符 到 行尾         d0     删除 字符 到 行首         J     删除 本行 回车 ， 合并 到 下行               用 输入 的 文本 替换 ， 进入 插入 模式         s     输入 文本 替换 当期 字符 ， n 个版   ns           S     删除 当前 行 进入 编辑 模式 ， n 个版   nS           cw     替换 右侧 一个 字 ， n 个版   ncw           cW   ,   c $     替换 右侧 所有 字         cb     替换 左侧 一个 字 ， n 个版   ncb           cd     替换 当前 行 ， n 个版   ncd   ， 试 过 但是 无效 ？ ！         c $     替换 到 行尾 所有         c0     替换 到 行首 所有               撤销         u     undo ， 撤销 最后 一个 操作         .     redo ， 重复 最有 一个 操作 ， 或者   CTRL   +   R                 复制   y   粘贴   p           yy     复制 一行 到 缓冲区 ， n 行版   nyy           + y     复制到 系统 剪贴板 ， n 行版   + nyy   ， mac 貌似 无效         p     将 缓冲区 内容 粘贴 到 光标 后面         P     到 前面                   插入 模式           插入 命令 有 ：         i     光标 左侧 插入         I     行 开头 插入         a     右侧 插入         A     行 结尾 插入         o     下 一行 插入         O     上 一行 插入               退出 插入 模式 命令   ESC ,   ctrl   +   [   ， 将 进入 编辑 模式           命令 模式       编辑 模式 下 按   :   号 ， 进入 命令 模式 ， 输入 命令 后 回车 即可 运行 命令 。                       -   文件 命令             -     : e   path - to - file     打开 文件             -     : w     保存 当前 文件             -     : w   path - to - file     另存为             -     : q     退出 ， 强制 版   : q !               -     : wq     保存 并 退出             -     : f   filename     重命名 文件             -     : r   filename     读取 filename 到 光标 后面     -   行号 与 文件             -     : 12     移动 到 第 12 行             -     : 12w   filename     第 12 行 写入 到 文件             -     : 12 , 32w   filename     将 第 12 - 32 行 写入 到 文件 ， 这里 可以 用   $   表示 最后 一行 ， 用   .   表示 当前 行 ， 还 可以 用   .+ 5   表示 当前 行后 的 第 5 行     -   命令 模式 下 的 搜索             -     : / str /   正向 搜索   str               -     : ? str ?   反向 搜索   str               -     : / str / w   file       正向 搜索 ， 并 将 第一个 包含 字符串   str   的 行 写入   file   文件             -     : / str1 / , / str2 / w   file     正向 搜索 ， 并 将 包含 字符串   str1   的 行至 包含 字符串   str2   的 行写     -   搜索 正则表达式             ^                                 放在 字符串 前面 ， 匹配 行首 的 字 ；   $                                 放在 字符串 后面 ， 匹配 行尾 的 字 ；     \ \ & lt ;                                 匹配 一个 字 的 字头 ；     \ \ & gt ;                                 匹配 一个 字 的 字 尾 ；   .                                 匹配 任何 单个 正文 字符 ；     [   str   ]                           匹配   str   中 的 任何 单个 字符 ；     [   ^ str   ]                         匹配 任何 不 在   str   中 的 单个 字符 ；     [   a - b   ]                           匹配   a   到   b   之间 的 任一 字符 ；   *                                 匹配 前 一个 字符 的     0     次 或 多次 出现 ；     \ \                                   转义 后面 的 字符 。                       正文 替换 :   : s   命令             : % s / str1 / str2 /     用 字符串   str2   替换 行中 首次 出现 的 字符串   str1             : s / str1 / str2 / g     用 字符串   str2   替换 行中 所有 出现 的 字符串   str1               g   放在 命令 末尾 ， 表示 对 搜索 字符串 的 每次 出现 进行 替换 , 不止 匹配 每行 中 的 第一次 出现 ； 不 加   g ， 表示 只 对 搜索 字符串 的 首次 出现 进行 替换 ； g   放在 命令 开头 ， 表示 对 正文 中 所有 包含 搜索 字符串 的 行 进行 替换 操作 ;           s   表示 后面 跟着 一串 替换 的 命令 ；       %   表示 替换 范围 是 所有 行 ， 即 全文 。                   统计 出现 次数 命令 ：   : % s / str1 / & amp ; / gn             删除 正文 命令 ： 略           恢复 意外 退出 未 保存 的 文件   : recover   命令               执行 shell 命令 ：   : ! some - command   ， 执行 完后 回到 vim               分屏             : split   缩写   : sp   上下 分屏         : vsplit   缩写   : vsp     左右分 屏       分屏 后 切换   ctrl   +   w     [ hjkl ]   ， 也 可以 直接 用   Ctrl + w + w   顺序 切换       设置 分屏 尺寸         ctrl   +   w   =     所有 屏幕 等 高 或 等 宽         ctrl   +   w   -     减少 高度         ctrl   +   w   +     增加 高度               可以 在 分屏 中 用   : q   关闭 文件 即可 关闭 分屏         : only     关闭 当前 分屏 外 所有 屏幕                   标签 页             : tabnew   [ filename ]     创建 新 标签 文件名 可 选 ， 有 表示 打开 文件 ,   和   : tabe   差不多         : tabc     关闭 当前 标签 close         : tabo     关闭 当前 标签 外 的 所有 标签 only         : tabn     下 一个 标签 next         : tabp     跳转 到 前 一个 标签 preview         : tabr     和     : tabfir     到 第一个   first         : tabl     最后 一个 last         : tabdo     为 每个 标签 执行命令                   选项 设置 ， 命令   : set   option                       autoindent                 设置 该 选项 ， 则 正文 自动 缩进   ignorecase                 设置 该 选项 ， 则 忽略 规则 表达式 中 大 小写字母 的 区别   number                         设置 该 选项 ， 则 显示 正文 行号   ruler                           设置 该 选项 ， 则 在 屏幕 底部 显示 光标 所在 行 、 列 的 位置   tabstop                       设置 按   Tab   键 跳 过 的 空格 数 。 例如   : set     tabstop     =   n ， n   默认值 为     8     mk                                 将 选项 保存 在 当前目录 的   . exrc   文件 中               外部 工具 集成             vim   +   diff   =   vimdiff     使用 方法   vimdiff   f1   f2     或者     vim   - d         在 已经 打开 的 文件 中 执行命令   : diffsplit   filename2   。       : diffpatch   ，   diffsplit   默认 上下 分屏 ， 可以 用   : vert   diffsplit   实现 左右分 屏         : diffoff   关闭 diff 的 颜色         : diffu [ pdate ]     更新 diff                 xxd   二进制 编辑         vim   - b     编辑 二进制 文件         % ! xxd     在 vim 内 切换 到 二进制 编辑         % ! xxd   - r     切 回到 文本编辑 模式                   参考             http : / / www . jianshu . com / p / bcbe916f97e1        ", "tags": "tools", "url": "/wiki/tools/linux.html"},
      
      
      {"title": "Linux基本安装", "text": "    Table   of   Contents           源码 安装 gcc6           源码 安装   Python27           安装   TensorFlow                 源码 安装 gcc6       以   redhat   环境 安装 gcc6 为例 ：           首先 下载   gcc - 6.3 . 0 . tar . gz       解压     tar   xzf   gcc - 6.3 . 0 . tar . gz         下载 必须 的 依赖     . / contrib / download _ prerequisites     ， 会 直接 从   gnu   官网 下载 ， 国内 可能 很 慢 ，     可以 修改 这个 文件 中 的 命令 ， 使用 镜像 的 源 ， 推荐 一个 源 挺快 的     https : / / ftp . mirrorservice . org / sites / sourceware . org / pub / gcc / infrastructure /   ， 也 可以 使用 国内 的 其他 源 ， 比如   USTC ,   THU   的 镜像 。       在 gcc 目录 外键 一个 目标 文件目录     cd   ..     & amp ;     mkdir   objdir         配置 gcc     .. / gcc - 6.3 . 0 / configure   - - prefix = $ HOME / gcc - 6.3 . 0     ， 可以 将   prefix 参数 修改 为 你 想要 的 位置       多线程 make     make   - j4     修改 j 参数 调整 线程 数 ， 这个 过程 需要 较长时间 ， 建议 用   nohup   后台 编译 。       安装     make   install             源码 安装   Python27           下载   Python - 2.7 . 13 . tgz       解压     tar   xzf   Python - 2.7 . 13 . tgz     然后 进入 解压 后 目录     cd   Python - 2.7 . 13         配置     . / configure   - - prefix = $ HOME / local / usr / Python - 2.7 . 13     - - enable - shared     可以 修改   prefix   指定 安装 目录 ， 使能 共享 库 ；     如果 要 安装   TensorFlow ， 需要 使能   ucs4   支持 ， 增加 参数     - - enable - unicode = ucs4     即可 。       安装     make   - j4   & amp ; & amp ;   make   install             安装   TensorFlow       可能 存在 TensorFlow 的   glibc   和   libcxx   版本 与 系统 自带 的 版本 不 一致 ， 会 导致   import   异常 ， 我 是 参考 这个 链接 解决 的 ：       https : / / stackoverflow . com / questions / 33655731 / error - while - importing - tensorflow - in - python2 - 7 - in - ubuntu - 12 - 04 - glibc - 2 - 17 - not - f   。 可能 不 一定 适合 你 的 情况 ， 但是 原理 是 一样 的 ， 即 手动 下载   tf   支持 的   glibc   和 libcxx 版本 ， 然后 设置 环境变量     LD _ LIBRARY _ PATH     使   python   加载 正确 的 版本 。 我 的 命令 如下             mkdir   ~ / libcenv     cd     ~ / libcenv   wget   http : / / launchpadlibrarian . net / 137699828 / libc6 _ 2.17 - 0ubuntu5 _ amd64 . deb   wget   http : / / launchpadlibrarian . net / 137699829 / libc6 - dev _ 2.17 - 0ubuntu5 _ amd64 . deb     ###   论坛 中 的 那个 链接 此时 已经 失效 ， 换 一个 版本 也 可以 ， 应该 只要 版本 高于 tf 要求 的 libcxx 版本 即可 ， 没有 测试 过 ， 不负责任     wget   ftp : / / rpmfind . net / linux / centos / 7.3 . 1611 / os / x86 _ 64 / Packages / libstd c++ - 4.8 . 5 - 11 . el7 . x86 _ 64 . rpm   ar   p   libc6 _ 2.17 - 0ubuntu5 _ amd64 . deb   data . tar . gz     |     tar   zx   ar   p   libc6 - dev _ 2.17 - 0ubuntu5 _ amd64 . deb   data . tar . gz     |     tar   zx   rpm2cpio   libstd c++ - 4.8 . 5 - 11 . el7 . x86 _ 64 . rpm   |     cpio   - idmv               done ！       最后 一步 ， 我 安装 python 的 时候 忘记 指定   - - enable - unicode = ucs4   ， 导致 报错     undefined   symbol :   PyUnicodeUCS4 _ AsUTF8String   ， 重新 编译   python 即可 ！ 对此 的 解释 可以 参考     http : / / blog . csdn . net / taolinke / article / details / 50472451         运行 时要 指定 环境变量 ， 你 也 可以 通过 shell 函数 来 做         LD _ LIBRARY _ PATH = \" $ HOME / libcenv / lib / x86 _ 64 - linux - gnu / : $ HOME / libcenv / usr / lib64 / \"   $ HOME / libcenv / lib / x86 _ 64 - linux - gnu / ld - 2.17 . so   $ HOME / local / usr / Python - 2.7 . 13 / bin / python    ", "tags": "tools", "url": "/wiki/tools/gcc.html"},
      
      
      {"title": "maven", "text": "    Table   of   Contents           关于           流程           创建 maven 项目           构建 项目           运行 项目                   核心 概念           Pom ,   project   object   Model .           Maven   插件           Maven   生命周期           Maven   依赖 管理           Maven   库                   maven 常用 参数 和 命令           参考 链接                 关于       Maven   是 一个 项目管理 和 构建 自动化 工具 。 但是 对于 我们 程序员 来说 ， 我们 最 关心 的 是 它 的 项目 构建 功能 。     所以 这里 我们 介绍 的 就是 怎样 用   maven   来 满足 我们 项目 的 日常 需要 。       流程       创建 maven 项目             mvn   archetype : generate   - DgroupId   =   com . mycompany . helloworld   - DartifactId   =   helloworld   - Dpackage   =   com . mycompany . helloworld   - Dversion   =     1   . 0 - SNAPSHOT               archetype : generate   目标 会 列出 一系列 的   archetype   让 你 选择 。   Archetype   可以 理解 成 项目 的 模型 。   Maven   为 我们 提供 了 很 多种 的 项目 模型 ， 包括 从 简单 的   Swing   到 复杂 的   Web   应用 。 我们 选择 默认 的   maven - archetype - quickstart   ， 是 编号   # 807 ， 不同 版本 这个 值会 不同 ， 不过 都 会 是 默认值 ，     所以 按 回车 就 好 了 。       创建 的 配置 属性 对应 的 意义 参考 pom 文件 说明 ， 这些 属性 是 我们 在 命令行 中 用   - D   选项 指定 的 。 该 选项 使用   - Dname = value   的 格式 。       构建 项目               cd     helloworld   mvn   package               当 你 第一次 运行   maven   的 时候 ， 它会 从 网上 的   maven   库   ( repository )   下载 需要 的 程序 ， 存放 在 你 电脑 的 本地 库   ( local   repository )   中 ， 所以 这个 时候 你 需要 有   Internet   连接 。 Maven   默认 的 本地 库是   ~ / . m2 / repository /   ， 在   Windows   下 是   % USER _ HOME % . m2 \ \ repository \ \   。       maven   在   helloworld   下面 建立 了 一个 新 的 目录   target /   ， 构建 打包 后 的   jar   文件   helloworld - 1.0 - SNAPSHOT . jar   就 存放 在 这个 目录 下 。 编译 后 的   class   文件 放在   target / classes /   目录 下面 ， 测试   class   文件 放在   target / test - classes /   目录 下面 。       运行 项目       为了 验证 我们 的 程序 能 运行 ， 执行 下面 的 命令 ：             java   - cp   target / helloworld - 1.0 - SNAPSHOT . jar   com . mycompany . helloworld . App               核心 概念       Pom ,   project   object   Model .       配置文件 ， maven 根据 该 文件 构建 项目 ， 可以 继承 。 各 节点 的 含义     -   project ： 顶级 元素     -   modelVersion ： object   model 版本 ， 强制     -   groupId ： 项目 创建 团体 或 组织 的 唯一 标识 ， 通常 是 域名 倒 写 ， 例如   org . apache . maven . plugins     -   artifactId   ： 是 项目 artifact 唯一 的 基 地址 名     -   packaging   ： artifact 打包 的 方式 ， 如 jar 、 war 、 ear 等等 。 默认 为 jar 。 这个 不仅 表示 项目 最终 产生 何种 后缀 的 文件 ，         也 表示 build 过程 使用 什么样 的 lifecycle 。     -   version   ： artifact 的 版本 ， 通常 能 看见 为 类似 0.0 . 1 - SNAPSHOT ， 其中 SNAPSHOT 表示 项目 开发 中 ， 为 开发 版本     -   name ：   表示 项目 的 展现 名 ， 在 maven 生成 的 文档 中 使用     -   url ： 表示 项目 的 地址 ， 在 maven 生成 的 文档 中 使用     -   description ：   表示 项目 的 描述 ， 在 maven 生成 的 文档 中 使用     -   dependencies ：   表示 依赖 ， 在子 节点 dependencies 中 添加 具体 依赖 的 groupId   artifactId 和 version     -   build ：   表示 build 配置     -   parent ：   表示 父 pom       在   POM   中 ， groupId ,   artifactId ,   packaging ,   version   叫作   maven   坐标 ， 它 能 唯一 的 确定 一个 项目 。 有 了   maven   坐标 ， 我们 就 可以 用 它 来 指定 我们 的 项目 所 依赖 的 其他 项目 ， 插件 ， 或者 父 项目 。 一般   maven   坐标 写成 如下 的 格式 ：   groupId : artifactId : packaging : version   。     像 我们 的 例子 就 会 写成 ：   com . mycompany . helloworld :   helloworld :   jar :   1.0 - SNAPSHOT   .       我们 的   helloworld   示例 很 简单 ， 但是 大 项目 一般 会 分成 几个 子项目 。 在 这种 情况 下 ， 每个 子项目 就 会 有 自己 的   POM   文件 ， 然后 它们 会 有 一个 共同 的 父 项目 。 这样 只要 构建 父 项目 就 能够 构建 所有 的 子项目 了 。 子项目 的   POM   会 继承 父 项目 的   POM 。 另外 ， 所有 的   POM 都 继承 了 一个   Super - POM 。 Super - POM   设置 了 一些 默认值 ， 它 遵循 了 惯例 优于 配置 的 原则 。 所以 尽管 我们 的 这个   POM   很 简单 ， 但是 这 只是 你 看得见 的 一部分 。 运行 时候 的   POM   要 复杂 的 多 。   如果 你 想 看到 运行 时候 的   POM   的 全部内容 的话 ， 可以 运行 下面 的 命令 ：   mvn   help : effective - pom   .       Maven   插件       我们 用 了     mvn   archetype : generate     命令 来 生成 一个 项目 。 那么 这里 的     archetype : generate     是 什么 意思 呢 ？ archetype   是 一个 插件 的 名字 ， generate 是 目标 ( goal ) 的 名字 。 这个 命令 的 意思 是 告诉   maven   执行   archetype   插件 的   generate   目标 。 插件 目标 通常 会 写成     pluginId : goalId         一个 目标 是 一个 工作 单元 ， 而 插件 则 是 一个 或者 多个 目标 的 集合 。 比如说 Jar 插件 ， Compiler 插件 ， Surefire 插件 等 。 从 看 名字 就 能 知道 ， Jar   插件 包含 建立 Jar 文件 的 目标 ，   Compiler   插件 包含 编译 源代码 和 单元测试 代码 的 目标 。 Surefire   插件 的话 ， 则 是 运行 单元测试 。       看到 这里 ， 估计 你 能 明白 了 ， mvn   本身 不会 做太多 的 事情 ， 它 不 知道 怎么样 编译 或者 怎么样 打包 。 它 把 构建 的 任务 交给 插件 去 做 。 插件 定义 了 常用 的 构建 逻辑 ， 能够 被 重复 利用 。 这样 做 的 好处 是 ， 一旦 插件 有 了 更新 ， 那么 所有 的   maven   用户 都 能 得到 更新 。       Maven   生命周期       我们 用 的 第二个 命令 是 ： mvn   package 。 这里 的   package   是 一个 maven 的 生命周期 阶段   ( lifecycle   phase   ) 。 生命周期 指 项目 的 构建 过程 ， 它 包含 了 一系列 的 有序 的 阶段   ( phase ) ， 而 一个 阶段 就是 构建 过程 中 的 一个 步骤 。       那么 生命周期 阶段 和 上面 说 的 插件 目标 之间 是 什么 关系 呢 ？ 插件 目标 可以 绑定 到 生命周期 阶段 上 。 一个 生命周期 阶段 可以 绑定 多个 插件 目标 。 当   maven   在 构建 过程 中 逐步 的 通过 每个 阶段 时 ， 会 执行 该 阶段 所有 的 插件 目标 。       maven   能 支持 不同 的 生命周期 ， 但是 最 常用 的 是 默认 的 Maven 生命周期   ( default   Maven   lifecycle   ) 。 如果 你 没有 对 它 进行 任何 的 插件 配置 或者 定制 的话 ， 那么 上面 的 命令   mvn   package   会 依次 执行 默认 生命周期 中 直到 包括   package   阶段 前 的 所有 阶段 的 插件 目标 ：     -   process - resources   阶段 ： resources : resources     -   compile   阶段 ： compiler : compile     -   process - classes   阶段 ： ( 默认 无 目标 )     -   process - test - resources   阶段 ： resources : testResources     -   test - compile   阶段 ： compiler : testCompile     -   test   阶段 ： surefire : test     -   prepare - package   阶段 ： ( 默认 无 目标 )     -   package   阶段 ： jar : jar       Maven   依赖 管理       之前 我们 说 过 ， maven   坐标 能够 确定 一个 项目 。 换句话说 ， 我们 可以 用 它 来 解决 依赖 关系 。 在   POM   中 ， 依赖 关系 是 在   dependencies   部分 中 定义 的 。 在 上面 的   POM   例子 中 ， 我们 用   dependencies   定义 了 对于   junit   的 依赖 ：               & lt ; dependencies & gt ;               & lt ; dependency & gt ;                   & lt ; groupId & gt ;   junit   & lt ; / groupId & gt ;                   & lt ; artifactId & gt ;   junit   & lt ; / artifactId & gt ;                   & lt ; version & gt ;   3.8 . 1   & lt ; / version & gt ;                   & lt ; scope & gt ;   test   & lt ; / scope & gt ;               & lt ; / dependency & gt ;       & lt ; / dependencies & gt ;                 那 这个 例子 很 简单 ， 但是 实际 开发 中 我们 会 有 复杂 得 多 的 依赖 关系 ， 因为 被 依赖 的   jar   文件 会 有 自己 的 依赖 关系 。 那么 我们 是不是 需要 把 那些 间接 依赖 的   jar   文件 也 都 定义 在 POM 中 呢 ？ 答案 是 不 需要 ， 因为   maven   提供 了 传递 依赖 的 特性 。       所谓 传递 依赖 是 指   maven   会 检查 被 依赖 的   jar   文件 ， 把 它 的 依赖 关系 纳入 最终 解决 的 依赖 关系 链中 。 针对 上面 的   junit   依赖 关系 ， 如果 你 看 一下   maven   的 本地 库 （ 我们 马上会 解释   maven   库 ） ~ / . m2 / repository / junit / junit / 3.8 . 1 /   ， 你 会 发现   maven   不但 下载 了   junit - 3.8 . 1 . jar ， 还 下载 了 它 的   POM   文件 。 这样   maven   就 能 检查   junit   的 依赖 关系 ， 把 它 所 需要 的 依赖 也 包括 进来 。       在   POM   的   dependencies   部分 中 ， scope   决定 了 依赖 关系 的 适用范围 。 我们 的 例子 中   junit   的   scope   是   test ， 那么 它 只会 在 执行   compiler : testCompile   and   surefire : test   目标 的 时候 才 会 被 加到   classpath   中 ， 在 执行   compiler : compile   目标 时 是 拿 不到   junit   的 。       我们 还 可以 指定   scope   为   provided ， 意思 是   JDK   或者 容器 会 提供 所 需 的 jar 文件 。 比如说 在 做 web 应用 开发 的 时候 ， 我们 在 编译 的 时候 需要   servlet   API   jar   文件 ， 但是 在 打包 的 时候 不 需要 把 这个   jar   文件 打 在   WAR   中 ， 因为 servlet 容器 或者 应用服务器 会 提供 的 。       scope   的 默认值 是   compile ， 即 任何 时候 都 会 被 包含 在   classpath   中 ， 在 打包 的 时候 也 会 被 包括 进去 。       Maven   库       当 第一次 运行   maven   命令 的 时候 ， 你 需要   Internet   连接 ， 因为 它 要 从 网上 下载 一些 文件 。 那么 它 从 哪里 下载 呢 ？ 它 是从   maven   默认 的 远程 库 ( http : / / repo1 . maven . org / maven2 )   下载 的 。 这个 远程 库有   maven   的 核心 插件 和 可供 下载 的   jar   文件 。       但是 不是 所有 的   jar   文件 都 是 可以 从 默认 的 远程 库 下载 的 ， 比如说 我们 自己 开发 的 项目 。 这个 时候 ， 有 两个 选择 ： 要么 在 公司 内部 设置 定制 库 ， 要么 手动 下载 和 安装 所 需 的 jar 文件 到 本地 库 。       本地 库是 指   maven   下载 了 插件 或者   jar   文件 后 存放 在 本地 机器 上 的 拷贝 。 在   Linux   上 ， 它 的 位置 在   ~ / . m2 / repository ， 在   Windows   XP   上 ， 在   C : \ \ Documents   and   Settings \ \ username . m2 \ \ repository   ， 在   Windows   7   上 ， 在   C : \ \ Users \ \ username . m2 \ \ repository 。 当   maven   查找 需要 的   jar   文件 时 ， 它会 先 在 本地 库中 寻找 ， 只有 在 找 不到 的 情况 下 ， 才 会 去 远程 库中 找 。       运行 下面 的 命令 能 把 我们 的   helloworld   项目 安装 到 本地 库 ：                         $ mvn     install               一旦 一个 项目 被 安装 到 了 本地 库后 ， 你 别的 项目 就 可以 通过   maven   坐标 和 这个 项目 建立 依赖 关系 。 比如 如果 我 现在 有 一个 新 项目 需要 用到   helloworld ， 那么 在 运行 了 上面 的   mvn   install   命令 后 ， 我 就 可以 如下 所示 来 建立 依赖 关系 ：                       & lt ; dependency & gt ;                   & lt ; groupId & gt ;   com . mycompany . helloworld   & lt ; / groupId & gt ;                   & lt ; artifactId & gt ;   helloworld   & lt ; / artifactId & gt ;                   & lt ; version & gt ;   1.0 - SNAPSHOT   & lt ; / version & gt ;               & lt ; / dependency & gt ;                 好 了 ， maven   的 核心 概念 就 简单 的 介绍 到 这里 。 至于 在   Eclipse   中 如何 使用   maven ， 这个 网上 很多 了 ， google   一下 就行 。       maven 常用 参数 和 命令           mavn 常用 参数                 mvn   - e   显示 详细 错误   mvn   - U   强制 更新 snapshot 类型 的 插件 或 依赖 库 （ 否则 maven 一天 只会 更新 一次 snapshot 依赖 ）   mvn   - o   运行 offline 模式 ， 不 联网 更新 依赖   mvn   - N 仅 在 当前 项目 模块 执行命令 ， 关闭 reactor   mvn   - pl   module _ name 在 指定 模块 上 执行命令   mvn   - ff   在 递归 执行命令 过程 中 ， 一旦 发生 错误 就 直接 退出   mvn   - Dxxx = yyy 指定 java 全局 属性   mvn   - Pxxx 引用 profile   xxx                   Lifecycle   中 的 命令                 mvn   test - compile   编译 测试代码   mvn   test   运行 程序 中 的 单元测试   mvn   compile   编译 项目   mvn   package   打包 ， 此时 target 目录 下会 出现 maven - quickstart - 1.0 - SNAPSHOT . jar 文件 ， 即 为 打包 后 文件   mvn   install   打包 并 安装 到 本地 仓库 ， 此时 本 机 仓库 会 新增 maven - quickstart - 1.0 - SNAPSHOT . jar 文件 。   每个 phase 都 可以 作为 goal ， 也 可以 联合 ， 如 之前 介绍 的 mvn   clean   install                   其他 命令                 mvn   archetype : generate   创建 maven 项目   mvn   package   打包 ， 上面 已经 介绍 过 了   mvn   package   - Prelease 打包 ， 并 生成 部署 用 的 包 ， 比如 deploy / * . tgz   mvn   install   打包 并 安装 到 本地 库   mvn   eclipse : eclipse   生成 eclipse 项目 文件   mvn   eclipse : clean   清除 eclipse 项目 文件   mvn   site   生成 项目 相关 信息 的 网站               参考 链接             http : / / www . oracle . com / technetwork / cn / community / java / apache - maven - getting - started - 1 - 406235 - zhs . html           http : / / www . oracle . com / technetwork / cn / community / java / apache - maven - getting - started - 2 - 405568 - zhs . html           http : / / www . trinea . cn / android / maven /           http : / / maven . apache . org / guides / index . html        ", "tags": "tools", "url": "/wiki/tools/maven.html"},
      
      
      {"title": "SBT - scala构建工具", "text": "    Table   of   Contents           关于           构建 项目           从 单个 文件 构建           构建 文件 build . sbt           目录 结构                   构建 定义           键                   Tasks   任务           两种 构建 模式           dependencies   依赖 管理           手动 管理           自动 管理           resolvers           configuration                   使用 pom   xml 文件 添加 依赖           路径           创建 文件 和 路径           路径 finder                           调试                 关于       sbt 是 scala 的 交互式 构建 工具 ， 类似 于 java 的 maven 。       构建 项目       从 单个 文件 构建       最 简单 的 方式 是从 只 包含 一个 scala 源文件 的 目录 构建 项目 。     例如 在 目录   helloword   创建 一个   HW . scala   文件               object       Hi       {           def       main     (     args     :       Array     [     String     ] )       =       println     (     & quot ; Hi ! & quot ;     )       }                 然后 运行   sbt   ， 和   run   ， 或者 直接 运行   sbt   run   命令 。       sbt 自动 寻找 下列 目录           Sources   in   the   base   directory       Sources   in   src / main / scala   or   src / main / java       Tests   in   src / test / scala   or   src / test / java       Data   files   in   src / main / resources   or   src / test / resources       jars   in   lib           构建 文件   build . sbt         位于 项目 根目录 ， 一个 简单 的 构建 文件 如下               lazy       val       root       =       (     project       in       file     (     & quot ; .& quot ;     ) ) .           settings     (               name       : =       & quot ; hello & quot ;     ,               version       : =       & quot ; 1.0 & quot ;     ,               scalaVersion       : =       & quot ; 2.11 . 7 & quot ;           )                 如果 是 要 打包 到 jar 文件 ， name 和 version 是 必须 的 。       添加 依赖               val       derby       =       & quot ; org . apache . derby & quot ;       %       & quot ; derby & quot ;       %       & quot ; 10.4 . 1.3 & quot ;         lazy       val       commonSettings       =       Seq     (           organization       : =       & quot ; com . example & quot ;     ,           version       : =       & quot ; 0.1 . 0 & quot ;     ,           scalaVersion       : =       & quot ; 2.11 . 7 & quot ;       )         lazy       val       root       =       (     project       in       file     (     & quot ; .& quot ;     ) ) .           settings     (     commonSettings     :       _     *     ) .           settings     (               name       : =       & quot ; hello & quot ;     ,               libraryDependencies       + =       derby           )                 依赖 库 的 写法 是 ：             groupID   %   artifactID   %   revision               如果 将 依赖 的 库 放在 lib 目录 下 ， 就 不 需要 添加 该 依赖 。       目录 结构           源码     sbt 的 源码 目录 结构 与 maven 一样 。                 src /       main /           resources /                 & lt ; files   to   include   in   main   jar   here & gt ;           scala /                 & lt ; main   Scala   sources & gt ;           java /                 & lt ; main   Java   sources & gt ;       test /           resources                 & lt ; files   to   include   in   test   jar   here & gt ;           scala /                 & lt ; test   Scala   sources & gt ;           java /                 & lt ; test   Java   sources & gt ;                       构建 定义 文件     包含 根目录 的   build . sbt   ， 其他 构建 文件 放在 project 目录 下 。               构建 输出 文件目录 是   target /   ， 在   . gitignore   中 应该 排除 该 目录               构建 定义       一个 构建 定义 是 一个 Project ， 拥有 一个 类型 为   Setting [ T ]   的 列表 ， Setting [ T ]   是 会 影响 到   sbt   保存 键值 对 的   map   的 一种 转换 ， T   是 每 一个   value   的 类型 。     参考 前面 的 构建 定义 示例 代码 。       每一项   Setting   都 定义 为 一个   Scala   表达式 。 在   settings   中 的 表达式 是 相互 独立 的 ， 而且 它们 仅仅 是 表达式 ， 不是 完整 的   Scala   语句 。 ? ? ? ? WHAT     这些 表达式 可以 用   val ， lazy   val ， def   声明 。   build . sbt   不 允许 使用 顶层 的   object   和   class 。 它们 必须 写 到   project /   目录 下 作为 完整 的   Scala   源文件 。       键       有 三种 类型 的   key ：           SettingKey [ T ] ： 一个   key   对应 一个 只 计算 一次 的   value （ 这个 值 在 加载 项目 的 时候 计算 ， 然后 一直 保存 着 ） 。       TaskKey [ T ] ： 一个   key   对应 一个 称之为   task   的   value ， 每次 都 会 重新 计算 ， 可能 存在 潜在 的 副作用 。       InputKey [ T ] ： 一个   key   对应 一个 可以 接收 命令行 参数 的   task 。           键 的 类型           内置 键 ，   build . sbt   会 隐式 包含   import   sbt . Keys ._   ， 所以 可以 通过   name   取 到   sbt . Keys . name 。       自定义 键 ， 创建 方法 ： settingKey ， taskKey   和   inputKey   创建 自定义   keys .                   lazy       val       hello       =       taskKey     [     Unit     ] (     & quot ; 一个   task   示例 & quot ;     )                 Tasks   任务       一个 简单 的 hello 任务 如下 ， 在 build . sbt 文件 中 加入 下列 代码               lazy       val       hello       =       taskKey     [     Unit     ] (     & quot ; Prints   &# 39 ; Hello   World &# 39 ; & quot ;     )         hello       : =       println     (     & quot ; hello   world ! & quot ;     )                 然后 执行     sbt   hello     就 可以 看到 结果 了 。       一个 任务 首先 需要 定义 一个   taskKey [ T ]   ， 在 这个 例子 中 返回 空 类型 ， 每 一个 任务 是 一个 scala 函数 ， 可以     返回 一个 结果 。 可以 在 其他 任务 中 通过   . value   属性 访问 另 一个 task 的 结果 。       两种 构建 模式           交互式 模式 ， 输入   sbt   命令 后面 不 跟 参数 ， 然后 进入 交互式 环境 ， 然后 运行 命令 构建 。       批处理 模式 ， 跟 参数 罗 。   sbt   clean   compile   \" testOnly   TestA   TestB \"         常见 命令 ， 更 多 参考                 clean       删除 所有 生成 的 文件   （ 在   target   目录 下 ） 。   compile   编译 源文件 （ 在   src / main / scala   和   src / main / java   目录 下 ） 。   test         编译 和 运行 所有 测试 。   console   进入 到 一个 包含 所有 编译 的 文件 和 所有 依赖 的   classpath   的   Scala   解析器 。 输入   : quit ，   Ctrl + D   （ Unix ） ， 或者   Ctrl + Z   （ Windows ）   返回 到   sbt 。   run   & lt ; 参数 & gt ; *       在 和   sbt   所处 的 同一个 虚拟机 上 执行 项目 的   main   class 。   package   将   src / main / resources   下 的 文件 和   src / main / scala   以及   src / main / java   中 编译 出来 的   class   文件 打包 成 一个   jar   文件 。   help   & lt ; 命令 & gt ;       显示 指定 的 命令 的 详细 帮助 信息 。 如果 没有 指定 命令 ， 会 显示 所有 命令 的 简介 。   reload     重新 加载 构建 定义 （ build . sbt ，   project / * . scala ，   project / * . sbt   这些 文件 中 定义 的 内容 ) 。 在 修改 了 构建 定义 文件 之后 需要 重新 加载 。               dependencies   依赖 管理       手动 管理       手动 将库 的 jar 包 复制到 lib 目录 下 就 可以 了 。 如果 要 更改 默认 路径 ， 需要 修改   unmanagedBase   ，     例如 修改 到   custom _ lib /   目录 可以 用 下述 命令               unmanagedBase       : =       baseDirectory     .     value       /       & quot ; custom _ lib & quot ;                 更 多 的 控制 可以 通过 重载 unmanagedJars 这个 task ， 默认 的 实现 是               unmanagedJars       in       Compile       : =       (     baseDirectory     .     value       * *       & quot ; * . jar & quot ;     ) .     classpath                 如果 要 添加 多个 路径 到 默认 路径 ， 可以 这样 写               unmanagedJars       in       Compile       ++ =       {               val       base       =       baseDirectory     .     value               val       baseDirectories       =       (     base       /       & quot ; libA & quot ;     )       +++       (     base       /       & quot ; b & quot ;       /       & quot ; lib & quot ;     )       +++       (     base       /       & quot ; libC & quot ;     )               val       customJars       =       (     baseDirectories       * *       & quot ; * . jar & quot ;     )       +++       (     base       /       & quot ; d & quot ;       /       & quot ; my . jar & quot ;     )               customJars     .     classpath       }                 这里 对 路径 的 语法 ， 参考 后面 的 路径       自动 管理       sbt 支持 三种 自动 管理 方式 ， 都 是 通过 Apache   ivy 来 实现 的 。           Declarations   in   your   project   definition       Maven   POM   files   ( dependency   definitions   only :   no   repositories )       Ivy   configuration   and   settings   files           可以 通过 下述 语句 声明 依赖 ， 其中 configuration 是 可选 的 。     多个 依赖 可以 通过   Seq   将 每 一个 依赖 作为 一个 元素 进行 添加 ， 注意 链接 操作 符号 的 区别 ，     libraryDependencies 是 一个   Seq   ?               libraryDependencies       + =       groupID       %       artifactID       %       revision       %       configuration       libraryDependencies       ++ =       Seq     (           groupID       % %       artifactID       %       revision     ,           groupID       % %       otherID       %       otherRevision       )                     If   you   are   using   a   dependency   that   was   built   with   sbt ,   double   the   first   %   to   be   % %       sbt   uses   the   standard   Maven2   repository   by   default .           revision 除了 可以 使用 常规 的 完整 版本号 外 ， 还 可以 使用   \" latest . integration \" ,   \" 2.9 .+ \" ,   or   \" [ 1.0 , ) \" 这种 形式 。       resolvers       可以 通过 设置 resolvers 来 添加 依赖 库 获取 的 位置 ， 格式 是       resolvers   + =   name   at   location   ， location 可以 是 合法 的 URI ， 例如               resolvers       + =       & quot ; Sonatype   OSS   Snapshots & quot ;       at       & quot ; https : / / oss . sonatype . org / content / repositories / snapshots & quot ;       resolvers       + =       & quot ; Local   Maven   Repository & quot ;       at       & quot ; file : / / & quot ;     +     Path     .     userHome     .     absolutePath     +     & quot ; / . m2 / repository & quot ;       externalResolvers       : =       Resolver     .     withDefaultResolvers     (     resolvers     .     value     ,       mavenCentral       =       false     )                 configuration           指定 URL                   libraryDependencies       + =       & quot ; slinky & quot ;       %       & quot ; slinky & quot ;       %       & quot ; 2.1 & quot ;       from       & quot ; https : / / slinky2 . googlecode . com / svn / artifacts / 2.1 / slinky . jar & quot ;       libraryDependencies       + =       & quot ; org . apache . felix & quot ;       %       & quot ; org . apache . felix . framework & quot ;       %       & quot ; 1.8 . 0 & quot ;       intransitive     ( )       libraryDependencies       + =       & quot ; org . testng & quot ;       %       & quot ; testng & quot ;       %       & quot ; 5.7 & quot ;       classifier       & quot ; jdk15 & quot ;       libraryDependencies       + =           & quot ; org . lwjgl . lwjgl & quot ;       %       & quot ; lwjgl - platform & quot ;       %       lwjglVersion       classifier       & quot ; natives - windows & quot ;       classifier       & quot ; natives - linux & quot ;       classifier       & quot ; natives - osx & quot ;       libraryDependencies       + =               & quot ; log4j & quot ;       %       & quot ; log4j & quot ;       %       & quot ; 1.2 . 15 & quot ;       exclude     (     & quot ; javax . jms & quot ;     ,       & quot ; jms & quot ;     )       libraryDependencies       + =                   & quot ; org . apache . felix & quot ;       %       & quot ; org . apache . felix . framework & quot ;       %       & quot ; 1.8 . 0 & quot ;       withSources     ( )       withJavadoc     ( )                 -       使用 pom   xml 文件 添加 依赖             externalPom ( )   externalPom ( Def . setting ( baseDirectory . value   /   & quot ; custom - name . xml & quot ; ) )               路径       创建 文件 和 路径       sbt   0.10 +   使用   java . io . File     文件类型 。     创建 文件 方法               val       source     :       File       =       file     (     & quot ; / home / user / code / A . scala & quot ;     )       def       readme     (     base     :       File     )     :       File       =       base       /       & quot ; README & quot ;                 sbt   添加 了   /   方法 ， 对应 于 两 参数 构造函数 。         baseDirectory     task   返回 bese 目录 绝对路径 。       路径 finder       一个 路径 finder 返回 一个   Seq [ File ]   。 例如               def       scalaSources     (     base     :       File     )     :       Seq     [     File     ]       =       {           val       finder     :       PathFinder       =       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * . scala & quot ;           finder     .     get       }                 The     * *     method   accepts   a     java . io . FileFilter     ， 筛选 目录 及 子目录 下 所有 文件 。     如果 只 访问 该 目录 可以 使用     *     函数 。     惰性 求值 使得 需要 调用   . get   才能 计算结果 。       name   filter   使用     *   表示 0 个 或 多个 字符 。 用   | |   表示 多个 filter 的 或 ， 用   - -   表示 排除 。               val       base       =       baseDirectory     .     value       (     base       /       & quot ; src & quot ;     )       *       & quot ; * Test * . scala & quot ;       (     base       /       & quot ; src & quot ;     )       * *       (     & quot ; * . scala & quot ;       | |       & quot ; * . java & quot ;     )       (     base     /     & quot ; src & quot ;     /     & quot ; main & quot ;     /     & quot ; resources & quot ;     )       *       (     & quot ; * . png & quot ;       - -       & quot ; logo . png & quot ;     )                 组合 多个 finder     +++   ,   排除 结果 可以 用     - - -   。     finder 有 一个 filter 方法 ， 用于 进一步 筛选               (     base       /       & quot ; lib & quot ;       +++       base       /       & quot ; target & quot ;     )       *       & quot ; * . jar & quot ;       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * . scala & quot ;     )       - - -       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; . svn & quot ;       * *       & quot ; * . scala & quot ;     )       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * & quot ;     )       filter       {       _     .     isDirectory       }       base       filter       ClasspathUtilities     .     isArchive         ` `     `           -   to   string   转换               -   toString               -   absString   ，               -   getPaths   ，   返回 Seq [ String ]         ##   插件       插件 用来 扩展 构建 定义 ， 可能 是 一个 新 的 task 。         添加 插件 申明 ， 在 项目 根目录 下 的   `     /     project `   目录 添加   `     .     sbt `   文件 ， 然后 在 其中 添加 语句         `     ` `     scala       addSbtPlugin     (     & quot ; com . typesafe . sbt & quot ;       %       & quot ; sbt - site & quot ;       %       & quot ; 0.7 . 0 & quot ;     )                     assembly   打包 插件 ， 以及 shade 例子 ：                   assemblyOption       in       assembly       : =       (     assemblyOption       in       assembly     ) .     value     .     copy     (     includeScala       =       false     )         val       jarStartWith       =       Seq     (     & quot ; pmml - & quot ;     ,     & quot ; guava - & quot ;     ,     & quot ; jpmml - & quot ;     ,       & quot ; json4s - & quot ;     )       assemblyExcludedJars       in       assembly       : =       {               val       cp       =       (     fullClasspath       in       assembly     ) .     value                 val       filtered       =       cp       filterNot       {     f       = & gt ;                       jarStartWith     .     map     (     s       = & gt ;       f     .     data     .     getName     .     startsWith     (     s     ) ) .     foldLeft     (     false     ) ( (     a     ,     b     )       = & gt ;       a       | |       b     )               }               cp     .     foreach     {     c       = & gt ;                       val       tag       =       if     (     filtered     .     contains     (     c     ) )       & quot ;     Excluded & quot ;       else       & quot ; +   Included & quot ;                       println     (     s & quot ;     $ tag       :       $ {     c     .     data     .     getName     }     & quot ;     )               }               filtered       }         val       shadedRootPackage       =       & quot ; com . tracholar & quot ;       assemblyShadeRules       in       assembly       : =       Seq     (               ShadeRule     .     rename     (     & quot ; com . google . common . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll     ,               ShadeRule     .     rename     (     & quot ; org . jpmml . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll     ,               ShadeRule     .     rename     (     & quot ; org . dmg . pmml . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll       )         assemblyMergeStrategy       in       assembly       : =       {               case       PathList     (     & quot ; org & quot ;     ,     & quot ; dmg & quot ;     ,       & quot ; pmml & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; org & quot ;     ,     & quot ; jpmml & quot ;     ,         xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; com & quot ;     ,     & quot ; google & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; com & quot ;     ,     & quot ; tracholar & quot ;     ,       & quot ; spark & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       x       = & gt ;       (     assemblyMergeStrategy       in       assembly     ) .     value     (     x     )       }       logLevel       in       assembly       : =       Level     .     Debug                 调试       传入     - jvm - debug   & lt ; port & gt ;     Turn   on   JVM   debugging ,   open   at   the   given   port .   参数 即可 远程 调试 ， 例如     sbt   - jvm - debug   5005   run    ", "tags": "tools", "url": "/wiki/tools/sbt.html"},
      
      
      {"title": "scala语言", "text": "    Table   of   Contents           关于           基础 语法           基本 数据结构           函数 组合 子   Functional   Combinators           函数 组合           偏 函数                   类型 ， 静态 类型           隐式 转换           隐式 函数           隐式 类   2.10 .+                   构建 工具   SBT           字符串   -   核心 数据结构           字符串 插值   String   Interpolation ， Scala   2.10 .+                   集合   -   核心 数据结构           List           Set           Seq           Map                   层次结构           常用 子类           一些 描述 特性 的 特质           可变   vs   不可 变           可变 集合           与 Java 转换                   使用 specs 进行 测试           并发 编程 *           线程 安全 的 三种 工具 。                   Java 跨平台 交互 ： 在 Java 中 使用 Scala           扩展 规格                   泛型 编程           Test           FlatSpec           FunSuit                         关于       学习 scala 后 ， 发现 scala 就是 灵活 版 的 java ， 他 通过 引入 函数 式 编程 的 一些 概念 来 达到 这个 目的 ，     并且 由于 基于 JVM ， 能够 复用 所有 的 java 库 ！ ！ 如果 你 嫌 java 臃肿 ， 不妨 试试 scala 。           相关 链接 ：       scala   API     http : / / www . scala - lang . org /         Twitter 教程     https : / / twitter . github . io / scala _ school / zh _ cn / index . html                     基础 语法           不 变量   val   ， 变量   var         基础 类型 ：       Int               流程 控制 ， 直接 看 例子                   for     (     i       & lt ; -       0       to       100     ) {               println     (     i     )       }                       def   创建 函数 ， 类型 标签                   def       addOne     (     m     :       Int     )     :       Int       =       m       +       1                     匿名 函数     ( x : Int )   = & gt ;   x + 1         函数 的 部分 应用                   def       adder     (     m     :     Int     ,       n     :     Int     )       =       m       +       n       val       add2       =       adder     (     2     ,       _ :     Int     )                     柯 理化 函数                   def       muliply     (     m     :       Int     )       (     n     :       Int     )       =       m       *       n                     可变 长 参数                   def       cap     (     args       String     * )       =       {               args     .     map       {                       arg       = & gt ;       arg     .     capitalize               }       }                     类     class         构造函数 不是 特殊 的 方法 ， 他们 是 除了 类 的 方法 定义 之外 的 代码 。                   class       Calculator     (     brand     :       String     )       {           / * *             *   A   constructor .             * /           val       color     :       String       =       if       (     brand       = =       & quot ; TI & quot ;     )       {               & quot ; blue & quot ;           }       else       if       (     brand       = =       & quot ; HP & quot ;     )       {               & quot ; black & quot ;           }       else       {               & quot ; white & quot ;           }             / /   An   instance   method .           def       add     (     m     :       Int     ,       n     :       Int     )     :       Int       =       m       +       n       }                     Scala 是 高度 面向 表达式 的 ： 大多数 东西 都 是 表达式 而 非 指令 。       继承                   class       ScientificCalculator     (     brand     :       String     )       extends       Calculator     (     brand     )       {           def       log     (     m     :       Double     ,       base     :       Double     )       =       math     .     log     (     m     )       /       math     .     log     (     base     )       }                     抽象类                   abstract       class       Shape       {                       def       getArea     ( )     :     Int             / /   subclass   should   define   this       }                         Traits   特质     很 像 接口 ， 通过   with   关键字 ， 一个 类 可以 扩展 多个 特质 。       ` ` ` scala     trait   Car   {         val   brand :   String     }       trait   Shiny   {         val   shineRefraction :   Int     }     class   BMW   extends   Car   with   Shiny   {         val   brand   =   \" BMW \"         val   shineRefraction   =   12     }     ` ` `     -   泛型 ， 方法 和 trait 都 可以 引入 类型 参数                       trait       Cache     [     K   ,     V     ]       {           def       get     (     key     :       K     )     :       V           def       put     (     key     :       K     ,       value     :       V     )           def       delete     (     key     :       K     )       }       def       remove     [     K     ] (     key     :       K     )                 如何 实现 像 java 那样 的 父类 占位 符 。           apply 方法                   class       Bar       {               def       apply     ( )       =       0       }       val       bar       =       new       Bar       bar     ( )       / /   res :   Int   =   0                     单例 对象 ， 工厂 模式                   object       Timer       {           var       count       =       0             def       currentCount     ( )     :       Long       =       {               count       + =       1               count           }       }       Timer     .     currentCount     ( )                     函数 即 对象 .     函数 是 一些 特质 的 集合 。 具体来说 ， 具有 一个 参数 的 函数 是 Function1 特质 的 一个 实例 。 这个 特征 定义 了   apply ( )   语法 糖 ， 让 你 调用 一个 对象 时 就 像 你 在 调用 一个 函数 。                   object       addOne       extends       Function1     [     Int   ,     Int     ]       {               def       apply     (     m     :       Int     )     :       Int       =       m       +       1       }       class       AddOne       extends       (     Int       = & gt ;       Int     )       {           def       apply     (     m     :       Int     )     :       Int       =       m       +       1       }                     包 ， 和 Java 的 一样       模式匹配     匹配 值                   val       times       =       1         times       match       {               case       1       = & gt ;       & quot ; one & quot ;               case       2       = & gt ;       & quot ; two & quot ;               case       _       = & gt ;       & quot ; some   others & quot ;       }         / /   守卫 匹配       times       match       {               case       i       if       i       = =       1       = & gt ;       & quot ; one & quot ;               case       i       if       i       = =       2       = & gt ;       & quot ; two & quot ;               case       _       = & gt ;       & quot ; some   others & quot ;       }                 匹配 类型               def       bigger     (     o     :       Any     )     :       Any       =       {           o       match       {               case       i     :       Int       if       i       & lt ;       0       = & gt ;       i       -       1               case       i     :       Int       = & gt ;       i       +       1               case       d     :       Double       if       d       & lt ;       0     .     0       = & gt ;       d       -       0.1               case       d     :       Double       = & gt ;       d       +       0.1               case       text     :       String       = & gt ;       text       +       & quot ; s & quot ;           }       }                 匹配 类 成员               def       calcType     (     calc     :       Calculator     )       =       calc       match       {           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 20B & quot ;       = & gt ;       & quot ; financial & quot ;           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 48G & quot ;       = & gt ;       & quot ; scientific & quot ;           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 30B & quot ;       = & gt ;       & quot ; business & quot ;           case       _       = & gt ;       & quot ; unknown & quot ;       }                     样本 类   case   class                   case       class       Calculator     (     brand     :       String     ,       model     :       String     )                 case   classes   are   designed   to   be   used   with   pattern   matching .   Let ’ s   simplify   our   calculator   classifier   example   from   earlier .     样本 类 就是 被 设计 用 在 模式匹配 中 的 。 让 我们 简化 之前 的 计算器 分类器 的 例子 。               def       calcType     (     calc     :       Calculator     )       =       calc       match       {           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 20B & quot ;     )       = & gt ;       & quot ; financial & quot ;           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 48G & quot ;     )       = & gt ;       & quot ; scientific & quot ;           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 30B & quot ;     )       = & gt ;       & quot ; business & quot ;           case       Calculator     (     ourBrand     ,       ourModel     )       = & gt ;       & quot ; Calculator :   % s   % s   is   of   unknown   type & quot ;     .     format     (     ourBrand     ,       ourModel     )       }                 我们 也 可以 将 匹配 的 值 重新命名 。                   case       c     @ Calculator     (     _     ,       _     )       = & gt ;       & quot ; Calculator :   % s   of   unknown   type & quot ;     .     format     (     c     )                     异常 ，   try   ...   catch   ...   finally       private [ spark ]   private   作用域 为 包含 spark 类 的 地方 才 可见           基本 数据结构           String   实际上 就是 java . lang . String       List   列表                   val       numbers       =       List     (     1     ,     2     ,     3     ,     4     )                     Set   集                   scala     & gt ;       Set     (     1     ,     2     ,     1     )       res0     :       scala . collection . immutable . Set     [     Int     ]       =       Set     (     1     ,       2     )                     Tuple   元组                   val       hostPort       =       (     & quot ; localhost & quot ;     ,       80     )       hostPort     .     _ 1         / /   localhost       hostPort     .     _ 2         / /   80                 与 样本 类 不同 ， 元组 不能 通过 名称 获取 字 段 ， 而是 使用 位置 下标 来 读取 对象 ； 而且 这个 下标 基于 1 ， 而 不是 基于 0 。     在 创建 两个 元素 的 元组 时 ， 可以 使用 特殊 语法 ：   1   - & gt ;   2   ， 见 映射           unpack ：   val   ( v1 ,   v2 )   =   ( 1 , 2 )   ,     函数参数 unpack 例子           _ *                     def       hello     (       names     :       String *     )       {           println     (       & quot ; Hello   & quot ;       +       names     .     mkString     (     & quot ;   and   & quot ;       )       )       }         scala     & gt ;       val       names       =       List     (     & quot ; john & quot ;     ,       & quot ; paul & quot ;     ,       & quot ; george & quot ;     ,       & quot ; ringo & quot ;     )       names     :       List     [     String     ]       =       List     (     john     ,       paul     ,       george     ,       ringo     )       scala     & gt ;       hello     (       names     :       _     *       )       Hello       john       and       paul       and       george       and       ringo                     Map   映射 ， 类似 于 python 的 字典 ， c 的 hash _ map                   Map     (     1       - & gt ;       2     )         / /   值 映射       Map     (     & quot ; foo & quot ;       - & gt ;       & quot ; bar & quot ;     )         / /   字符串 映射       Map     (     1       - & gt ;       Map     (     & quot ; foo & quot ;       - & gt ;       & quot ; bar & quot ;     ) )         / /   映射 到 映射       Map     (     & quot ; timesTwo & quot ;       - & gt ;       {     timesTwo     (     _     ) } )           / /   映射 到 函数                   Map   中要 获取 键 对应 的 值 ， 需要 使用   Map . get   方法 。     -   选项   Option     Option   是 一个 表示 有 可能 包含 值 的 容器 。     Option   本身 是 泛型 的 ， 有 两个 子类     Some [ T ]     或     None   。     在 模式匹配 中 会 用到 。               val       result       =       res1       match       {               case       Some     (     n     )       = & gt ;       n     *     2               case       None       = & gt ;       0       }                 Option 基本 的 接口 是 这样 的 ：               trait       Option     [     T     ]       {           def       isDefined     :       Boolean           def       get     :       T           def       getOrElse     (     t     :       T     )     :       T       }                 Option 本身 是 泛型 的 ， 并且 有 两个 子类 ：   Some [ T ]   或   None 。       Map . get   使用   Option   作为 其 返回值 ， 表示 这个 方法 也许 不会     返回 你 请求 的 值 。     类似 于 Haskell 的   Maybe   ？       函数 组合 子   Functional   Combinators           map   组合 子     例子 ：   List ( 1 , 2 , 3 , 4 )   map   { i : Int   = & gt ;   i * i }   ，     或者 这样 调用                   numbers     .     map     ( (     i     :     Int     )       = & gt ;       i       *       2     )                     foreach ，   很 像 map ， 但是 没有 返回值 。 仅 用于 有 副作用 的 函数 ？                   numbers     .     foreach     ( (     i     :     Int     )       = & gt ;       i       *       2     )                     filter ， 一处 任何 传入 函数 计算结果 为   false   的 元素 。       zip ， 将 两个 列表 的 内容 聚合 到 一个 对偶 列表 中 。                   List     (     1     ,     2     ,     3     ) .     zip     (     List     (     & quot ; a & quot ;     ,     & quot ; b & quot ;     ,     & quot ; c & quot ;     ) )         / / [ ( 1 , a ) , ( 2 , b ) , ( 3 , c ) ]                       partition   ,   使用 给定 的 谓词 函数 （ 返回 true 和 false 的 函数 ） 分割 列表 ， 返回 tuple         find   ， 返回 集合 中 第一个 匹配 谓词 函数 的 元素         drop     和     dropWile   ，   drop   删除 前 i 个 元素 ，   dropWhile   将 删除         元素 直到 不 满足条件 为止 。       foldLeft ，   左 折叠 。 需要 传入 一个 初始值 和 一个二元 函数       foldRight ， 右 折叠       flatten ， 展平 。       flatMap ， 等价 于   flatten   .   map           函数 组合             compose   组合 其它 函数 形成 新 的 函数   f ( g ( x ) )   。                   val       fg       =       f       _       compose       g       _                   println   是 啥 ？ 为 甚 不能 组合 。     -     andThen   ， 与   compose   很 像 ， 只是 执行 顺序 相反 ， 先 执行 第一个 。       偏 函数       不是 部分 应用 函数 ， 篇 函数 是 指 只能 接受 该 类型 的 某些 特定 的 值 。       isDefinedAt   用来 确定 该 函数 能否 接受 一个 给定 的 参数 。               val       one     :       PartialFunction     [     Int   ,     String     ]       =       {       case       1       = & gt ;       & quot ; one & quot ;       }       one     .     isDefinedAt     (     1     )           / /   true       one     .     isDefinedAt     (     2     )           / /   false                 PartialFunctions 可以 使用 orElse 组成 新 的 函数 ， 得到 的 PartialFunction 反映 了 是否 对 给定 参数 进行 了 定义 。               scala     & gt ;       val       two     :       PartialFunction     [     Int   ,     String     ]       =       {       case       2       = & gt ;       & quot ; two & quot ;       }       two     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       three     :       PartialFunction     [     Int   ,     String     ]       =       {       case       3       = & gt ;       & quot ; three & quot ;       }       three     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       wildcard     :       PartialFunction     [     Int   ,     String     ]       =       {       case       _       = & gt ;       & quot ; something   else & quot ;       }       wildcard     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       partial       =       one       orElse       two       orElse       three       orElse       wildcard       partial     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       partial     (     5     )       res24     :       String       =       something       else         scala     & gt ;       partial     (     3     )       res25     :       String       =       three         scala     & gt ;       partial     (     2     )       res26     :       String       =       two         scala     & gt ;       partial     (     1     )       res27     :       String       =       one         scala     & gt ;       partial     (     0     )       res28     :       String       =       something       else                 模式匹配 其实 是 一个 偏 函数 ！ 偏 函数 是 函数 的 子类 ， 所以 所有 在 使用 函数 的 地方 都 可以 使用 偏 函数 ， 即 模式匹配 ！       类型 ， 静态 类型       随着 类型 系统 表达能力 的 提高 ， 我们 可以 生产 更 可靠 的 代码 。     所有 的 类型信息 会 在 编译 时 被 删去 ， 因为 它 已 不再 需要 。 这 就是 所谓 的 擦除 。           参数 化 多态 ， 秩 1 多态性 rank - one 。 下面 是 一个 错误 的 例子 ， 将会报 编译 错误 。                   def       foo     [     A   ,     B     ] (     f     :       A - & gt ; List     [     A     ] ,       b     :       B     )       =       f     (     b     )       def       foo     [     A     ] (     f     :       A - & gt ; List     [     A     ] ,       b     :       Int     )       =       f     (     i     )                     类型 推断     Hindley   Milner 算法 。   Scala 编译器 为 我们 做 类型 推断 ，     使得 可以 不 明确 指定 返回 类型 。                   def       id     [     T     ] (     x       :       T     )       =       x       val       x       =       id     (     & quot ; hey & quot ;     )                     变性   Variance ， 如果 T ' 是 T 的 子类 ， 那么 Container [ T ' ] 和 Container [ T ] 的 关系 呢 ？       协变 ，   C [ T ' ] 也 是 C [ T ] 的 子类 ，   [ + T ]       逆变 ，   C [ T ' ] 是 C [ T ] 的 父类 ，   [ - T ]       不变 ，   没有 关系 ，   [ T ]                   逆变 的 例子 ， 函数 特质 。 参数 用 父类 ， 调用 用 子类 ， 表明 以 父类 为 类型 参数 的 函数     是 以 子类 为 类型 参数 的 函数 的 子类 。 有点 绕 ， 理解 一下 。           边界 ， 指定 泛型 的 大 类型 ？   T   & lt ; :   SomeType     指定 T 是 SomeType 的 子类 。                   scala     & gt ;       def       cacophony     [     T     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )       & lt ;     console     & gt ; :     7     :       error :       value       sound       is       not       a       member       of       type       parameter       T                     def       cacophony     [     T     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )                                                                                                                       ^         scala     & gt ;       def       biophony     [     T       & lt ; :       Animal     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )       biophony     :       [     T       & lt ; :       Animal     ]     (     things :       Seq     [     T     ] )     Seq     [     java . lang . String     ]         scala     & gt ;       biophony     (     Seq     (     new       Chicken     ,       new       Bird     ) )       res5     :       Seq     [     java . lang . String     ]       =       List     (     cluck     ,       call     )                   T   : & gt ;   SomeType     指定 T 是 SomeType 的 超类 。     List   同样   定义 了   : : [ B   & gt ; :   T ] ( x :   B )     来 返回 一个 List [ B ] ， 例如 下面 这个 例子 中 ，     flock 是 Bird 类型 ， Bird 是 Animal 的 子类 。   : :   操作 后 返回 的 是 超类 Animal 的 列表 。               scala     & gt ;       new       Animal       : :       flock       res59     :       List     [     Animal     ]       =       List     (     Animal     @     11     f8d3a8     ,       Bird     @     7     e1ec70e     ,       Bird     @     169     ea8d2     )                     量化   Quantification 。     有时候 ， 不 关心 类型 变量 时 ， 可以 用 通配符 取而代之 ， 注意 区分 变量 和 类型 变量 。     个人 理解 ： 下面 这个 例子 与 类型 无关 ， 只 与 List 的 接口 有关 ， 所以 不 影响 类型 推导 系统 。           可以 为 通配符 指定 边界 。               def       count     [     A     ] (     l     :       List     [     A     ] )       =       l     .     size       def       count     (     l     :       List     [     _     ] )       =       l     .     size         def       hashcodes     (     l     :       Seq     [     _       & lt ; :       AnyRef     ] )       =       l       map       (     _     .     hashCode     )                     View   bounds （ type   classes ） ，     & lt ; %   .     在 隐式 函数 可以 帮助 满足 类型 推断 时 ， 它们 允许 按 需 的 函数 应用 。                   class       Container     [     A       & lt ; %       Int     ]       {       def       addIt     (     x     :       A     )       =       123       +       x       }                         更 多 类型 限制 ， 我 已经 晕 了 ， 不要 问 我 ， 自己 看 教程 ！               关于 类型 ， 还有 一些 内容 ， 看 教程     https : / / twitter . github . io / scala _ school / zh _ cn / advanced - types . html                 断言 assert 和 require ， 通常 用 require 做 参数 检查 ， 而用 assert 做 测试 相关 的 。                       def       assert     (     assertion     :       Boolean     )       {           if       ( !     assertion     )               throw       new       java     .     lang     .     AssertionError     (     & quot ; assertion   failed & quot ;     )       }         def       assume     (     assumption     :       Boolean     )       {           if       ( !     assumption     )               throw       new       java     .     lang     .     AssertionError     (     & quot ; assumption   failed & quot ;     )       }         def       require     (     requirement     :       Boolean     )       {           if       ( !     requirement     )               throw       new       IllegalArgumentException     (     & quot ; requirement   failed & quot ;     )       }                 隐式 转换       隐式 函数               implicit       def       intToString     (     x     :     Int     )       :       x . toString                 隐式 类   2.10 .+       隐式 类 的 主 方法 可以 用于 隐式 类型转换 。               object       Helpers       {           implicit       class       IntWithTimes     (     x     :       Int     )       {               def       times     [     A     ] (     f     :       = & gt ;       A     )     :       Unit       =       {                   def       loop     (     current     :       Int     )     :       Unit       =                       if     (     current       & gt ;       0     )       {                           f                           loop     (     current       -       1     )                       }                   loop     (     x     )               }           }       }         import       Helpers ._       5       times       println     (     & quot ; HI & quot ;     )         HI       HI       HI       HI       HI                 可以 利用   Scala . math   库   Numeric   对 数字 类型 变量 进行 限制       构建 工具   SBT           安装 命令   brew   install   sbt         项目 布局       项目   –   项目 定义 文件       project / build / . scala   –   主 项目 定义 文件       project / build . properties   –   项目 、 sbt 和 Scala 版本 定义       src / main   –   你 的 应用 程序代码 出现 在 这里 ， 在 子目录 表明 代码 的 语言 （ 如 src / main / scala ,   src / main / java ）       src / main / resources   –   你 想要 添加 到 jar 包中 的 静态 文件 （ 如 日志 配置 ）       src / test   –   就 像 src / main ， 不过 是 对 测试       lib _ managed   –   你 的 项目 依赖 的 jar 文件 。 由 sbt   update 时 填充       target   –   生成物 的 目标 路径 （ 如 自动 生成 的 thrift 代码 ， 类 文件 ， jar 包 ）                           字符串   -   核心 数据结构       scala 的 字符串 很多 是 直接 借助于 java 的 String 类 ， 但是 还有 一些 scala 的 特性 需要 说明 一下 。       字符串 插值   String   Interpolation ， Scala   2.10 .+       scala 提供 三种 字符串 插值 方法 ， s ,   f   and   raw 。     -   s 支持 局部变量 和 表达式 。     -   f 表明 对 变量 进行 格式化 ， 类似 于 printf 的 功能 。 类型 安全 ， 如果 不 匹配 ， 将会 报错 。   % s   是 通用 的 ？ ！     -   raw 字符串 就是 不会 对 转义字符 转义 。 相当于 python 里面 的 r               / /   插入 局部变量       val       name       =       & quot ; James & quot ;       println     (     s & quot ; Hello ,       $ name     & quot ;     )         / /   Hello ,   James       / /   插入 表达式       println     (     s & quot ; 1   +   1   =       $ {     1       +       1     }     & quot ;     )         val       height       =       1.9 d       val       name       =       & quot ; James & quot ;       / /   f   使用 类似 于 printf 格式 字符       println     (     f & quot ;     $ name     % s   is       $ height     % 2.2 f   meters   tall & quot ;     )         / /   James   is   1.90   meters   tall         println     (     raw & quot ; a \ \ nb & quot ;     )       / /   Output :   a \ \ nb                 集合   -   核心 数据结构       List           创建 集合                   List     (     1     ,     2     ,     3     ,     4     )       1       : :       2       : :       3       : :       Nil         val       L       =       1       to       1000       toList                   : :   是 将 前面 的 数据 prepend 到 后面 的 列表 中 ， 等同于   + :   。               连接 集合 ，     L1   ++   L2   ，   可以 连接 元素 类型 不同 的 集合 ， 最终 生成 的 集合 类型 是 这 两个         集合 元素 类型 的 超集 。 和   : : :   一样                 z   / :   L     相当于     foldLeft   z   L   ,     ( 0   / :   L ) ( ( a , b ) = & gt ; a + b )   求和             z   : \ \   L     右 折叠             : +     append 操作 ，     L   : +   6                 索引 操作 ：   . apply ( n : Int )     取 下标 n 的 元素 ， 可以 通过   ( )   进行 访问 ， 如   L ( 0 )             内置 的 数学 函数 ，   . max   ,     . min   ,     . sum   ,     . product         内置 的 基本 属性 ，   . length   ,     . size   ,     . head   ,     . last           filter   ,     flatMap   ,     map   ,     withFilter   ,     zip   ,     zipWithIndex   （ 相当于 python 的 enumerater ） 。         其中   flatMap   =   flatten   .   map   ， 因此 穿 进去 的 函数 需要 返回 一个   GenTraversableOnce   ， 比如 返回 一个 列表 。           Set             +     增加 一个 元素 ， 返回 新 的 集合         -     减少 一个 元素         & amp ;     交集     |     并集 （   ++   ）     & amp ; ~     差集   (   - -   )       与 list 一样 的 折叠 、 map 、 reduce 等 集合 相关 操作       索引   apply ( e : A )   ，   ( e : A )   一样           Seq       貌似 与 list 没 啥 区别 ， 需要 再 仔细 看看 。               scala     & gt ;       Seq     (     1     ,       1     ,       2     )       res3     :       Seq     [     Int     ]       =       List     (     1     ,       1     ,       2     )                 请 注意 返回 的 是 一个 列表 。 因为 Seq 是 一个 特质 ； 而 列表 是 序列 的 很 好 实现 。       \" . mkString ( seq ) \"   方法 可以 实现 python 的   join   方法 的 功能 。       Map           创建 MAP                 Map ( &# 39 ; a &# 39 ;   - & gt ;   1 ,   &# 39 ; b &# 39 ;   - & gt ;   2 )               层次结构               traverable ,     foreach     实现 遍历           基本操作         def   head   :   A     返回 第一个 元素         def   tail   :   Traversable [ A ]     除去 第一个 元素 剩下 的 集合                   函数 组合 子             def   map   [ B ]   ( f :   ( A )   = & gt ;   B )   :   CC [ B ]     返回 每个 元素 都 被   f   转化 的 集合         def   foreach [ U ] ( f :   Elem   = & gt ;   U ) :   Unit     在 集合 中 的 每个 元素 上 执行   f   。         def   find   ( p :   ( A )   = & gt ;   Boolean )   :   Option [ A ]     返回 匹配 谓词 函数 的 第一个 元素         def   filter   ( p :   ( A )   = & gt ;   Boolean )   :   Traversable [ A ]     返回 所有 匹配 谓词 函数 的 元素 集合                   划分 ：             def   partition   ( p :   ( A )   = & gt ;   Boolean )   :   ( Traversable [ A ] ,   Traversable [ A ] )     按照 谓词 函数 把 一个 集合 分割 成 两 部分         def   groupBy   [ K ]   ( f :   ( A )   = & gt ;   K )   :   Map [ K ,   Traversable [ A ] ]     按照 Key 函数 将 一个 集合 分为 多个 .     S . groupBy ( x = & gt ; x % 3 )                 转换 ：         def   toArray   :   Array [ A ]           def   toArray   [ B   & gt ; :   A ]   ( implicit   arg0 :   ClassManifest [ B ] )   :   Array [ B ]           def   toBuffer   [ B   & gt ; :   A ]   :   Buffer [ B ]           def   toIndexedSeq   [ B   & gt ; :   A ]   :   IndexedSeq [ B ]           def   toIterable   :   Iterable [ A ]           def   toIterator   :   Iterator [ A ]           def   toList   :   List [ A ]           def   toMap   [ T ,   U ]   ( implicit   ev :   & lt ; : & lt ; [ A ,   ( T ,   U ) ] )   :   Map [ T ,   U ]     例如 转换 命令行 参数 ，   List ( \" A = 3 \" , \" B = 5 \" ) . map ( l   = & gt ;   l . split ( \" = \" ) ) . toMap           def   toSeq   :   Seq [ A ]           def   toSet   [ B   & gt ; :   A ]   :   Set [ B ]           def   toStream   :   Stream [ A ]           def   toString   ( )   :   String           def   toTraversable   :   Traversable [ A ]                 iterable ,     iterator ( )     返回 一个 迭代 器 ， 通常 不会 用 ， 一般 会 用 函数 组合 子 和   for                             def       hasNext     ( )     :       Boolean       def       next     ( )     :       A                     Seq   序列 ， 有 顺序 的 对象 序列       Set   没有 重复 的 对象 集合                   def       contains     (     key     :       A     )     :       Boolean       def       + (     elem     :       A     )     :       Set     [     A     ]       def       - (     elem     :       A     )     :       Set     [     A     ]                     Map   键值 对           常用 子类           HashSet :   implements   immutable   sets   using   a       hash   trie           HashMap :   implements   immutable   maps   using   a   hash   trie       TreeMap   是 SortedMap 子类       Vector   快速 随机 访问             继承     Seq   ,     IndexedSeq   ,     Iterable   ,     Traversable         Range   等 间隔 的 Int 有序 序列 。             继承     traverable   ,     Iterable   ,     Seq   .                   val       r0       =       0       until       10       val       r1       =       0       until       10       by       2       new       Range     (     start     :       Int     ,       end     :       Int     ,       step     :       Int     )                 一些 描述 特性 的 特质             IndexedSeq     快速 随机 访问 元素 和 一个 快速 的 长度 操作         LinearSeq     通过 head 快速访问 第一个 元素 ， 也 有 一个 快速 的 tail 操作 。           可变   vs   不可 变       不可 变       优点       在 多线程 中 不会 改变     缺点       一点 也 不能 改变     Scala 允许 我们 是 务实 的 ， 它 鼓励 不变性 ， 但 不 惩罚 我们 需要 的 可变性 。 这 和 var   vs .   val 非常 相似 。 我们 总是 先 从 val 开始 并 在 必要 时 回退 为 var 。       我们 赞成 使用 不可 改变 的 版本 的 集合 ， 但 如果 性能 使然 ， 也 可以 切换 到 可变 的 。 使用 不可 变 集合 意味着 你 在 多线程 不会 意外 地 改变 事物 。       可变 集合           ListBuffer 和 ArrayBuffer       LinkedList   and   DoubleLinkedList       PriorityQueue       Stack   和   ArrayStack       StringBuilder   有趣 的 是 ， StringBuilder 的 是 一个 集合           与 Java 转换       您 可以 通过 JavaConverters   package 轻松 地 在 Java 和 Scala 的 集合 类型 之间 转换 。 它用 asScala   装饰 常用 的 Java 集合 以和用 asJava   方法 装饰 Scala 集合 。                     import       scala . collection . JavaConverters ._             val       sl       =       new       scala     .     collection     .     mutable     .     ListBuffer     [     Int     ]             val       jl       :       java . util . List     [     Int     ]       =       sl     .     asJava             val       sl2       :       scala . collection . mutable . Buffer     [     Int     ]       =       jl     .     asScala             assert     (     sl       eq       sl2     )                 双向 转换 ：               scala     .     collection     .     Iterable       & lt ; = & gt ;       java     .     lang     .     Iterable       scala     .     collection     .     Iterable       & lt ; = & gt ;       java     .     util     .     Collection       scala     .     collection     .     Iterator       & lt ; = & gt ;       java     .     util     . {       Iterator     ,       Enumeration       }       scala     .     collection     .     mutable     .     Buffer       & lt ; = & gt ;       java     .     util     .     List       scala     .     collection     .     mutable     .     Set       & lt ; = & gt ;       java     .     util     .     Set       scala     .     collection     .     mutable     .     Map       & lt ; = & gt ;       java     .     util     . {       Map     ,       Dictionary       }       scala     .     collection     .     mutable     .     ConcurrentMap       & lt ; = & gt ;       java     .     util     .     concurrent     .     ConcurrentMap                 此外 ， 也 提供 了 以下 单向 转换               scala     .     collection     .     Seq       = & gt ;       java     .     util     .     List       scala     .     collection     .     mutable     .     Seq       = & gt ;       java     .     util     .     List       scala     .     collection     .     Set       = & gt ;       java     .     util     .     Set       scala     .     collection     .     Map       = & gt ;       java     .     util     .     Map                 使用 specs 进行 测试       貌似 现在 包为 specs2 ， 导入 单元测试 规范   org . specs2 . mutable . Specification                 org     .     specs2     .     mutable     .     _         object       ArithmeticSpec       extends       Specification       {           & quot ; Arithmetic & quot ;       should       {               & quot ; add   two   numbers & quot ;       in       {                   1       +       1       mustEqual       2               }               & quot ; add   three   numbers & quot ;       in       {                   1       +       1       +       1       mustEqual       3               }           }       }                 并发 编程 *           Runnable / Callable 定义 如下 ， 区别 在于 Runnable 没有 返回值 ， 而 Callable 有 。                   trait       Runnable       {           def       run     ( )     :       Unit       }         trait       Callable     [     V     ]       {           def       call     ( )     :       V       }                     线程 ， Scala 并发 是 建立 在 Java 并发 模型 基础 上 的 。 在 Sun   JVM 上 ， 对 IO 密集 的 任务 ， 我们 可以 在 一台 机器运行 成千上万 个 线程 。         一个 线程 需要 一个   Runnable   。 你 必须 调用 线程 的     start     方法 来 运行 Runnable 。                   val       hello       =       new       Thread     (     new       Runnable       {           def       run     ( )       {               println     (     & quot ; hello   world & quot ;     )           }       } )         hello     .     start     ( )                 创建 一个 自己 的 线程 步骤 ， 首先 创建 一个 实现 Runnable 接口 的 类 ， 然后 将 该类 的 实例 作为 参数 传给   new   Thread ( )   即可 ，     然后 再 调用 创建 的 线程 的   . start ( )   方法 就 可以 运行 了 。       也 可以 利用 java 的 线程 的 执行 服务 构建 一个 线程 池 。   java . util . concurrent . { Executors ,   ExecutorService }                 val       pool     :       ExecutorService       =       Executors     .     newFixedThreadPool     (     poolSize     )       pool     .     execute     (     new       MyRunableClass     ( ) )                     Futures .   Future   代表 异步 计算 。 你 可以 把 你 的 计算 包装 在 Future 中 ， 当 你 需要 计算结果 的 时候 ， 你 只 需 调用 一个 阻塞 的   get ( )   方法 就 可以 了 。 一个   Executor   返回 一个   Future   。           线程 安全 的 三种 工具 。           mutex   互斥 锁 。       volatile       AtomicReference                   / /   synchronized       class       Person     (     var       name     :       String     )       {           def       set     (     changedName     :       String     )       {               this     .     synchronized       {                   name       =       changedName               }           }       }         / /   volatile       class       Person     (     @ volatile       var       name     :       String     )       {           def       set     (     changedName     :       String     )       {               name       =       changedName           }       }         / /   AtomicReference       import       java . util . concurrent . atomic . AtomicReference         class       Person     (     val       name     :       AtomicReference     [     String     ] )       {           def       set     (     changedName     :       String     )       {               name     .     set     (     changedName     )           }       }                 略       Java 跨平台 交互 ： 在 Java 中 使用 Scala       扩展 规格       泛型 编程           使用   ClassTag   ， 用 类型 作为 参数     Spark   rdd . objectFile   源码             从 classTag 创建对象 的 方法 还 没 搞清楚 ， 参看   代码   ！ ：               import       scala . reflect .     {     classTag     ,       ClassTag     }         def       objectFile     [     T :       ClassTag     ] (                   path     :       String     ,                   minPartitions     :       Int       =       defaultMinPartitions     )     :       RDD     [     T     ]       =       withScope       {               assertNotStopped     ( )               sequenceFile     (     path     ,       classOf     [     NullWritable     ] ,       classOf     [     BytesWritable     ] ,       minPartitions     )                   .     flatMap     (     x       = & gt ;       Utils     .     deserialize     [     Array     [     T     ] ] (     x     .     _ 2     .     getBytes     ,       Utils     .     getContextOrSparkClassLoader     ) )           }           private       implicit       def       arrayToArrayWritable     [     T       & lt ; %       Writable :       ClassTag     ] (     arr     :       Traversable     [     T     ] )           :       ArrayWritable       =       {           def       anyToWritable     [     U       & lt ; %       Writable     ] (     u     :       U     )     :       Writable       =       u             new       ArrayWritable     (     classTag     [     T     ] .     runtimeClass     .     asInstanceOf     [     Class     [     Writable     ] ] ,                   arr     .     map     (     x       = & gt ;       anyToWritable     (     x     ) ) .     toArray     )       }                       . asInstanceOf [ T ]     进行 类型转换 。           Test       包     http : / / www . scalatest . org /         build . sbt   引入 测试 包               libraryDependencies       + =       & quot ; org . scalatest & quot ;       %       & quot ; scalatest _ 2.10 & quot ;       %       & quot ; 2.2 . 6 & quot ;       %       & quot ; test & quot ;                 FlatSpec       \" X   should   Y , \"   \" A   must   B , \"           assert       assertResult       assertThrows           Achieving   success       FunSuit               import       org . scalatest . FunSuite         class       SetSuite       extends       FunSuite       {             test     (     & quot ; An   empty   Set   should   have   size   0 & quot ;     )       {               assert     (     Set     .     empty     .     size       = =       0     )           }             test     (     & quot ; Invoking   head   on   an   empty   Set   should   produce   NoSuchElementException & quot ;     )       {               assertThrows     [     NoSuchElementException     ]       {                   Set     .     empty     .     head               }           }       }                 现在 用 的 是 3.0 ，   intellij   好像 支持 得 不好 。 最好 还是 用   2.2 . 6 吧 ！  ", "tags": "tools", "url": "/wiki/tools/scala.html"},
      
      
      {"title": "scala语言底层实现所设计的数据结构", "text": "    Table   of   Contents           关于           Hash   Tries           hashtable                 关于       闲得无聊 ， 学习 一下 。       Hash   Tries       scala   里面 不可 变 的 的 HashMap   和   HashSet   使用   HashTries 实现       hashtable       可变 的 HashMap 和 HashSet 使用 hashtable 实现 。  ", "tags": "tools", "url": "/wiki/tools/scala-data-implement.html"},
      
      
      {"title": "Spark", "text": "    Table   of   Contents           安装           启动 主机 和 worker           Spark   shell                   SparkContext           RDD           RDD   操作           RDD 持久 化           理解 闭包           KV 值 操作           通用 的 变换           Action                   共享 变量           提交 spark 任务           Spark   Streaming           Spark   SQLContext ，           DataFrame                   MLlib           spark . ml   包           基础 类           特征提取           特征 变换           特征选择           分类   org . apache . spark . ml . classification           回归   org . apache . spark . ml . regression           聚类   org . apache . spark . ml . clustering           协同 过滤           DataFrame                   spark . mLlib           基本 数据结构           模型 评估                   TIPS           使用 log4j                         安装       从 Spark 官网 下载安装 包 ， 然后 解压 即可 。 非常简单       启动 主机 和 worker       进入 spark 目录 ， 然后 运行 脚本             . / sbin / start - master . sh               即可 。 进程 会 在 后台 运行 ， 你 可以 通过     http : / / localhost : 8080     进行 监控 。       启动 worker 的 脚本 是             . / bin / spark - class   org . apache . spark . deploy . worker . Worker   spark : / / IsP : PORT               其中 IP 和 PORT 可以 在 监控 页面 看到 。       关闭 worker 很 简单 ， 直接 关闭 worker 运行 的 shell 或者 ctr   +   c 中断 即可 。     关闭 主机 需要 运行 脚本             . / sbin / stop - master . sh               Spark   shell       启动 scala 版 的 shell 命令 为   . / bin / spark - shell   ， python 版 的 命令 为   . / bin / pyspark         SparkContext       sc 是 spark 的 入口 ， 通过   SparkConf   来 创建 它 。               val       sparkConf       =       new       SparkConf     ( ) .     setAppName     (     & quot ; FromPostgreSql & quot ;     )           .     setMaster     (     & quot ; local [ 4 ] & quot ;     )           .     set     (     & quot ; spark . executor . memory & quot ;     ,       & quot ; 2g & quot ;     )       val       sc       =       new       SparkCsontext     (     sparkConf     )                 对 了 ， 目前 spark 只 支持 的 scala 版本 是 2.10 . x ， 所以 用 2.11 . x 版本 可能 会 出错 。       使用   sc . stop ( )   方法 停止 SparkContext 。 貌似 不 执行 stop ， 本地 用   sbt   run   运行 时会 出现 错误信息 ，     但是 提交 jar 方式 运行 没 问题 。     参考   https : / / stackoverflow . com / questions / 28362341 / error - utils - uncaught - exception - in - thread - sparklistenerbus   .           issue       使用   sbt   run   方式 运行 任务 ， 如果 涉及 到   saveAsTextFile   操作 时 ， 会 出错 ， 原因 未知 。                   RDD           RDD ， 全 称为 Resilient   Distributed   Datasets ， 是 一个 容错 的 、 并行 的 数据结构 ， 可以 让 用户 显式 地 将 数据 存储 到 磁盘 和 内存 中 ， 并 能 控制数据 的 分区 。       in - memory   cache .     cache ( )         RDD   常用 操作         count ( )           foreach   ,     map   ,     flatMap   ,     filter   ,               并行 化 容器 ， 可以 通过   SparkContext . parallelize     方法 创建 分布式 便于 并行计算 的 数据结构 。 也 可以 用来 将 scala 的 容器 转换 为 RDD 结构 的 tips                   val       data       =       Array     (     1     ,     2     ,     4     ,     5     ,     6     ,     7     )       val       distData       =       sc     .     parallelize     (     data     )                     从 外部 数据库 创建 ， 支持 本地 文件系统 ， HDFS ， Cassandra ，   HBase ，   Amazon   S3 ，   等 。         支持 的 文件格式 包括 文本文件 ，   SequenceFiles ， 其他 Hadoop 输入 格式 。         其中 文本格式 可以 通过   SparkContext . textFile ( URI   [ ,   partition _ number ] )   方法 创建 RDD 。       支持 本地 文件 和 网络 文件 的 URI ， \" / home / user / path - to - file \" ,   \" hdfs : / / path - to - file \"       支持 文件夹 ， 压缩文件 ， 通配符 等 方式 。 例如 \" / path - to - file / * . gz \" ,   \" / path - to - file / directory \"       指定 分区 数目 ， 每 一个 分区 是 64MB ， 默认 创建 一个 分区 。       也 可以 通过     SparkContext . wholeTextFiles     读取 一个 目录 下 的 所有 文本文件 ， 返回 的 是   ( filename ,   content ) ，         而   textFile     则 返回 所有 的 行       其他 Hadoop 输入 格式 可以 使用     SparkContext . hadoopRDD     方法 。       其他 基于     org . apache . hadoop . mapreduce     API   的 输入 格式 可以 通过       SparkContext . newAPIHadoopRDD     方法 创建         RDD . saveAsObjectFile     和     SparkContext . objectFile     支持 保存 RDD 为 简单 的 序列化 java 对象 。                   RDD   操作           支持 两种 操作   map ，   reduce       变换 ： 从 一个 已经 存在 的 数据 创建 新 的 数据 ， 如     map   ,     reduce   ,     reduceByKey   。 所有 的 变换 操作 都 是 惰性 求值 ， 而且 不 保存         中间 结果 。 如果 重新 计算 ， 中间 结果 也 会 重新 计算 。 如果 需要 保存 中间 结果 可以 通过   RDD . persist ( )   方法 指明 保存 该 RDD 。       传递函数 给 spark ， 不同 的 语言 不同       scala 中 可以 通过 以下 几种 方式       匿名 函数       单例 模式 对象 的 一个 静态方法       一个 类 的 实例 对象 的 一个 成员 方法 ， 这种 情况 需要 传递 整个 对象 过去 。 同样 ， 如果 函数 应用 了 外部 的 对象 的 一个 域 ， 那么 也 需要 传递 整个 对象 。         为了 避免 这个 问题 ， 可以 创建 该域 的 一个 本地 拷贝 。                                   class       MyClass       {           val       field       =       & quot ; Hello & quot ;           def       doStuff     (     rdd     :       RDD     [     String     ] )     :       RDD     [     String     ]       =       {       rdd     .     map     (     x       = & gt ;       field       +       x     )       }       }         / /   修改 后 的 doStuff   函数       def       doStuff     (     rdd     :       RDD     [     String     ] )     :       RDD     [     String     ]       =       {           val       field _       =       this     .     field           rdd     .     map     (     x       = & gt ;       field _       +       x     )       }                       -   java ,     ` org . apache . spark . api . java . function `   对象 ， 或者 java   8   的 lambda 表达式   -   python ，   lambda 表达式 ， 本地 函数 ， 模块 的 顶级 函数 ， 对象 的 方法                   重新 分区 ，   repartition   会 重新分配 所有 数据 ， 如果 是 降低 分区 数目 ， 可以 用   coalesce   ， 它会 避免 移动 所有 数据 ，         而 只是 移动 丢弃 的 分区 的 数据 ， 参考   stackoverflow 的 讨论   。           RDD 持久 化       持久 化 的 两个 方法     . cache ( )   和   . persist ( StorageLevel . SOME _ LEVEL )   ， 存储 级别 有 ：           MEMORY _ ONLY   ：   默认 级别 ， 以   deserialized   Java   objects   保存 在 内存 （ JVM ） ， 内存 放不下 的 部分 每次 也 是 重新 计算       MEMORY _ AND _ DISK   ：   保存 在 内存 ， 放不下 的 放在 磁盘       MEMORY _ ONLY _ SER   ：   序列化 后 再 保存 在 内存 ， 放不下 重新 计算       MEMORY _ AND _ DISK _ SER   ： 与 上 一个 术语 差异 在于 放不下 的 放 磁盘       DISK _ ONLY   ：   只放 磁盘       MEMORY _ ONLY _ 2 ,   MEMORY _ AND _ DISK _ 2 ,   etc .   ：   多 保存 一个 备份       OFF _ HEAP   ( experimental )   ：   Store   RDD   in   serialized   format   in   Tachyon           在 python 中 都 是 用 pickle 序列化 ， 只有 这 一种 。     手动 移除 cache 的 方法 是     RDD . unpersist ( )   ， 如果 不 手动 移除 ， Spark   也 会 自动 处理 cache 的 。       理解 闭包           在 RDD 的 foreach 中 ， 对外部 变量 的 引用 实际上 是 复制 了 该 对象 到 executor 中 ， 然后 引用 executor 中 的 那个 对 像 ， 所以 不会 改变 本想 引用 的 那个 对象 。         可以 使用   Accumulator   来 实现 改变 主 对象 。       输出 RDD 到 stdout ， 同样 存在 一个 问题 ， 在 foreach 和 map 中 的 prinln 是 输出 到 executor 的 stdout 。 可以 通过   RDD . collect ( ) . foreach ( println )   方法 实现 ，         如果 该 只是 打印 一部分 ， 可以 通过   RDD . take ( 100 ) . foreach ( println )     来 实现 。           KV 值 操作           由于 KV 类型 可以 是 很多 不同 类型 ， 通用 的 操作 不 多 ， 最 常用 的 是     shuffle     操作 ， 例如   grouping   和   aggregating   by   key 。       在 spark 中 通过 创建 Tuple2 对象 实现 K - V ， 例如 在 下述 代码 中                   val       lines       =       sc     .     textFile     (     & quot ; data . txt & quot ;     )       val       pairs       =       lines     .     map     (     s       = & gt ;       (     s     ,       1     ) )       val       counts       =       pairs     .     reduceByKey     ( (     a     ,       b     )       = & gt ;       a       +       b     )                 注意 ， 在 使用 自定义 的 对象 作为 key 的 时候 ， 需要 确保   . equals ( )   方法 与   hashCode ( )   方法 兼容 。       通用 的 变换           map ( func )       filter ( func )       flatMap ( func ) ,   相当于 先 做 map ， 然后 做 flat 操作       mapPartitions ( func ) ， map 到 每 一个 分区       mapPartitionsWithIndex ( func ) ，   带有 index 的 版本       sample ，   采样       union ， 并集       intersection ， 交集       distinc ,   去 重       groupByKey ， 输入 ( K , V ) ， 输出 ( K ,   Iter   )       reduceByKey ( func ) ， 输入 ( K , V )       aggregateByKey       sortByKey         join ( otherDataset   [ ,   numTasks ] )   ,     ( K , V ) ,   ( K , W )   - & gt ;   ( K ,   ( V , W ) )           cogroup       cartesian   笛卡尔 积 ？       pipe       coalesce       repartition     略           Action           reduce       collect       count       first       take ( n )       takeSample       takeOrdered       saveAsTextFile ( path )       saveAsSequenceFile ( path ) ,   java   and   scala       countByKey ， 对 每 一个 key 单独 计数       foreach ( func )           共享 变量           broadcast 变量 ， 不同 的 executor 共享                   val       broadcastVar       =       sc     .     broadcast     (     Array     (     1     ,       2     ,       3     ) )       broadcastVar     .     value                 优点 在于 ， 不同于 简单 复制 ， 可以 采用 P2P 协议 来 提升 在 多个 节点 之间 复制 的 性能 ！ 对于 很大 的 共享 对象 ， 性能 提升 很 明显 ！       https : / / stackoverflow . com / questions / 26884871 / advantage - of - broadcast - variables             Accumulator ,                   val       accum       =       sc     .     accumulator     (     0     ,       & quot ; My   Accumulator & quot ;     )       sc     .     parallelize     (     Array     (     1     ,       2     ,       3     ,       4     ) ) .     foreach     (     x       = & gt ;       accum       + =       x     )       accum     .     value                 一般 需要 实现 自己 的 AccumulatorParam 子类 ，               object       VectorAccumulatorParam       extends       AccumulatorParam     [     Vector     ]       {           def       zero     (     initialValue     :       Vector     )     :       Vector       =       {               Vector     .     zeros     (     initialValue     .     size     )           }           def       addInPlace     (     v1     :       Vector     ,       v2     :       Vector     )     :       Vector       =       {               v1       + =       v2           }       }         / /   Then ,   create   an   Accumulator   of   this   type :       val       vecAccum       =       sc     .     accumulator     (     new       Vector     ( ... ) ) (     VectorAccumulatorParam     )                 提交 spark 任务       使用   bin / spark - submit   脚本 提交 ， 语法             . / bin / spark - submit     \ \         - - class   & lt ; main - class & gt ;     \ \         - - master   & lt ; master - url & gt ;     \ \         - - deploy - mode   & lt ; deploy - mode & gt ;     \ \         - - conf   & lt ; key & gt ;   =   & lt ; value & gt ;     \ \         ...     #   other   options         & lt ; application - jar & gt ;     \ \           [   application - arguments   ]                     For   Python   applications ,   simply   pass   a   . py   file   in   the   place   of       instead   of   a   JAR ,   and   add   Python   . zip ,   . egg   or   . py   files   to   the   search   path   with   - - py - files .           Spark   Streaming       简单 地说 ， 就是 用来 从 其他 地方 拉 数据 的 。     输入 数据流   = & gt ;   Spark   streaming   = & gt ;   batches   of   input   data   = & gt ;   Spark   engine   = & gt ;   batches   of   processed   data       Spark   SQLContext ，           从 SparkContext 创建                   org     .     apache     .     spark     .     sql     .     SQLContext       val       sc     :       SparkContext       / /   An   existing   SparkContext .       val       sqlContext       =       new       org     .     apache     .     spark     .     sql     .     SQLContext     (     sc     )                     使用   . sql   函数 进行 SQL 查询 ， Spark   SQL 支持 的 语法                   SELECT       [     DISTINCT     ]       [     column       names     ]     |     [     wildcard     ]       FROM       [     kesypace       name     . ]     table       name       [     JOIN       clause       table       name       ON       join       condition     ]       [     WHERE       condition     ]       [     GROUP       BY       column       name     ]       [     HAVING       conditions     ]       [     ORDER       BY       column       names       [     ASC       |       DSC     ] ]                 如果 使用 join 进行 查询 ， 则 支持 的 语法 为 ：               SELECT       statement       FROM       statement       [     JOIN       |       INNER       JOIN       |       LEFT       JOIN       |       LEFT       SEMI       JOIN       |       LEFT       OUTER       JOIN       |       RIGHT       JOIN       |       RIGHT       OUTER       JOIN       |       FULL       JOIN       |       FULL       OUTER       JOIN     ]       ON       join       condition                 -       DataFrame       Spark   DataFrame 的 设计 灵感 正是 基于 R 与 Pandas 。     我们 通过 外部 Json 文件创建 一个 DataFrame ：               val       dataFrame       =       sqlContext     .     load     (     & quot ; / example / data . json & quot ;     ,       & quot ; json & quot ;     )       dataFrame     .     show     ( )                     With   a   SQLContext ,   applications   can   create   DataFrames   from   an   existing   RDD ,   from   a   Hive   table ,   or   from   data   sources .                   / /   Create   the   DataFrame       val       df       =       sqlContext     .     read     .     json     (     & quot ; examples / src / main / resources / people . json & quot ;     )         / /   Show   the   content   of   the   DataFrame       df     .     show     ( )       / /   age     name       / /   null   Michael       / /   30       Andy       / /   19       Justin         / /   Print   the   schema   in   a   tree   format       df     .     printSchema     ( )       / /   root       / /   | - -   age :   long   ( nullable   =   true )       / /   | - -   name :   string   ( nullable   =   true )         / /   Select   only   the   & quot ; name & quot ;   column       df     .     select     (     & quot ; name & quot ;     ) .     show     ( )       / /   name       / /   Michael       / /   Andy       / /   Justin         / /   Select   everybody ,   but   increment   the   age   by   1       df     .     select     (     df     (     & quot ; name & quot ;     ) ,       df     (     & quot ; age & quot ;     )       +       1     ) .     show     ( )       / /   name         ( age   +   1 )       / /   Michael   null       / /   Andy         31       / /   Justin     20         / /   Select   people   older   than   21       df     .     filter     (     df     (     & quot ; age & quot ;     )       & gt ;       21     ) .     show     ( )       / /   age   name       / /   30     Andy         / /   Count   people   by   age       df     .     groupBy     (     & quot ; age & quot ;     ) .     count     ( ) .     show     ( )       / /   age     count       / /   null   1       / /   19       1       / /   30       1                     直接 在 文件 上 运行 SQL ！                   val       df       =       sqlContext     .     sql     (     & quot ; SELECT   *   FROM   parquet . ` examples / src / main / resources / users . parquet ` & quot ;     )                     注册 UDF                   sqlContext     .     udf     .     register     (     & quot ; strLen & quot ;     ,       (     s     :       String     )       = & gt ;       s     .     length     ( ) )                 MLlib           不同 的 包 的 特点 ， 推荐   spark . ml           spark . mllib     contains   the   original   API   built   on   top   of   RDDs .   在 2.0 版本 不 在 支持 新 特性 了 ， 不再 维护 。         spark . ml     provides   higher - level   API   built   on   top   of     DataFrames     for   constructing   ML   pipelines .                   spark . ml   包       基础 类           基于 DataFrame ， 借助于 抽象 ， 将 模型 抽象 为 三个 基本 类 ， estimators （ 实现 fit 方法 ） ,   transformers （ 实现 transform 方法 ） ,   pipelines       一个 正常 的 模型 应该 同时 实现     fit     和     transform     两个 方法         transform     将 生成 一个 新 的 DataFrame ， 包含 了 预测 的 结果         fit     的 DataFrame 需要 包含 两列   featuresCol   和   labelCol   默认 名字 为   label             transform     之前 的 DataFrame 需要 包含 一列   featuresCol ， 默认 名字 为 features ， 输出 三列 （ 依赖于 参数 ） ， 三列 有 默认 名字 ， 都 可以 通过 setter 函数 进行 设置 。           predictedCol   预测 的 标签 ， 默认 名字 为     prediction         rawPredictedCol   预测 的 裸 数据 ？ 向量 ？ 逻辑 回归 是   wx   貌似 ， 默认 名字 为     rawPrediction         probabilityCol   预测 的 概率 ， 默认 名字 为     probability                     模型 参数 封装 类     Param   ， 他 的 一个 常用 子类 是     ParamMap   ， 实现 了 Map 接口 ， 可以 通过     get ,   put   进行 操作 。     在 2.0 版本 开始 ， Spark 对 Estimators 和 Transformers 提供 统一 的 参数 API 。                       val       paramMap       =       ParamMap     (     lr     .     maxIter       - & gt ;       20     )           .     put     (     lr     .     maxIter     ,       30     )       / /   Specify   1   Param .     This   overwrites   the   original   maxIter .           .     put     (     lr     .     regParam       - & gt ;       0.1     ,       lr     .     threshold       - & gt ;       0.55     )       / /   Specify   multiple   Params .                         paramMap       =       {     lr     .     maxIter     :       20     }       paramMap     [     lr     .     maxIter     ]       =       30       #   Specify   1   Param ,   overwriting   the   original   maxIter .       paramMap     .     update     ( {     lr     .     regParam     :       0.1     ,       lr     .     threshold     :       0.55     } )       #   Specify   multiple   Params .                       pipeline     将 不同 模型 （ transform ） 堆叠 起来 ， 类似 于 sklearn 里面 的 pipeline 。     pipeline 保存 了 一个 Array [ PipelineStage ] ， 可以 通过   . setStage ( Array [ _   & lt ; :   PipelineStage ] )   函数 进行 设置 。     pipeline 实现 了 estimator 的 fit 接口 和 transformer 的 transform 接口 。       模型 持久 化     save ,   load           PipelineStage   抽象类 ， 啥 也 没干 ？ ？ ？ ？ ？ ？ ？ ？ ！ ！ ！ ！ ！   transformer   还是 它 的 子类 ！ ！             UnaryTransformer     单列 转换 对象 ， 是 transformer 的 子 抽象类 ， 也 实现 了 pipelinestage 接口 。         有 两个 变量   inputCol   和   outputCol   代表 输入输出 列 的 名字 。         有 几个 常用 的 实例 ， 例如 Tokenizer ， HashingTF 等 。               模型 的 保存 和 加载 ， 利用 类 的 静态方法   . load   加载 ( MLReader 的 实现 ) ， 而用 实例 的   . save   方法 （ MLWriter 的 实现 ） 保存 模型 到 文件 。               模型 评估     Evaluator   ( 实现   evaluate ( dataFrame )   方法 ) ，     RegressionEvaluator   回归 ，     BinaryClassificationEvaluator   二元 分类 ，       MulticlassClassificationEvaluator     多元 分类 。             BinaryClassificationEvaluator     除了   evaluate   方法 之外 ， 还有 几个 重要 的 属性 和 属性 setter 。 标签 列名   labelCol   ， 度量 名称     metricName     默认 为 areaUnderROC ， 即 AUC 。   rawPredictionCol     预测 结果 列名 。 以及 相应 的 setter 和 getter 。         MulticlassClassificationEvaluator   ， 三个 属性     labelCol   ，   metricName     （ supports   \" f1 \"   ( default ) ,   \" precision \" ,   \" recall \" ,   \" weightedPrecision \" ,   \" weightedRecall \" ） ，   predictionCol           RegressionEvaluator   ， 三个 属性       labelCol   ，   metricName     （ \" rmse \"   ( default ) :   root   mean   squared   error ，   \" mse \" :   mean   squared   error ，   \" r2 \" :   R2   metric ，   \" mae \" :   mean   absolute   error ） ，   predictionCol                     交叉 验证 选择 模型 超 参数 。 交叉 验证     CrossValidator     类 ， 有 4 个 基本 方法             . setEstimator           . setEvaluator           . setEstimatorParamMaps ( paramGrid )     参数 网络         . setNumFolds ( k )     k - fold 交叉 验证 的 参数 k     同是 他 也 是 一个 estimator ， 调用 它 的   fit   方法 训练 模型 ， 返回 训练 好 的 模型 CrossValidatorModel 或 模型 序列 。     他 也 是 一个 transformer ， 调用   transform   方法 直接 执行 多个 transform 。                   训练 集 和 测试 集 的 分割     TrainValidationSplit   与 交叉 验证 类 类似 ， 取代   . setNumFolds   的 是 函数   . setTrainRatio ( ratio )   。               参数 网格 可以 通过     ParamGridBuilder   对象 创建 ， 他 有 三个 方法 ，   addGrid ( param ,   values : Array )   添加 一个 参数 网格 ，       baseOn ( paramPair )   设置 指定 参数 为 固定值 ，   build ( )   方法 返回 一个   Array [ ParamMap ]   数组               特征提取           TF - IDF ( HashingTF   and   IDF ) ， 传统 的 词 统计 是 通过 维护 一个 查找 的 词典 （ hash 表 或者 查找 树 实现 ） ，     HashTF 则 是 直接 通过 对 特征 计算 hash 函数 映射 到 低维 索引 。 还 可以 通过 第二个 hash 函数 确定 是否 存在 冲突 。     有 什么 优势 ？ ？ ？ 使用 的 类   HashingTF ,   IDF ,   Tokenizer         Word2Vec ， 低维词 向量 学习 ， 对应 的 类 ：   Word2Vec         CountVectorizer ， 直接 统计 ：   CountVectorizer             特征 变换           Tokenizer ： 将 文本 转换 为 一个 一个 的 词 。 例如 中文 分词 就算 一个 ， 对于 英文 可以 简单 的 用 空白 字符 分割 就行 。 可用 的 类 有 ：         Tokenizer     常规 Tokenizer         RegexTokenizer     正则 式 Tokenizer               StopWordsRemover ： 停止 词 的 移除 。   StopWordsRemover   ， 可以 通过   setCaseSensitive   设置 大小写 敏感 ，     和   setStopWords ( value :   Array [ String ] )   设置 停止 词 词典 。       n - gram ： 将 输入 的 一串 词 转换 为 n - gram 。   NGram         Binarizer ： 通过 阈值 将 数值 特征 变成 二值 特征 。 类   Binarizer   ， 主要 方法 ：   setThreshold         PCA ： PCA   降维 。   PCA   ， 方法   setK         PolynomialExpansion ： 将 特征 展开 为 多项式 特征 ， 实现 特征 交叉 。   x1 , x2 - & gt ; x1 ^ 2 , x2 ^ 2 , x1x2   。   PolynomialExpansion   方法 :   setDegree         DCT ： 离散 余弦 变换 。   DCT           StringIndexer     将 字符串 类型 的 变量 （ 或者 label ） 转换 为 索引 序号 ， 序号 会 按照 频率 排序 ， 不是 字典 序 。 对于 不 在 词典 的 string ， 默认 抛出 异常 ， 也 可以 通过   setHandleInvalid ( \" skip \" )   直接 丢弃 。         IndexToString     和   StringIndexer   配合 使用 可以 让 字符串 类型 的 变量 的 处理 变得 透明 ， 这个 是 将 index 变成 原来 的 字符串         OneHotEncoder   ： 将 单个 数字 转换 为 0 - 1 编码 的 向量 。   1 - & gt ; ( 0 , 1 , 0 , 0 )   。 常用 在 类别 特征 的 变换 。         VectorIndexer   ： 将 输入 向量 中 的 类别 特征 自动编码 为 index 。 比较 高端 ， 需要 学习 一下 ！   setMaxCategories ( 4 )   表示 特征 的 值 数目 超过 4 个 就 认为 是 连续 特征 ， 否则 认为 需要 编码 。         Normalizer   ： 归一化 。 需要 指定 p - norm 的 值   setP   。 按照 p 范数 归一化 ， 默认 为 2 。 可以 用 在 输出 概率 或 score 时 归一化 ？         StandardScaler   ： 标准化 特征 到 方差 为 1 ， 也 可以 将 均值 设为 0 .   方法 ：   setWithStd ( bool ) ,   setWithMean ( bool )           MinMaxScaler   ： 归一化 到 0 - 1 之间 。 也 可以 指定 min 和 max         MaxAbsScaler   ： @ since ( 2.0 . 0 ) ， 除以 最大值 的 绝对值 ， 从而 将 特征 归一化 到 [ - 1 , 1 ]         Bucketizer   ： 分桶 。 方法   setSplits ( splits )   来 设置 分割 点 ， 分割 点 需要 严格 递增 。         ElementwiseProduct   ： 对 输入 向量 乘以 一个 权值 。 方法   setScalingVec   设置 权值 。         SQLTransformer   ： 让 你 用 SQL 语句 进行 变换 特征 。 例子 ：                   val       sqlTrans       =       new       SQLTransformer     ( ) .     setStatement     (           & quot ; SELECT   * ,   ( v1   +   v2 )   AS   v3 ,   ( v1   *   v2 )   AS   v4   FROM   __ THIS __& quot ;     )                       VectorAssembler   ： 将 多个 特征 合并 到 一个 向量 ， 也 可以 合并 向量 。 通过   setInputCols ( Array [ String ]   设置 要 合并 的 列 。         QuantileDiscretizer   ： 首先 对 特征 采样 ， 然后 根据 采样 值 将 特征 按照 等量 分桶 （ 等频 离散 化 ） 。 基于 采样 ， 所以 每次 不同 。 方法   setNumBuckets ( i : Int )   设置 桶 的 个数 。           特征选择             VectorSlicer   ： 通过 slice 选择 特征 ， 人工 指定 indices 。 方法   setIndices   设置 选择 的 indices 。 字符串 indices 通过   setNames   方法 设置 索引 。         RFormula   ： 通过 R 模型 公式 选择 特征 ， 例如   clicked   ~   country   +   hour   ， 输出 列是 默认 是 公式 的 响应 变量 名字 。 它会 对 字符串 one - hot 编码 ， 对 数值 列 转换 为 double 类型 。 需要 人工 指定 哪些 特征 。         ChiSqSelector   ： 通过 卡方 独立性 检验 来 选择 特征 。 方法   setNumTopFeatures   指定 要 选择 的 卡方值 前 多少 个 。                   val       labelIndexer       =       new       StringIndexer     ( )               .     setInputCol     (     & quot ; label & quot ;     )               .     setOutputCol     (     & quot ; indexedLabel & quot ;     )               .     fit     (     training     )       val       rf       =       new       RandomForestClassifier     ( )               .     setPredictionCol     (     & quot ; indexedPrediction & quot ;     )               .     setLabelCol     (     & quot ; indexedLabel & quot ;     )       setRFParam     (     rf     ,       param     )       val       labelConverter       =       new       IndexToString     ( )               .     setInputCol     (     & quot ; indexedPrediction & quot ;     )               .     setOutputCol     (     & quot ; prediction & quot ;     )               .     setLabels     (     labelIndexer     .     labels     )       val       pipeline       =       new       Pipeline     ( )               .     setStages     (     Array     (     labelIndexer     ,       rf     ,       labelConverter     ) )                 分类     org . apache . spark . ml . classification             逻辑 回归 ：   LogisticRegression   ， 参数   maxIter ,   regParam ,   elasticNetParam     分别 是 最大 迭代 次数 、 正则 项 系数 、 elastic 网 的 参数 。       决策树 ：   DecisionTreeClassifier         随机 森林 ：   RandomForestClassifier         GBDT ：   GBTClassifier         多层 感知器 （ 全 连接 神经网络 ） ：   MultilayerPerceptronClassifier   ，   setLayers   指定 每层 的 节点 数目 。   有没有 预 训练 ？ 需要 研究 一下 ！ ！ ！         One - vs - All ：   OneVsRest   ， 将 二 分类 变成 多 分类 模型 ， 采用 One - vs - all 策略 。 方法   setClassifier   设置 二 分类器 。       朴素 贝叶斯 ：   NaiveBayes   ，           回归     org . apache . spark . ml . regression             线性 回归 ：   LinearRegression         广义 线性 回归 ：   GeneralizedLinearRegression     @ since ( 2.0 . 0 )       决策树 回归 ：   DecisionTreeRegressor         随机 森林 回归 ：   RandomForestRegressor         GBDT 回归 ：   GBTRegressor         Survival   regression ：   AFTSurvivalRegression   ， 什么 东西 ？       Isotonic   回归 ：   IsotonicRegression   ， 什么 东西 ？           聚类     org . apache . spark . ml . clustering               KMeans   ， K - means 聚类 ， 通过   setK   设置 类 的 数目 。   K - means ++ 的 分布式 实现   。         LDA   ： Latent   Dirichlet   allocation         BisectingKMeans   ： Bisecting   k - means   聚类 ， @ since ( 2.0 . 0 )   不 懂 ？         GaussianMixture   ： GMM   模型 。 @ since ( 2.0 . 0 )           协同 过滤             ALS   ： ALS 算法 ， 2.0 才 加到 ml 包 里面 ， 之前 在 mllib 包 。           DataFrame       DataFrame 相当于   RDD [ Row ] ， 而 Row 相当于 一个 可以 包含 各种 不同 数据 的 Seq 。     DataFrame 通过 collect 函数 之后 就是 Array [ Row ]       通过 工厂 方法   SQLContext . createDataFrame   创建 DataFrame ， 可以 从 一下 几个 数据源 创建           从   List ( label ,   FeatureVector )   序列 创建       从     JavaRDD   创建       从     RDD     创建       从     List [ Row ]     创建       从     RDD [ Row ]     创建                   val       training       =       sqlContext     .     createDataFrame     (     Seq     (           (     1.0     ,       Vectors     .     dense     (     0.0     ,       1.1     ,       0.1     ) ) ,           (     0.0     ,       Vectors     .     dense     (     2.0     ,       1.0     ,       -     1.0     ) ) ,           (     0.0     ,       Vectors     .     dense     (     2.0     ,       1.3     ,       1.0     ) ) ,           (     1.0     ,       Vectors     .     dense     (     0.0     ,       1.2     ,       -     0.5     ) )       ) ) .     toDF     (     & quot ; label & quot ;     ,       & quot ; features & quot ;     )                 spark 的 DataFrame 每 一列 可以 存储 向量 ！ 甚至 图像 ！ 任意 值 都行 ！ ！               SQL 操作           select ( col1 ,   col2 ,   ... )   选取 部分 列       sample   采样       sort   排序       unionAll   融合 其他 表       orderBy       limit       join   内 连接       groupyBy       filter ( sql 表达式 )                   lazy   val   rdd   对象 ， 可以 通过 RDD 接口 操作               df . sqlContext   可以 访问 创建 该 DataFrame   的 SQLContext 对象 ， rdd . sparkContext   可以 访问 创建 RDD 的 SparkContext 对象 。               保存 到 磁盘                       df     .     rdd     .     map       {       转换 操作       }       .     saveAsTextFile     (     filepath     )                 spark . mLlib           LogisticRegressionWithLBFGS       LogisticRegressionModel ，   要   model . clearThreshold     predict 才 会 输出 概率 ， 否则 输出 的 是 判决 后 的 值           基本 数据结构           Vector ,   可以 通过 工厂 对象   Vectors   创建 ， 普通 向量   Vectors . dense   ， 稀疏 向量   Vectors . sparse   ， 通过   . toArray   方法 转换 为   Array [ Double ]         LabeledPoint ,   二元 组     ( label : Double ,   features :   Vector )         Matrix ，   可以 通过 工厂 对象   Matrices   创建 ， 普通 矩阵     Matrices . dense   ， 稀疏 矩阵   Matrices . sparse         RowMatrix ， 前面 的 向量 和 矩阵 都 是 存在 单机 中 ， 这种 和 下面 的 矩阵 是 分布式 存储 的 。       IndexedRowMatrix ， indexedrow 是 ( long ,   vector ) 的 包装 使得 index 是 有 意义 的       CoordinateMatrix ，       BlockMatrix                   val       rows     :       RDD     [     Vector     ]       =       ...       / /   an   RDD   of   local   vectors       / /   Create   a   RowMatrix   from   an   RDD [ Vector ] .       val       mat     :       RowMatrix       =       new       RowMatrix     (     rows     )           val       rows     :       RDD     [     IndexedRow     ]       =       ...       / /   an   RDD   of   indexed   rows       / /   Create   an   IndexedRowMatrix   from   an   RDD [ IndexedRow ] .       val       mat     :       IndexedRowMatrix       =       new       IndexedRowMatrix     (     rows     )                 模型 评估       包名   org . apache . spark . mllib . evaluation             两 分类   BinaryClassificationMetrics       多 分类   MulticlassMetrics           TIPS       使用 log4j               package       org . apache . log4j     ;             public       class       Logger       {                 / /   Creation   & amp ;   retrieval   methods :               public       static       Logger       getRootLogger     ( ) ;               public       static       Logger       getLogger     (     String       name     ) ;                 / /   printing   methods :               public       void       trace     (     Object       message     ) ;               public       void       debug     (     Object       message     ) ;               public       void       info     (     Object       message     ) ;               public       void       warn     (     Object       message     ) ;               public       void       error     (     Object       message     ) ;               public       void       fatal     (     Object       message     ) ;                 / /   generic   printing   method :               public       void       log     (     Level       l     ,       Object       message     ) ;       }         / /   例子       import       org . apache . log4j . Logger       val       log       =       Logger     .     getLogger     (     getClass     .     getName     )       log     .     info     (     & quot ; info & quot ;     )                         如果 对 RDD 操作 里面 有 随机 的 因素 在 里面 ， 那么 每次 操作 会 不 一样 ！ ！               Spark   in   Action   [ BOOK ]     https : / / zhangyi . gitbooks . io / spark - in - action             Spark   Programming   Guide     https : / / spark . apache . org / docs / latest / programming - guide . html        ", "tags": "tools", "url": "/wiki/tools/spark.html"},
      
      
      {"title": "ssh使用技巧汇总", "text": "    Table   of   Contents           关于           常用命令                 关于       记录 ssh 使用 技巧       常用命令           生成 公钥                 ssh - keygen      ", "tags": "tools", "url": "/wiki/tools/ssh.html"},
      
      
      {"title": "YARN学习", "text": "    Table   of   Contents           关于           开发 指南           参考                 关于       YARN   是 HADOOP2 代 ， 相比 第一代 产品 ， 可以 开发 除了   MAPREDUCE   外 更 复杂 的 分布式 程序 ，     XGBoost   on   yarn   就是 一个 例子 ！       开发 指南               YARN   用   ResourceManager   来 分配资源 ， NodeManager   来 管理 计算 节点 ！     ApplicationMaster   用于 跟踪 任务 ， 管理 每 一个 在 YARN 上 运行 的 程序 实例 ，     通过 调用   ResourceManager   和   NodeManager   进行 管理 。       要 使用 一个   YARN   集群 ， 首先 需要 来自 包含 一个 应用程序 的 客户 的 请求 。 ResourceManager   协商 一个 容器 的 必要 资源 ， 启动 一个   ApplicationMaster   来 表示 已 提交 的 应用程序 。 通过 使用 一个 资源 请求 协议 ， ApplicationMaster   协商 每个 节点 上供 应用程序 使用 的 资源 容器 。 执行 应用程序 时 ， ApplicationMaster   监视 容器 直到 完成 。 当 应用程序 完成 时 ， ApplicationMaster   从   ResourceManager   注销 其 容器 ， 执行 周期 就 完成 了 。       具体 而言 ， 就是 要 创建 一个   ApplicationMaster 类 ， 例如   XGBoost 的 例子   。       创建 一个     application   submission   client   ，     提交 应用 给   YARN   的   ResourceManager ( RM ) 。     这个 步骤 通过 创建 一个   YarnClient   实例 实现 ！       Following   are   the   important   interfaces :           Client & lt ; - - & gt ; ResourceManager           By   using   YarnClient   objects .           ApplicationMaster & lt ; - - & gt ; ResourceManager           By   using   AMRMClientAsync   objects ,   handling   events   asynchronously   by   AMRMClientAsync . CallbackHandler           ApplicationMaster & lt ; - - & gt ; NodeManager           Launch   containers .   Communicate   with   NodeManagers   by   using   NMClientAsync   objects ,   handling   container   events   by   NMClientAsync . CallbackHandler       一个 简单 的 例子 ：               / /   Create   yarnClient       YarnClient       yarnClient       =       YarnClient     .     createYarnClient     ( ) ;       yarnClient     .     init     (     conf     ) ;       yarnClient     .     start     ( ) ;         YarnClientApplication       app       =       yarnClient     .     createApplication     ( ) ;       GetNewApplicationResponse       appResponse       =       app     .     getNewApplicationResponse     ( ) ;         / * *       -   * * ApplicationSubmissionContext * * :   defines   all   the   information   needed   by   the   RM   to   launch   the   AM .   A   client   needs   to   set   the   following   into   the   context :               -   Application   info :   id ,   name               -   Queue ,   priority   info :   Queue   to   which   the   application   will   be   submitted ,   the   priority   to   be   assigned   for   the   application .               -   User :   The   user   submitting   the   application               -   ContainerLaunchContext :   The   information   defining   the   container   in   which   the   AM   will   be   launched   and   run .   The   ContainerLaunchContext ,   as   mentioned   previously ,   defines   all   the   required   information   needed   to   run   the   application   such   as   the   local   * Resources   ( binaries ,   jars ,   files   etc . ) ,   Environment   settings   ( CLASSPATH   etc . ) ,   the   Command   to   be   executed   and   security   T * okens   ( RECT ) .       * /       ApplicationSubmissionContext       appContext       =       app     .     getApplicationSubmissionContext     ( ) ;       ApplicationId       appId       =       appContext     .     getApplicationId     ( ) ;         appContext     .     setKeepContainersAcrossApplicationAttempts     (     keepContainers     ) ;       appContext     .     setApplicationName     (     appName     ) ;                 ApplicationMaster   通过   AMRMClientAsync . CallbackHandler   处理 资源 相关 事件 ， 通常 在   onContainersAllocated   事件 启动 任务 。       本地 文件 可以 通过   Client   上 传到   HDFS ， 然后   ContainerLaunchContext . setLocalResources   创建 为 本地 资源 ， 进行 调用 ！       参考           IBM 博客     https : / / www . ibm . com / developerworks / cn / data / library / bd - hadoopyarn / index . html           Hadoop :   Writing   YARN   Applications        ", "tags": "tools", "url": "/wiki/tools/yarn.html"},
      
      
      {"title": "科学上网", "text": "    Table   of   Contents               proxifier   注册码             L6Z8A - XY2J4 - BTZ3P - ZZ7DF - A2Q9C （ Portable   Edition ）   5EZ8G - C3WL5 - B56YG - SCXM9 - 6QZAP （ Standard   Edition ）   P427L - 9Y552 - 5433E - 8DSR3 - 58Z68 （ MAC ）      ", "tags": "tools", "url": "/wiki/tools/gfw.html"},
      
      
      
      
      
        
      
        
        
      
      {"title": "第00讲: 关于《机器学习教程》", "text": "    Table   of   Contents           关于 教程 的 使用 和 面向对象           关于 作业           支持 与 赞助                 机器 学习 现在 大火 ， 得益于 天时地利人和 ， 我 也 算是 半路出家 的 人 ， 周围 也 有 一些 朋友 想 学习 机器 学习 。 所以 有感于 此 ， 打算 写 一个 教程 ， 将 自己 学习 的 路线 和 知识 分享 给 大家 ， 一方面 利 人 ， 另一方面 也 是 督促 自己 学习 和 深入 思考 。 费恩曼 貌似 说 过 这样 一句 话 ， 只有 当 你 用 自己 的 语言 将 一个 知识 描述 出来 ， 才能 说 你 真正 理解 了 它 。 费恩曼 思维 活跃 ， 写得 物理 书 也 是 我 最 喜欢 的 物理 教材 了 。 当时 看到 费恩曼 先生 的 《 费恩曼 物理学 讲义 》 才 知道 教材 还 能 写 得 这么 生动 ， 而且 看 他 写 得 物理 教材 ， 更 侧重 物理 图像 的 理解 ， 而 不是 像 国内 很多 教材 那样 ， 强调 各种 数学 推导 。       关于 教程 的 使用 和 面向对象       国家 已经 把 人工智能 提升 到 国家 战略 了 ， 而且 从 目前 的 趋势 来看 ， 未来 很多 工作 大有 可能 会 被 机器 取代 ， 因此 使用 机器 学习 解决 实际 问题 的 人 需求量 将会 越来越 大 。 甚至 可以 说 ， 未来 懂 机器 学习 就 像 现在 懂 编程 一样 ， 会 成为 技术人员 的 标配 。 因此 ， 本 教程 的 主要 面向对象 将会 降低 到 懂 一些 编程 的 所有 技术人员 ， 如果 你 已 是 机器 学习 的 老 司机 ， 在 学习 新 的 算法 的 时候 ， 使用 本 教程 也 会会 是 一个 不错 的 选择 。       关于 作业       从 我 个人 学习 的 经历 和 对 周围 人 自学 机器 学习 的 观察 来看 ， 没有 一定 的 实践 ， 只是 看 视频 、 看书 、 看 博客 ， 很难 有 较大 的 收获 。 我 学习 过 Andrew   Ng 的 公开课 ， 也 看过 斯坦福 的 CS224d 以及 CS231n 等 有名 的 课程 ， 深感 国外 课程设计 地 多么 精细 ， 尤其 是 对 它们 的 作业 设计 ， 十分 佩服 。 相比 国内 的 教程 ， 很少 有 设计 作业 的 内容 。 但是 ， 作业 过多 也 是 一件 麻烦事 ， 因为 不 知道 重点 ， 想 通过 作业 来 加深 理解 ， 看到 十几道 题 和 从零开始 的 代码 ， 也 是 很 头疼 ， 可能 并 不是 每个 人 都 能花 那么 多 精力 去 折腾 。 所以 ， 我 尽量 吧 作业控制 在 最少 的 范围 内 。 如果 你 真的 想 学习 ， 每一 讲 的 作业 是 非常 有 必要 要 做 得 ， 每一 讲 的 作业 不 多 ， 通常 是 1 - 2 个 思考题 或者 一个 编程 实践 题 ， 编程 的 启动 代码 我 都 会 提供 ， 你 只 需要 完成 关键 代码 即可 。       评论 区 可以 用来 向 我 提问 ， 或者 和 学习 的 小伙伴 们 互相 交流 ， 但 不 建议 直接 贴 作业 的 答案 。       支持 与 赞助       最后 ， 如果 您 觉得 这个 教程 有用 ， 不妨 推荐 给 同学 或 朋友 。 如果 这个 教程 对 您 帮助 很大 ， 不妨 赞助 我 一杯 咖啡 ， 支持 我 继续 提供 更好 的 教程 。  ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro.html"},
      
      
      {"title": "第01.0讲：一个例子入门机器学习", "text": "    Table   of   Contents           关于           机器 学习 的 本质           从 0 开始 机器 学习           任务 与 数据           建模           简单 规则 模型           决策树 模型                   预测           总结                   机器 学习 的 应用 举例           点击率 预估           大 数据 风控           人脸识别           出租车 派单                   复现 代码           数据 加载           鸢尾花 数据分析           决策树 建模                   思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 入门 机器 学习 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 机器 学习 ？       机器 学习 能 做 什么 ？       机器 学习 在 代码 上 具体 如何 实现 ？           学习 本 讲 ， 希望 你           年满 18 岁 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           机器 学习 的 本质       现在 人们 通常 将 机器 学习 和 人工智能 联系 在 一起 ， 实际上 ， 人工智能 涉及 的 领域 更加 宽泛 ， 机器 学习 只是 其中 一种 手段 。 人工智能 的 起源 可以 追溯到 上 世纪 50 年代 ， 1956 年 举办 的 达茂思 会议 (   Dartmouth   Conference   ) ， 在 这次 会议 上 ， 信息论 之父 Shannon 和 IBM 科学家 Nathan   Rochester 等 人 ， 一起 探讨 了 一个 议题 ： 精确 地 描述 学习 过程 和 智能 的 特征 并用 机器 进行 模拟 。 说人话 ！ 就是 用 机器 模拟出 人类 的 智能 ！       人工智能 发展 的 初期 ， 研究者 致力于 将 人类 的 知识 表达 为 一些 逻辑 规则 ， 然后 利用 搜索 进行 逻辑推理 ， 进而 实现 智能 ， 到 后来 演变 到 利用 知识库 构造 专家系统 ， 实现 所谓 的 智能 。 这 期间 ， 比较 有名 的 成就 有 IBM 的 国际象棋 程序 深蓝 打败 国际象棋 冠军 。 这一 阶段 的 人工智能 实现 ， 更 像 人类 的 演绎推理 ， 利用 少量 的 规则 ， 加上 知识库 ， 进行 推演 ， 从而 得出结论 。 但是 ， 规则 的 归纳 需要 人类 专家 干预 ， 限制 了 这种 模式 的 发展 。 2000 年 以后 ， 随着 互联网 和 摩尔定律 的 发展 ， 产生 了 大量 的 数据 和 计算资源 ， 使得 人们 可以 利用 机器 从 数据 中 自动 归纳 出 规则 ， 也 就是 数据 驱动 的 智能 。 这 其中 的 工具 就是 机器 学习 ！       所以 ，   机器 学习 就是 利用 一种 程序 从 数据 中 自动 归纳 出 有 价值 的 知识 的 一种 方法   。       所谓 演绎推理 ( Deductive   Reasoning ) ， 就 是从 一般性 的 前提 出发 ， 通过 推导 即 “ 演绎 ” ， 得出 具体 陈述 或 个别 结论 的 过程 。 演绎推理 的 逻辑 形式 对于 理性 的 重要 意义 在于 ， 它 对 人 的 思维 保持 严密性 、 一贯性 有着 不可 替代 的 校正 作用 。 我们 熟知 的 很多 数学 证明 方法 ， 例如 通过 简单 的 几条 公理 ， 推导 出 整个 欧式 几何 大厦 的 推理 过程 ， 就是 典型 的 演绎推理 。   下面 是 演绎推理 里面 一个 典型 的 三段论 推理 的 例子 ：               知识分子 都 是 应该 受到 尊重 的 ，       人民 教师 都 是 知识分子       所以 ， 人民 教师 都 是 应该 受到 尊重 的 。               演绎推理 的 核心思想 就 是从 一般 到 特殊 ， 将 一些 已经 为 真的 通用性 结论 应用 到 具体 的 问题 当中 ， 得到 具体 的 情况 下 的 结论 。 这种 推理 方式 保证 了 推理 的 严密性 ！ 在 上述 例子 中 ， 前 两条 就是 一般性 结论 ， 知识分子 是 比 人民 教师 更大 的 概念 ， 第三条 的 结论 就是 将 第一条 结论 应用 到 人民 教师 这个 具体 的 个体 上 得到 的 更 具体 的 结论 ！ 有趣 的 是 ， 柯南道 尔 的 著名 小说 中 《 福尔摩斯 》 中 的 大 侦探 福尔摩斯 也 十分 推崇 “ 演绎法 ” ！ 为此 ， 老美 还 专门 拍 了 一部 剧 《 福尔摩斯 ： 基本 演绎法 》 ！   只不过 ， 福尔摩斯 所 声称 的 一般性 结论 和 推理 方式 非常 人能 理解 ！       而 归纳法 是 根据 一类 事物 的 部分 对象 具有 某种 性质 的 有限 观察 ， 推出 这 类 事物 的 所有 对象 都 具有 这种 性质 的 推理 ， 叫做 归纳推理 （ 简称 归纳 ） 。 归纳 是从 特殊 到 一般 的 过程 ， 它 属于 合情 推理 。 通常 归纳法 难以 保证 结论 是 可靠 的 ， 例如 ， 下面 就是 经典 的 归纳法 的 例子 ：               欧洲 看到 过 的 天鹅 都 是 白色 的 。       所以 所有 的 天鹅 都 是 白色 的 ！                 黑天鹅 事件   ： 17 世纪 之前 ， 欧洲 看到 过 的 天鹅 都 是 白色 的 ， 所以 当时 欧洲人 归纳 出 一个 结论 ： 天鹅 都 是 白色 的 ！   直到 后来 ， 欧洲人 发现 了 澳洲 ， 看到 了 当地 的 黑天鹅 ， 人们 才 认识 到 这个 结论 是 错误 的 ！       从 有限 的 经验 归纳 出来 的 结论 当然 不见得 是 可靠 的 ， 但是 数学 上 也 有 完全 归纳法 ， 可以 保证 结论 是 可靠 的 的 例子 ， 我们 以前 学过 的 数学 归纳法 。 从 逻辑推理 的 角度 来看 ， 我们 现在 所用 的 机器 学习 就是 先 观察 到 一些 数据 ， 然后 从 这 有限 的 数据 中 归纳 出 一些 有用 的 规律 的 过程 ！   因此 ，   机器 学习 本质 上 就是 在 做 归纳推理 ， 并且 是 不 完全 的 归纳法   ！ 我们 前面 说 到 ， 这种 不 完全 归纳法 无法 保证 结论 的 正确性 ， 所以 如果 机器 学习 模型 预测 错 了 ， 请 不要 怪 他 ， 因为 它 是 在 做 不 完全 归纳 ， 肯定 会 犯错 的 ！ 但是 这 并 不 意味着 就 没有 用 ， 事实上 我们 人类 很多 经验 都 是 通过 不 完全 归纳法 归纳 出来 的 ， 甚至 可以 说 几乎 所有 实际 的 经验 都 来源于 不 完全 归纳 ， 完全 归纳法 只有 在 数学 上 才 存在 。 只要 归纳 的 结论 大多数 情况 下 是 对 的 ， 那么 他 就是 有用 的 ！       接下来 ， 我们 就 用 一个 实际 的 例子 来 解释 机器 学习 是 如何 从 数据 中学 到 有用 知识 的 。       从 0 开始 机器 学习       接下来 ， 我们 将 利用 一个 简单 的 分类 任务 ， 给 读者 展示 机器 学习 如何 从 数据 中学 到 有用 知识 的 。       任务 与 数据       本 任务 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               建模       我们 的 目标 是 ， 建立 一个 模型 ， 输入 鸢尾花 的 4 个 属性 变量 ， 能够 对 鸢尾花 的 种类 进行 判别 。 这样 一旦 模型 建立 好 了 之后 ， 对 新 看到 的 鸢尾花 ， 只要 测量 了 这 4 个 属性 ， 就 可以 利用 模型 对 它 的 类别 进行 预测 了 。       数学 地 角度 来说 ， 我们 要 确定 一个 函数   $ f :   R ^ 4   \ \ rightarrow   \ \ { 0 , 1 , 2 \ \ } $ ， 输入 是 一个 4 维 向量   $ \ \ vec { x }   =   ( x _ 1 ,   x _ 2 ,   x _ 3 ,   x _ 4 ) $ ， 每 一维 代表 一个 属性 变量 的 值 ， 输出 一个 分类 变量   $ y   \ \ in   \ \ { 0 , 1 , 2 \ \ } $ ， 代表 该 样本 属于 哪个 类别 。 所谓 的 建模 过程 ， 就是 利用 我们 已经 观测 到 的 数据 集 ， 去 确定 这个 函数   $ f $   的 具体 形式 。 这里 每 一个 属性 我们 都 称作 一个 特征 ， 输出 分类 变量 我们 称做 目标 （ 或 建模 目标 ） ， 这里 的 函数   $ f $   就是 我们 通常 所说 的 模型 。       简单 规则 模型       在 建立 复杂 模型 之前 ， 我们 先 来 建立 一种 简单 规则 模型 。 所谓 的 简单 规则 ， 就是 对 一个 属性 ， 通过 规则 判定 ， 确定 该 样本 属于 哪 一个 类 。 比如 ， 我们 可以 进行 数据分析 ， 将 收集 的 数据 绘制 到 以 花瓣 长度 为 横坐标 、 花瓣 宽度 为 纵坐标 的 坐标 图上               可以 看到 ， 这 三种 花 在 花瓣 长度 和 宽度 上 都 有 明显 差异 ， 我们 可以 继续 分析 ， 统计 出 每 一种 花 的 萼片 长度 、 萼片 宽度 、 花瓣 长度 、 花瓣 宽度 的 平均值 。               通过 上述 分析 ， 可以 看到 三种 花 的 花瓣 长度 ( petal   length ， 对应 绿色 的 柱子 ) 平均值 差异 比较 大 ， setosa 的 平均 花瓣 长度 在 1.5 cm 左右 ， versicolor 的 平均 花瓣 长度 在 4.2 cm 左右 ， 而   virginica 的 平均 花瓣 长度 在 5.6 cm 左右 。 因此 ， 一种 简单 规则 模型 可以 归纳 为       $ $     target   =     \ \ begin { cases }     0 ,   \ \ text { petal   length }   \ \ lt   2.8   \ \ \ \     1 ,   \ \ text { petal   length }   \ \ in   [ 2.8 ,   4.9 )   \ \ \ \     2 ,   \ \ text { petal   length }   \ \ ge   4.9     \ \ end { cases }     $ $       这里 分割 点 的 值取 的 是 两种 花 平均 中 的 平均数 。 这 实际上 就是 一个 分段 函数 ， 输入 时 花瓣 长度 ， 输出 就是 花 的 类别 ， 因此 这 就是 一个 模型 。       好 了 ， 到 目前为止 ， 你 已经 学会 了 数据挖掘 过程 中 的 最 简单 情景 了 。 通过 数据分析 ， 归纳 出 规则 ， 然后 将 规则 编码 成 一个 函数 ， 从而 得到 一个 预测 模型 ， 可以 用来 做 预测 。       很快 ， 我们 会 发现 ， 这种 方法 需要 人工 进行 数据分析 ， 总结 出 规则 ， 那么 能不能够 让 程序 自动 地 找到 这些 规则 ， 甚至 发现 更 复杂 的 规则 呢 ？ 答案 是 肯定 的 ， 决策树 就是 这样 一种 模型 ， 自动 地 发现 这些 规则 ， 甚至 复杂 的 组合 规则 。       决策树 模型       下图 是 上面 我们 人工 挖掘 出来 的 规则 模型 对应 的 决策树 模型 ， 可以 看到 它 由 很多 节点 构成 ， 包括 中间 节点 ( 椭圆形 ) 和 叶子 节点 ( 方形 ) 。 中间 节点 是 一个 规则 ， 每个 规则 是 一个 逻辑 判断 ， 例如 最 上面 的 中间 节点 （ 也 叫做 根 节点 ） 的 规则 是 花瓣 长度 小于 2.8 厘米 。 如果 规则 满足 ， 则 进入 左边 的 叶子 节点 ， 否则 就 沿着 右子 树 继续 判断 ， 我们 把 这个 过程 称作 分裂 。 叶子 节点 对应 模型 的 输出 结果 ， 最 左边 的 叶子 节点 输出 的 是 setosa ， 表明 满足 该 叶子 节点 规则 就 预测 为 setosa 。 每 一个 叶子 节点 都 对应 一条 从根 节点 出发 的 路径 ， 路径 上 长度 称作 叶子 节点 的 深度 （ 也 可以 认为 是 规则 的 数目 ） ， 叶子 节点 的 最大 深度 称作 树 的 深度 ， 图中 这棵树 的 深度 为 2 。 一个 样本 过来 ， 沿着 根 节点 开始 ， 不断 地 按照 规则 往 下 移动 ， 直到 到达 叶子 节点 。 叶子 节点 对应 的 输出 结果 就是 模型 对 这个 样本 的 预测 结果 。 这些 规则 可以 用 一棵树 状 的 图 描述 ， 因此 叫做 决策树 模型 。               决策树 节点 上 的 规则 可以 从 数据 中 通过 算法 自动 地 发现 ， 或者说 可以 通过 算法 自动 地 生成 一棵 决策树 ， 这个 过程 称作 模型 的 训练 。 决策树 如何 发现 这些 规则 我们 暂时 不要 去 深究 ， 作为 一个 入门 课程 ， 我们 重点 是 了解 模型 能干 啥 。 这里 ， 我们 利用   scikit - learn   软件包 里面 的 决策树 模型 工具 ， 建立 模型 。 模型 训练 好 了 之后 ， 我们 可以 将 决策树 画 出来 ， 进行 观察 。               算法 自动 学习 出来 的 决策树 是 一颗 二叉树 ， 根 节点 对应 规则 是 petal   length   ( cm )   & lt ; =   2.45 。 满足 这 条 规则 的 样本 ， 就 到 了 左子 树 。 左子 树 是 一个 叶子 节点 ， 图中 的 values = [ 50 ,   0 ,   0 ] 表示 整个 数据 集 的 150 个 样本 中 满足 这 条 规则 的 样本 只有 50 个 ， 且 全部 为 setosa 这个 类别 ， 因此 叶子 节点 预测 输出 的 类别 是   setosa 。 这 和 前面 我们 通过 数据分析 得出 的 规则   petal   length   & lt ;   2.8   则 为 setosa ， 非常 接近 。     < ! - -   ####   线性 模型     通过 前面 的 数据分析 ， 可以 看出 花瓣 长度 （ petal   length ） 越长 ， 越有 可能 是 virginica ； 花瓣 宽度 （ petal   width ） 越长 ， 越有 可能 是 virginica 。 那么 如何 综合 考虑 这 两个 因素 呢 ？ 答案 是 把 它们 加 起来 ， 得到 一个 分数 ， 这个 分数 越大 ， 那么 表明 越有 可能 是 virginica ， 这个 分数 越小 ， 那么 表明 越有 可能 是 setosa ！ 如果 要 区分 versicolor ， 只有 这 两个 因素 是 不够 得 ， 还要 把 其他 因素 也 加 起来 ！ 显然 ， 不同 的 因素 的 重要性 是 不同 的 ， 那么 我们 可以 对 每种 因素 赋予 不同 的 权重 。 这 就是 线性 模型 ， 对 每 一类 ， 将 所有 因素 加权 相加 ， 计算 一个 分数 ， 分数 越高 表明 样本 属于 这 一类 的 可能性 越大 。 因为 这个 得分 的 计算 是 一个 线性 函数 ， 所以 叫做 线性 模型 。 用 数学公式 表述 如下     $ $   score _ i   =   b _ i   +   \ \ vec { w }   _   i ^ T   \ \ vec { x }   $ $     为 每个 类别 计算 一个 得分 后 ， 选择 出 得分 最大 的 类 作为 预测 的 类 。 模型 的 权重 $ \ \ vec { w }   _   i $ 称作 模型 参数 ， 通过 优化 算法 优化 得到 。 优化 模型 参数 的 过程 就 叫做 模型 训练 ！ 模型 训练 的 过程 不是 这次 的 重点 ， 我们 推 到 以后 再 讲 。     线性 模型 （ 确切 地 说 是 线性 多 分类 模型 ） 里面 最 典型 的 是 多 分类 逻辑 回归 ， 它 将 每个 类 的 分数 做 归一化 ， 这个 归一化 后 的 分数 被 解释 为 样本 归属于 该类 别的 概率     $ $   P ( i | \ \ vec { x } )   =   \ \ frac { e ^ { score _ i } } { \ \ sum _ i   e ^ { score _ i } }   $ $     下面 利用 ` scikit - learn ` 多 分类 逻辑 回归 工具   ` LogisticRegression `   进行 建模 。          wzxhzdk : 0            ! [ svg ] ( / wiki / static / images / iris - 4 . svg )       从 模型 参数 来看 ， sepal   length   和   sepal   width   数值 大 的 类别 setosa 的 得分 更高 ， petal   length   数值 较大 的   versicolor 的 得分 更高 ， petal   length   和   petal   width   数值 大 的   virginica   得分 更高 。   - - >       预测       一旦 模型 建立 好 了 之后 ， 我们 就 可以 利用 模型 进行 预测 了 ， 所谓 的 预测 ， 是 指 对于 一个 新 的 样本 ， 比如 我 在 某个 路边 看到 了 一朵 鸢尾花 ， 不 知道 到底 是 哪 一类 ， 就 可以 利用 这个 模型 进行 预测 。 首先 ， 我们 需要 测量 模型 预测 所 需要 的 4 个 数据 （ 特征 ） ， 花萼 的 长度 和 宽度 ， 花瓣 的 长度 和 宽度 ， 然后 输入 的 模型 中 去 。       对 预测 前面 简单 规则 模型 ， 只 需要 花瓣 的 长度 数据 即可 预测 。 对于 决策树 模型 ， 实际上 只 需要 花瓣 的 长度 和 宽度 数据 也 可 预测 ， 如果 我们 将 决策树 深度 变得 更深 ， 那么 就 可能 要 用到 所有 数据 。 首先 ， 决策树 从根 节点 开始 搜索 ， 根 节点 对应 一条 规则   petal   length   ( cm )   & lt ; =   2.45 ， 如果 满足 这 条 规则 ， 就 到 左子 树 ， 预测 输出 为 setosa 。 如果 不 满足 ， 那么 就 到 右子 树 ， 右子 树根 节点 还是 一个 规则   petal   width   ( cm )   & lt ; =   1.75 。 我们 重复 这个 过程 ， 直到 找到 该 样本 满足 规则 的 叶子 节点 ， 叶子 节点 对应 的 输出 值 就是 模型 预测 结果 。       总结       这里 我们 以 鸢尾花 分类 任务 为例 ， 构建 了 一个 决策树 模型 进行 预测 。 总结 起来 ， 所谓 的 建模 过程 ， 就是 利用 已有 的 标注 数据 （ 已知 目标 变量 的 值 的 数据 ） ， 自动 学习 到 一个 函数   $ f : R ^ n   \ \ rightarrow   Y $ ， 根据 观察 到 的 特征向量 ， 计算 得到 目标 变量 的 值 。 这个 任务 就是 一个 3 分类 的 函数 。 虽然 这个 任务 简单 ， 但是 和 更 复杂 的 任务 一样 都 具有 以下 3 个 基本 步骤 ：           收集 （ 标注 ） 数据       建立 模型       预测           不同 的 业务 可能 收集 到 的 数据 不同 ， 收集 到 的 原始数据 需要 加工 成 模型 能用 的 数据 （ 即 特征 ） 。 不同 的 任务 建模 目标 也 不 一样 ， 比如 预测 性别 ， 那么 目标 变量 是 男 和 女 ； 预测 年龄 ， 那么 目标 变量 是 个 0 - 100 之间 的 连续 值 ； 预测 股价 涨跌 ， 那么 目标 变量 就是 涨 和 跌 。 根据 建模 目标 可以 将 问题 分为 两大类 ， 一类 是 和 这个 鸢尾花 分类 问题 类似 ， 目标 变量 取值 有 有限 的 ， 称作 分类 问题 ； 另一类 和 预测 房价 一样 目标 变量 取值 是 无限 的 ， 称作 回归 。       相同 数据 和 目标 的 情况 下 ， 也 可以 选择 不同 的 模型 ， 决策树 是 一个 久经沙场 的 模型 ， 它 的 两个 变体   随机 森林   和   梯度 提升 树   应用 非常 广泛 。 近年来 的 深度 神经网络 ， 可以 利用 非常 原始 的 数据 进行 建模 ， 减少 了 人工 特征 的 工作量 （ 毕竟 去 想 很多 可能 有用 的 变量 是 一件 不少 的 体力活 ） 。 但是 本质 上 ， 他们 都 在 干同 一件 事情 ， 从 数据 中 发现 规律 ， 这个 规律 可以 表达 为 一个 函数 ， 因此 也 叫 函数 拟合 ！       机器 学习 的 应用 举例       具体来讲 ， 机器 学习 可以 用来 做 很多 事情 ， 目前 已经 有 成功 案例 的 就 有 很多 。       点击率 预估       点击率 预估 是 很 早就 开始 应用 机器 学习 的 场景 之一 ， 也 是 机器 学习 在 工业界 应用 最为 广泛 的 场景 。 可以 说 ， 过去 5 年 ， 60 % 的 算法 工程师 都 是 在 做 各种 点击率 预估 。 点击率 预估 在 广告 、 搜索 、 推荐 三个 场景 中 应用 最为 广泛 。       在 互联网 广告 中 ， 如何 决定 给 用户 展示 哪 种 广告 一直 是 互联网 广告商 十分 在意 的 事情 。 例如 ， 当 你 在 百度 中 搜索 “ 手机 ” 时 ， 排 在 搜索 前面 的 几条 都 是 广告 ， 但是 广告位 是 有限 的 ， 现在 假定 只有 一个 广告位 ， 而 手机 广告 总共 有 1000 个 ， 那么 应该 展示 哪个 广告 呢 ？ 一种 简单 的 策略 就是 看 每个 广告 过去 的 点击率 ， 点击率 = 点击 人数 / 展示 人数 ， 点击率 高 的 广告 出现 的 概率 高 。 这个 简单 的 方法 没有 个性化 ， 那么 很 容易 通过 引入 个性化 来 提高 广告 的 总体 点击率 。 例如 ， 对于 收入水平 低 的 用户 ， 更 应该 展示 低价 的 手机 ， 而 收入水平 高 的 用户 ， 则 更 应该 展示 高价 的 手机 。 这 表明 ， 如果 我们 能够 知道 用户 对 每 一个 广告 的 个性化 点击率 而 不是 总体 的 点击率 ， 那么 我们 可以 让 展示 的 广告 更 有效率 ， 例如 为 广告 运营商 带来 更 多 的 广告费 。 但是 ， 个性化 点击率 很难 通过 历史 行为 统计 出来 ， 因为 给 同一个 用户 展示 同一个 广告 的 次数 不 可能 很多 ， 而且 广告 是 如此 之多 ， 以至于 每 一个 用户 看过 的 广告 占 总 的 广告 比例 非常低 。 例如 ， 百度 声称 在 使用 百度 推广 的 企业 有 62 万家 ， 某个 用户 看过 的 广告 都 只是 沧海一粟 。       在 搜索 排序 中 ， 同样 会 有 上述 问题 ， 你 在 京东 上 搜索 手机 时 ， 也 会 根据 你 的 点击率 给 你 排个序 。 与 广告 不同 的 是 ， 电子商务 排序 中 还 会 考虑 你 的 下单 率 ， 因为 在 电子商务 中 ， 下单 才能 带来 实实在在 的 收入 。 在 推荐 系统 中 ， 推荐 位置 同样 是 有限 的 ， 推荐 哪些 商品 和 不 推荐 哪些 商品 都 是 需要 用户 的 个性化 估计 点击率 和 下单 率 。 所以 最终 会 根据 点击率 和 下单 率 加权 取得 分 最高 的 商品 展示 在 推荐 位置 上 。       点击率 预估 和 下单 率 预估 的 方法 没有 太 大 差异 ， 这里 以 广告 点击率 预估 为例 进行 说明 。 广告 运营商 根据 用户 和 广告 的 历史 交互 数据 ， 可以 知道 用户 是否 点击 了 某个 广告 。 然后 可以 找出 用户 的 一些 信息 ， 例如 性别 、 年龄 等 人口学 属性 ， 还有 广告 的 标签 和 分类 等 信息 。 利用 机器 学习 的 算法 ， 可以 建立 一个 数学模型 ， 该 模型 可以 根据 这些 信息 计算 出 用户 的 个性化 点击率 。 如下 图 所示 ， 是 一个 点击率 预估 的 决策树 模型 ， 实际 场景 中 的 决策树 比 这个 要 复杂 得 多 ， 但是 基本原理 并 没有 太 大 差异 。 决策树 的 根 节点 是 一个 跟 用户 性别 有关 的 规则 ， 假设 我们 从 历史数据 中 收集 了 10000 个 样本 ， 这些 样本 每 一条 都 是 一次 广告 的 展示 。 性别 为 男则 进入 左子 树 ， 否则 则 进入 右子 树 。 假设 收集 的 样本 中有 4000 个 样本 对应 的 用户 性别 为 男性 ， 那么 左子 树有 4000 个 样本 ， 右子 树有 6000 个 样本 。 左子 树 的 根 节点 是 一条 与 广告 有关 的 规则 ， “ 广告 类别 = 汽车 ” ， 如果 广告 类别 是 汽车 则 进入 最 左边 的 叶子 节点 ， 这样 的 样本 有 1000 个 ， 其中 25 个 用户 点击 了 广告 ， 剩下 的 975 个 样本 用户 没有 点击 广告 。 因此 ， 这个 叶子 节点 输出 的 点击率 为 2.5 % 。 这个 结果 是从 训练 的 样本 中 计算 得到 的 ， 预测 时 ， 如果 用户 和 广告 满足 这个 规则 “ 用户 性别 = 男   & amp ; & amp ;   广告 类别 = 汽车 ” ， 那么 模型 就 预测 点击率 为 2.5 % 。 这 就是 最 简单 的 点击率 预估 模型 ， 可以 看到 ， 这 跟 很多 数据 分析师 干得 事情 是 十分相似 的 ， 最大 的 区别 在于 这些 规则 是 通过 算法 自动 生成 的 ， 可以 设想 ， 当要 考虑 的 因素 成千上万 时 ， 分析师 是 多么 的 抓狂 ， 但是 机器 不会 ， 机器 可以 生成 大量 的 这种 规则 ， 让 预测 更加 精准 ， 在 这件 事情 上 ， 机器 比 人 更加 擅长 。               大 数据 风控       在 传统 银行 中 ， 对 个人 和 企业 的 贷款 需要 进行 风险 控制 ， 为此 往往 需要 花费 大量 人力 进行 背景 调查 。 由于 这种 调查 成本 很 高 ， 所以 银行 很难 解决 小额贷款 的 风控 问题 ， 因为 成本 太高 ， 买卖 不划算 ， 银行 要么 要 你 提供 不动产 抵押 才能 贷款 ， 要么 直接 不 给 你 贷 。 在 互联网 时代 ， 人们 大量 的 信息 都 被 数字信息 记录 了 下来 ， 通过 一些 技术手段 可以 将 传统 金融 风控 所 需要 的 信息提取 出来 ， 利用 机器 学习 的 方法 ， 预测 用户 的 还款 意愿 和 还款 能力 ， 这样 就 可以 低成本 地 完成 大量 用户 的 贷款 申请 审核 工作 ， 使得 个人 小额贷款 成为 一个 赚钱 的 买卖 。       人脸识别       现在 的 数码相机 和 智能机 上 的 相机 都 具备 了 人脸识别 的 能力 ， 所谓 人脸识别 就是 可以 检测 出 图片 中 哪些 部分 是 人脸 ， 哪些地方 不是 。 一般 的 相机 在 拍照 时 ， 都 是 实时 地 识别 人脸 ， 一旦 识别 出来 后 ， 会 用 一个 正 方向 框住 人脸 。               这件 事情 看起来 十分 玄乎 ， 但是 一旦 你 了解 了 机器 学习 工作 原理 之后 ， 这 背后 的 基本原理 就 变得 简单 了 。 现在 几乎 涉及 到 图像 、 视频 等 检测 的 场景 ， 背后 最 主要 的 算法 都 是 机器 学习 。 人脸识别 的 大致 原理 是 ， 如果 有 一个 模型 （ 函数 ） ， 输入 一张 图片 ， 它会 告诉 你 这张 图片 是否 为 人脸 ， 那么 人脸 检测 就 简单 了 。 为了 从 一张 大图 中 找到 哪些 区域 是 人脸 ， 可以 先 把 这 张大 图 按照 不同 尺寸 不同 分辨率 截取 很多 小 的 图片 块 ， 对 每个 块 应用 上述 模型 ， 预测 这个 小 块 是否是 人脸 ， 然后 把 所有 是 人脸 的 块 都 标记 出来 ， 最后 把 这些 块 融合 成 一个 就 可以 了 ( 参考 上 图 ) 。       那 怎么 得到 这样 一个 模型 呢 ？ 答案 是 机器 学习 ！ 可以 先 收集 各种各样 的 图片 ， 然后 人工 标记 哪些 是 人脸 ， 哪些 不是 ， 得到 一个 标注 的 人脸识别 数据 集 。 图片 在 计算机 中是 用 数字 表示 的 ， 我们 知道 每 一张 图片 都 是 有 很多 像素点 构成 ， 对于 黑白 度 图像 ， 每 一个 像素 对应 一个 数值 ， 这个 数值 越大 ， 表明 这个 像素点 越亮 ， 这个 数值 越小 ， 表明 这个 像素点 越暗 ， 这些 敏感 交替 的 像素 一起 构成 了 整个 图片 ( 参考 上 图 ) 。 这里 每个 像素 对应 的 数值 都 可以 看做 特征 ， 虽然 我们 无法 像 分类 鸢尾花 那样 ， 认为 某个 像素 高 就 更 有 可能 是 人脸 ， 但是 和 分类 鸢尾花 一样 ， 都 是 要 找到 一个 函数 对 输入 的 这些 值 进行 计算 ， 输出 一个 结果 。 只不过 ， 计算 人脸 的 这个 函数 要 比 鸢尾花 的 复杂 得 多得多 ， 但 本质 上 都 是 在 找到 这个 函数 ， 而且 实现 的 方法 惊人 地 相似 ！ 只要 把 这些 特征 全部 扔 到 机器 学习 模型 中 ， 让 机器 自动 地 从 这些 标记 的 数据 中学 到 一个 可以 识别 人脸 的 函数 就 可以 了 。 当然 实际 的 系统 比 这个 要 复杂 得 多 ， 但是 基本原理 差不太多 。 这 在 另外 一个 方面 也 体现 了 机器 学习 相比 人工 规则 的 优越性 ， 人工 规则 实在 是 难以 找到 这样 一个 复杂 的 函数 来 识别 是否 为 人脸 。       计算机 视觉 用 计算机 来 实现 人 的 视觉 的 研究 领域 ， 里面 很多 问题 都 和 上述 人脸识别 的 问题 类似 ， 最终 都 归结为 拟合 一个 函数 的 问题 ， 这时 机器 学习 就 派上用场 了 。       出租车 派单       在 互联网 出租车 出来 以前 ， 打车 靠 的 是 运气 ， 当 你 碰到 一个 空车 并且 司机 成功 看到 你 的 时候 ， 这笔 交易 就算 成功 了 。 很多 时候 ， 这种 匹配 都 很 困难 ， 尤其 是 在 偏僻 的 地方 和 高峰期 。 当 用户 和 司机 都 通过 手机 APP 接入 互联网 之后 ， 这种 匹配 效率 就 大大提高 了 。 设想 这样 一个 问题 ， 在 一个 区域 有 5 个 乘客 和 7 个 司机 ， 每个 司机 只能 派 一单 ， 但是 可以 把 同一个 订单 派 给 多个 司机 ， 最先 抢到 的 司机 就算 接单 了 。 由于 司机 可能 因为 距离 太远 、 目的地 太偏 、 甚至 心情 不好 等 原因 拒绝 接单 ， 所以 这 5 单 即使 都 派出 去 也 不 简单 都 能 在 这 一次 派 单中 成交 ， 没 能 成交 的 订单 就 需要 等待 下 一次 派单 。 派单 的 一个 简单 的 目标 是 让 这 5 单 尽可能 地 成交 ， 如果 定义 接单 率 是 成交 的 单数 / 总 单数 ， 那么 目标 就是 让 总体 的 接单 率 最大化 。 那么 问题 来 了 ， 怎么 把 这 5 单派 给 司机 使得 总体 接单 率 最高 呢 ？ 最 简单 的 方法 是 就近 原则 ， 派 给 最近 的 司机 。 但 这种 策略 不见得 是 最优 的 ， 并 不是 所有 司机 都 愿意 接单 ， 有些 司机 不 愿意 接近 距离 的 单 ， 有些 司机 不 愿意 接 目的地 很 偏 的 单 。 这种 只 依靠 单一 因素 的 策略 匹配 成功率 不会 很 高 ， 那么 如何 把 所有 能够 考虑 的 因素 都 考虑 进来 设计 派单 策略 呢 ？ 答案 是 用 机器 学习 的 方法 预测 司机 的 接单 率 ！ 收集 历史 上 派 单 的 记录 ， 记录 里面 会 有 司机 是否 接单 ， 然后 将 需要 考虑 的 因素 计算出来 ， 作为 模型 的 特征 x ， 模型 预测 的 目标 是 不 接单 ( y = 0 ) 还是 接单 ( y = 1 ) 。 然后 就 可以 利用 算法 自动 归纳 出 一个 函数 ， 输入 是 这些 特征 ， 输出 是 接单 还是 不 接单 。 也 可以 像 点击率 预估 模型 那样 ， 输出 接单 的 概率 ， 这个 概率 就 可以 用来 设计 更好 的 派 单 策略 ！ 这个 思路 就是 滴滴 在 2017 年 数据挖掘 大会 ( KDD ) 上 提出 的 派 单 模型 的 一部分 。       类似 的 问题 还有 外卖 骑手 的 接单 问题 ， 这 类 问题 已经 大量 使用 机器 学习 的 方法 进行 优化 了 。 机器 学习 算法 正在 影响 我们 生活 的 方方面面 ， 从 打车 到 点 外卖 ， 几乎 无处不在 。       复现 代码       数据 加载               from       sklearn . datasets       import       load _ iris       import       pandas       as       pd       import       numpy       as       np       import       matplotlib . pyplot       as       plt       import       seaborn       as       sn         plt     .     style     .     use     (     &# 39 ; seaborn - talk &# 39 ;     )         iris       =       load _ iris     ( )       df       =       pd     .     DataFrame     (     iris     .     data     ,       columns     =     iris     .     feature _ names     )       df     [     &# 39 ; target &# 39 ;     ]       =       iris     .     target         #   随机 选取 几条 数据       idx       =       range     (     df     .     shape     [     0     ] )       np     .     random     .     shuffle     (     idx     )         print     (     iris     .     target _ names     )       print     (     df     .     iloc     [     idx     ]     .     head     (     10     ) )                 鸢尾花 数据分析               ax       =       plt     .     gca     ( )       df     .     groupby     (     by     =     &# 39 ; target &# 39 ;     )     .     plot     (     kind     =     &# 39 ; line &# 39 ;     ,       x     =     &# 39 ; petal   length   ( cm ) &# 39 ;     ,       y     =     &# 39 ; petal   width   ( cm ) &# 39 ;     ,       style     =     &# 39 ; .&# 39 ;     ,       ax     =     ax     )       ax     .     set _ xlim     ( [     0     ,     8     ] )       ax     .     set _ ylim     ( [     0     ,     3     ] )       plt     .     legend     (     iris     .     target _ names     ,       loc     =     &# 39 ; best &# 39 ;     )       plt     .     xlabel     (     &# 39 ; petal   length   ( cm ) &# 39 ;     )       plt     .     ylabel     (     &# 39 ; petal   width   ( cm ) &# 39 ;     )         df     .     groupby     (     by     =     &# 39 ; target &# 39 ;     )     .     mean     ( )     .     plot     (     kind     =     &# 39 ; bar &# 39 ;     )       plt     .     xticks     ( [     0     ,     1     ,     2     ] ,       iris     .     target _ names     ) ;       plt     .     xlabel     (     &# 39 ; &# 39 ;     )       plt     .     ylabel     (     u     &# 39 ; 平均值 &# 39 ;     )                 决策树 建模       决策树 模型 分为 回归 和 分类 ， 如果 目标 变量 是 类别 变量 ， 只能 取 有限 的 几个 值 ， 这样 的 问题 称为 分类 ， 我们 这个 任务 就是 分类 问题 。 而 如果 目标 变量 是 可取 连续 值 变量 ， 例如 预测 房价 ， 那么 这样 的 问题 就是 回归 。 这里 我们 只用 分类 就 好 了 ， 对应 的 类 是     DecisionTreeClassifier   ， 为了 便于 观察 ， 我们 限定 树 的 深度 为 2 。 为了 让 决策树 模型 能够 从 数据 中学 会 规则 ， 我们 需要 调用 模型 的     fit     方法 ， 并 将 数据 （ 包括 特征   iris . data   和 目标   iris . target   ） 传给 它 。       模型 从 数据 中 自动 学会 这些 规则 的 过程 ， 我们 称之为   训练   或者   拟合   。 因此 ，   fit   方法 实际上 就是 在 做   模型 训练   ！               from       sklearn . tree       import       DecisionTreeClassifier     ,       export _ graphviz       import       graphviz         clf       =       DecisionTreeClassifier     (     max _ depth     =     2     )       clf     .     fit     (     iris     .     data     ,       iris     .     target     )         dot _ data       =       export _ graphviz     (     clf     ,       feature _ names     =     iris     .     feature _ names     ,                                                             class _ names       =       iris     .     target _ names     ,                                                             out _ file     =     None     ,       filled     =     True     )         graphviz     .     Source     (     dot _ data     )                 思考 与 实践       一个 编程 小 练习 ， 探索 决策树 的 深度 与 预测 的 准确率 的 关系 ， 并 解释一下 观察 到 的 现象 的 原因 。       在 编程 之前 ， 请 先 配置 好 环境 ：           安装   Python     https : / / www . python . org / downloads / release / python - 2713 /         安装 Python 包       scikit - learn     http : / / scikit - learn . org / stable / install . html         pandas   数据处理 包       matplotlib   绘图 包       seaborn     可视化 包 ( 本 作业 暂时 不用 )       graphviz   可视化 包 ( 本 作业 暂时 不用 )                   可以 用 python 包 管理器   pip   安装 相关 包 ，   pip   默认 会 使用 国外 的 软件 源 ， 在 国内 下载 较慢 ， 建议 使用 国内 的 镜像 ：           USTC ：   https : / / lug . ustc . edu . cn / wiki / mirrors / help / pypi         THU ：   https : / / mirrors . tuna . tsinghua . edu . cn / help / pypi /                     from       sklearn . datasets       import       load _ iris       import       pandas       as       pd       import       numpy       as       np       import       matplotlib . pyplot       as       plt       from       sklearn . tree       import       DecisionTreeClassifier         plt     .     style     .     use     (     &# 39 ; seaborn - talk &# 39 ;     )         iris       =       load _ iris     ( )       df       =       pd     .     DataFrame     (     iris     .     data     ,       columns     =     iris     .     feature _ names     )       df     [     &# 39 ; target &# 39 ;     ]       =       iris     .     target         depths       =       range     (     2     ,     10     )       errors       =       [ ]         & quot ; & quot ; & quot ; 下面 是 你 的 代码 ， 请 完成 功能 ：             1 .   用 深度 为 2 - 10 的 不同 决策树 分别 对 数据 进行 建模 ， 计算 每 一颗 决策树 的 预测 准确率 。 准确率 是 预测 正确 的 样本 数目   /   总 样本 数目 。             2 .   画出 深度 - 误差 的 折线图               Hint :                   1 .   sklearn 每 一个 模型 都 会 有 一个 ` predict ` 方法 ， 可以 用来 预测 结果 。                   2 .   matplotlib   的 画图 函数   ` plt . plot `   是 有用 的 画图 工具 。       & quot ; & quot ; & quot ;        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-ml-example.html"},
      
      
      {"title": "第01.1讲：python快速入门", "text": "    Table   of   Contents           关于           安装           WINDOWS 系统           MAC           Linux                   编码 工具           Hello   word           小 任务                   数据类型 与 变量           列表 、 集合 和 字典           列表 推导 与 字典 推导                   函数 和 类           科学计算 库 ： numpy           绘 图库 ： matplotlib           思考 与 实践                 关于       在 本 讲 中 ， 你 讲 快速 了解 python 的 基本 编程 语法 和 工具 。 本 教程 是 为了 给 机器 学习 算法 学习 提供 支持 的 ， 除了 讲解 python 语法 外 ， 还会 涉及 做 算法 相关 的 工具 。       安装       WINDOWS 系统       对于 windows 系统 ， 可以 从   官网 下载   ， 安装 过程 可以 参考   这个 教程         MAC       MAC 自带 有 python ， 如果 没有 可以 通过 下述 两种 方式 安装           和 window 系统 一样 ， 通过 官网 下载安装       通过 brew 安装     brew   install   python             Linux       linux 一般 自带 python ， 如果 没有 可以 通过 下述 两种 方式 安装           通过 官网 下载安装       通过 包 管理器 安装 ， Ubuntu     sudo   apt - get   install   python   ,   CentOS     yum   install   python             编码 工具       用 常用 的 带 语法 高亮 的 代码 编辑器 就行 ， windows 推荐     notepad ++   ， mac 推荐   atom 。       如果 喜欢 IDE 可以 用 VS 或者 IDEA 。 IDE 的 好处 是 有 代码 提示 ， 写 起来 方便 。       Python 还有 一个 神奇 ， 叫做 IPython 、 Jupyter ， 愿意 折腾 的 可以 试试 。       Hello   word       和 所有 编程语言 一样 ， 首先 来 实现 一个   Hello   word 程序 ， 将 下面 的 代码 保存 到   hello . py   文件 中               print     (     & quot ; Hello   word ! & quot ;     )                 然后 打开 终端 ， 进入   hello . py   所在 目录 ， 执行命令   python   hello . py   就 可以 看到 输出 结果 了 ！ 没错 ， 你 没有 看 错 ， 整个 文件 只有 这 一行 ！             Hello   word !               你 也 可以 使用 python 交互式 命令行 ， 在 学习 python 的 语法 时 很 方便 。 使用 方法 是 打开 终端 ( WINDOW   下 是 CMD 程序 ) ， 然后 输入 python 并 回车 即可 进入 交互式 环境             Python   2.7 . 10   ( default ,   Oct   23   2015 ,   19 : 19 : 21 )   [ GCC   4.2 . 1   Compatible   Apple   LLVM   7.0 . 0   ( clang - 700.0 . 59.5 ) ]   on   darwin   Type   & quot ; help & quot ; ,   & quot ; copyright & quot ; ,   & quot ; credits & quot ;   or   & quot ; license & quot ;   for   more   information .   & gt ; & gt ; & gt ; |               然后 输入 Hello   Word 程序 并 回车 即可 看到 效果               & gt ; & gt ; & gt ;       print     (     & quot ; Hello   word ! & quot ;     )       Hello       word     !                 小 任务       将 问候语 改为 问候 你 自己 的 名字 ， 例如   Hello   Jack !       数据类型 与 变量       和 很多 编程语言 一样 ， 如果 你 要 存储 数据 ， 都 需要 一些 基础 的 数据类型 。 python 有 很多 基础 数据类型 ： 整型 、 浮点数 、 字符串 、 布尔值 ( True ,   False ) 、 空型 ( None ) 。 例如 你 要 表达 一个 公司 有 多少 人 ， 那么 需要 一个 整数 ； 如果 你 要 表达 公司 的 盈利 ， 那么 需要 一个 浮点数 ； 如果 你 要 表达 一个 用户 是否 为 管理员 ， 那么 你 需要 一个 布尔 类型 ； 最后 如果 你 要 表达 什么 都 没有 ， 你 需要 空型 。               #   整数       v       =       123         #   浮点数       pi       =       3.14159         #   字符串 ， 可以 用 单引号 和 双引号 ， 两者 是 等价 的       s       =       &# 39 ; Hello   word &# 39 ;       s       =       & quot ; hello   word & quot ;         #   字符串 支持 格式化 ,   python2 . 7 版本 用 百分号 ， python3 用 format 函数       s       =       &# 39 ; v   =       % d     &# 39 ;       %       v       s       =       &# 39 ; v   =   { 0 } &# 39 ;     .     format     (     v     )         #   布尔值       v       =       True       if       v     :               print     (     &# 39 ; True &# 39 ;     )         #   空型       v       =       None                 列表 、 集合 和 字典       python 原生 支持 3 种 常用 的 集合 类型 ： 列表 、 集合 和 字典 。 列表 就是 很多 元素 的 集合 ， 集合 和 列表 一样 ， 只是 集合 要求 元素 互不 相同 ， 字典 是 key - value 结构 。               #   一个 列表       arr       =       [     1     ,     3     ,     5     ,     7     ,     9     ,       3     ,       5     ]         #   列表 遍历       for       i       in       arr     :               print     (     i     )       #   1   3   5   7   9   3   5         #   集合       s       =       set     (     arr     )       for       i       in       s     :               print     (     i     )       #   1   3   5   7   9         #   字典       d       =       {     &# 39 ; A &# 39 ;       :       1     ,       &# 39 ; B &# 39 ;       :       2     ,       &# 39 ; C &# 39 ;       :       4     }       for       k       in       d     :               print     (     k     ,       d     [     k     ] )                 列表 推导 与 字典 推导       python 对 集合 的 遍历 支持 更 方便 的 语法 ， 叫做 列表 推导 和 字典 推导 。               #   列表 推导       arr _ new       =       [     i     *     2       for       i       in       arr     ]       #   将 arr 中 每个 元素 乘以 2         #   字典 推导       d _ new       =       {     k       :       v     *     2       for       k     ,     v       in       d     .     items     ( ) }                 函数 和 类       python 通过 关键字   def   定义 函数 ， 通过 关键字 class 定义 类 ， self 代表 类 自己 ， 类似 于 其他 编程语言 中 的 this 。               #   乘以 2 的 函数       def       times _ two     (     x     ) :               return       x       *       2         #   定义 一个 类       class       TimesTwo     (     object     ) :               def       __ init __     (     self     ) :                       pass               def       times _ two     (     self     ,       x     ) :                       return       x     *     2                 科学计算 库 ： numpy       利用 numpy 可以 方便 地 进行 向量 和 矩阵 操作 。               import       numpy       as       np       x       =       np     .     array     ( [     1.0     ,     2.0     ,     3.0     ,     5.0     ] )       y       =       np     .     array     ( [     2.0     ,     3     ,     4     ,     5     ] )       print     (     np     .     dot     (     x     ,       y     ) )       #   计算 向量 x 和 y 的 内积                 绘 图库 ： matplotlib       利用 matplotlib 会 图库 可以 方便 地画 出 各种 图形 。               import       matplotlib . pyplot       as       plt       x       =       np     .     array     ( [     1.0     ,     2.0     ,     3.0     ,     5.0     ] )       y       =       np     .     array     ( [     2.0     ,     3     ,     4     ,     5     ] )         plt     .     plot     (     x     ,       y     )       plt     .     show     ( )                 思考 与 实践       对比 for 循环 方式 对 数组 迭代 和 用 numpy 的 速度 。               import       numpy       as       np       import       time         def       for _ loop     (     arr     ) :               & quot ; & quot ; & quot ; 实现 for 循环 将 数组 arr 里面 所有 元素 平方                 arr :   np . array               & quot ; & quot ; & quot ;               pass         def       np _ loop     (     arr     ) :               & quot ; & quot ; & quot ; 实现 numpy 将 arr 所有 元素 平方               arr :   np . array               & quot ; & quot ; & quot ;               pass             arr       =       np     .     random     .     rand     (     100000000     )       start       =       time     .     clock     ( )       for _ loop     (     arr     )       print     (     &# 39 ; time   for   function   for _ loop :   { : . 3f } &# 39 ;     .     format     (     time     .     clock     ( )       -       start     ) )         start       =       time     .     clock     ( )       np _ loop     (     arr     )       print     (     &# 39 ; time   for   function   np _ loop :   { : . 3f } &# 39 ;     .     format     (     time     .     clock     ( )       -       start     ) )        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-python.html"},
      
      
      {"title": "第02.0讲：决策树模型", "text": "    Table   of   Contents           关于           回顾 决策树           决策树 生成 算法           信息熵           数据量 解释           平均 信息量 解释                   信息 增益 准则                   决策树 的 几何 解释           决策树 的 可 解释性           思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 深入 理解 决策树 模型 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 决策树 模型 ？       构建 决策树 模型 的 算法 是 怎么 实现 的 ？           学习 本 讲 ， 希望 你           至少 有 高中数学 水平 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           本 讲 所用 的 数据 集 还是 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               回顾 决策树       在 上 一 讲 中 ， 我们 简单 了解 了 一下 决策树 的 基本概念 。 如下 图 所示 ， 是 我们 上 一 讲 通过 数据分析 ， 设计 出来 的 简单 规则 模型 对应 的 决策树 。 决策树 首先 是 一颗 树 ， 树由 很多 节点 构成 。 这些 节点 分为 两类 ， 中间 节点 （ 椭圆形 ） 和 叶子 节点 （ 方形 ） 。 中间 节点 代表 一条 规则 ， 叶子 节点 代表 模型 的 决策 输出 。 有 多少 个 叶子 结点 ， 就 代表 有 多少 条 规则 。 这个 决策树 实际上 代表 3 条 规则 ， 每条 规则 可以 用 一个   IF - THEN   条件 语句 表示 ：                   IF   花瓣 长度 ( petal   length )   & lt ;   2.8 ,   THEN   sentosa       IF   花瓣 长度 ( petal   length )   & gt ; =   2.8   AND   花瓣 长度 ( petal   length )   & lt ;   4.9 ,   THEN   versicolor       IF   花瓣 长度 ( petal   length )   & gt ; =   4.9 ,   THEN   virginica           这 三个 规则 ， 共同 定义 了 一个 分段 函数 ！       $ $     y   =     \ \ begin { cases }     0 ,   \ \ text { petal   length }   \ \ lt   2.8   \ \ \ \     1 ,   \ \ text { petal   length }   \ \ in   [ 2.8 ,   4.9 )   \ \ \ \     2 ,   \ \ text { petal   length }   \ \ ge   4.9     \ \ end { cases }     $ $       对于 决策树 ， 我们 先 定义 几个 基本概念 ， 便于 后面 表述 ：           子 节点 ： 和 节点 相连 的 后继 节点 ， 比如 节点 ( petal   length   & lt ;   4.9 ) 是 节点 ( petal   length   & lt ;   2.8 ) 的 子 节点       父 节点 ： 当前 节点 是子 节点 的 父 节点 。 比如 节点 ( petal   length   & lt ;   2.8 ) 是 节点 ( petal   length   & lt ;   4.9 ) 的 父 节点       边 ： 相连 相邻 两个 节点 中间 的 部分 叫边 ， 这条 边 有 方向 ， 从父 节点 指向 子 节点 。       中间 节点 ： 有子 节点 的 节点 ， 它 不 直接 输出 预测 结果 的 节点 ， 对应 一个 规则 。 图中 椭圆形 的 节点 都 是 中间 节点 。       叶子 节点 ： 没有 子 节点 的 节点 ， 直接 输出 预测 结果 的 节点 ， 对应 一个 复杂 的 规则 ， 通常 是 多个 规则 的 组合 。 图中 方形 的 节点 都 是 叶子 节点 。       根 节点 ： 没有 父 节点 的 中间 节点 。 节点 ( petal   length   & lt ;   2.8 ) 就是 上述 决策树 的 根 节点 。       路径 ： 从根 节点 出发 ， 沿着 子 节点 移动 ， 到达 叶子 节点 时所 经历 的 所有 节点 序列 就是 一条 路径 ， 图中 黄色 区域 表示 出 一条 路径 。 路径 上边 的 数量 叫做 路径 长度 ， 实际上 也 等于 中间 节点 的 数目 。       深度 ： 最大 的 路径 长度 ， 比如 上述 决策树 深度 为 2 .       规则 ： 一个 可以 判定 真假 的 表达式 就 叫 规则 ， 比如 花瓣 长度 ( petal   length )   & lt ;   2.8       组合 规则 ： 多个 规则 通过 且 ( AND ) 和 或 ( OR ) 连接起来 的 语句 叫做 组合 规则 ， 比如   花瓣 长度 ( petal   length )   & gt ; =   2.8   AND   花瓣 长度 ( petal   length )   & lt ;   4.9           决策树 生成 算法       在 前面 一讲 ， 我们 通过 数据 可视化 分析 ， 找到 了 针对 花瓣 长度 ( petal   length ) 的 3 个 规则 。 这些 规则 的 过程 能不能够 自动化 的 从 已经 标注 好 的 数据 中 找到 呢 ？ 如果 可以 的话 ， 那么 决策树 就 可以 自动化 地 生成 了 ， 不用 再 去 做 数据分析 了 。 答案 是 肯定 的 ， 这 就是 决策树 生成 算法 。       假设 我们 对 花瓣 长度 来 设计 一个 规则 ， 一个 规则 可以 看做 将 它 的 取值 划分 成 两个 区间 ， 我们 期望 这种 划分 能够 将 数据 划分 成 两个 更加 容易 区分 类别 的 集合 ， 这里 的 关键 是 找到 划分 的 分裂 点 的 值 。 划分 方法 很多 ， 最 简单 的 方法 是 ， 选择 每 一个 可能 分裂 点 进行 划分 ， 然后 找出 里面 最好 的 分裂 点 。               如图所示 ， 是从 所有 可能 的 分裂 中 随机 取 的 两个 不同 的 分裂 方式 ， 第一种 采用 的 分裂 点 是 2 ， 在 标注 类别 的 训练样本 中 ， 三个 类别 的 样本 数目 都 是 50 个 ， 通过 分裂 后 ， 落到 左边 子 节点 的 样本 数目 分别 是 [ 50 ,   0 ,   0 ] ， 即 只有 第 1 类 的 样本 ， 看起来 区分 性 还 不错 ， 把 第一类 完美 地 识别 出来 了 ； 落到 右边 子 节点 的 样本 数目 分别 是 [ 0 ,   50 ,   50 ] ， 第 2 类 和 第 3 类 暂时 还 无法 区分 ， 不过 不用 担心 ， 我们 可以 沿着 右子 节点 继续 分裂 下去 就 可能 把 这 两类 也 分开 来 。 第二种 分裂 方式 采用 的 分裂 点 是 4 ， 相比 于 第一种 分裂 方式 ， 它 把 11 个 第 2 类 样本 也 放到 左边 了 ， 看起来 区分 性 没 第一种 好 。 那么 问题 来 了 ， 怎么 衡量 一种 分裂 方式 比 另外 一种 好 呢 ？       信息熵       信息熵 可以 用来 度量 一个 概率分布 $ \ \ { p _ i ,   i = 1 , 2 , ... \ \ } $ 的 不 确定 度 ， 熵 的 定义 是       $ $     H ( \ \ { p _ i \ \ } )   =   - \ \ sum _ i   p _ i   \ \ log   p _ i     $ $       为什么 熵 可以 度量 不 确定 度 呢 ？ 我们 来 看看 最 简单 的 一个 例子 ， 假设 我们 抛 一枚 硬币 ， 如果 硬币 是 均匀 的 ， 那么 正面 和 反面 出现 的 概率 都 是 0.5 ， 我们 计算 一下 熵 $ H   =   -   0.5 \ \ log   0.5   -   0.5 \ \ log0 . 5 = \ \ log   2   =   1 $ （ 这里 为 方便 记 ， 对数 的 底取 为 2 ） 。 如果 这个 硬币 不 那么 均匀 ， 假设 正面 朝上 的 概率 为 0.9 ， 反面 朝上 的 概率 为 0.1 ， 我们 再 来 计算 一下 熵   $ H   =     -   0.9 \ \ log   0.9   -   0.1 \ \ log0 . 1 = 0.427 $ 。 熵 变小 了 ！ 直观 来看 是 不确定性 减小 了 ！ 因为 抛 一枚 均匀 的 硬币 ， 确实 很难 猜测 它 是 正面 还是 反面 ， 但是 如果 非常 不 均匀 的 硬币 ， 正面 朝上 的 概率 是 0.9 ， 那么 我们 有 很大 的 把握 猜测 它 是 正面 ！ 如果 我们 把 熵 H 随着 正面 朝上 概率 p 画 一个 函数 图像 ， 可以 看到 它 在 0.5 处取 最大值 ， 直观 理解 是 均匀 的 硬币 最难 猜 ， 熵 最大 ； 相反 在 p = 0 和 1 处取 最小值 ， 直观 理解 是 只有 一面 的 硬币 最好 猜 ， 熵 最小 ！               数据量 解释       信息熵 的 另外 一个 解释 是 ， 要 描述 一个 随机 事情 所 需要 的 最少 数据量 （ 比特 数 ） 。 比特 是 计算机 表达 数据量 的 一个 单位 ， 计算机 中 都 是 用 0 和 1 表示 数据 ， 1 个 这样 的 0 / 1 单元 就是 1 比特 。 所以 这句 话 也 可以 这样 理解 ， 如果 我要 用 计算机 存储 这样 一个 随机 事件 的 结果 ， 最少 要用 的 数据量 。 对于 一个 均匀 硬币 ， 我要 记录 结果 是 正面 还是 反面 ， 只要 用 0 表示 反面 ， 1 表示 正面 ， 所以 需要 1 个 比特 （ 恰好 等于 熵 ） 就 可以 记录 结果 了 。 但是 如果 一枚 非常 不 均匀 的 硬币 ， 正面 朝上 的 概率 为 1 ， 那么 我们 根本 不 需要 记录 就 可以 知道 它 的 结果 肯定 是 正面 ， 也 就是 需要 0 个 比特 ， 对应 的 熵 为 0 ！ 如果 正面 朝上 的 概率 是 0.9 呢 ？ 貌似 也 需要 1 个 比特 才能 记录 这个 事件 的 结果 ， 如果 只 记录 1 个 这样 的 事件 ， 确实 如此 。 如果 我们 要 记录 很多 个 这样 的 事件 时 ， 那么 我们 可以 利用 数据 压缩工具 对 结果 进行 压缩 ， 就 好像 我们 压缩 自己 电脑 上 一个 普通 文件 那样 。 这种 压缩 有个 下界 ， 信息论 之父 Shanon 告诉 我们 ， 这个 下界 就是 熵 ， 平均 一个 事件 至少 需要 H ( p ) 个 比特 的 数据 来 记录 ！       一个 简单 的 压缩 例子 ： 设想 正面 朝上 的 概率 是 0.9999 ， 记录 10000 个 这样 的 抛 硬币 事件 ， 因此 记录 的 数据 中 大约 只有 1 个 为 0 ， 其他 全部 是 1 。 如果 我们 直接 记录 原始数据 ， 就 需要 10000 个 比特 。 如果 我们 只 记录 这个 唯一 的 0 出现 的 位置 ， 所 需要 的 数据量 不会 超过 32 比特 （ 计算机 中 表示 一个 整数 所 需要 的 比特 数 ） 。 记录 数据量 被 压缩 了 近 300 倍 ！       平均 信息量 解释       定义 ： 如果 一个 事件 发生 的 概率 为 p ， 那么 这个 事件 的 信息量 为   $   -   \ \ log   p $ 。       例如 ， 概率 为 1 的 事件 信息量 为 0 ， 因为 它 肯定 会 发生 ， 所以 这件 事情 发生 了 没有 带来 任何 信息 。 如果 我 告诉 你 明天 太阳 会 从 东方 升起 ， 你 肯定 会 认为 这是 一句 废话 ， 没有 任何 信息量 。 从 概率 的 角度 来 解释 ， 因为 太阳 从 东方 升起 的 概率 几乎 就是 1 ， 那么 这句 话 带来 的 信息量 是   $   -   \ \ log   1   =   0 $ 比特 ！ 相反 ， 如果 一件 事情 发生 的 概率 非常 小 ， 几乎 等于 0 ， 如果 它 发生 了 ， 带来 的 信息量 就 非常 大 ， 因为 $   -   \ \ log   0   = \ \ infty   $ ！ 如果 我 告诉 你 一个 人 把 狗 咬 了 ， 那么 你 肯定 会 认为 背后 有 非常 多 事情 没 暴露 出来 ， 这个 消息 的 信息量 非常 大 ， 这 就是 为什么 狗 咬 人 不是 新闻 ， 人 咬 狗 才 是 新闻 ， 这 可以 用 信息量 解释 为 新闻 的 信息量 越大 ， 大家 越 关注 。       一个 概率分布 $ \ \ {   p _ 1 ,   p _ 2 ,   ... ,   p _ n \ \ } $ 对应 了 n 个 事件 ， 比如 投 一个 六面体 骰子 ， 对应 6 个 事件 ， 每个 事件 的 概率 是 1 / 6 ， 根据 信息量 的 定义 ， 每个 事件 的 信息量 就是   $   \ \ log   6 $ ， 这 6 个 事件 的 平均 信息量 等于       $ $     - \ \ sum _ i   p _ i   \ \ log   p _ i   =   - \ \ sum   \ \ frac { 1 } { 6 }   \ \ log   \ \ frac { 1 } { 6 }   =   \ \ log   6     $ $       这 正是 这个 概率分布 的 熵 ！     < ! - -   ####   信息 增量 解释   信息熵 的 减少 量 可以 解释 为 一个 事件 的 结果 带来 的 信息量 。 比如 ， 一枚 均匀 的 硬币 朝上 这个 结果 的 信息量 是 多少 呢 ？ 在 我们 不 知道 这个 结果 时 ， 朝上 和 朝 下 的 概率 都 是 0.5 ， 不 确定 度 H = 1 ； 在 我们 知道 朝上 这个 结果 后 ， 不 确定 度 变成 H = 0 了 ！ 所以 ， 这里 不 确定 度 ( 熵 ) 减少 量 是 1 比特 ， 实际上 就是 因为 “ 一枚 均匀 的 硬币 朝上 ” 这个 结果 带来 了 1 比特 的 信息量 。 信息量 就是 不确定性 ( 熵 ) 的 减少 量 ！   - - >       信息 增益 准则       利用 信息熵 ， 我们 可以 度量 每 一种 分裂 方法 的 好坏 。 因为 熵 的 意义 是 不 确定 度 ， 那么 我们 计算 分裂 前后 这种 不 确定 度 的 减少 量 ， 不 确定 度 的 减少 越 多 ， 信息量 越大 ， 那么 分裂 后越 容易 区分 每 一类 ， 所以 分裂 方式 就 越 好 。 基于 这个 思想 ， 我们 定义 分裂 带来 的 信息 增益       $ $     \ \ Delta   I   =   \ \ text { 分裂 前 的 不 确定 度 }   -   \ \ text { 分列 后 的 不 确定 度 }   \ \ \ \             =   H ( P )   -   [ r _ 1   H ( C _ 1 )   +   r _ 2   H ( C _ 2 ) ]     $ $       P 是 分裂 前 数据 集中 每 一类 的 概率分布 ， 在 上述 的 第一种 分裂 中 ， 分裂 前 三个 类别 的 样本 都 是 50 ， 所以 每 一类 都 是 1 / 3 的 概率 ， 所以 $ H ( P )   =   \ \ log   3   =   1.098 $ 。 C1 是 分裂 后分 到 左子 树 的 数据 中 每 一类 的 概率分布 ， 在 分裂 规则 petal   length & lt ; 2 下 ， 分到 左子 树 的 样本 有 50 个 ， 且 全部 是 第一类 ， 所以 三个 类别 的 概率分布 是 $ \ \ { 1 ,   0 ,   0   \ \ } $ ， $ H ( C _ 1 )   =   1 \ \ log1   +   0 \ \ log   0   +   0 \ \ log0   =   0 ( 0   \ \ log   0   =   0 ) $ 。 $ r _ 1 $ 是 左子 树 的 样本 占 分裂 前 的 比例 ， $ r _ 1   =   50 / 150 = 1 / 3 $ 。 同样 ， 分到 右子 树 的 样本 分别 是 [ 0 ,   50 ,   50 ] ， 所以 $ H ( C _ 2 )   =   2 / 3 \ \ log   2   =   0.462 ,   r _ 2 = 2 / 3 $ 。 因为 分裂 后 有 两个 样本 集合 ， 所以 需要 把 这 两个 集合 的 不 确定 度 平均 一下 ， 平均 的 方式 是 按照 样本数 目的 加权 平均 。 根据 信息 增益 公式 ， 可以 得到   $ \ \ Delta   I   =   1.098   -   [ 1 / 3   *   \ \ log   0   +   0.462 * 2 / 3 ]   =   0.79 $ 。       类似 的 ， 我们 可以 计算 第二种 分裂 方式 的 信息 增益 ， 按照 上述 逻辑 计算结果 为 $ \ \ Delta   I   =   0.500 $ 。 这 表明 第二种 分裂 方式 的 信息 增益 没有 第一种 好 ， 直观 解释 就是 第二种 分裂 方式 带来 的 信息量 没有 第一种 多 ， 这 与 我们 直观 感受 一致 。       信息量 解释 ： 不 确定 度 的 减少 就是 信息量 ， 比如 在 抛 一枚 均匀 的 硬币 时 ， 知道 结果 之前 ， 不 确定 度为 1 比特 ， 知道 结果 后 ， 不 确定 度为 0 ， 不 确定 度 减少 量 是 1 比特 ， 也 就是 这个 结果 带来 的 信息量 。 信息 增益 是 分裂 前后 不 确定 度 的 减少 量 ， 所以 实际上 就是 分裂 带来 的 信息量 。       利用 信息 增益 准则 ， 我们 可以 设计 出 最佳 分裂 点 算法 ， 对于 某个 特征 ， 在 所有 可能 的 分裂 点 计算 分裂 的 信息 增益 ， 信息 增益 最大 的 分裂 点 就是 最佳 的 分裂 点 ！ 可能 的 分裂 点 可以 将 该 特征 所有 取值 按照 顺序排列 $ \ \ { z _ 1 ,   z _ 2 ,   .. ,   z _ k ,   ... , z _ n \ \ } $ ， 任何 两个 值 的 平均值 $   ( z   _   i   +   z   _   { i + 1 } ) / 2 $ 都 可以 作为 候选 分裂 点 。       最后 一个 问题 ， 这么 多个 特征 我们 应该 先 选择 那个 特征 分裂 呢 ？ 只有 先 选择 好 分裂 那个 特征 ， 才能 利用 上面 的 步骤 选出 最佳 分裂 点 。 方法 也 很 简单 ， 我们 对 每 一个 特征 $ x _ i $ 都 计算 它 的 最佳 分裂 点 $ t _ i $ 和 最大 信息 增益 $ \ \ Delta   I _ i $ ， 然后 将 信息 增益 最大 的 特征 $ i   ^   *   $ 作为 当前 最佳 分裂 特征 即可 。 因此 ， 决策树 生成 算法 中 每 一次 进行 分裂 的 过程 可以 归纳 为 如下 步骤 ：           计算 分裂 前 的 信息熵 H ( P )       对 每 一个 特征 的 每 一个 可能 的 分裂 点 ， 计算 分裂 后 的 信息 增益 ， 找到 最大 增益 的 特征 $ x _ i $ 和 分裂 点 $ t _ i $       得到 分裂 规则   $ x _ i   & lt ;   t _ i $   和 最大 信息 增益           利用 上述 分裂 算法 ， 可以 很 容易 得到 决策树 生成 算法 ， 从 训练 集合 开始 ， 运行 上述 分裂 算法 找到 一个 最佳 分裂 规则 ， 将 集合 分裂 成 两个 集合 。 如下 图 所示 ， 一 开始 训练 集中 3 类 数据 都 是 50 个 样本 ， 分裂 算法 找到 的 当前 最佳 分裂 规则 是 ( petal   length & lt ; 2.45 ) 。 这个 规则 将 训练 集 分为 两个 规则 B 和 C ， 集合 B 中有 50 个 样本 ， 全是 第一类 ， 集合 C 中有 100 个 样本 ， 其中 50 个 是 第 2 类 ， 50 个 是 第 3 类 。 接着 ， 对 集合 B 和 C 继续 分别 运行 分裂 算法 ， 相当于 将 B 看做 新 的 训练 集合 ， 生成 左子 树 ； 将 C 看做 新 的 训练 集合 ， 生成 右子 树 。 将 这个 步骤 不断 迭代 下去 ， 直到 达到 某个 准则 为止 。 例如 在 第一次 分裂 后 ， 分 到 左边 的 B 集合 里面 只有 一类 ， 已经 没有 不确定性 了 ， 没有 必要 继续 分裂 ， 这 就是 第一个 停止 准则 ： 如果 集合 只有 一类 样本 ， 那么 就 停止 分裂 。 而 分到 右边 的 C 集合 没有 满足 停止 准则 ， 所以 把 C 集合 看做 一个 新 的 训练 集 ， 运行 分裂 算法 找到 最佳 分裂 规则 为 ( petal   width & lt ; 1.75 ) ， C 集合 继续 分裂 成 两个 子集 ， 这 两个 子集 满足 第 2 个 停止 准则 ： 如果 决策树 的 深度 达到 某个 阈值 ， 那么 就 停止 分裂 。 这里 我们 要求 深度 不能 超过 2 ， 所以 这 两个 子集 就 停止 分裂 了 。 除了 这 两个 准则 外 ， 还有 一些 准则 ， 比如 每个 叶子 节点 的 样本 数目 不能 少于 10 个 ， 因为 叶子 节点 上 样本 数目 太 少 ， 那么 满足 这 条 组合 规则 的 样本 太少 了 ， 这 说明 这 条 规则 没有 代表性 。 叶子 节点 上 类别 最多 的 类别 ， 作为 在 输入 满足 这个 叶子 节点 对应 规则 时 ， 模型 的 输出 结果 。 例如 ， 最 左边 的 叶子 节点 全是 第一类 ， 因此 这个 叶子 节点 预测 的 结果 是 第一类 ， 也就是说 如果 输入 满足 规则 ( petal   length & lt ; 2.45 ) ， 那么 模型 的 预测 结果 就是 第一类 。 如果 所有 的 分裂 都 达到 停止 准则 了 ， 那么 一棵 决策树 就 生成 好 了 ， 然后 就 可以 用 这个 决策树 模型 对 未知 数据 进行 预测 啦 ！       决策树 生成 算法 步骤 总结 如下 ：           初始化 A = 训练 集       如果 集合 A 的 数据量 小于 一个 阈值 或者 只有 一类 样本 ， 停止 分裂 。 否则 进入 第 3 步 。       对 集合 A ， 依次 选取 第 i 个 特征 进行 分裂 ， 利用 分裂 算法 计算 该 特征 的 信息 增益 和 分裂 规则 ， 选取 信息 增益 最大 的 特征 作为 当前 分裂 特征 ， 对应 的 分裂 规则 作为 当前 的 分裂 规则 。       利用 第 3 步 生成 的 分裂 规则 ， 将 集合 A 划分 为 两个 子集 B 和 C 。 对 这 两个 子集 继续 应用 生成 算法 生成 左子 树 和 右子 树 。                   生成 算法 的 每 一个 分裂 步骤 可以 看做 对 所有 的 可能 的 分裂 规则 进行 穷举 ， 在 鸢尾花 识别 这个 任务 中 ， 特征 有 4 个 ， 假设 在 一 开始 分裂 的 时候 ， 每个 特征 有 100 个 可能 的 分裂 点 ， 那么 就 有 400 个 可能 的 分裂 方式 ， 生成 算法 就是 穷举 这 400 个 分裂 规则 ， 利用 信息 增益 准则 来 评估 这些 规则 ， 找出 最佳 规则 作为 决策树 的 规则 ！ 这个 方法 看起来 很 笨 ， 不像 我们 可以 观察 一下 分布图 就 知道 最佳 分裂 点 了 ， 但是 这是 目前 的 计算机 最 有效 的 处理 方式 了 。       决策树 的 几何 解释       在 前面 ， 我们 一直 把 决策树 看做 很多 规则 构成 的 分段 函数 。 接下来 ， 我 将 介绍 决策树 的 一个 直观 的 几何 解释 。 如果 我们 用 决策树 分类 的 特征 做 为 坐标轴 ， 例如 我们 以 花瓣 长度 ( petal   length ) 为 横轴 ， 花瓣 宽度 ( petal   width ) 为 纵轴 。 我们 将 数据 绘制 于 这样 一个二维 平面 中 ， 如下 图 所示 ， 每 一个 类别 的 数据 都 用 不同 的 颜色 表示 出来 。 上述 的 决策树 的 第一个 规则 是 ( petal   length & lt ; 2.45 ) ， 这 相当于 用图 中 垂直于 横轴 的 绿色 虚线 将 这个 平面 分割 为 左右两半 ， 左边 对应 于 ( petal   length & lt ; 2.45 ) ， 而 右边 对应 于 ( petal   length & gt ; 2.45 ) ； 第二个 基本 规则 是 ( petal   width & lt ; 1.75 ) ， 因为 这个 规则 是 在 ( petal   length & gt ; 2.45 ) 条件 下 应用 的 ， 所以 相当于 用图 中 平行 于 横轴 的 红色 虚线 将 前面 分 隔开 的 右半 空间 继续 分割 为 两 部分 ， 上边 对应 于 组合 规则 ( petal   length & gt ; 2.45   & amp ; & amp ;   petal   width & gt ; 1.75 ) ， 下边 对应 于 组合 规则 ( petal   length & gt ; 2.45   & amp ; & amp ;   petal   width & lt ; 1.75 ) 。 经过 这 两个 划分 ， 分别 形成 3 个 区域 ， 每个 区域 对应 一个 组合 规则 ， 同时 也 对应 决策树 的 一个 叶子 节点 ！ 从 几何 上 ， 可以 直观 地 看出 ， 这个 决策树 确实 将 这三类 数据 区分 开 了 ！               因此 ， 从 几何 上 来看 ， 决策树 可以 看做 对 特征 构成 的 空间 进行 划分 ， 划分 的 边界对应 于 特征 的 分裂 点 ， 划分 后 形成 的 每 一个 区域 与 决策树 的 叶子 节点 一一对应 ！       决策树 的 可 解释性       前面 我们 说 到 ， 机器 学习 就是 在 寻找 一个 可以 描述 数据 规律 的 函数 ， 复杂 的 函数 基本上 是 个 黑盒子 ， 很难 被 人 理解 。 决策树 是 其中 为数不多 可 解释性 高 的 模型 ， 因为 它 就是 很多 规则 嘛 ！ 当 一个 决策树 模型 训练 好 了 ， 我们 可以 将 决策树 模型 画 出来 。 在 预测 时 ， 我们 根据 样本 的 特征 可以 知道 这个 样本 是 被 决策树 的 哪条 规则 命中 ， 从而 得到 预测 结果 的 。 例如 ， 如果 一个 样本 满足 规则 ( petal   length & lt ; 2.45 ) ， 那么 模型 的 预测 结果 是 山 鸢尾 。 如果 有人 问 你 模型 怎么 判断 的 ， 那么 你 可以 很 肯定 的 告诉 他 ， 模型 是 根据 这 朵花 的 花瓣 长度 小于 2.45 厘米 来 判断 的 。 这个 好用 的 性质 并 不是 所有 模型 都 具备 ， 例如 后面 要 讲 到 的 神经网络 ， 可 解释性 就 很 低 。       思考 与 实践           证明 对于 任何 一种 分裂 方式 ， 信息 增益 非负 。       实现 上述 基于 信息 增益 的 决策树 生成 算法 ， 启动 代码 已 提供                   from       sklearn . datasets       import       load _ iris       import       numpy       as       np       from       scipy       import       stats         def       H     (     arr     ) :               & quot ; & quot ; & quot ; 计算 熵 的 函数 & quot ; & quot ; & quot ;               n       =       sum     (     arr     )               arr       =       [     1.0     *     i     /     n       for       i       in       arr     ]               return       sum     ( [     -       p       *       np     .     log     (     p     )       for       p       in       arr       if       p       & gt ;     0     ] )         def       InformactionInc     (     x     ,       y     ,       split     ) :               & quot ; & quot ; & quot ; 计算 利用 split 分裂 特征 x 时 ， 信息 增益               x   :   np . array   特征 数组 ,   size =   ( 样本数 ,   )               y   :   np . array   类别 ，   size = ( 样本数 ,   )               split   :   double   候选 分裂 点 的 值                 return   :   信息 增益               & quot ; & quot ; & quot ;               delta       =       0                 #   Your   Code                 #   end                 return       delta         def       FindSpliter     (     X     ,       y     ) :               & quot ; & quot ; & quot ; 寻找 最佳 分裂 特征 和 分裂 点 算法               X   :   np . array   特征 ， size   =   ( 样本数 ,   特征 数 )               y   :   np . array   类别 ， size   =   ( 样本数 ,   )               return   ( 分裂 特征 索引   i ,   分裂 点 的 值   t )               & quot ; & quot ; & quot ;               n _ samples       ,       n _ features       =       X     .     shape                 max _ infor _ inc       =       0               best _ feature       =       0               best _ split _ value       =       None                 #   Your   Code                 #   end                 return       best _ feature     ,       best _ split _ value           def       TreeGenerate     (     X     ,       y     ,       depth     =     0     ,       max _ depth     =     3     ) :               & quot ; & quot ; & quot ; 决策树 生成 算法 ， 返回 一棵树               & quot ; & quot ; & quot ;               #   满足 不 分裂 准则               if       X     .     shape     [     0     ]       & lt ;       10       or       np     .     unique     (     y     )     .     shape     [     0     ]       = =       1       or       depth       & gt ; =       max _ depth     :                       node       =       {     &# 39 ; prediction &# 39 ;       :       stats     .     mode     (     y     ) [     0     ] [     0     ] }                       return       node                 #   继续 分裂               best _ feature     ,       best _ split _ value       =       FindSpliter     (     X     ,       y     )               mask       =       X     [ : ,       best _ feature     ]       & lt ;       best _ split _ value               node       =       {                       &# 39 ; left &# 39 ;       :       TreeGenerate     (     X     [     mask     ,       : ] ,       y     [     mask     ] ,       depth     +     1     ) ,                       &# 39 ; right &# 39 ;       :       TreeGenerate     (     X     [     ~     mask     ,       : ] ,       y     [     ~     mask     ] ,       depth     +     1     )               }               return       node         def       TreePredict     (     X     ,       T     ) :               & quot ; & quot ; & quot ; 实现 决策树 预测 算法 & quot ; & quot ; & quot ;               pass           iris       =       load _ iris     ( )       T       =       TreeGenerate     (     iris     .     data     ,       iris     .     target     )       print     (     T     )         print     (       &# 39 ; ACC :   { } &# 39 ;     .     format     (     np     .     mean     (     TreePredict     (     X     ,       T     )       = =       iris     .     target     ) )       )        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-dt.html"},
      
      
      {"title": "第03.0讲：机器学习建模实战", "text": "    Table   of   Contents           关于           分类 准确率           交叉 验证           过 拟合 与 决策树 减枝           过 拟合 的 方差 偏差 解释                   特征 工程           类别 特征           缺失 值                   决策树 的 其他 分裂 准则           信息 增益 率           基尼系数                   复现 代码           过 拟合 现象 代码                   思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 深入 理解 机器 学习 建模 的 整个 过程 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 过 拟合 与 泛化 ？ 为什么 要 划分 训练 集 、 测试 集 、 验证 集 。       什么 是 交叉 验证 ？       如何 评估 一个 分类 模型 的 好坏 ？       方差 - 偏差 分解       决策树 的 减枝 与 泛化           学习 本 讲 ， 希望 你           至少 有 高中数学 水平 。       了解 决策树 ， 如果 还 不 了解 ， 可以 参看   决策树 模型         如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           本 讲 所用 的 数据 集 还是 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               分类 准确率       在 上 一 讲 中 ， 我们 说 到 决策树 生成 算法 ， 每 一步 都 是 穷举 所有 可能 的 分裂 规则 ， 利用 信息 增益 准则 找到 最佳 规则 ， 将 最佳 规则 加入 决策树 中 ， 持续 这个 步骤 知道 满足 停止 准则 。 在 前面 ， 我们 介绍 了 三个 停止 准则 ， 分别 是 ： 如果 集合 中 只有 一类 样本 ， 就 停止 分裂 ； 如果 集合 中 样本 个数 少于 某个 阈值 ， 就 停止 分裂 ； 如果 树 的 深度 达到 某个 阈值 ， 就 停止 分裂 。 显然 ， 如果 停止 准则 不同 ， 那么 我们 得到 的 决策树 也 不同 ， 这么 多 决策树 如何 评估 哪 一个 好 ， 哪 一个 不好 呢 ？       以 上述 鸢尾花 分类 任务 为例 ， 分类 得 好 和 坏 可以 通过 分类 转确 率来 衡量 ， 分类 准确率 = 分类 正确 的 样本 数目 / 总 样本 数目 。 如下 图 所示 ， 分别 是 3 棵 决策树 ， 深度 分别 为 0 、 1 、 2 ， 叶子 节点 中 的 3 个 数字 分别 代表 训练样本 中 3 类花 的 样本 数目 。 第 一棵 决策树 只有 一个 叶子 节点 ， 也 就是 对 所有 样本 都 预测 同一个 值 （ 训练样本 中 最 多 的 那 一类 ， 在 这个 例子 中 3 类 样本 一样 更 多 ， 任意 一个 预测 类别 都 一样 ） ， 假设 预测 为 第 0 类 ， 那么 准确率 只有 1 / 3 。 第二 棵 决策树 深度 为 1 ， 根据 花瓣 长度 是否 小于 2.45 将 特征 空间 划分 为 两 部分 ， 每 一部分 对应 一个 叶子 节点 ， 左边 的 叶子 节点 预测 为 第 0 类 ， 而 右边 的 叶子 节点 预测 为 第 1 类 （ 由于 第 1 类 样本 和 第 2 类 样本 一样 多 ， 所以 预测 为 第 1 类 和 第 2 类 效果 是 一样 的 ） 。 因此 ， 分到 左子 树 的 50 个 样本 都 会 预测 为 第 0 类 ， 都 预测 准确 ， 而 分到 右子 树 的 100 个 样本 只有 一半 预测 准确 ， 总 的 预测 准确 样本数 是 100 ， 准确率 为 2 / 3 。 随着 决策树 深度 的 加深 ， 训练样本 预测 的 准确率 会 越来越 高 ， 知道 每个 叶子 节点 上 都 只有 一类 样本 ， 那么 训练样本 的 预测 准确率 将 达到 100 % ！ 如果 以 训练 集上 的 准确率 为 评估 指标 ， 那么 显然 越深 的 决策树 预测 效果 越 好 。 而且 可以 想象 ， 对 任何 一个 训练 集 ， 总 可以 不断 地 将 决策树 加深 ， 直到 每个 叶子 节点 上 都 只有 一类 样本 。 那么 ， 训练 集上 的 准确率 越高 是否 就 代表 模型 的 预测 能力 越好 呢 ？               交叉 验证       我们 前面 讲过 ， 机器 学习 就 是从 数据 中 自动 发现 一些 有 意义 的 规律 。 这些 规律 是 有 意义 的 ， 意味着 可以 用来 指导 实践 ， 用来 对 没有 见到 过 的 数据 进行 预测 。 以 鸢尾花 为例 ， 我们 希望 通过 机器 学习 从 已经 观测 到 的 那 150 个 样本 ， 找到 一些 关于 4 个 属性 与 类别 的 规律 ， 这样 我们 就 可以 通过 这 4 个 属性 对 没有 见 过 的 鸢尾花 预测 它 的 类别 了 。 我们 用 X 表示 用来 预测 的 属性 ， 也 就是 特征 ， 用 Y 表示 预测 的 目标 类别 ， 也 就是 标签 ， 那么 在 所有 可能 的 数据 中 ， 可以 用 一个 联合 概率分布 P ( X ,   Y ) 来 表示 X 和 Y 的 概率 关系 ， 它 的 意义 是 在 所有 的 数据 中 特征 等于 X 且 类别 为 Y 的 概率 。 这些 所有 可能 的 数据 构成 的 集合 我们 称作 总体 ， P ( X ,   Y ) 就是 总体 的 概率分布 ， 已经 观测 到 的 数据   D = { ( x , y ) |   ( x , y ) ~ P ( X ,   Y ) }   只是 总体 的 一个 采样 结果 ， 它们 只是 总体 很少 的 一部分 并且 通常 假设 这些 样本 是 独立 选取 的 。 你 可以 理解 为 1 个 样本 是否 选取 与 其他 样本 没有 关系 ， 因此 样本 间 统计 独立 ， 又 因为 他们 都 来自 同一个 总体 ， 所以 他们 是 独立 同 分布 ( i . i . d ) 的 ！ 机器 学习 的 目标 就 是从 这 有限 的 观测 数据 D 中 学习 到 关于 总体 的 规律 。               因此 ， 我们 当然 不 希望 模型 学到 的 只是 训练 集上 看到 的 有限 的 规律 ， 甚至 只有 少数 样本 表现 出来 的 假 规律 。 这 也 是因为 机器 学习 采用 的 是 不 完全 归纳法 ， 所以 存在 被 经验 误导 的 可能性 ， 我们 希望 模型 学到 这种 错误 规律 的 风险 尽可能 小 ， 这样 模型 才 是 有 价值 的 ， 所以 我们 应该 在 全量 的 总体 上来 评估 模型 的 效果 才 靠 谱 。 以 鸢尾花 任务 为例 ， 确切 来说 就是 要求 模型 在 未知 的 来自 同一个 总体 的 样本 上 ， 分类 准确率 尽可能 高 。 如果 我们 用 函数 h ( x ) 表示 训练 好 的 模型 ， 对于 样本 的 特征 为 x ， 模型 的 预测 结果 $ \ \ hat { y }   =   h ( x ) $ 。 用 示性 函数 $ I ( h ( x )   =   y ) $ 表示 模型 是否 预测 正确 ， 预测 正确 结果 为 1 ， 预测 错误 结果 为 0 . 那么 模型 在 训练 集上 的 准确率 为       $ $     \ \ hat { P }   _   c   =   E _ { ( x ,   y )   \ \ sim   D }   I ( h ( x )   =   y )     $ $       上述 准确率 并 不是 一个 评估 模型 效果 好坏 的 指标 ， 而 应该 用 模型 在 总体 上 的 准确率       $ $     P   _   c   =   E _ { ( x ,   y )   \ \ sim   P ( x ,   y ) }   I ( h ( x )   =   y )     $ $       这 两个 准确率 的 唯一 区别 就是 求 期望 是 在 观测 数据 上 还是 在 总体 上 计算 的 结果 。 在 总体 上 的 期望值 才 有 意义 ， 才能 表达 模型 在 未知 数据 上 的 预测 效果 。 但是 ， 这个 期望值 我们 无法 求 ， 因为 总体 对 我们 来说 是 未知 的 ， 那 怎么办 呢 ？ 一个 简单 的 方法 是 ， 将 观测 数据 划分 成 两 部分 ， 一部分 用来 训练 模型 ， 而用 另外 一部分 计算 准确率 作为 模型 在 总体 上 准确率 的 估计值 。 这样 划分 的 两个 集合 我们 叫做 训练 集 和 测试 集 ， 在 训练 集上 训练 模型 ， 而 在 测试 集上 评估 模型 的 效果 。       有 了 测试 集后 ， 就 可以 用 测试 集上 的 预测 效果 作为 模型 在 总体 上 的 预测 效果 的 一个 较 好 的 估计 。 前面 说 过 ， 在 训练 集上 ， 深度 越深 的 决策树 准确率 约 高 ， 但是 在 测试 集上 的 效果 就 不见得 。 那么 我们 能 不能 通过 测试 集上 的 效果 来 选择 最佳 的 模型 呢 ？ 答案 是 不能 的 ！ 模型 的 建立 过程 中 不能 涉及 到 任何 测试 集上 的 信息 ， 否则 测试 集上 的 评估 结果 就 不能 很 好 地 反应 出 模型 真实 的 性能 。 为了 从 很多 模型 中 选择 一个 最好 的 ， 我们 还 需要 将 训练 集 继续 划分 成 训练 集 和 验证 集 ！ 用 训练 集 训练 多个 模型 ， 用 验证 集 选出 最好 的 一个 模型 ， 测试 集 只能 用于 评估 ！ 但是 在 实际 建模 任务 中 ， 我们 会 用 真实 的 未 观测 数据 来 评估 模型 ， 比如 训练 好 一个 推荐 的 模型 ， 上线 之后 运行 一段时间 来 验证 实际效果 ， 因此 也 有 只 将 训练 集 划分 成 训练 集 和 验证 集 两个 集合 的 做法 ， 而 用线 上 效果 来 评估 模型 。 而 在 学术论文 当中 ， 为了 与 同行 比较 效果 ， 那么 就 需要 划分 成 3 个 集合 ， 用 验证 集 选择 模型 ， 而用 测试 集 报告 本 论文 方法 的 效果 与 同行 进行 比较 。 曾经 有 学者 在 选择 模型 用到 了 测试 集 的 数据 ， 最后 被 发现 了 ， 这是 严重 的 学术 不端 行为 ！       将 训练 集 划分 成 训练 集 和 验证 集 选择 模型 的 方法 也 叫做 交叉 验证 ( Cross   Validate ) ， 交叉 验证 还有 一些 其他 方法 ， 这里 再 介绍 两种 ： k 折叠 和 留 一法 。 k 折叠 是 为了 解决 训练 集 数目 少 的 问题 ， 如果 训练 集 数目 很大 ， 上述 简单 的 划分 就 可以 了 ， 但是 如果 训练 集较 小 ， 某 一次 划分 带来 的 统计 波动 很大 ， 使得 这种 验证 方法 不 稳定 。 为了 解决 这个 问题 ， 可以 将 训练 集 随机 划分 成 k 个 相等 的 集合 ， 用 其中 k - 1 个 集合 训练 模型 ， 而用 剩下 一个 计算 评估 指标 ， 但是 这个 评估 指标 并 不是 最终 选择 模型 的 指标 。 选择 评估 集合 可以 有 k 种 不同 的 选择 方式 （ k 个 集合 任何 一个 都 可以 作为 评估 集合 ） ， 因此 可以 用 相同 的 超 参数 （ 我们 将 决策树 深度 、 每个 叶子 节点 最少 样本 数目 等等 这种 在 训练 模型 之前 就 需要 确定 的 参数 叫做 模型 的 超 参数 ） 训练 k 次 ， 可 得到 k 个 准确率 ， 然后 用 这 k 个 准确率 的 平均值 作为 最终 的 评估 指标 来 选择   不同 的 超 参数   。               留一法 可以 看做 k 等于 训练样本 数目 的 特殊 情况 ， 也 就是 每次 只 留下 一个样 本来 评估 。 在 k 折叠 交叉 验证 中 ， k 越大 ， 那么 评估 集上 估计 的 统计 误差 就 越 小 ， 因为 评估 指标 是 k 个 评估 结果 的 平均值 ！ 但是 计算 消耗 的 资源 就 越 多 ， 留一法 是 评估 指标 最 接近 总体 上 的 评估 指标 的 ， 但是 计算 消耗 的 资源 也 最 多 ， 一般 应 根据 训练 集合 的 大小 来 选择 合适 的 k 值 ， 一般 k 取 3 到 10 是 比较 合理 的 。       以 鸢尾花 任务 为例 ， 为了 选择 最佳 的 决策树 深度 这个 超 参数 ， 我们 可以 利用 5 折叠 交叉 验证 。 对 每 一个 深度 ， 利用 5 折叠 交叉 验证 计算 出 k 折叠 平均 准确率 ， 平均 准确率 最高 的 那个 深度 值 就是 最佳 的 ！ 然后 我们 可以 将 深度 设置 为 这个 最佳值 ， 在 全量 训练样本 中 重新 训练 决策树 模型 ， 作为 最终 的 预测 模型 ！       过 拟合 与 决策树 减枝       将 数据 集 划分 成 训练 集合 测试 集 ， 可以 评估 训练 集上 训练 的 模型 的 好坏 。 训练 集上 效果 越好 并 不 代表 在 测试 集上 的 效果 越 好 ， 当然 也 不 代表 在 总体 上 的 效果 越 好 。 下图 是 在 鸢尾花 任务 中 ， 将 数据 按照 8 : 2 的 比例 随机 划分 为 训练 集 和 测试 集 ， 限制 决策树 的 最大 深度 为 不同 值时 ， 训练 集 ( train ) 和 测试 集 ( test ) 上 的 误差 变化 。 随着 深度 的 增加 ， 训练 集上 的 误差 逐渐 降低 至 0 ， 而 测试 集上 的 误差 先变 低后 变高 并 发送 波动 （ 实际效果 跟 划分 结果 有关 ） 。 而 一般 的 数据 集中 ， 测试 集上 的 效果 会 随着 训练 集 误差 降低 反而 增加 ！ 这种 训练 集 和 测试 集 效果 不 一致 的 现象 ， 我们 说 模型 发生 了 过 拟合 现象 。 模型 过度 拟合 了 训练 集 ， 但是 在 未 见 过 的 测试 集上 的 效果 随 训练 集 拟合 精度 提高 反而 下降 了 ！               过 拟合 是 机器 学习 建模 中 经常 遇到 的 问题 ， 过 拟合 现象 可以 通过 交叉 验证 发现 ， 同时 通过 限制 模型 复杂度 等 措施 在 一定 程度 上 降低 过 拟合 的 程度 。 例如 可以 限制 决策树 深度 、 每个 叶子 节点 上 的 样本 数目 等 措施 ， 减少 模型 过 拟合 风险 。 此外 还 可以 通过 对 决策树 减枝 的 方法 解决 过 拟合 问题 。 事实上 ， 通过 限制 叶子 节点 上 最少 样本 数目 就是 在 做 减枝 ， 防止 决策树 过度生长 ， 这种 在 生成 决策树 过程 中 减少 决策树 的 分支 的 方法 叫做 预减 枝 。 也 可以 先 让 决策树 充分 生长 ， 然后 测试 决策树 每个 分支 上 的 最后 一次 分裂 是否 有 足够 大 的 信息 增益 ， 如果 没有 就 合并 这 两个 叶子 节点 ， 这种 方法 叫 后 减枝 。 这 两种 减枝 的 方法 都 是 通过 限制 模型 复杂度 减少 决策树 过 拟合 的 风险 。       关于 模型 选择 有个 奥卡姆 剃刀 原理 ， 这个 原理 说 如果 两个 模型 都 能 解释 数据 ， 那么 应该 选择 最 简单 的 那 一个 。 用过 拟合 来 解释 就是说 更 复杂 的 那个 模型 有 更 高 的 过 拟合 风险 ， 因此 如果 模型 效果 没有 明显 提升 的话 ， 不 应该 选择 更 复杂 的 模型 。       过 拟合 的 方差 偏差 解释       根据 偏差 - 方差 分解 ， 模型 的 预测 误差 可以 分解 为 偏差 和 方差 。 对 给定 的 待 预测 样本 $ x $ ， 估计 出来 的 预测 函数 $ \ \ hat { f } $ 会 随着 训练 集 的 不同 而 改变 ， 是 一个 随机变量 。 假设 实际 的 关系 是 $ y   =   f ( x )   +   \ \ epsilon $ ， $ \ \ epsilon $ 是 噪声 随机变量 ， 所以 $ y $ 也 是 一个 随机变量 ， 但是 $ f ( x ) $ 是 常数 。 那么 模型 预测值 $ \ \ hat { f } $ 和 实际 值 $ y $ 之间 的 期望 误差 可以 分解 为       $ $     \ \ begin { align }     E [ ( y   -   \ \ hat { f } ( x ) ) ^ 2 ]   & amp ; =   E [ y ^ 2   +   \ \ hat { f } ^ 2   -   2y \ \ hat { f } ]   \ \ \ \     & amp ; = Var [ y ] +   Var [ \ \ hat { f } ]   + [ Ey ] ^ 2   + [ E \ \ hat { f } ] ^ 2   -   2   f   E \ \ hat { f }   \ \ \ \     & amp ; =   Var [ y ]   +   Var [ \ \ hat { f } ]   +   ( f   -   E \ \ hat { f } ) ^ 2   \ \ \ \     & amp ; =   \ \ sigma ^ 2   +   Var [ \ \ hat { f } ]   +   Bias [ \ \ hat { f } ] ^ 2     \ \ end { align }     $ $       误差 可以 分为 三项 ， 第一项 是 该 问题 由于 信息 缺失 等 问题 带来 的 固有 误差 ， 无法 消除 ； 第二项 是 $ \ \ hat { f } $ 因为 训练 集 选取 的 不同 所 带来 的 统计 涨落 误差 ， 称为 方差 ； 第三项 是 把 所有 可能 的 训练 集都 训练 一遍 ， 得到 的 函数 预测值 平均 消除 统计 涨落 后 还 无法 消除 的 偏差 ！       模型 的 过 拟合 可以 看做 方差 很大 ， 对 特定 的 训练 集合 误差 很小 而 对 其他 训练 集合 误差 很大 ， 所以 对 某个 固定 的 观测 样本 ， 模型 预测 的 结果 的 波动 受 训练 集 的 选择 影响 很大 ， 也 就是 模型 的 方差 很大 。               特征 工程       类别 特征       在 实际 问题 中 常常 会 碰到 类别 特征 ， 例如 性别 分为 男和女 ， 职业 有 学生 、 白领 、 蓝领 、 无业 等 ， 文章 的 分类 有 社会 、 科技 、 经济 等等 。 这 一类 的 特征 和 鸢尾花 任务 的 特征 不同 的 是 ， 这 类 特征 的 取值 是 有限 个 并且 没有 明确 的 数值 关系 。 对于 类别 特征 ， 我们 通过 one - hot 编码 的 方式 将 它们 对应 为 数值 向量 。 one - hot 编码 的 方法 是 ， 如果 这个 特征 有 n 个 不同 的 取值 ， 那么 就 用 一个 n 维 向量 来 表示 这 一个 特征 ， 每 一维 对应 一个 取值 ， 如果 这个 特征 取 第 k 个值 ， 那么 就 将 这个 特征 第 k 维置 1 ， 而 其他 位置 都 置 0 。 以 上述 的 职业 特征 为例 ， 假设 职业 取值 有 5 个 ， 分别 是 学生 、 白领 、 蓝领 、 无业 、 未知 。 那么 我们 需要 用 一个 5 维 的 向量 来 编码 职业 这个 特征 ， 对于 某个 样本 职业 特征 为 白领 ， 那么 这个 5 维 的 向量 为 [ 0 ,   1 ,   0 ,   0 ,   0 ] ， 除了 第 2 位为 1 其他 全为 0 。 通过 这种 编码 之后 ， 类别 特征 也 可以 和 连续 值 特征 一样 进行 相同 的 处理 了 ， 这个 方法 在 以后 介绍 的 模型 中 会 大量 用到 。       不过 ， 决策树 还有 一个 更 方便 的 处理 类别 特征 的 方法 ， 直接 分裂 为 多个 子树 而 不是 两个 ！ 这样一来 ， 决策树 的 中间 节点 可能 存在 多个 分支 ， 每个 分支 对 应该 特征 的 一个 取值 ， 那么 也 就是 一个 等于 规则 ， 如下 图 所示 ， 最 左边 的 分支 对应 “ 职业 = 学生 ” 这 条 规则 。               决策树 处理 类别 特征 的 另外 一个 方法 是 子集 划分 ， 这个 方法 在 微软 的 LightGBM 中 被 用到 。 一个 类别 特征 的 n 个 不同 的 取值 构成 的 集合 ， 划分 成 两个 不同 取值 不同 的 子集 的 方法 有 $ 2 ^ n $ 个 ， 因此 遍历 每 一种 划分 要求 的 计算 复杂度 很 高 。 这种 问题 的 难点 来自 于 n 个 取值 的 无序 性 ， 试想 如果 n 个 取值 是 有序 的 ， 那么 这 就 与 之前 见到 的 连续 值 特征 一样 的 处理 就 好 了 。 基于 这个 思考 ， 可以 考虑 根据 特征 与 目标 变量 的 相关性 为 这些 值 赋予 一定 的 序 关系 。 例如 ， 在 建模 用户 的 收入水平 问题 中 ， 可以 根据 高 收入 占 比 排序 ， 将 职业 的 5 个 取值 排序 ， 假设 从 低 到 高 分别 是 ： 无业 、 未知 、 学生 、 蓝领 、 白领 。 我们 只 将 这 5 个值 划分 成 两个 子集 A 和 B ， 其中 A 中 的 职业 中 高 收入 占 比 都 比 B 要 低 ， 因此 这种 划分 方式 只有 4 种 ， 一般 情况 下 只有 n - 1 种 划分 方式 。 相比 于 遍历 无需 集合 所有 子集 大大减少 了 计算 量 。               缺失 值       在 实际 问题 中 第二类 常 遇到 问题 是 特征 缺失 值 ， 一般 用 NULL 表示 一个 样本 的 某个 特征值 缺失 ， 这种 缺失 可能 是 多种 原因 造成 的 ， 例如 确实 未知 、 录入 的 时候 忘记 填 了 、 脏 数据 等等 。 对于 缺失 值 的 处理 ， 一般 可以 根据 先验 知识 填充 对应 的 值 。 例如 在 年龄特征 缺失 时 ， 可以 用 这个 任务 下 的 用户 的 平均值 、 中位数 等 全局 统计 指标 填充 ； 在 性别 缺失 的 时候 ， 可以 用 数据 中 的 众数 填充 ， 例如 在 唯品 会上 的 建模 ， 性别 大多数 女性 ， 所以 对 性别 缺失 值用 女性 填充 。 对于 类别 特征 ， 还 可以 将 缺失 值 当做 一个 新 的 特殊 类别 处理 ， 例如 性别 取值 为 男 和 女 ， 缺失 值 可以 作为 第 3 个 取值 对待 。       对于 决策树 模型 ， 缺失 值 还有 一些 其他 处理 方式 。 在 C4 . 5 算法 中 ， 缺失 值 的 样本 会 同时 进入 到 分裂 后 的 各 分支 中 ， 为了 确保 缺失 值 样本 与非 缺失 值 样本 贡献 相同 ， 保证 公平 ， 同时 进入 各 分支 的 缺失 值 样本 会 被 加权 ， 权重 归一到 1 ， 保证 所有 的 缺失 值 贡献 之 和 为 1 ， 与非 缺失 值 的 贡献 相同 。 为此 ， 可以 在 初始化 时为 每 一个 样本 赋予 一个 权重 $ w _ i = 1 $ ， 在 以后 的 每 一次 分裂 中 ， 如果 只分 到 一个 分支 的 样本 ， 权重 不变 ； 而 分到 多个 分支 的 样本 在 每个 分支 的 权重 会 被 再次 加权 ， 变为 $ r _ i   *   w _ i $ ， $ r _ i $ 是非 缺失 样本 在 该 分支 的 占 比 。 以 职业 为例 ， 学生 、 白领 、 蓝领 、 无业 ， 未知 就是 缺失 值 ， 那么 用多叉树 进行 分裂 就 会 得到 4 个 分支 ， 假设 职业 不 缺失 的 样本 有 100 个 ， 在 每个 分支 分别 为 30 、 30 、 30 、 10 ， 而 职业 缺失 的 样本 有 10 个 ， 这 10 个 样本 会 同时 进入 这 4 个 分支 ， 且 同一个 样本 在 每个 分支 的 权重 会 从 原来 的 $ w _ i $ 更新 为 $ r _ i   *   w _ i $ ， 4 个 分支 的 比例 系数 $ r _ i $ 分别 为 0.3 、 0.3 、 0.3 、 0.1 ！ 而 在 计算 信息 增益 的 时候 ， 只 在 非 缺失 值 样本 上 计算 ， 并且 计算 的 时候 概率 不再 是 简单 的 数目 之 比 ， 而且 要 考虑 权重 ， 最后 将 结果 再 乘 上 非 缺失 样本 的 比例 。       决策树 另外 一种 处理 确实 值 的 方式 是 只 将 缺失 值 放到 一个 分支 ， 这个 方法 在 著名 的 XGBoost 中 被 采用 。 多个 分支 应该 选择 哪个 分支 呢 ？ 很 简单 ， 每个 分支 都 试一下 ， 选择 信息 增益 ( 或者 其他 准则 ) 最大 的 那 一种 方法 即可 ！       决策树 的 其他 分裂 准则       信息 增益 率       信息 增益 准则 在 二叉树 ( 即 每个 中间 节点 都 只有 两个 子 节点 的 树 ) 中是 一个 很 好 的 判断 分裂 好坏 的 准则 ， 但是 在 多叉树 中 就 不见得 了 。 在 多叉树 中 ， 取值 很多 的 类别 特征 会 天然 地有 很 高 的 信息 增益 ， 这 使得 这 类 特征 被 选择 进行 分裂 的 概率 比 连续 特征 和 只有 两个 取值 的 类别 要 高 很多 。 为了 解决 这个 问题 ， 信息 增益 率 准则 被 提出 来 了 。 信息 增益 率 是 在 信息 增益 的 基础 上 除以 属性 取值 的 \" 固有 值 \" ， 属性 a 的 固有 值 定义 为       $ $     IV ( a )   =   -   \ \ sum _ v   \ \ frac { | D _ v | } { D }   \ \ log   _   2   \ \ frac { | D _ v | } { D }     $ $       上式 中 D 和 $ D _ v $ 分别 是 待 分裂 样本 总数 和 其中 属性 a = v 的 样本 数目 ， 也 就是 根据 样本 中 属性 a 的 取值 形成 一个 概率分布 $ P ( v )   =   \ \ frac { | D _ v | } { D } $ ， 这个 分布 的 熵 就是 属性 a 的 固有 值 。 因此 ， 取值 越多 的 属性 IV 通常 越大 ， 以此 在 一定 程度 上 纠正 信息 增益 对 取值 多 的 属性 的 选择 偏好 。 利用 IV ， 信息 增益 率 可以 写为       $ $     Gain \ \ _   ratio ( D ,   a )   =   \ \ frac { Gain ( D ,   a ) } { IV ( a ) }     $ $       其中 $ Gain ( D ,   a ) $ 就是 前面 所说 的 对 集合 D 按照 属性 a 划分 的 信息 增益 $ \ \ Delta   I $ 。 与 信息 增益 相反 ， 信息 增益 率会 天然 的 偏好 取值 数目 少 的 属性 ， 在 C4 . 5 算法 中 ， 使用 了 一种 启发式 方法 ， 先 找出 信息 增益 高于 平均水平 的 属性 ， 然后 从中 选出 信息 增益 率 最高 的 ， 相当于 在 信息 增益 和 信息 增益 率 两者 中取 了 一个 折中 的 方案 。       基尼系数       决策树 的 基尼系数 与 经济学 上 度量 贫富差距 的 基尼系数 并 不是 一 回事 ， 这里 的 基尼系数 是 说 对 一个 集合 D ， D 中 的 样本 属于 不同 类别 ， 随机 取 两个 样本 他们 的 类别 不同 的 概率 。 这个 概率 越低 ， 说明 集合 的 纯度 越高 ， 这个 概率 等于 1 说明 集合 中 只有 一个 类别 。 假设 集合 D 中 每 一类 的 比例 为 $ p _ k $ ， 那么 根据 这种 概率 定义 可 得 基尼系数 为       $ $     \ \ begin { align }     Gini ( D )   & amp ; =   \ \ sum _ k   p _ k   \ \ sum _ { k '   \ \ ne   k }   p _ { k ' }   \ \ \ \     & amp ; = \ \ sum _ k   p _ k ( 1 - p _ k )   \ \ \ \     & amp ; = 1   -   \ \ sum _ k   p _ k ^ 2     \ \ end { align }     $ $       按照 某个 分裂 规则 分裂 后 ， 每个 集合 都 可以 计算 一个 基尼系数 $ Gini ( D _ v ) $ ， 我们 用 平均 基尼系数 $ \ \ sum _ v   r _ v   Gini ( D _ v ) $ 来 衡量 按照 属性 a 的 某个 分裂 规则 分裂 后 的 总体 不 纯度 ， 其中 $ r _ v $ 是 每个 集合 的 样本 占 总体 的 比例 。 这个 平均 基尼系数 越低 ， 说明 分裂 规则 越 好 ！ 因此 ， 在 决策树 分裂 算法 中 ， 用 分裂 后 最小 平均 基尼系数 选择 最佳 属性 和 分裂 点 即可 ， 这 就是 CART （ 分类 回归 树 ） 中 用到 的 分裂 准则 ， 在 著名 的 GBDT 和 XGBoost 中 也 经常 用到 。       复现 代码       过 拟合 现象 代码               import       numpy       as       np       import       pandas       as       pd       import       matplotlib . pyplot       as       plt         from       sklearn . tree       import       DecisionTreeClassifier       from       sklearn . datasets       import       load _ iris         iris       =       load _ iris     ( )         mask       =       np     .     random     .     randn     (     iris     .     data     .     shape     [     0     ] )       & lt ;       0.8       #   20 %   for   test       train _ x     ,       train _ y       =       iris     .     data     [     mask     ] ,       iris     .     target     [     mask     ]       test _ x     ,       test _ y       =       iris     .     data     [     ~     mask     ] ,       iris     .     target     [     ~     mask     ]         train _ err       =       [ ]       test _ err       =       [ ]       depths       =       range     (     1     ,       10     )       for       n       in       depths     :               clf       =       DecisionTreeClassifier     (     max _ depth     =     n     )               clf     .     fit     (     train _ x     ,       train _ y     )               train _ err     .     append     (     1       -       clf     .     score     (     train _ x     ,       train _ y     ) )               test _ err     .     append     (     1       -       clf     .     score     (     test _ x     ,       test _ y     ) )         plt     .     plot     (     depths     ,       train _ err     ,       &# 39 ; . - &# 39 ;     )       plt     .     plot     (     depths     ,       test _ err     ,       &# 39 ; . - &# 39 ;     )         plt     .     legend     ( [     &# 39 ; train   error &# 39 ;     ,       &# 39 ; test   error &# 39 ;     ] )       plt     .     xlabel     (     &# 39 ; depth &# 39 ;     )       plt     .     ylabel     (     &# 39 ; error &# 39 ;     )         plt     .     show     ( )                 思考 与 实践           举例说明 在 最终 的 决策树 中 ， 缺失 值 样本 在 所有 叶子 节点 中 的 权重 之 和 等于 1 ！       实现 k 折叠 交叉 验证 ， 选择 最佳 决策树 模型 ， 最佳 模型 参数 包括 决策树 深度 、 叶子 节点 上 最少 样本 数目 、 分裂 准则 这 三个 。                   import       numpy       as       np       import       pandas       as       pd       import       matplotlib . pyplot       as       plt         from       sklearn . tree       import       DecisionTreeClassifier       from       sklearn . datasets       import       load _ iris       from       sklearn . cross _ validation       import       KFold           iris       =       load _ iris     ( )         mask       =       np     .     random     .     randn     (     iris     .     data     .     shape     [     0     ] )       & lt ;       0.8       #   20 %   for   test       train _ x     ,       train _ y       =       iris     .     data     [     mask     ] ,       iris     .     target     [     mask     ]       test _ x     ,       test _ y       =       iris     .     data     [     ~     mask     ] ,       iris     .     target     [     ~     mask     ]         & quot ; & quot ; & quot ; 实现 K - Fold 算法       可以 利用 sklearn 的   sklearn . model _ selection . KFold   实现 集合 划分 ， 然后 依次 执行 fit 训练 模型 ，       和   score   评估 在 评估 集上 的 效果 ， 将 k 次 的 效果 平均 得到 在 该组 超 参数 下 的 平均 效果 。 最后 从 多组 超 参数 中       选择 出 最佳 的 超 参数 重新 训练 模型 ， 在 测试 集上 评估 结果 。       & quot ; & quot ; & quot ;       best _ err       =       1       best _ params       =       {     &# 39 ; depth &# 39 ;     :     0     ,       &# 39 ; min _ samples _ leaf &# 39 ;     :       1     ,       &# 39 ; criterion &# 39 ;     :       &# 39 ; entropy &# 39 ;     }       k       =       5       clf       =       DecisionTreeClassifier     ( )         ###   YOUR   CODE ,   定义   kfold   对象         ###   END   YOUR   CODE         for       depth       in       range     (     1     ,     10     ) :               for       min _ samples _ leaf       in       [     1     ,     3     ,     10     ,     30     ] :                       for       criterion       in       [     &# 39 ; entropy &# 39 ;     ,       &# 39 ; gini &# 39 ;     ] :                                                       ###   YOUR   CODE ， 实现 K - Fold 算法                                 ###   END   YOUR   CODE         print     (     &# 39 ; Best   error :       % . 4f     ,   depth =     % d     ,   min _ samples _ leaf =     % d     ,   criterion =     % s     &# 39 ;     .     format     (     best _ err     ,       best _ params     [     &# 39 ; depth &# 39 ;     ] ,       best _ params     [     &# 39 ; min _ samples _ leaf &# 39 ;     ] ,       best _ params     [     &# 39 ; criterion &# 39 ;     ] ) )         ###   YOUR   CODE ,   在 上述 最佳 超 参数 下 重新 在 训练 集上 训练 模型 ， 并 在 测试 集上 评估 效果         ###   END   YOUR   CODE        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/ml-in-action.html"},
      
      
      {"title": "第10.0讲：强化学习简介", "text": "    Table   of   Contents           关于           马尔科夫 决策 过程           MDP 的 例子           确定性 环境           随机 策略                   哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程           状态值 函数           动作 值 函数           HJB 方程           确定性 环境 的 HJB 方程                           动态 规划           值 迭代           压缩 映像 原理                   策略 迭代           两种 迭代法 的 对比           环境 未知 的 问题                   强化 学习 的 解释           强化 学习 的 应用           有 注意力 的 图像识别 系统                   思考 与 实践                 关于       强化 学习 近年 大火 ， 最早 是因为 AlphaGo 使用 强化 学习 打败 人类 围棋 冠军 引发 的 。 在 那 之后 ， 强化 学习 在 工业 场景 应用 越来越 多 ， 原来 很多 做 搜索 、 推荐 、 广告 等 一直 在 用 监督 学习 的 业务 ， 也 开始 使用 强化 学习 来 优化 用户 体验 和 平台 收益 了 。 强化 学习 实际上 很 早就 提出 了 ， 事实上 ， 强化 学习 来源于 控制论 。 控制论 之 父 叫做   维纳   ， 学过 信号处理 的 可能 知道 他 ， 维纳滤波 就是 用 他 的 名字 命名 的 。 控制论 最早 源于 航天 ， 人们 要 控制 火箭 发射装置 ， 将 火箭 发射 到 地球 之外 ； 控制论 还 源于 机器人 控制 ， 人们 需要 用 算法 对 机器人 的 行为 进行 控制 。 所以 ， 很多 讲 强化 学习 的 文献 也 会 说 强化 学习 是 在 优化 控制策略 。       在 这 篇文章 中 ， 您 将 学习 到           强化 学习 是 什么 ？ 可以 干什么 ？       马尔科夫 决策 过程 ( MDP ) 的 重要 概念       哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程       已知 环境 下 HJB 方程 的 求解 算法 ： 值 迭代 和 策略 迭代           我 期望 您 至少 有 ：           高中数学 水平 且 年满 18 岁 ， 部分 内容 需要 你 了解 监督 学习 ， 你 可以 通过 本 教程 前面 的 章节 进行 学习 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           马尔科夫 决策 过程       考虑 下面 的 迷宫 游戏 问题 ， 你 控制 一个 机器人 在 一个二维 迷宫 中 运动 ， 迷宫 中有 正常 的 陆地 、 火坑 、 石柱 、 钻石 。 你 可以 控制 机器人 上下左右 运动 ， 机器人 不能 走 到 迷宫 外面 ， 一次 最 多 只能 运动 一步 ， 如果 不 小心 掉 到 火坑 中 ， 游戏 结束 ， 如果 找到 了 钻石 ， 那么 可以 得到 奖励 ， 并且 游戏 结束 ！ 由于 你 的 控制 是 通过 语音指令 控制 ， 机器人 有 一定 概率 会 判断 出错 。 比如 你 说 让 机器人 往 左 走 ， 机器人 有 一定 概率 会 往右 走 ， 所以 机器人 的 移动 和 你 的 指令 之间 并 不是 完美 匹配 的 。 你 的 目标 是 通过 设计 策略 ， 让 机器人 尽快 地 找到 钻石 ， 获得 奖励 。               上述 问题 有 几个 关键 要素 ：           状态 ： 机器人 所处 的 位置 是 有限 的 ， 我们 把 每 一个 位置 称作 一个 状态 ， 那么 一共 有 15 个 状态 （ 有 一个 是 石柱 ， 机器人 无法 到达 这个 位置 ） 。 其中 两个 是 火坑 ， 一个 是 钻石 ， 由于 机器人 进入 这些 状态 就 会 结束 游戏 ， 我们 称为 终态 。 我们 用 S 来 表示 状态 ， 那么 S 可以 有 15 个 取值 ， 我们 一次 用 数字 标识 这 15 个 状态 ， 那么 $ S   \ \ in   \ \ { 1 , 2 , ... , 15   \ \ }   $ 。       动作 ： 在 每个 不是 终态 的 状态 下 ， 我们 都 有 4 个 控制 动作 ， 上 、 下 、 左 、 右 ， 我们 也 可以 将 这些 动作 依次 编号 为 1 到 4 ， 用 A 表示 动作 ， 那么 $ A   \ \ in   \ \ { 1 , 2 , 3 , 4   \ \ } $ 。       转移 概率 ： 因为 我们 是 通过 声音 控制 机器人 的 ， 所以 机器人 可能 听错 ， 可以 认为 是 语音 识别 技术 尚 不 成熟 的 原因 。 那么 在 某个 状态 S ， 采取 动作 A 之后 ， 机器人 到达 的 状态 并 不是 完全 确定 的 。 例如 当 机器人 在 左上角 时 ， 采取 “ 右 ” 这个 动作 时 ， 机器人 也 有 一定 概率 会 向下 移动 ， 进入状态 5 （ 假设 状态 按照 从左到右 顺序 编号 ） 。 为了 描述 机器人 的 这种 不 确定 运动 ， 可以 用 一个 转移 概率 来 表示 。 我们 用 P ( S ' | S ,   A ) 表示 在 状态 S 下 ， 采取 动作 A 的 条件 下 ， 机器人 进入状态 S ' 的 概率 。 例如 在 刚才 这个 例子 中 ， P ( S ' = 5 | S = 1 ,   A = 右 ) 就 表示 在 状态 1 （ 也 就是 左上角 ） ， 采取 动作 右 的 条件 下 ， 进入状态 5 （ 也 就是 第二行 第一个 状态 ） 的 概率 。 所以 ， 这个 转移 概率 描述 的 是 机器人 所 处 状态 在 我们 的 控制 下 变化 的 规律 。       回报 ： 在 机器人 每 一步 运动 到 下 一个 状态 时 ， 环境 会 给 我们 一个 奖励 或者 惩罚 ， 例如 如果 进入 火坑 游戏 就 会 结束 ， 而 拿到 钻石 就 会 得到 奖励 ， 这种 奖励 或者 惩罚 我们 用 数量 来 量化 。 我们 可以 用 正数 表示 奖励 ， 负数 表示 惩罚 ， 这 就是 回报 。 一般 情况 下 ， 回报 可能 跟 状态 和 动作 都 有 关系 ， 所以 我们 用 一个 函数 来 表示     R ( S ,   A ,   S ' ) ， 它 表示 在 在 状态 S 下 采取 动作 A 到达 状态 S ' 时 ， 获得 的 回报 。 注意 ， 一般 情况 下 ， 回报 是 在 每 一个 动作 执行 后 跳转 到 新 的 状态 就 会 收到 的 ， 而 不是 只有 最后 到达 终点 时才 有 。 在 这个 例子 中 ， 只有 最终 的 那 一步 才 有 回报 ， 可以 认为 其他 的 每 一步 的 回报 为 0 。 你 也 可以 给 其他 的 步骤 给 一个 负 的 回报 ， 负 的 回报 可以 解释 为 每 一步 付出 的 成本 ， 例如 时间 成本 、 机器人 的 能量消耗 等等 。       马尔科夫 性 ： 描述 环境 的   转移 概率   只 跟 当前 状态 和 动作 有关 ， 而 与 之前 经过 的 状态 和 动作 无关 ， 这种 性质 叫做 马尔科夫 性 （ 因为 是 一个 叫 马尔科夫 的 人 最早 提出 来 的 ） 。 在 这个 例子 中 ， 机器人 在 我们 的 控制 下要 进入 的 状态 的 概率 只 与 当前 的 状态 和 我们 的 控制 动作 有关 ， 而 与 在 这 之前 机器人 经历 过 的 状态 无关 。 例如 ， 当 机器人 处于 状态 1 时 ， 那么 接下来 ， 采取 动作 \" 右 \" 后 ， 状态 变到 2 还是 5 的 概率 与 机器人 到达 状态 1 之间 的 所有 事情 都 无关 ！ 这种 无关 ， 并 不是 说 最终 获得 的 回报 与 之前 的 状态 无关 ， 而是 说 当前 这 一步 的   转移 概率   与 之前 的 状态 无关 ！ 并 不是 所有 的 决策 事情 都 有 马尔科夫 性 的 ， 比如 股票 ， 今天 涨跌 不但 与 昨天 有关 ， 还 与 之前 的 多天 有关 。 如果 我们 用 $ S _ t ,   A _ t $ 分别 代表 t 时刻 的 状态 和 采用 的 动作 ， 那么 马尔科夫 性 可以 表示 为           $ $     P ( S _ { t + 1 } | S _ t ,   A _ t ,   S _ { t - 1 } ,   A _ { t - 1 } ,   ... , S _ 1 ,   A _ 1 )   =   P ( S _ { t + 1 } | S _ t ,   A _ t )     $ $       这个 公式 的 左边 是 在 前面 一系列 的 状态 和 动作 的 条件 下 ， 下 一步 转移 到 状态 $ S _ { t + 1 } $ 的 概率 ， 右边 是 在 当前 状态 $ S _ t $ 下 ， 采取 动作 $ A _ t $ 后 ， 转移 到 状态 $ S _ { t + 1 } $ 的 概率 ，   而 不管 更 前面 的 状态 和 动作 是 什么   。 这 两者 相等 ， 就是说 转移 概率 只 跟 当前 状态 和 动作 有关 ， 而 与 之前 经过 的 状态 和 动作 无关 ， 即 上面 说 的 马尔科夫 性 。       马尔科夫 性 也 叫 无 记忆性 ， 想象 一个 得 了 一种 奇怪 失忆症 的 病人 ， 他 每天 醒来时 ， 就 会 将 以前 发生 的 事情 全部 忘记 。 马尔科夫 性 就 像 一个 不 记事 的 系统 ， 这种 系统 只是 对 实际 问题 的 一种 理想 近似 ， 可以 简化 数学模型 。               上述 问题 可以 抽象 为 上述 4 个 要素 和 1 个 性质 的 普遍 问题 ， 因为 环境 具有 马尔科夫 性 ， 我们 需要 做出 决策 对 机器人 进行 控制 ， 所以 叫做 马尔科夫 决策 过程 ( Markov   decision   process ， MDP ) 。 对于 这 类 问题 ， 我们 在 每 一步 决定 要 采取 那个 动作 的 策略 可以 用 一个 策略 函数 来 表示 $ \ \ pi ( A   |   S ) $ ， 这个 函数 的 意义 是 在 状态 S 下 ， 采用 动作 A 的 概率 。 对于 确定性 的 策略 可以 看做 它 的 一个 特例 ， 即 只有 一个 动作 的 概率 为 1 其他 为 0 . 例如 ， 我们 可以 定义 一个 确定性 策略 如下 ： 如果 右边 能 走 ， 就 往 “ 右 ” ， 如果 右边 不能 走 ， 就 往 “ 下 ” 。 用 数学公式 表示 为       $ $     \ \ pi ( S )   =   \ \ begin { cases }                             \ \ text { 右 } ,   S   \ \ in   \ \ { 1 , 2 , 3 , 6 , 8 , 9 , 10 , 12 , 13 , 14   \ \ }   \ \ \ \                             \ \ text { 下 } ,   S   \ \ in   \ \ { 4 , 7 , 11   \ \ }                             \ \ end { cases }     $ $       MDP 问题 的 目标 是 找到 这样 一个 策略 ， 在 这个 策略 下 ， 总 的 期望 折扣 回报 最大化 ！ 总 期望 折扣 回报 定义 如下       $ $     R   =   E   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t ) ,   \ \ gamma   \ \ in   ( 0 ,   1 ]     $ $       也 就是 在 这个 策略 下 ， 将 所有 获得 的 回报 打个 折 之后 全部 加 起来 。 越往后 的 折扣 越大 ， 这 是因为 我们 更 关心 离 当前 时间 更近 的 回报 。 这个 回报 越大 ， 说明 这个 策略 越 好 ， 使得 回报 最大 的 策略 就是 最优 策略 。 因为 环境 有 一定 的 随机性 ， 在 这个 策略 下 实际 的 回报 是 随机 的 ， 所以 我们 对 回报 求 期望 ， 得到 期望 折扣 回报 。       以 上述 迷宫 为例 ， 我们 假定 游戏 结束 后 回报 全部 为 0 ， 状态 也 不 改变 了 。 那么 上述 求和 只到 游戏 结束 ， 假设 折扣 因子 为 1 ， 跳 到 火坑 的 回报 为 - 1 ， 找到 钻石 的 回报 为 + 1 ， 其他 情况 回报 为 0 。 那么 一个 找到 钻石 的 策略 的 总 回报 就是 1 ！ 而 跳 到 火坑 的 策略 的 总 回报 为 - 1 .   由于 环境 的 随机性 ， 实际 的 策略 下 这 两种 情况 都 有 可能 ， 所以 一般 期望 回报 在 - 1 到 1 之间 。 一个 策略 越 好 ， 那么 期望 回报 会 更 接近 1 ， 期望 回报 最大 的 策略 就是 我们 要 寻找 的 最优 策略 。       MDP 的 例子       设想 你 在 跟 朋友 玩 斗地主 （ 一种 流行 的 扑克牌 游戏 ， 你 可以 类比 于 所有 你 熟悉 的 牌类 游戏 ） ， 影响 你 每 一步 出牌 的 信息 包括 每个 人 手中 的 剩下 的 牌 的 张数 ， 每 一个 人 已经 出过 的 牌 ， 还有 你 自己 目前 手中 的 牌 ， 这些 信息 构成 了 一个 状态 ( S ) 。 你 的 出牌 动作 可以 从 所有 可能 的 出牌 方式 中 选择 一个 ， 比如 3 带 1 或者 一对 A 都 是 一个 动作 ( A ) 。 可以 想象 ， 状态 数目 和 动作 数目 都 非常 大 ， 但是 不用 担心 ， 计算机 很 容易 处理 大量 的 状态 和 动作 的 任务 ， 但是 前提 是 你 要 把 MDP 中 的 所有 元素 定义 清楚 。 你 每次 出 牌 之后 ， 到 你 下 一轮 出牌 时 ， 完成 了 一个 状态 转移 ， 转移 到 的 状态 就是 下 一轮 出牌 时 ， 每个 人 剩下 的 牌 的 张数 ， 每个 人 已经 出过 的 牌 ， 还有 你 自己 手中 当时 的 牌 。 由于 你 无法 决定 其他人 的 出牌 ， 所以 对 你 的 某 一个 出 牌 动作 ， 转移 到 的 状态 也 是 不 确定 的 ， 存在 着 转移 概率 ( P ) 。 但是 这个 转移 概率 在 当前 的 状态 和 你 的 动作 给定 的 情况 下 ， 而 与 之前 的 状态 和 动作 无关 ， 因为 根据 我们 对 状态 的 定义 ， 它 已经 包含 了 这些 信息 ， 所以 不用 管 之前 的 状态 了 ， 这 表明 这个 任务 满足 马尔科夫 性 。 我们 的 目标 是 要 最终 赢得 这 一局 牌 ， 可以 将 赢 了 的 回报 ( R ) 定义 为 + 1 ， 而 将 输 了 的 回报 定义 为 - 1 ， 其他 每 一步 的 回报 都 是 0 。 那么 玩 斗地主 游戏 的 过程 具有 上述 4 个 要素 和 1 个 性质 ， 因此 在 计算机 设计 斗地主 算法 时 ， 就 可以 用 马尔科夫 决策 过程 来 描述 ！       设想 你 是 一只 AlphaGo ， 正在 和 人类 下围棋 ， 影响 你 每 一步 落子 的 信息 就是 当前 的 棋局 状态 ( S ) ， 而 你 每 一个 落子 动作 ( A ) 都 是从 所有 可能 的 落子 位置 中 选择 的 一个 。 从 你 当前 落子 到 下 一步 落子 时 ， 状态 发生 了 转移 ， 又 有 你 不 知道 对手 会下 哪 一步 ， 所以 存在 状态 转移 概率 ( P ) 。 这个 转移 概率 只 与 当前 的 棋局 状态 和 你 的 落子 动作 有关 ， 所以 具有 马尔科夫 性 。 我们 的 目标 是 最终 要 赢得 这 一局 牌 ， 每 一局 牌 结束 时 都 会 有 一个 回报 ( R ) 。 因此 ， AlphaGo 下围棋 的 过程 具有 上述 4 个 要素 和 1 个 性质 ， 因此 可以 用 马尔科夫 决策 过程 ( MDP ) 来 描述 。 事实上 ， 在 Google 最新版 的 AlphaGo 程序 中 ， 正是 通过 MDP 来 对 下围棋 这个 过程 进行 建模 的 。       设想 你 是 一个 航天 控制系统 设计 工程师 ， 你 想 让 火箭 按照 预期 将 卫星 发射 到 预定 轨道 当中 。 火箭 在 每 一个 时刻 上 所处 的 位置 和 速度 共同 构成 了 一个 状态 ( S ) ， 而 你 设计 的 控制算法 每个 时刻 所 给出 的 推动 策略 ( 加速 喷火 还是 减速 喷火 等等 ) 就是 一个 动作 ( A ) 。 这个 问题 中 ， 状态 是 连续 的 ， 动作 既 可以 是 有限 的 也 可以 是 连续 的 。 每 一个 动作 执行 后 ， 由于 受到 环境 的 干扰 （ 阻力 ， 控制 误差 等等 因素 ） ， 下 一个 时刻 火箭 的 状态 存在 着 一个 转移 概率 ( P ) 。 这个 转移 概率 在 当前 时刻 的 状态 和 动作 给定 的 情况 下 与 之前 火箭 的 状态 和 控制 动作 无关 ， 这 表明 任务 满足 马尔科夫 性 。 我们 的 目标 是 用 最少 的 能量消耗 将 火箭 发射 到 预定 轨道 ， 每 一步 的 回报 ( R ) 是 负 的 ， 因为 每 一步 消耗 了 燃料 ， 总 的 回报 就是 负 的 消耗 总 能量 。 最大化 总 回报 就是 最小化 能量消耗 ， 因此 上述 火箭 控制系统 可以 用 马尔科夫 决策 过程 来 描述 。       这样 的 例子 还有 很多 ， 可以 看到 ， 很多 实际 的 决策 和 控制 问题 ， 都 可以 看做 马尔科夫 决策 过程 ( MDP ) ， 因此 MDP 应用 非常 广泛 ， 从 斗地主 到 火箭 发射 ， 几乎 无所不在 。       确定性 环境       假设 我们 可以 直接 控制 上述 机器人 ， 那么 在 某个 状态 S ， 采用 动作 A 后 ， 机器人 到达 的 状态 S ' 就是 确定 的 ， 也 就是 转移 概率 P ( S ' | S ,   A ) 只 可能 取 0 和 1 两个 值 ！ 这种 情况 下 ， 我们 说 环境 是 确定 的 ， 这 就是 我们 正常 的 迷宫 游戏 。       在 环境 是 确定 的 ， 没有 随机 干扰 情况 下 ， 我们 很 容易 看出 ， 下图 所示 的 一个 策略 就是 最佳 策略 ， 箭头 所指 的 方向 就是 在 当前 状态 下 ， 应该 采取 的 动作 。 因为 在 这个 路劲 上 ， 除了 最终 的 状态 ， 其他 状态 回报 都 是 0 ， 只有 到达 最终 状态 的 时候 有 一个 + 1 的 回报 ， 所以 这个 策略 的 总 回报 等于 1 。 不难想象 ， 这个 策略 并 不是   唯一   最优 的 策略 ， 例如 在 这条 路径 上 往回 走 一段 路程 之后 再 往前走 ， 总 回报 还是 等于 1 ， 因为 中间 所有 的 状态 的 回报 都 是 0 。       如果 我们 想要 得到 最短 的 路径 ， 该 怎么办 呢 ？ 方法 很 简单 ， 我们 可以 让 除了 到达 终态 的 步骤 外 ， 其他 每 一步 的 回报 为 - 0.1 即可 ！ 虽然 这样一来 ， 但是 只有 最短 路径 的 策略 可以 获得 最大 总 回报 ， 容易 计算 出 最大 总 回报 等于 0.5 。 你 可以 把 每 一步 回报 为 负数 看做 成本 ， 例如 你 控制 的 机器 要 消耗 电 ， 不必要 的 步骤 就是 在 浪费 你 的 时间 ， 增加 时间 成本 。 在 最 短 路径 找到 钻石 实际上 是 在 说 用 最少 的 成本 得到 最大 的 回报 ！       另外 一个 方法 是 让 折扣 因子 小于 1 ， 比如 取 0.9 ， 每 一步 的 回报 还 保持 为 0 不变 ， 那么 路径 越长 ， 最后 得到 的 那个 回报 就 会 被 衰减 得 更 多 ， 只有 最短 路径 才能 得到 最大 回报 ， 此时 最大 回报 为       $ $     R   =   0   +   0   +   0   +   0   +   0   +   0.9 ^ 6   =   0.531     $ $       折扣 因子 实际上 是 再 将 未来 的 回报 折现 到 现在 ， 想想 给 你 的 回报 是 很多年 后 的 1 万块 钱 ， 那么 同样 是 一万块 钱 ， 年限 不同 决定 了 其 价值 也 不 相同 （ 想想 一下 2000 年 的 1 万元 和 2010 年 的 1 万元 的 价值 明显 是 不 一样 的 ， 因为 货币 会 贬值 ） 。 为了 统一 衡量 这些 不同 年限 的 回报 ， 可以 将 他们 折现 到 当下 ， 看 他们 在 现在 值 多少 钱 。   折扣 因子 就是 折现 率 ， 它 将 未来 的 回报 折现 到 现在 便于 统一 比较   ！       因此 ， 从 这个 例子 来看 ， 在 对 实际 问题 建模 的 时候 ， 合理 设计 回报 也 是 一个 很 重要 的 事情 。               随机 策略       这个 简单 的 例子 我们 很 容易 利用 上帝 视角 ， 发现 最佳 的 策略 $ \ \ pi ^   *   $ 使得 总 回报 最大 ， 就是 上图 中 的 最 短 路径 。 但是 ， 设想 一下 ， 我们 是 图 中 那个 机器人 ， 身在 此 山中 ， 云深 不知 处 ， 无法 开 上帝 视角 ， 不 知道 上下左右 分别 是 什么 坑 ， 只 知道 自己 所处 的 位置 是否 有 坑 ， 因此 就 不 知道 采取 某个 动作 之后 会 得到 什么 ， 除非 我们 去 尝试 一下 。 在 这种 情况 下 ， 我们 该 采用 什么 策略 呢 ？ 没有 太好 的 办法 ， 只有 采用 随机 策略 去 尝试 。       所谓 随机 策略 就是 每个 状态 下 ， 选择 上下左右 四个 动作 的 概率 都 不 为 0 ， 比如 在 对 环境 一无所知 的 情况 下 ， 可以 将 每个 方向 的 概率 都 设为 一样 。 当 你 经过 尝试 或者 通过 其他 途径 获得 了 一些 关于 环境 的 信息 ， 你 也 可以 让 某个 方向 上 的 概率 更大 一些 ， 例如 有人 告诉 我 ， 钻石 就 在 最 右下角 ， 那么 我们 可以 不用 采用 那么 随机 的 策略 ， 让 向 右 和 向下 这 两个 动作 的 概率 更大 一些 。 这 表明 ， 我们 知道 的 信息 越 多 ， 可以 采用 的 策略 就 越 不用 那么 随机 。 如果 我们 对 环境 十分 清楚 ， 例如 我们 知道 每 一个 状态 下 每 一个 动作 执行 后 会 转移 到 哪个 状态 ( 即 知道 转移 概率 P ) ， 并且 知道 哪些 状态 是 坑 ， 哪些 是 钻石 ( 也 即 是 知道 了 回报 函数 R ) ， 也 就是 开 了 上帝 视角 ， 知道 环境 的   转移 概率   和 环境 的   回报 函数   ， 那么 我们 原则上 可以 计算 出 这个 最佳 的 策略 。 那么 问题 来 了 ，   在 环境 的 转移 概率 和 回报 函数 已知 的 情况 下   ， 怎么 求解 最佳 策略 呢 ？       哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程       在 回答 上述 问题 之前 ， 我们 先 来 介绍 一个 方程 ， 叫做 哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程 ， 很 明显 这个 方程 跟 三个 人 有关 ， 这 三个 人 都 是 很 有名 的 数学家 ， 哈密顿 还是 个 数学 物理学家 ， 是 哈密顿 力学 的 创始人 。 这个 方程 刻画 了 最优 策略 要 满足 的 充分 必要条件 ， 一旦 这个 方程 求解 出来 了 ， 那么 最优 策略 也 就 知道 了 。 在 介绍 这个 方程 之前 ， 我们 先 来 介绍 两个 在 强化 学习 中 非常 重要 的 概念 ， 状态值 函数 和 动作 值 函数 ， 熟悉 这 两个 概念 对 以后 的 学习 十分 重要 ， 我们 会 不断 的 碰到 这 两个 概念 。       状态值 函数       当 我们 知道 环境 的 状态 转移 概率 时 ， 那么 很 直接 的 想法 是 查看 四周 ， 找到 最好 的 一个 状态 ， 然后 选择 可以 到达 这个 最好 的 状态 的 动作 。 例如 在 状态 8 的 时候 ， 周围 的 状态 只有 3 个 ， 12 是 火坑 ， 5 和 9 是 普通 的 状态 ， 显然 12 这个 状态 不能 跳 ， 状态 9 看起来 比 5 好 ， 因为 它 离 钻石 更近 一些 。 但是 在 复杂 的 问题 中 ， 我们 很 难用 这样 一个 简单 的 法则 来 表达 状态 是 好 还是 坏 ， 因为 在 复杂 的 问题 中 ， 随机性 可能 比较 大 ， 最 短距离 可能 并 不是 最佳 的 。 例如 状态 6 比 2 到达 钻石 的 距离 短 ， 但是 如果 状态 6 到达 10 的 转移 概率 很 低 很 低 ， 那么 状态 2 可能 比 6 更好 。 这 表明 最短 路径 并 不是 一个 很 好 的 评估 一个 状态 指标 。 那么 如何 精确 评估 一个 状态 好 还是 坏 呢 ？ 答案 就是 状态值 函数 。       定义 ： 状态值 函数 V ( S ) 是从 状态 S 出发 ， 所 能 获得 的 最大 期望 回报 ！       $ $     V ( S )   =   \ \ max   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S   \ \ right ]     $ $       我们 假设 环境 是 确定性 的 ， 没有 随机性 ， 这样 可以 省去 求 期望 的 步骤 ， 便于 理解 状态值 函数 。 对于 终止 状态 ， 我们 定义 它们 的 状态值 函数 值恒 等于 0 。 假设 折扣 因子 等于 1 ， 每 一步 跳转 的 回报 函数 都 为 - 0.1 ， 仍然 假定 环境 是 确定 的 。 我们 来看 一下 从 状态 13 出发 ， 可能 到达 的 状态 有 3 个 ， 分别 是 12 、 9 、 14 ， 我们 来 计算 一下 这 三个 状态 的 状态值 函数 。 因为 12 是 火坑 ， 是 终止 状态 ， 所以 V ( 12 ) = 0 。 对于 状态 9 ， 最快 可以 通过 3 步 到达 钻石 ， 获得 的 总 回报 是 V ( 9 ) = - 0.1 - 0.1 + 1 = 0.8 。 而 状态 14 最快 一步 就 能 到达 钻石 ， 所以 总 回报 就是 这 一步 的 回报 V ( 14 ) = 1 . 很 明显 ， V ( 14 ) & gt ; V ( 9 ) & gt ; V ( 12 ) ， 所以 这 3 个 状态 中 ， 状态 14 最好 ， 这 跟 我们 的 直觉 一致 。   这 表明 状态值 函数 可以 评估 两个 不同 状态 的 好坏   。               状态值 函数 的 求 最大值 操作 表明 采用 的 是 最优 策略 $ \ \ pi ^   *   $ 下 的 总 折扣 回报 ， 对于 某个 具体 的 策略 $ \ \ pi $ ， 还 可以 定义 策略 值 函数 为 ： 从 状态 S 出发 ， 采用 策略 $ \ \ pi $ 选择 动作 ， 所 能 获得 的 期望 回报 ！       $ $     V ^ { \ \ pi } ( S )   =   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ t   \ \ sim   \ \ pi   \ \ right ]     $ $       $ V ^ { \ \ pi } ( S ) $ 和 $ V ( S ) $ 都 是从 状态 S 出发 获得 的 最大 期望 回报 ， 不同 的 是 $ V ^ { \ \ pi } ( S ) $ 要求 后面 采取 的 策略 是 给定 的 策略 $ \ \ pi $ ， 而 $ V ( S ) $ 要求 的 是 最好 的 策略 。 假设 从 状态 S = 13 出发 ， 策略 $ \ \ pi _ 1 $ 是 一直 往上走 ， 如果 上面 不能 走 就 往右 走 （ 下图 绿色 箭头 ） 。 策略 $ \ \ pi _ 2 $ 是 一直 往右 走 ， 右边 走 不了 就 往下走 ( 下图 黄色 箭头 ) 。 在 策略 1 下 ， 机器人 会 跳 到 火坑 里面 （ 这里 还是 认为 是 确定性 环境 ， 每 一步 回报 为 - 0.1 ， 火坑 回报 - 1 ， 钻石 回报 + 1 ） ， 所以 $ V ^ { \ \ pi _ 1 } ( 13 ) = - 1.2 $ ， 而 在 策略 2 下 ， 机器人 将 找到 钻石 ， 所以 $ V ^ { \ \ pi _ 2 } ( 13 ) = 0.9 $ 。 显然 在 状态 13 下 ， 策略 2 更好 ， 对应 的 策略 值 函数 也 越 大 。 因此 ，   策略 值 函数 $ V ^ { \ \ pi } ( S ) $ 可以 用来 评估 一个 策略 好不好   。               状态值 函数 $ V ( S ) $ 与 策略 值 函数 $ V ^ { \ \ pi } ( S ) $ 不同 的 是 ， 前者 采用 的 是 最佳 策略 ， 而 后者 采用 的 是 具体 的 某个 策略 $ \ \ pi $ 。 两者 的 关系 是 ， $ V ( S ) $ 是 采用 最佳 策略 $ \ \ pi   ^   *   $ 下 的 策略 值 函数 ， 即 $ V ( S )   =   V ^ { \ \ pi   ^   *   } ( S ) $ 。       动作 值 函数       前面 的 状态值 函数 可以 用来 评估 一个 状态 好不好 ， 那么 在 一个 给定 的 状态 下 ， 怎么 评估 某个 动作 好不好 呢 ？ 答案 是 动作 值 函数 ！       定义 ： 动作 值 函数 Q ( S ,   A ) 是从 状态 S 出发 ， 采用 动作 A 后 ， 所 能 获得 的 最大 期望 回报 ！ 动作 值 函数 也 称 Q 函数 。       $ $     Q ( S ,   A )   =   \ \ max   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ 1 = A   \ \ right ]     $ $               求和 包括 两 部分 ， 第一 部分 是 第一步 采用 动作 A 后 调到 状态 S ' 所 获得 的 单步 回报 $ R ( S ,   A ,   S ' ) $ ， 第二 部分 是从 S ' 开始 采用 最优 策略 所 获得 的 最大 回报 ， 这 一部分 可以 用 状态 S ' 的 值 函数 表示 $ V ( S ' ) $ 。 所以 总 回报 为 $ R ( S ,   A ,   S ' )   +   \ \ gamma   V ( S ' ) $ 。 考虑 环境 的 随机性 后 ， 需要 对 所有 的 可能 状态 S ' 求 期望 ， 于是 有 如下 关系       $ $     Q ( S ,   A )   =   \ \ sum _ { s ' }   P ( s ' |   S ,   A ) [   R ( S ,   A ,   s ' )   +   \ \ gamma   V ( s ' )   ]     $ $       以 确定性 环境 为例 （ 如上图 所示 ） ， 设 单步 回报 为 - 0.1 ， 在 初始状态 S 时 ， 可以 有 3 个 动作 ， 分别 是 上 、 左 、 右 ， 当 第一步 采用 动作 上后 ， 所 获得 的 回报 包括 两 部分 ， 第一 部分 是 执行 动作 上后 调到 状态 S ' 的 回报 R ( S , 上 , S ' ) ， 第二 部分 是从 S ' 开始 采用 最优 策略 所 获得 的 回报 ， 这 正是 状态值 函数 的 定义 ， 所以 这部分 回报 是 V ( S ' ) 。 综上 可 得 动作 值 函数 $ Q ( S , 上 )   =   R ( S , 上 , S ' )   +   \ \ gamma   V ( S ' ) $ 。       根据 值 函数 的 定义 ， 对 初始状态 S ， 其值 函数 V ( S ) 是 之后 每 一步 都 采用 最优 策略 带来 的 总 回报 期望值 。 而 动作 值 函数 Q ( S ,   A ) 要求 第一步 必须 采用 动作 A ， 从 第二步 开始 才 采用 最优 策略 ， 如果 第一步 采用 的 动作 A 就是 最优 策略 的 动作 ， 那么 动作 值 函数 就 等于 状态值 函数 ！ 也 就是       $ $     V ( S )   =   \ \ max _ a   Q ( S ,   a )     $ $       并且 可以 得到 最优 动作 $ A ^   *   =   \ \ arg \ \ max _ a   Q ( S ,   a ) $ 。               这 两个 关系 表明 ， 在 环境 已知 的 情况 下 ， 即 转移 概率 P 和 回报 函数 R 已知 ， Q 和 V 可以 互相转化 ， 只 要求 出 其中 一个 ， 另 一个 也 就 求 出来 了 。       同理 可以 定义 在 具体 策略 $ \ \ pi $ 下 的 动作 值 函数 为 ： 从 状态 S 出发 ， 采用 动作 A 后 ， 然后 在 以后 的 决策 中 采用 策略 $ \ \ pi $ 选择 动作 ， 所 能 获得 的 期望 回报 ！       $ $     Q ^ { \ \ pi } ( S ,   A )   =   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ 1 = A ,   A _ t   \ \ sim   \ \ pi   \ \ right ]     $ $       与 动作 值 函数 的 差别 在于 从 第二步 开始 ， 动作 按照 策略 $ \ \ pi $ 给出 而 不是 最优 策略 。       HJB 方程       利用 动作 值 函数 可以 评估 一个 动作 的 好坏 ， 给出 最优 策略 。 因此 ， 如果 我们 能够 求出 Q 函数 ， 那么 最优 策略 也 就 出来 了 ； 如果 能够 求 出 状态值 函数 ， 根据 前面 这 两个 值 函数 的 关系 ， 也 可以 得到 Q 函数 ， 从而 得到 最优 策略 。 那么 怎么 求 出 状态值 函数 呢 ？ 答案 是 利用 值 函数 的 递推 关系 ， 构建 HJB 方程 。       利用 状态值 函数 和 动作 值 函数 的 两个 关系 可 得       $ $     \ \ begin { align }     V ( s )   & amp ; =   \ \ max _ a   Q ( s ,   a )   \ \ \ \               & amp ; =   \ \ max _ a   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )   ]     \ \ end { align }     $ $       最后 一个 式子 就是 HJB 方程 ， 也 有 叫 贝尔曼 方程 的 。 这个 式子 中 ， 未知 的 是 状态值 函数 V ( s ) ， 已知 的 是 环境 的 状态 转移 概率 P 和 回报 R ， 由于 存在 求 max 操作 ， 使得 它 是 关于 状态值 函数 的 非线性 方程 ！ 如果 我们 从 这个 方程 中 求解 出 状态值 函数 V ( s ) ， 那么 最优 策略 的 问题 就 引刃 而解 了 ！       确定性 环境 的 HJB 方程       如果 环境 是 确定性 的 ， 即 采用 某个 动作 后 转移 到 的 状态 是 唯一 的 ， 在 迷宫 的 例子 中 ， 相当于 我 可以 直接 控制 机器人 的 运动 ， 而 不是 通过 语音 来 间接 控制 。 那么 上述 HJB 方程 的 求 期望 步骤 可以 省略 ， HJB 方程 变为       $ $     V ( s )   =   \ \ max _ a   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )     $ $       s ' 是 在 状态 s 下 采用 动作 a 后 转移 到 的 状态 ， 在 确定性 迷宫 问题 中 ， 考虑 s = 14 的 例子 ， HJB 方程 就是说 ( 记住   V ( 15 ) = 0 )       $ $     V ( 14 )   =   \ \ max   \ \ { \ \ gamma   V ( 13 ) ,   \ \ gamma   V ( 10 ) ,   1   \ \ }     $ $       对 每 一个 状态 s ， 都 可以 写出 上述 类似 的 非线性 方程 ， 我们 可以 得到 14 个 这样 的 非线性 方程 构成 的 非 线性方程组 。 那么 怎么 求解 这种 非线性 方程 呢 ？ 可能 有些 对 算法 比较 熟悉 的 同学 已经 从 上述 表达式 看 出来 了 ， 这 就是 动态 规划 ！       动态 规划       采用 动态 规划 求解 HJB 方程 有 两类 方法 ， 分别 是 值 迭代 和 策略 迭代 。 在 一定 条件 下 ， 他们 都 能 收敛 到 最优 解 。       值 迭代       值 迭代 的 基本 思想 是 ， 将 HJB 方程 看做 如下 函数 的 不动点       $ $     f ( V )   =   \ \ max _ a   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )   ]     $ $       V 是 一个 向量 ， 包含 多个 元素 ， 这个 函数 是 一个多 变量 非线性 函数 。 函数 f 的 不动点 是 指 满足 方程 $ f ( x )   =   x $ 的 解 。 根据   压缩 映像 原理   ， 如果 函数 f 是 压缩 映象 ， 那么 对 任何 初始值 $ V _ 0 $ ， 可以 不断 地 应用 函数 f 迭代 下去 $ V _ { k + 1 }   =   f ( V _ { k } ) $ ， $ V _ k $ 必 收敛 于 函数 f 的 不动点 ！ 而 f 的 不动点 就是 满足 HJB 方程 的 状态值 函数 ， 所以 有值 迭代 算法           初始化 $ V _ 0 ( i ) = 0 ,   i = 1 , 2 , ... $       利用 HJB 方程 迭代   $ V _ { k + 1 } ( s )   =   \ \ max _ a   R ( s ,   a ,   s ' )   +   \ \ gamma   V _ { k } ( s ' ) $       重复 第 2 步 直到 收敛 ！           当 $ \ \ gamma & lt ; 1 $ 时 ， 函数 f 一定 是 压缩 映像 ， 上述 算法 必 收敛 于 不动点 ！ 不动点 算法 也 是 一个 常用 的 求解 非线性 方程 或 线性方程 的 算法 。 下面 简单 介绍 一下 它 的 原理 。       压缩 映像 原理         注意 ： 理解 本 部分 需要 本科 及 以上 数学 功底 ， 高中数学 能力 的 请 跳 过 ， 接受 上述 结论 即可 ！         一个 定义 在 巴拿赫 空间 B 上 的 自 映射 f : B   →   B   是 压缩 映象 是 说 ， 对 任意 两个 $ x ,   y   \ \ in   B $ ， 有       $ $     d (   f ( x ) ,   f ( y )   )       \ \ le   \ \ gamma   d (   x ,   y   ) ,   0   & lt ;   \ \ gamma   & lt ;   1     $ $       d 是 距离 测度 ， 也就是说 在 这个 映射 下 ， 像 的 距离 比原 像 的 距离 短 ， 就 像 在 压缩 一样 ， 所以 叫做 压缩 映像 ( 也 说 映射 ) 。       压缩 映象 有个 不动点 定理 ， 说 如果 f 是 巴拿赫 空间 的 压缩 映象 ， 那么 必 存在 唯一 的 不动点 x 满足 不动点 方程 $ f ( x )   =   x $ 。 存在 性 我 就 不 证明 了 ， 可以 简单 解释一下 为什么 会 收敛 到 这个 不动点 。 假设 $ x _ 0 $ 是 某个 初始 点 ， $ x _ k   =   f ( x _ { k - 1 } ) $ ， $ x ^   *   $ 是 不动点 ， 那么 根据 压缩 映象 的 定义 ， 对 任意 正整数 k 有       $ $     | |   x _ { k }   -   x ^   *   | |   =       | |   f ( x _ { k - 1 } )   -   f ( x ^   * )   | |   \ \ \ \     & lt ;   \ \ gamma   | |   x _ { k - 1 }   -   x ^   *   | |   & lt ;   ...   & lt ;   \ \ gamma ^ k   | | x _ 0   -   x ^   *   | |   \ \ \ \     \ \ rightarrow   0   ( k   \ \ rightarrow   \ \ infty )     $ $       有 兴趣 的 同学 不妨 利用 类似 的 技巧 证明 一下 值 迭代 在 $ \ \ gamma & lt ; 1 $ 是 收敛 的 。       这个 性质 可以 用来 求解 非线性 方程 ， 下面 是 一个 简单 的 计算 $ \ \ sqrt { n } $ 例子 。 计算机 只会 加减乘除 ， 其他 数学 运算 都 要 表示 为 这 四则运算 才能 计算 ， 那么 怎么 计算 $ \ \ sqrt { n } $ 。 计算方法 有 很多 ， 这里 介绍 利用 压缩 映像 不动点 的 性质 的 计算方法 。 $ \ \ sqrt { n } $ 可以 看做 方程 $ x ^ 2   =   2 $ 的 解 ， 这个 方程 等价 于   $ x   =   0.5 ( n / x   +   x ) $ 。 因此 ， x 是 函数 $ f ( x )   =   0.5 ( n / x   +   x ) $ 的 不动点 。 容易 验证 f 是 非线性 函数 ， 且 是 压缩 映像 。 那么 就 可以 令 x0 = 1 ， 不断 地 应用 $ x _ k   =   f ( x _ { k - 1 } ) $ 迭代 下去 就 可以 了 ， 最终 就 会 收敛 到 $ \ \ sqrt { n } $ ！       策略 迭代       策略 迭代 是 另外 一种 求解 HJB 这个 线性方程 的 方法 ， 因为 这个 非线性 方程 的 所有 非线性 来自 于求 最大值 ， 前面 说 过 ， 如果 我们 把 策略 固定 ， HJB 方程 的 求 最大值 就 消失 了 ， 变成 线性方程 ！       $ $     V ^ { \ \ pi } ( s )   =   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ { \ \ pi } ( s ' )   ]     $ $       线性方程 很 容易 解决 ， 从 小学 就 开始 学了 ， 不停 地 代换 消元 即可 ， 这 就是 高斯消 元法 。 这个 过程 ， 称作 策略 评估 ， 因为 计算出来 的 是 某个 策略 下 的 值 函数 ， 值 函数 可以 来 评估 策略 的 好坏 ， 所以 叫 策略 评估 。       当 $ V ^ { \ \ pi } $ 计算 好 了 之后 ， 我们 又 可以 通过 它 得到 一个 新 的 策略       $ $     \ \ pi ' ( s )   =   \ \ arg \ \ max _ a   Q ^ { \ \ pi } ( s ,   a )     $ $       因为 Q 函数 和 V 函数 可以 知一求 二 ， 前面 知道 了 V 函数 ， 利用 V 和 Q 的 关系 很 容易 得到 Q ， 然后 就 可以 计算 这个 新 的 策略 了 。 可以 证明 这个 策略 不会 比 之前 的 策略 差 ， 也 就是 对 所有 状态 s ， 两个 策略 的 值 函数 满足 $ V ^ { \ \ pi } ( s )   \ \ le   V ^ { \ \ pi ' } ( s ) $ ， 因此 只要 不断 地 迭代 下去 ， 也 可以 得到 最优 的 策略 ！ 这 一步 叫做 策略 提升 ， 因为 新 的 策略 效果 提升 啦 ！       综合 这 两步 ， 我们 就 得到 策略 迭代 算法 ：           初始化 一个 策略 $ \ \ pi $       通过 解 线性方程 计算 策略 $ \ \ pi $ 的 值 函数 V 和 Q       从 Q 函数 中 得到 新 的 策略 ， 更新 到 $ \ \ pi $ 中       重复   2 - 3   直到 策略 收敛 ！           两种 迭代法 的 对比       这 两种 迭代法   本质 都 是 在 求解 HJB 方程   ， 得到 值 函数 的 值 ， 因为 状态 是 有限 的 ， 所以 值 函数 就是 一个 向量 。 得到 值 函数 之后 ， 就 可以 得到 最优 策略 了 。 值 迭代 只有 一个 迭代 ， 反复 使用 HJB 方程 进行 迭代 ， 直到 收敛 就 可以 了 。 而 策略 迭代 先 固定 策略 ， 接 线性方程 得到 值 函数 ， 然后 利用 值 函数 来 提升 策略 ， 这 两个 步骤 反复 迭代 ， 直到 找到 最优 策略 。 策略 迭代 通常 的 迭代 次数 会 比值 迭代 要少 ， 但是 内部 接 线性方程 耗时 会 比较 多 ， 两种 迭代 方法 都 在 一定 条件 下 可以 收敛 到 最佳 策略 。 在 实践 部分 ， 我们 将 实现 这 两个 算法 ， 初始 代码 和 环境 已经 准备 好 了 ， 你 只 需要 是 想 这 两个 算法 就行 。       环境 未知 的 问题       前面 我们 讲 到 ， 如果 环境 已知 ， 也 就是 转移 概率 P 和 回报 函数 R 已知 ， 我们 可以 通过 求解 HJB 方程 得到 值 函数 ， 进而 得到 最优 策略 。 但是 如果 我们 不 知道 环境 会 对 我们 做出 如何 反馈 ， 就 像 身 在 迷宫 中 的 机器人 ， 看不到 全貌 。 那么 我们 该 如何 得到 值 函数 和 最优 策略 呢 ？ 答案 是 通过 蒙特卡罗 模拟 ， 估计 出 环境 的 转移 概率 P 和 回报 函数 R 。 因为 状态 是 有限 的 ， 动作 也 是 有限 的 ， 所以 只要 用 很多 个 机器人 采用 完全 随机 的 策略 进行 尝试 ， 那么 根据 尝试 的 结果 ， 可以 估计 出 转移 概率 和 回报 函数 。 假设 随机 尝试 了 很多很多 次 ， 每 一次 采取 动作 都 会 得到 一个 四元组 ( S ' ,   A ' ,   S ' ,   r ) 。 例如 在 迷宫 问题 中 ， 从 动作 1 开始 ， 采用 向 右 ( A = 4 ) ， 到达 状态 2 ， 环境 回报 为 - 1 . 那么 这个 四元组 就是 ( 1 , 4 , 2 , - 1 ) 。 当 得到 很多 这样 的 四元组 后 ， 就 可以 统计 每 一对 ( S ,   A ) 转移 到 S ' 的 次数 N ( S ,   A ,   S ' ) 和 遇到 的 所有 ( S ,   A ) 的 次数 N ( S ,   A ) ， 从而 得到 概率 和 回报       $ $     P ( S ' | S ,   A )   =   \ \ frac {   N ( S ,   A ,   S ' )   } { \ \ sum _ { s ' }   N ( S ,   A ,   s ' ) }   \ \ \ \     R ( S ,   A ,   S ' )   =   r     $ $       例如 ， 从 状态 1 出发 ， 采用 动作 4 ( 向 右 ) ， 有 90 次 转移 到 了 状态 2 ， 有 10 次 转移 到 了 状态 5 ， 所以 可以 估计 出 P ( 1 , 4 , 2 ) = 0.9 ,   P ( 1 , 4 , 5 ) = 0.1 ,   P ( 1 , 4 ,   其他 状态 ) = 0 。 这些 可能 的 状态 转移 带来 的 回报 都 是 - 1 ， 所以 R ( 1 , 4 , 所有 状态 ) = - 1 。       一旦 我们 通过 上述 模拟 方法 得到 对 环境 的 估计 ， 那么 就 可以 采用 上述 动态 规划 方法 求解 出值 函数 ， 进而 得到 最优 策略 ！       强化 学习 的 解释       强化 学习 和 传统 的 监督 学习 方式 有所不同 ， 它 没有 一个 很强 的 监督 信号 告诉 模型 ， 要 拟合 一个 什么样 的 函数 ？ 也 不 像 无 监督 学习 完全 没有 反馈 ， 强化 学习 有 一个 弱 的 反馈 信号 告诉 你 ， 你 采取 的 动作 是 对 的 还是 错 的 ， 就 像 一只 被 训练 接 铁饼 的 小狗 ， 如果 它 不 接住 铁饼 ， 将会 受到 一个 惩罚 ， 相反 ， 如果 它 接住 了 铁饼 ， 或者 主动 去 追 铁饼 ， 将会 收到 一个 奖励 。 这种 反馈 机制 ， 让 小狗 虽然 一 开始 不 知道 要 干什么 ， 但是 经过 不断 的 尝试 - 失败 ， 不断 地 探索 ， 它 将 最终 知道 自己 要 追逐 的 目标 是 什么 。 而 监督 学习 就是 有 一个 老师 ， 先 示范 一下 告诉 小狗 要 做 什么 ， 然后 让 小狗 跟着 做 。 无 监督 学习 则 既 没有 老师 ， 也 没有 反馈 ， 所以 小狗 也 不 知道 要 干 啥 ， 它 只会 做 自己 熟悉 的 事情 。 从 这个 角度 来看 ， 强化 学习 是 在 无 监督 学习 和 监督 学习 中间 的 一种 半 监督 学习 。       从 实现 来看 ， 强化 学习 与 监督 学习 不同 的 是 ， 它 有 动作 ！ 也 就是 有 主观 能动性 ， 监督 学习 则 是 被动 地 接受 老师 教给 的 知识 ， 没有 充分利用 主官 能动性 。 监督 学习 从 一 开始 就 知道 学习 的 目标 精确 的 是 什么 ， 比如 做 人脸识别 ， 你 一 开始 就 知道 这张 图片 是不是 人脸 ， 模型 要 做 的 就是 建立 一个 函数 输出 是 人脸 还 是不是 人脸 。 而 强化 学习 是 要 建模 策略 函数 ， 在 一 开始 是 不 知道 哪个 策略 是 对 的 还是 错 的 ， 只有 尝试 之后 ， 收到 环境 的 反馈 后 ， 才 知道 这种 尝试 是否 正确 。 这种 反馈 是 通过 和 环境 交互 得到 的 ， 而 不是 我们 “ 误差 冒泡 ” 的 算法 中 给出 的 。 拿 人脸识别 来说 ， 监督 学习 就是 已经 有 很多 标注 的 人脸 和 非 人脸 照片 ， 你 去 训练 一个 模型 ， 拟合 这个 输入 和 输出 。 而 强化 学习 则 是 你 不 知道 这个 任务 是 干什么 ， 只是 给 你 图片 ， 可以 和 我 互动 ， 我 不会 直接 告诉 你 答案 ， 你 可以 通过观察 我 的 表情 来 得到 一部分 关于 猜 对 还是 猜错 的 信息 。       从 应用 场景 来看 ， 监督 学习 主要 是 来 做 预测 的 ， 而 强化 学习 主要 是 来 做 决策 的 。 监督 学习 可以 先 预测 ， 然后 根据 预测 的 结果 信息 然后 进行 决策 。 而 强化 学习 则 直接 给出 策略 。 因此 从 这种 角度 来看 ， 似乎 强化 学习 更加 直接 。 例如 在 做 搜索 排序 ， 监督 学习 会 拟合 一个 得分 ， 然后 你 根据 这个 得分 从大到 小 排序 ， 或者 在 加上 一些 其他 策略 。 但是 显然 这种 不是 最优 的 ， 比如 得分 很 高 的 都 是 一些 非常 相似 的 东西 ， 都 排 在 前面 是 没什么 意义 的 。 而 强化 学习 直接 拟合 排序 策略 ， 将 监督 学习 人工 决策 的 过程 也 干 了 ， 因此 可能 得到 更好 的 效果 。       强化 学习 与 深度 学习 的 关系 ： 实际上 强化 学习 和 深度 学习 是 对 机器 学习 算法 两个 完全 不同 的 分类 方式 。 强化 学习 是 与 监督 学习 和 无 监督 学习 一个 范畴 的 ， 而 深度 学习 则 是从 学习 的 模型 层面 上 进行 划分 ， 是 与 决策树 、 浅层 模型 一个 范畴 的 。 强化 学习 是 一种 学习 的 通用 方法 ， 不 限制 用 什么 模型 ， 而 深度 学习 时 特指 用 某类 模型 而 没有 说用 什么 学习 方法 。 强化 学习 可以 和 深度 学习 模型 结合 起来 ， 就是 深度 强化 学习 ， AlphaGo 的 创始人 之一 曾 在 ICML 大会 上 说 AI = DL ( 深度 学习 )   +   RL ( 强化 学习 ) ， 可见 强化 学习 和 深度 学习 的 重要 地位 。       强化 学习 的 应用       有 注意力 的 图像识别 系统       在 监督 学习 中 ， 我们 说 到 一个 模型 就是 在 拟合 一个 函数 ， 输入 是 一些 特征 ， 图像 的 数字 表示 就是 它 的 特征 ， 图像 在 计算机 中 都 是 用 很多 数字 表示 ， 如果 是 黑白 图像 ， 每 一个 像素 是 一个 数值 ， 数值 大小 表示 明暗 。 图像识别 就是 在 拟合 这样 一个 函数 ， 输入 是 图像 ， 输出 是 图像 的 类别 。 对于 一张 高清 图片 ， 往往 需要 降低 分辨率 ， 减少 计算 量 ， 或者 分成 很多 子块 ， 进行 分类 ， 然后 再 把 字块 识别 的 结果 融合 起来 。 那么 怎么 选择 子快 呢 ？ 最 简单 的 方法 是 随机 按照 不同 位置 不同 分辨率 选出 很多 子块 ， 一种 更好 的 方案 是 第一个 子块 可以 根据 整个 图像 的 低分辨率 选取 ， 接下来 每次 选取 的 位置 根据 之间 看到 的 所有 子块 共同 决定 ， 这样 每次 选择 哪个 字块 就可以看 做 马尔科夫 决策 过程 。 状态 ( S ) 是 我 之前 看过 的 所有 图片 ， 动作 ( A ) 是 我 选择 的 字块 的 位置 （ 假设 字块 的 大小 被 固定 ） ， 回报 ( R ) 是 我 最终 的 识别 是否 准确 ， 识别 对 了 回报 为 1 ， 否则 为 0 。 这 就是 Google 在 2014 年 再 论文   Recurrent   Models   of   Visual   Attention   中 所用 的 方案 。 他 把 每次 查看 的 字块 的 机制 叫做 注意力 机制 ， 就 像 我们 人来 识别 图片 中 的 内容 时 ， 目光 的 焦点 会 在 图片 中 不断 跳动 ， 每 一次 跳动 就可以看 做 我们 的 一次 决策 过程 。             < ! - -   ###   超越 经验 的 推荐 算法     ###   阿里巴巴 鲁班 系统           ###   AlphaGo     ###   AI 玩游戏     - - >       思考 与 实践           考虑 如下 图 所示 的 MDP 问题 ， S 是 初始状态 ， G 是 终止 状态 ， 对于 非 终止 状态 ， 每个 状态 可以 采取 两个 动作 ： 左 或者 右 ， 每 一步 的 回报 都 是 - 1 。 其中 中间 有 一个 状态 ， 采取 的 动作 和 实际 运动 方向 是 相反 的 ， 也 就是 动作 是 向 左 ， 而 实际 运动 是 向 右 ， 其他 两个 状态 正常 。 一个 随机 策略 $ \ \ pi $ 定义 如下 ： 每 一次 都 随机 地以 概率 p 选择 动作 “ 右 ” ， 以 概率 ( 1 - p ) 选择 动作 “ 左 ” 。 试 推导 $ V ^ { \ \ pi } ( S ) $ 的 最大值 和 此时 的 概率 p 的 值 。 参考答案 ： p = 0.59 ,   V = - 11.6 。                       编程 实现 值 迭代 和 策略 迭代 算法           构建 一个 简单 的 模拟 环境 ， 有 nS 个 状态 ， 0 ， 1 ， ... ， nS - 1 ； 其中 nS - 1 是 终止 状态 。 该 环境 下 一共 两个 动作 ： 0 向 左 运动 ， 1 向 右 运动 ， 每个 动作 都 有 概率 p0 不动 ， p1 的 概率 会往 反 方向 运动 ,   1 - p0 - p1 概率 正常 运动 。                   0       1       2       ...       9                       ← · →       ← · →       ← · →       ← · →       终点                           import       numpy       as       np         nS       =       10       nA       =       2         # 不要 改 这个 参数       Done       =       nS       -       1       p0       =       0.1       p1       =       0.1       P       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       #   转移 概率       R       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       -       1.0       #   回报 都 是 - 1       gamma       =       1         #   环境 构建       for       s       in       range     (     nS     ) :               if       s       = =       Done     :       #   终止 态 转移 概率 都 为 0                       continue               for       a       in       range     (     nA     ) :                       inc       =       a       *       2       -       1       #   步长                                       P     [     s     ,       a     ,       s     ]       + =       p0       #   不 动                       P     [     s     ,       a     ,       max     (     0     ,       s       -       inc     ) ]       + =       p1       #   反 方向                       P     [     s     ,       a     ,       max     (     0     ,       s       +       inc     ) ]       + =       1       -       p0       -       p1       #   正常 运动             V       =       np     .     zeros     (     nS     )       #   值 迭代       for       it       in       range     (     1000     ) :                 ##   YOUR   CODE   HERE                 ##   END               pass         print       &# 39 ; iteral   steps : &# 39 ;     ,       it       print       V                 #   策略 迭代         pi       =       np     .     zeros     (     nS     ,       dtype     =     int     )       # 初始 策略 全部 往 左         for       it       in       range     (     100     ) :               V       =       np     .     zeros     (     nS     )               #   策略 评估 ， 解 线性方程                 ##   YOUR   CODE   HERE                 ##   END                 #   策略 提升                 ##   YOUR   CODE   HERE                 ##   END         print       &# 39 ; pi   = &# 39 ;     ,       pi       print       &# 39 ; V   = &# 39 ;     ,       V        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/basic-rl.html"},
      
      
      {"title": "第11.0讲：时间差分学习", "text": "    Table   of   Contents           关于           蒙特卡罗 方法                 关于       强化 学习 近年 大火 ， 最早 是因为 AlphaGo 使用 强化 学习 打败 人类 围棋 冠军 引发 的 。 在 那 之后 ， 强化 学习 在 工业 场景 应用 越来越 多 ， 原来 很多 做 搜索 、 推荐 、 广告 等 一直 在 用 监督 学习 的 业务 ， 也 开始 使用 强化 学习 来 优化 用户 体验 和 平台 收益 了 。 强化 学习 实际上 很 早就 提出 了 ， 事实上 ， 强化 学习 来源于 控制论 。 控制论 之 父 叫做   维纳   ， 学过 信号处理 的 可能 知道 他 ， 维纳滤波 就是 用 他 的 名字 命名 的 。 控制论 最早 源于 航天 ， 人们 要 控制 火箭 发射装置 ， 将 火箭 发射 到 地球 之外 ； 控制论 还 源于 机器人 控制 ， 人们 需要 用 算法 对 机器人 的 行为 进行 控制 。 所以 ， 很多 讲 强化 学习 的 文献 也 会 说 强化 学习 是 在 优化 控制策略 。       在 这 篇文章 中 ， 您 将 学习 到           环境 未知 情况 下 的 更好 的 学习 方法 ？       蒙特卡罗 方法 和 时间 差分 学习       SARSA ,   Q - learning       on - policy   与   off - policy   学习           我 期望 您 至少 有 ：           高中数学 水平 且 年满 18 岁 ， 部分 内容 需要 你 了解 监督 学习 ， 和 强化 学习 基本概念 ， 你 可以 通过 本 教程 前面 的 章节 进行 学习 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           蒙特卡罗 方法       在 上 一 讲 中 ， 我们 说 到 ， 对于 一个 MDP 问题 ， 如果 环境 已知 ， 也 就是 知道 环境 的 转移 概率 P 和 回报 函数 R ， 可以 通过 求解 贝尔曼 方程 得到 值 函数 ， 从而 得到 最优 策略 ， 求解 的 方法 有 两种 ， 分别 是 值 迭代 和 策略 迭代 。 如果 环境 未知 ， 我们 提到 一种 通过 随机 尝试 的 方法 ， 先 估计 出 环境 ， 然后 转化 为 环境 已知 问题 求解 。 这种 随机 尝试 效率 很差 ， 需要 尝试 很 多次 ， 并且 随机 尝试 本身 就 有 成本 ， 而且 需要 等到 尝试 很 多次 之后 ， 才能 得到 一个 比较 好 的 策略 。 那么 ， 能 不能 在 每 一次 尝试 之后 ， 都 能 将 策略 提升 到 一个 更好 的 策略 呢 ？ 因为 每 一次 尝试 ， 环境 的 反馈 都 会 提供 一部分 对 环境 的 信息 ， 如果 我们 能 利用 好 这部分 信息 ， 就 有 可能 从中 获取 到 有 价值 的 信息 来 提升 我们 的 策略 。       还是 以 走 迷宫 为例 （ 如图所示 ） ， 并且 考虑 确定性 迷宫 这种 简单 情况 。 我们 的 目标 是 估计 出 每个 状态 s 下 ， 采取 每 一个 动作 a 的 动作 值 函数 Q ， 即 从 s 出发 ， 第一步 采取 动作 a 所能 获得 的 最大 回报 。 那么 ， 一个 简单 的 想法 是 ， 从 某个 状态 s 出发 ， 例如 状态 1 ， 如果 某 一次 行动 ， 采用 动作 a = 向 右 获得 了 钻石 ， 那么 动作 a = 向 右 更 有 可能 有 较大 的 Q ； 相反 如果 a = 向下 掉 到 了 火坑 ， 那么 动作 a = 向下 更 有 可能 有 较 小 的 Q 。 这 表明 ， 从 某 一次 的 尝试 中 ， 已经 能够 获得 一些 关于 环境 的 信息 ， 虽然 信息量 没有 多到 足以 完全 确定 最优 策略 的 地步 ， 但是 这个 信息 已经 可以 用来 更新 我们 的 策略 了 。 例如 ， 在 以后 的 尝试 中 ， 可以 给 动作 a = 向 右 以 更 大 的 概率 ， 而 给 动作 a = 向下 以 更 小 的 概率 。                     为什么 估计 动作 值 函数 而 不是 状态值 函数 ？         我们 的 目标 是 得到 动作 值 函数 Q ( s ,   a ) 而 不是 状态值 函数 V ( s ) ， 因为 只有 状态值 函数 V ( s ) 的 情况 下 ， 我们 还 需要 知道 转移 概率 P 才能 得到 动作 值 函数 ， 反过来 则 简单 得 多 。 在 环境 未知 ， 也 就是 转移 概率 你 无法 知道 ， 所以 要 估计 动作 值 函数           如果 我们 从 状态 s 出发 ， 采用 动作 a 尝试 了 n 次 ， 每 一次 的 总 回报 为 $ G _ i $ ， 那么 容易 估计 出 期望 回报       $ $     Q _ n ( s ,   a )   =   \ \ frac { 1 } { n } \ \ sum _ { i = 1 } ^ n   G _ i     $ $       想象 一下 ， 这 n 次 行动 是 一次 执行 的 ， 上 式 需要 这 n 次 尝试 完全 结束 后 ， 才能 估计 Q ， 事实上 可以 把 上式 改写 为 递归 形式       $ $     Q _ n ( s ,   a )   =   \ \ frac { 1 } { n } ( G _ n   +   ( n - 1 )   Q _ { n - 1 } ( s ,   a ) )   \ \ \ \     =   Q _ { n - 1 } ( s ,   a )   +   \ \ frac { 1 } { n } \ \ left (   G _ n   -   Q _ { n - 1 } ( s ,   a )   \ \ right )     $ $       也 就 说 ， 每 一次 尝试 实际 得到 的 回报 可以 用来 调整 之前 对 Q 函数 的 估计 ， 每 一次 对 Q 函数 的 改变 就是 上 一次 估计 的 误差 $ G _ n   -   Q _ { n - 1 } ( s ,   a ) $ 乘 上 一个 系数 $ \ \ alpha   =   \ \ frac { 1 } { n } $ 。       这种 利用 经验 从 未知 环境 中 估计值 函数 的 方法 叫做 蒙特卡罗 法 。               梯度 下降              ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/td-learning.html"},
      
      
      
        
        
      
      {"title": "第0讲: 关于《Python教程》", "text": "    Table   of   Contents               Python 据说 已经 列入 中学 信息学 课程 中 ， 加上 人工智能 ， python 已经 成为 目前 仅次于 Java 的 编程语言 了 。 Python 简单 方便 ， 但是 对于 很多 人 ， 入门 还是 比较 困难 ， 所以 写 了 这样 一个 简单 的 教程 ， 帮助 大家 入门 Python 。  ", "tags": "tutorial/python", "url": "/wiki/tutorial/python/about.html"},
      
      
      {"title": "第1.0讲：Python初探", "text": "    Table   of   Contents           关于           为什么 编程           机器 适合 做 什么                   python 的 第一个 程序           安装 本地 python 环境           WINDOWS                   运行 python 程序 的 方法           命令行                         关于       如果 你 没有 什么 编程 基础 ， 想 学习 python 编程 ， 那么 本 教程 很 适合 你 。       为什么 编程       初次 听到 编程 的 时候 ， 我 认为 是 一件 很 高端 很 困难 的 事情 ， 相信 很多 人 都 有 类似 的 感受 。 编程语言 是 什么 ？ 人 与 人 之间 通过 语言 和 文字 进行 交流 ， 我 把 我 的 想法 通过 语言 告诉 你 ， 或者 通过 文字 告诉 你 。 但是 ， 一台 机器 ， 我们 怎么 把 自己 的 想法 告诉 他 呢 ？ 到 目前为止 ， 能够 直接 理解 自然语言 的 机器 尚未 制造 出来 ， 一些 可以 理解 自然语言 的 机器 最终 也 是 利用 编程语言 将 处理 自然语言 的 逻辑 告诉 机器 。 所以 ， 编程语言 是 目前 能够 直接 和 计算机 进行 交流 最 重要 的 方式 。       机器 适合 做 什么       设想 你 正在 浏览 一个 图片 类 的 网站 ， 你 想 把 页面 上 的 所有 图片 都 保存 下来 ， 或者 你 想 把 整个 网站 所有 的 图片 都 下载 下来 。 显然 一个 一个 用 鼠标 点击 下载 十分 费时费力 ， 那么 我们 能 不能 告诉 机器 自动 帮 我 把 所有 图片 都 下载 下来 呢 ？ 显然 目前 通过 自然语言 是 做 不到 的 ， 如果 你 对 他 说 ， “ 嘿 ， 快 帮 我 把 这个 网站 上 的 图片 都 下 下来 ！ ” ， 它 是 不会 理睬 你 的 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       设想 你 的 老板 要 你 统计 每天 公司 内 的 业务 报表 ， 并 把 分析 结果 发送 邮件 给 他 。 分析 过程 是 如此 的 程序化 ， 每 一天 做 得 事情 都 是 如此 的 相似 。 你 感叹 道 ： “ 为啥 让 我 天天 做 一些 没有 技术含量 的 事情 ！ ” 这是 ， 你 想象 如果 电脑 每天 能够 自动 把 这事 给 干 了 ， 就 可以 喝 喝茶 聊聊天 就 把 工作 给 完成 了 该 多 好 。 但是 ， 机器 仍然 不会 理睬 你 ， 它 不 懂用 意念 交流 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       设想 你 在 毕业 季 寻找 工作 ， 每天 都 关注 学校 BBS 或者 就业 论坛 中新 的 招聘会 信息 和 内 推 信息 。 为了 不错 过 每 一次 机会 ， 你 不得不 每隔 半小时 刷 一下 论坛 ， 有时候 一不小心 忘记 了 ， 就 把 一个 好 机会 给 偷偷 错过 了 ， 十分 懊恼 。 这时 ， 你 想 如果 有 新 的 信息 机器 就 能 直接 通知 你 就 好 了 。 但是 ， 机器 仍然 不会 理睬 你 ， 它 不 懂用 电话 交流 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       实际上 ， 编程 未来 将 成为 每 一个 人 的 基本技能 ， 就 像 现在 的 英语 一样 。 你 不 需要 深入 理解 计算机 、 理解 逻辑 门 ， 只 需要 熟悉 一门 语言 ， 就 可以 操纵 机器 为所欲为 ， 为什么 不 试试 python 呢 ？       python 的 第一个 程序       编程语言 是 用来 和 机器 交流 的 ， 那么 我们 先来 让 机器 给 我们 打个招呼 如何 ？ 如果 我们 跟 机器 用 自然语言 说 ： “ Hello ， 机器 ！ ” ， 它 听不懂 ， 但是 我们 可以 用 python 让 机器 跟 我们 打个招呼 ： “ Hello ,   World ! ” 。 用 Python 让 机器 输出 \" Hello ,   World ! \" 非常简单 ， 你 只 需写 上 一条 语句 即可 ！               print     (     & quot ; Hello ,   World ! & quot ;     )                       Hello ,   World !               你 可以 通过 在线 python 运行 环境 ， 先 动手 试试 ， 在 后面 将 告诉 你 怎么 在 本地 安装 python 环境 。 你 可以 试着 修改 双引号 里面 的 内容 ， 替换成 任何 你 想 让 机器 说 的话 。 赶紧 动手 试试 吧 。         runoob 的 python 在线 运行 环境         安装 本地 python 环境       安装 python 环境 的 步骤 取决于 你 的 操作系统 ， 对 不同 的 系统 我 将 分别 介绍       WINDOWS       首先 去 python 官方网站 下载安装 包 ：   https : / / www . python . org / downloads /   。 目前 python 有 2.7 和 3.6 两个 版本 ， 这 两个 版本 都 可以 ， 一般 下载 2.7 的 版本 即可 。               下载 下来 ， 按照 正常 的 软件 安装 即可 。       运行 python 程序 的 方法       命令行       用 记事本 或者 将 python 代码 文件 保存 到 一个 文本文件 ，  ", "tags": "tutorial/python", "url": "/wiki/tutorial/python/intro.html"},
      
      
      
      
      
      
        
      
      {"title": "github pages使用指南", "text": "  Github   Pages   使用指南  ", "tags": "web", "url": "/wiki/web/github-pages.html"},
      
      
      {"title": "wordpress开发指南", "text": "    Table   of   Contents           分类目录           为 分类目录 添加 元 数据                         分类目录       为 分类目录 添加 元 数据       钩子               / / 新建 分类目录       add _ action ( &# 39 ; category _ add _ form _ fields &# 39 ; ,   &# 39 ; cat _ add _ form _ fields _ cb &# 39 ; ) ;       / / 编辑 分类目录       add _ action (   &# 39 ; category _ edit _ form _ fields &# 39 ; ,   &# 39 ; cat _ edit _ form _ fields _ cb &# 39 ; ) ;         / /   保存       add _ action (   &# 39 ; edited _ category &# 39 ; ,   &# 39 ; cat _ form _ save _ fields _ cb &# 39 ; ) ;       add _ action (   &# 39 ; create _ category &# 39 ; ,   &# 39 ; cat _ form _ save _ fields _ cb &# 39 ; ) ;                 创建 和 更新 元 数据 函数               get _ term _ meta (   $ term _ id ,   $ meta _ key ,   $ single ) ;       add _ term _ meta ( $ term _ id ,   $ meta _ key ,   $ meta _ value ) ;       update _ term _ meta ( $ term _ id ,   $ meta _ key ,   $ meta _ value ) ;        ", "tags": "web", "url": "/wiki/web/wordpress.html"},
      
      
      {"title": "在github pages中使用simiki指南", "text": "  起因       开始 使用 simiki 是因为 一篇 博客 — —   程序员 的 知识 管理   ，     这篇 博客 对 我 启发 很大 。 是 的 ， 程序员 或者 一般 的 工程师 经常 需要 配置 一些 开发 环境 之类 的     工作 ， 如果 能够 将 这些 过程 记录下来 ， 日后 再 配置 的 时候 会 减少 很多 不必要 的 时间 浪费 。     此外 ， 如果 能 将 平常 一些 琐碎 的 知识 记录下来 也 是 不错 的 。 使用 simiki 可以 将 这些 工作 通过     wiki 来 实现 ， 并且 可以 将 数据 保存 在 本地 ， 不用 担心 数据 丢失 之类 的 风险 。         请 不要 FORK 我 的 WIKI ！ ！     因为 这个 WIKI 里面 的 内容 大多 是 我 原创 的 ， 请 不要 FORK ， 谢谢 。       工具 准备       为了 使用 simiki ， 需要 准备 好 基本 环境 。               安装 python ， 不同 的 平台 安装 方式 不同 ， 都 很 容易 。               安装 simiki 库 及其 依赖 库 ， 我 使用 pip 进行 安装 ， 只 需要 一条 命令   pip   install   simiki   即可 。               注册 github 账户 ， 并且 创建   & lt ; username & gt ; . github . io   代码 项目 。 完成 之后 ， 你 应该 可以 通过 该           子 域名 访问 到 自己 的 page 页面 ， 具体 细节 请 参考   官方 文档   。               环境 配置 过程       在 github 中 创建   wiki   项目 ， 并 创建   gh - pages   分支 [ 1 ] 。             git   clone   git @ github . com : tracholar / wiki . git   git   checkout   - b   gh - pages   git   rm   - rf   .               切换 到 master 分支 初始化 simiki ， 生成 content 和 themes 目录 和 几个 文件 。 并 在 output 目录 生成 静态 文件 。             git   checkout   master   simiki   init   simiki   g               部署       windows 中 的 部署       为了 部署 方便 ， 在 master 分钟 中将 output 目录 过滤 掉 ，     在 . gitignore 文件 中 添加 一行             output               即可 。 此外 ， 需要 将 output 目录 push 到 gh - pages 分支 ， 可以 利用 我 写 的 一个     批处理 脚本   deploy . bat   。     这个 脚本 提供 两个 功能 ， 初始化 和 部署 。             deploy   [ option ]           - i     初始化           message     以 message 作为 git   commit 信息 提交 并 推送 到 github               如果 你 要 使用 请 把 脚本 中库 的 URL 改成 你 的 URL 。     初始化 之前 请 删除 前面 生成 的 output 目录 ， 然后 执行 以下 命令 。             deploy   - i               然后 重新 提交 部署             deploy   init - version               然后 就 可以 在 你 的 项目 页面 中 看到 wiki 了 ， 对 我 来说 是   tracholar . github . io / wiki 。           Tips :   注意 文件夹 和 文件 命名 统一 用 小写 ， 否则 你 会 后悔 ， 因为 windows 不 区分 大小写     而 linux 是 区分 的 。           linux / MAC   中 的 部署       还 没试 过 ， 等 试过 之后 再 写 ， 你 可以 参考   官方 指南       使用   ghp - import   部署 ， 更 方便 ， 只是 不 支持 windows 。       公司 发了 mac ， 我 又 写 了 个 bash 脚本   deploy . sh   。     使用 方法 跟 windows 上 的 一样 ， 不过 要   chmod   + x   deploy . sh   给 脚本 增加 执行 权限 。     当然 ， 你 也 需要 把 git 仓库 的 地址 改成 你 自己 的 地址 。       我 的 效果       请 访问   https : / / tracholar . github . io / wiki   观看 。         请 不要 FORK 我 的 WIKI ！ ！     因为 这个 WIKI 里面 的 内容 大多 是 我 原创 的 ， 请 不要 FORK ， 谢谢 。       [ 1 ]   https : / / help . github . com / articles / creating - project - pages - manually /    ", "tags": "web", "url": "/wiki/web/simiki.html"},
      
      
]};