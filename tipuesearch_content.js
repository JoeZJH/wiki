/*DEBUG
setencoding(utf-8)
*/


var tipuesearch = {"pages": [
    
      
      
      
        
      
      {"title": "关于这个WIKI", "text": "  关于       @ tracholar   过去 学 物理 ， 后来 搞 通信 ， 现在 搞 数据挖掘 。       硕士论文 :   光通信 中 量子 接收机 的 理论 与 初步 实验 研究   是 做 量子 接收机 ， 现在 做 数据挖掘 。       这 是 我 的 个人 知识 WIKI ， 采用   @ simiki       制作 而成 ， 用于 保存 自己 在 学习 和 工作 中 的 琐碎 知识 ， 如果 希望 阅读 我 的 观点 和     深入分析 ， 请 移步 到   我 的 个人 博客   。       如果 想 和 我 一样 使用 simiki 制作 一个 自己 的 wiki ， 除了 可以 参考 官方 文档 外 ， 也     可以 参考   在 github   pages 中 使用 simiki 指南   。  ", "tags": "about", "url": "/wiki/about/intro.html"},
      
      
      {"title": "关于阅读论文的几点个人经验", "text": "    Table   of   Contents           保持 阅读 论文 重要性           第一步   精读 10 篇 以上 经典 论文           第二步   快速 阅读 多个 领域 类 的 文章           第三步   有 选择地 阅读                 保持 阅读 论文 重要性       对于 从事 数据挖掘 、 人工智能 相关 领域 的 从业人员 来说 , 保持 对 新 知识 的 热情 很 重要 。 学习 新 的 算法 和 模型 的 一个 重要途径 就是 阅读 论文 。 本文 将 自己 从 研究生 三年 到 工作 这 三年 的 阅读 论文 的 个人 经验 和 体会 记录下来 , 希望 对 读者 有所 帮助 。       第一步   精读 10 篇 以上 经典 论文       对于 一个 积累 不足 的 读者 来说 , 想要 实现 快速 阅读 论文 、 快速 掌握 公众 号 里面 的 算法 和 模型 实际上 是 很 困难 的 。 所以 我 建议 的 第一步 , 先 精读 10 篇 以上 经典 论文 。       有 几点 需要 解释一下 :     1 .   10 篇 只是 一个 大概 的 数字 , 不 一定 要 完全 按照 这个 数字 来     2 .   阅读 的 时候 做好 笔记 , 记录 你 认为 有 价值 的 、 重要 的 知识点 , 以及 不太 明白 的 知识点 , 以及 自己 的 思考 和 体会 。 这 一步 非常 重要 , 很多 人 不 注意 就 会 把 精读 变成 快读 了 , 或者 根本 读 不 下去 。     3 .   有 相关 的 重要文献 , 也 一起 读 一下 , 重点 是 跟 你 精读 论文 相关 的 部分 , 其他 部分 可以 不用 深究 , 看 个人 情况 就 好 。     4 .   经典 论文 可以 找人 推荐 、 或者 在 知乎 上 找 一些 人 推荐 的 文章 ,   一般 看 引用 量 就 能 知道 , 有 一个 很 好 的 地方 可以 找到 经典 论文 ,   一些 著名 的 公开课 如 CS231n ,   CS224n ,   CS234   等 都 会 提供 很多 参考文献 , 而 课程 的 PPT 可以 当做 很 好 的 索引 将 这些 论文 串联 起来     5 .   自己 动手 实现 里面 重要 的 算法 。 这 一点 也 是 很 重要 的 , 完全 复现 论文 是 一件 工作量 很大 的 事情 , 而且 对于 刚 开始 学习 不久 的 人 也 很 困难 , 但是 我们 可以 只 实现 里面 比较 重要 的 核心 算法 , 或者 你 自己 觉得 有趣 的 点 。 这 一步 的 核心 精神 是 通过 动手 , 加深 理解     6 .   当 阅读 一定量 的 时候 要 形成 自己 的 理解 , 形成 自己 的 知识结构 , 知识点 之间 缺乏 联系 那 是 知识 孤岛 , 所以 平常 要 多 想想 不同 算法 、 模型 之间 的 联系 , 比如 思考 :   是否 存在 一种 通用 的 方法 把 它们 都 归类 到 一起 , 是否 两种 模型 一种 是 另外 一种 的 特例 ?   新 的 算法 和 经典 算法 之间 的 关系 是 什么 ?   等等 这些 问题       第二步   快速 阅读 多个 领域 类 的 文章       有 了 第一步 的 基础 后 , 就 有 一些 积累 了 , 可以 开始 泛读 快速 扩大 自己 的 知识面 了 。 论文 可以 通过 reference 、 公开课 、 网上 有心人 收集 的 论文 列表 、 综述 等 方式 。       有 几点 需要 解释一下 :     1 .   泛读 的 重点 , 理解 论文 的 核心 创新 点 , 记录 笔记     2 .   通过 大量 阅读 , 总结 出 更 一般化 的 经验 , 极大 地 扩充 知识面     3 .   泛读 的 同时 , 发现 值得 精读 的 文章 也 要 按照 精读 的 方法 阅读       第三步   有 选择地 阅读       有 了 第二步 之后 , 有 了 一定 知识 储备 , 可以 有 选择地 选择 一些 文章 进行 精读 和 复现 论文 结论 , 很多 论文 的 结论 并不一定 靠 谱 , 这 一步 可以 帮助 你 真正 成为 领域专家 , 并且 可以 创造 新 的 知识 、 算法 和 模型 。       有 几点 需要 解释一下 :     1 .   选读 的 同事 也 要 一定 的 泛读     2 .   积极 跟踪 最新 论文 , 重要 的 国际 会议 , 建议 每篇 文章 的 标题 至少 刷 一遍 , 其次 最好 刷 一遍 摘要 , 筛选 出 感兴趣 的 文章     3 .   这 一步 的 目的 是 建立 自己 的 知识 网 , 并 牢固 他们 , 甚至 为 别人 创造 新 的 知 ( 挖 ) 识 ( 新 ) 点 ( 坑 ) 。  ", "tags": "about", "url": "/wiki/about/read-paper.html"},
      
      
      {"title": "原则", "text": "    Table   of   Contents           原则           关于 学习 和 生活 的 原则           关于 投资 的 原则                 原则       读 了 瑞 · 达利 欧 的 《 原则 》 , 觉得很有 感悟 , 可以 借鉴 一下 。     原则 就要 写 下来 , 并 及时 改进 。 错误 也 要 及时 记录 , 及时 改进 。       关于 学习 和 生活 的 原则           读书 / 论文 一定 要 记录 点 东西 , 比如 笔记 、 读后感 、 思考 etc       看 论文 的 原则       精读 论文 做 详细 笔记 , 并 写 代码 实现 , 至少 要 用 构造 出来 的 toy   data 复现 基本原理 , 如果 能 复现 论文 的 完整 结果 , 当然 最好       泛读 论文 做 简略 笔记 , 并 写出 与 其他 对比 论文 的 相似 点 和 差异 点 , 或者 写出 在 自己 工作 中 可能 的 用途               看书 的 原则       教材 类 的 书 一定 要记 详细 的 笔记       非 教材 类 的 书 , 可以 记录 一下 重要 观点 , 以及 自己 的 理解               一定 要 看 原始数据 , 并 亲自 使用 原始数据 按照 作者 的 思路 进行 分析 , 查看 能否 得到 相同 的 结论       如果 一件 事情 有 截止 时间 , 在 你 知道 的 时刻 开始 , 就要 开始 行动 起来 并 在 大脑 中 构思 , 哪怕 只是 写个 标题 列个 大纲       构建 一个 名义 锚 , 比如 : 写 PPT , 每天 至少 完成 一张 PPT , 以 页数 做 名义 锚 ;   写 文档 , 每天 至少 写 1000 字 ;   etc                   关于 投资 的 原则           每次 投资 金额 不要 过大 , 宁愿 多 几次 操作 , 目前 认为 不 应该 超过 全部 资金 的 1 %       除了 定 投 之外 , 手动 买入 时机 一定 要 在 大跌 的 时候 操作 , 一般 要 超过 2 % 的 跌幅 以上       除非 特定 情况 , 不要 空仓 也 不要 满仓 , 让 自己 有 操作 余地       进行 压力 测试 , 权益 类 最大 跌幅 20 % 、 50 % 的 总 资金 损失 , 债务 类 最大 跌幅 5 % 、 10 % 的 总 资金 损失 , 汇率 波动 5 % 、 10 % 的 总 资金 损失       资产 配置 中 每 一种 资产 至少 要 有 一种 相关系数 很 低 或者 负相关 的 其他 资产      ", "tags": "about", "url": "/wiki/about/principles.html"},
      
      
      {"title": "短中长期规划", "text": "  短期           熟悉 spark 开发 流程 ， 学会 使用 mllib 在线 上 建模   ✅       cs224d     ✅       cs231n     ✅       cs294 ,   cs234   RL       GAN       TensorFlow   系统性 学习 , 并 完成 记录 和 博客   [ TODO ]       《 货币 金融学 》           中期           deep   learning   overview   ：   Bengio   的 书 及 相关 论文       machine   learning   overview   ：   周志华 的 书 ， prml ,   element   of   ml       分布式 工具 和 环境 ，   Spark ，   Hbase ，   Hadoop ，   Hive       分布式 算法 ， 优化 算法 等       金融学 基础知识           长期           ML   技术 栈 ， 从 工程 到 算法 到 系统       金融学 基础知识       从 财务 和 技术 两 方面 分析 主流 IT 企业           2018 年           6 篇 深入 思考 博客   [ 完成 30 % ]       10 篇   关键 论文 结果 的 复现   [ 未 完成 ]           总结 :   缺乏 短期 规划 ,   和 短期 不断 的 调整 与 反馈       2019 年           3 篇 深入 思考 博客       10 篇 重要 论文 中 的 关键 部分 复现       3 篇 金融学 书       10   项 宏观经济 数据 及 分析 报告       每周 计划 、 调整 与 反馈      ", "tags": "about", "url": "/wiki/about/plan.html"},
      
      
      
      
      
        
      
      {"title": "Faiss", "text": "    Table   of   Contents          ", "tags": "algo", "url": "/wiki/algo/faiss.html"},
      
      
      {"title": "最近邻搜索", "text": "    Table   of   Contents           关于           近似 方法                 关于         最近 邻 搜索   算法 在 文本 、 图像 检索 中 应用 广泛 ， 在 机器 学习 中 也 有 很多 应用 场景 ， 例如   knn   分类 。       近似 方法             LSH ( Locality - sensitive   hashing )         随机 投影 方法 ：   SimHash              ", "tags": "algo", "url": "/wiki/algo/nearest-search.html"},
      
      
      
      
      
        
      
      {"title": "以太坊网络的命令行工具", "text": "    Table   of   Contents           客户端           Geth           MAC   OS           WINDOWS           LINUX                   ETH           MAC           在 LINUX 上 安装 ：           在 WINDOWS 上 安装                   Python           其他 选项                   运行           连接 到 一个 私人 测试 网           日志           了解 有关 运行 节点 的 更 多 信息                   用法 示例           创建 帐户           获取 帐户 的 余额           一次 检查 所有 余额                         这些 是 区块 链 开发者 的 工具 。   命令行 工具 将 允许 您 连接 服务器 或 在 以太 坊 区块 链上 运行 应用程序   -   或者 您 自己 的 私人 区块 链 。       客户端       为了 安全 起 见   ， 为 以太 坊 创建 了 三个 独立 的 实现 。   客户端 具有 几乎 相同 的 功能 ， 因此 您 选择 的 客户端 可以 在 平台 、 语言 以及 您 计划 的 网络 使用 方面 进行 个人 选择 。       如果 您 正在 构建 一个 需要 对 以太 坊 网络 提供 最大 正常 运行 时间 保证 的 业务 ， 那么 我们 建议您 至少 运行 两个 客户端 的 一个 实例 以 确保 可靠性 。       Geth               Go 语言 的 实现 被 称为 Geth   。   Geth 已经 过 安全 审计 ， 将 成为 面向 最终用户 的     Mist   Browser     的 未来 基础 ， 所以 如果 你 有 网站 开发 经验 ， 并且 有 兴趣 为 dapps 构建 前端 ， 你 应该 试用 Geth 。       MAC   OS       安装 Homebrew   ， 并 确保 它 是 最新 的 ：             brew   update   brew   upgrade               然后 使用 这些 命令 来 安装 以太 坊 ：             brew   tap   ethereum / ethereum   brew   install   ethereum               有关 更 多 信息 ， 请参阅   Mac   OSX   Geth 的 完整 文档         WINDOWS       下载 最新 的 稳定   二进制 文件     ， 解压缩 ， 下载 压缩文件 ， 从 压缩包 中 提取 geth . exe ， 打开 一个 命令 终端 并 输入 ：             chdir   & lt ; path   to   extracted   binary & gt ;   open   geth . exe               有关 更 多 信息 ， 请参阅   Windows   Geth 上 的 完整 文档         LINUX       在 Ubuntu 上 ， 执行 这些 命令 ：             sudo   apt - get   install   software - properties - common   sudo   add - apt - repository   - y   ppa : ethereum / ethereum   sudo   apt - get   update   sudo   apt - get   install   ethereum               有关 其他 环境 和 更 多 说明 ， 请参阅   Geth   linux 的 完整 文档         ETH               C++ 实现 称为 Eth   。   如果 您 希望 通过 并行 运行 两种 不同 的 实现 来 增加 安全性 ， 或者 认真对待 GPU 挖掘 ， 那么 C++ “ Eth ” 客户端 就是 为您服务 的 。       MAC       安装 Homebrew   ， 然后 确保 它 是 最新 的 ：             brew   update   brew   upgrade               然后 使用 这些 命令 来 安装 cpp - ethereum ：             brew   tap   ethereum / ethereum   brew   install   cpp - ethereum   brew   linkapps   cpp - ethereum                 cpp - ethereum 文档   提供 了 有关 OS   X   Homebrew 软件包 和 从 Source 构建 OS   X 的 详细信息 。       在 LINUX 上 安装 ：       如果 您 使用 Apt - get ， 请 将 其 粘贴 到 终端 中 ：             apt - get   install   cpp - ethereum               cpp - ethereum 文档 详细 介绍 了 关于 Ubuntu 的 PPA 以及 从 Source 构建 Linux 的 信息   。       在 WINDOWS 上 安装       cpp - ethereum 文档 包含 有关 从 源代码 构建 Windows 的 详细信息 。       Python       Python 的 实现 被 称为 Pyethapp 。   如果 您 有 兴趣 了解 以太 坊 如何 工作 以及 如何 扩展 它 ， 那么 这个 客户端 的 代码 基础 可能 是 最具 可读性 的 ， 并且 拥有 一个 具有 快速 开发周期 的 合同 测试 程序库 。   这 并 不 意味着 高端 的 使用 ， 因为 这个 客户端 的 性能 没有 那么 高 的 清晰度 和 可靠性 。   如果 你 是 一个 Python 开发者 ， 希望 构建 分布式 的 应用程序 ， 或者 对 以太 坊 有 兴趣 进行 研究 或 学术研究 ， 那么 这是 一个 很 好 的 客户端 ： 我们 邀请 你 来 看看 并 为 之 作出贡献   。       其他 选项           奇偶校验 技术   （   Parity   Technologies ） 实施 了 一个 Rust 实施       由 Blockapps 开发 的 一个 Haskell 实现       如果 您 有 兴趣 开发 一个 完全 在 Web 浏览器 中 运行 的 轻型 应用程序 ， 那么 我们 推荐 使用 EthereumJS 作为 基础 。       如果 您 想 创建 一个 小型 硬件 项目 ， 请 查看 Raspberry   Pi 的 实现       如果 你 想 为 非 Ubuntu   Linux 安装 geth ， 那么 我们 建议 你 从 源代码 开始 构建       如果 你 想 在 Mac 上 有 更 多 的 灵活性 ， 试试 Homebrew           运行       Geth 和 Eth 是 运行 完整 的 以太 坊 节点 的 多用途 命令行 工具 。   它们 提供 了 多个 接口 ：   命令行 子 命令 和 选项 ，   JSON - RPC 服务器 和 交互式 控制台   。       对于 本 指南 ， 我们 将 重点 介绍 控制台 ， 这是 一个 包含 您 可能 需要 的 所有 主要 功能 的 JavaScript 环境 。   根据 您 的 客户端 ， 粘贴 以下 任一 命令 ：       Geth             geth   console               Eth       Eth 仍然 有 一个 内置 的 控制台 ， 但 它 很快 就 会 被 移除 。   开始 使用             eth               然后 使用   geth   attach     （ 如果 你 也 有 geth ） 或者 下面 的 npm 控制台 。   Eth 可能 需要 一些 时间 才能 启动 。             npm   install   - g   ethereum - console   ethconsole               第一次 启动 命令 行时 ， 您 将 获得 许可证 。   在 您 使用 之前 ， 您 必须 先 接受 此 许可证 ， 请 仔细阅读 。         注意 ： 如果 您 只 想 测试 技术 并 玩耍 ， 请勿 使用 主 网络 。   进一步 阅读 以 了解 如何 部署 私有 测试 网络 ， 而 不用 花费 太 多 时间 。         连接 到 一个 私人 测试 网       有时 你 可能 不想 连接 到 现场 公共 网络 ;   相反 ， 您 可以 选择 创建 自己 的 私人 测试 网 。   如果 您 不 需要 测试 公共 合同 ， 只想 尝试 或 开发技术 ， 这 非常 有用 。   由于 您 是 您 专用 网络 的 唯一 成员 ， 因此 您 有 责任 查找 所有 块 ， 验证 所有 交易 并 执行 所有 智能 合约 。   这 使得 开发 更 容易 ， 因为 您 可以 灵活 控制 个人 区块 链中 的 交易 。       Geth             geth   - - datadir   ~ / . ethereum _ private   init   ~ / dev / genesis . json   geth   - - fast   - - cache   512   - - ipcpath   ~ / Library / Ethereum / geth . ipc   - - networkid   1234   - - datadir   ~ / . ethereum _ private   console               Eth             eth   - - private   12345   - - genesis - json   ~ / test / genesis . json   - - db - path   ~ / . ethereum _ experiment               将 12345 替换 为 您 想要 用作 网络 ID 的 任意 数字 。   最好 改变 生成 块 的 内容 ， 因为 如果 有人 不 小心 使用 真正 的 链 连接 到 你 的 测试 网络 ， 你 的 本地 副本 将 被 认为 是 一个 旧 的 分支 ， 并 更新 为 “ 真实 ” 的 分支   。   改变 数据 地址 也 会 改变 您 本地 的 区块 链 副本 ， 否则 ， 为了 成功 地 挖掘 一个 区块 ， 您 需要 对付 区块 链 本地 副本 中 存在 的 最后 一个 区块   -   这 可能 需要 几个 小时 。       如果 你 想 创建 一个 私人 网络 ， 你 应该 出于 安全 原因 使用 不同 的 起源 块 （ genesis   block :   一个 包含 Ether 所有 交易 的 数据库 ） 。   你 可以 阅读 我们 的 博客 文章 如何 生成 您 的 文件   。   在 不久 的 将来 ， 我们 将 提供 更好 的 方法 来 获得 其他 的 起源 块 。       这些 命令 可以 防止 任何人 不 知道 您 选择 的 密码 ， 网络 ID 和 起源 文件 连接 到 您 或 为 您 提供 不 需要 的 数据 。   如果 你 想 连接 到 其他 节点 ， 并 创建 一个 多台计算机 的 小型 私人 网络 ， 他们 都 将 需要 使用 相同 的 网络 ID 和 一个 相同 的 起源 块 。   您 还 必须 帮助 每个 节点 找到 其他 节点 。   要 做到 这 一点 ， 首先 你 需要 自己 的 节点 URL ：             admin . nodeInfo . NodeUrl               该 命令 将会 返回 你 的 节点 url   -   记下 它 ， 然后 在 其他 客户端 上 ， 告诉 他们 通过 执行 这个 命令 来 添加 你 的 节点 ：             admin . addPeer ( & quot ; YOURNODEURL & quot ; )               您 不 需要 将 每个 客户端 添加 到 彼此 ， 因为 一旦 连接 ， 他们 将 共享 关于 他们 连接 到 的 任何 其他 同伴 的 信息 。       如果 您 使用 的 是 Eth ， 那么 只 需 找出 您 的 IP 并 执行 以下 命令 ：             web3 . admin . net . connect ( & quot ; YOURIP : 30303 & quot ; )               日志       如果 你 正在 运行 Geth ， 你 会 注意 到 在 你 的 控制 台上 弹出 了 很多 日志 条目   -   有时 在 你 输入 的 时候 。   这 是因为 所有 警告 和 进度 信息 都 由 客户端 实时 记录 到 您 的 终端 中 。   如果 要 将 日志 保存 到 可以 稍后 查看 的 文件 ， 请 使用 以下 命令 ：             geth   console   2 & gt ; & gt ; geth . log               Geth 支持 多个 终端 窗口 ， 你 可以 用 一个 日志 和 另 一个 控制台 启动 一个 新 的 日志 。   这 将 给 你 与 原来 的 控制台 完全相同 的 功能 ， 但 没有 混乱 。   要 做到 这 一点 ， 打开 一个 新 的 终端 窗口 并 输入 ：             geth   attach               控制台 具有 自动 完成 和 历史 命令 支持 。   您 可以 通过 按 Tab 键 完成 一个 命令 ， 然后 geth 会 自动 补全 当前 的 语句 ， 当有 多个 补全 选择 时 显示 可用 补全 列表 。   您 可以 使用 向上 和 向下 箭头键 导航 您 的 命令 历史记录 。       了解 有关 运行 节点 的 更 多 信息             备份 还原           连接 到 网络             用法 示例       创建 帐户       为了 使用 以太 坊 网络 ， 你 需要 生成 一个 帐户 。   有 很多 方法 可以 解决 这个 问题   ， 但 最 简单 的 方法 是 通过 控制台 。       注意 ： 如果 您 是 在 奥运 期间 或 开发 早期 运行 以太网 ，   请 不要 重复使用   Frontier 客户端 软件 1.0 发布 之前 生成 的 密钥   ， 否则 可能 会 受到 重播 攻击 （ replay   attacks ）   。   备份 这些 密钥 ， 并 使用 Frontier 发布 客户端 创建 新 密钥 。             personal . newAccount ( & quot ; Write   here   a   good ,   randomly   generated ,   passphrase ! & quot ; )               注意 ： 拿 起 一个 很 好 的 密码 并 写 下来 。   如果 您 丢失 了 用于 加密 帐户 的 密码 ， 则 无法访问 该 帐户 。   重复 ： 没有 安全网 。   如果 没有 有效 的 密码 ， 无法访问 您 的 帐户 ， 并且 此处 没有 “ 忘记 我 的 密码 ” 选项 。   有关 详细信息 ， 请参阅 此 XKCD   。         不要 忘记 你 的   PASSPHRASE ！         您 可以 创建 尽可能 多 或 尽可能少 的 帐户 。   按照 惯例 我们 称 您 创建 您 的 主要 帐户 的 第一个 帐户 。   您 可以 使用 以下 命令 查看 所有 帐户 ：             web3 . eth . accounts               账户 的 排序 反映 了 他们 创建 的 时间 。   密钥 文件 存储 在   DATADIR / keystore   下 ， 可以 通过 复制 其中 包含 的 文件 在 客户端 之间 传输 。   这些 文件 使用 密码 进行 加密 ， 如果 包含 任何 数量 的 ether ， 则 应 备份 。   但是 ， 请 注意 ， 如果 您 传输 个别 密钥 文件 ， 则 提交 的 帐户 顺序 可能 会 发生变化 ， 您 可能 无法 在 同一个 位置 结束 同一个 帐户 。   所以 请 注意 ， 只要 您 不 将 外部 密钥 文件 复制到 您 的 密钥 库 ， 依靠 帐户 索引 就是 合理 的 。       获取 帐户 的 余额       控制 台上 的 所有 命令 实际上 都 是 在 JavaScript 中 运行 ， 因此 您 可以 创建 变量 和 函数 。   您 也 可以 将 任何 “ eth ” 函数 编写 为 “ web3 . eth ” ， 因为 它 实际上 是 主 “ web3 ” 对象 的 一部分 。       试试 这个 例子 ：               var       primaryAccount       =       web3     .     eth     .     accounts     [     0     ]                 您 现在 有 一个 名为   primaryAccount   的 变量 ， 您 可以 在 其他 调用 中 使用 该 变量 。   要 获得 任何 帐户 的 余额 ， 请 使用   eth . getBalance   函数 ， 如下 所示 ：               web3     .     eth     .     getBalance     (     primaryAccount     )                 你 的 余额 应该 返回 0 ， 因为 你 刚 创建 它 。   为了 进行 下 一步 操作 ， 您 需要 在 帐户 中 拥有 一些 以太网 帐户 ， 以便 支付 gas 费用 。   在 下 一节 中 ， 您 将 了解 什么 是 gas ， 以及 如何 与 网络 进行 交互 。       一次 检查 所有 余额       命令行 工具 是 JavaScript 环境 ， 这 意味着 您 可以 像 创建 JavaScript 一样 创建 函数 。   例如 ， 如果 您 想要 一次 查看 所有 帐户 的 余额 ， 请 使用 此 JavaScript 代码段 。       它 将 遍历 每个 帐户 ， 并 在 以太网 中 打印 其 余额 ， 可以 使用 以下 代码 ：               function       checkAllBalances     ( )       {                 web3     .     eth     .     getAccounts     (     function     (     err     ,       accounts     )       {                           accounts     .     forEach     (     function     (     id     )       {                                     web3     .     eth     .     getBalance     (     id     ,       function     (     err     ,       balance     )       {                                               console     .     log     (     & quot ; & quot ;       +       id       +       & quot ; : \ \ tbalance :   & quot ;       +       web3     .     fromWei     (     balance     ,       & quot ; ether & quot ;     )       +       & quot ;   ether & quot ;     ) ;                                       } ) ;                       } ) ;               } ) ;       } ;                 一旦 你 执行 上面 的 行 ， 你 需要 检查 所有 的 余额 是 调用 下面 的 函数 ：               checkAllBalances     ( )                 提示 ： 如果 您 有 许多 这样 的 小巧 方便 的 脚本 ， 您 可以 将 它们 保存 到 一个 文件 中 ， 然后 使用 loadScript 一次 加载 它们 ：               loadScript     (     &# 39 ; / some / script / here . js &# 39 ;     )        ", "tags": "blockchain", "url": "/wiki/blockchain/getch-eth.html"},
      
      
      {"title": "使用命令行构建智能合约", "text": "    Table   of   Contents           使用 命令行 构建 智能 合约           你 的 第一个 公民 ： Greeter           Solc 编译器           编译 你 的 合同           使用 在线 编译器                   运行 Greeter           让 其他人 与 您 的 代码 进行 交互           清理 后 自己 ：                                 使用 命令行 构建 智能 合约       这个 页面 将 帮助 你 在 以太 坊 命令行 建立 一个 Hello ， World 合约 。 如果 您 不 知道 如何 使用 命令行 ， 我们 建议您 跳过 本 教程 ， 而是 使用 图形用户界面 构建 自定义 标记 。       智能 合约 是 以太 坊 区块 链上 的 账户 持有 对象 。 它们 包含 代码 功能 ， 可以 与 其他 合同 进行 交互 ， 做出 决定 ， 存储 数据 ， 并 将 ether 发送给 其他人 。 契约 是 由 他们 的 创造者 定义 的 ， 但 他们 的 执行 ， 以及 他们 所 提供 的 服务 ， 都 是 由 以太网 本身 提供 的 。 只要 整个 网络 存在 ， 它们 就 会 存在 并且 可 执行 ， 只有 被 编程 为 自毁 ， 它们 才 会 消失 。       你 可以 用 合同 做 什么 ？ 事实上 ， 你 几乎 可以 做 任何 事情 ， 但是 对于 我们 的 入门 指南 ， 我们 来 做 一些 简单 的 事情 ： 首先 你 将 创建 一个 经典 的 “ Hello     World ” 合约 ， 然后 你 可以 建立 你 自己 的 密码 令牌 发送给 任何 你 喜欢 的 人 。 一旦 你 掌握 了 这 一点 ， 那么 你 将 通过 众筹 筹集资金 ， 如果 成功 ， 将 提供 一个 完全 透明 和 民主 的 组织 ， 只会 服从 自己 的 公民 ， 永远 不会 摆脱 它 的 宪法 ， 不能 被 检查 或 关闭 。 而 所有 这些 都 在 不到 300 行 的 代码 中 。       在 你 开始 之前 ：             安装 以太 坊 CLI           详细 了解 合同             进入   geth   控制台 之前 ， 请 确认 GUI 已 关闭 。   运行   geth   开始 同步 过程 （ 第一次 运行 可能 需要 一段时间 ） 。       那么 现在 就 开始 吧 。       你 的 第一个 公民 ： Greeter       现在 你 已经 掌握 了 以太 坊 的 基本知识 ， 让 我们 进入 你 的 第一个 严肃 的 合同 。 Frontier 是 一个 很大 的 开放 领域 ， 有时 你 可能 会 感到 孤独 ， 所以 我们 的 第一步 就是 创造 一个 自动 的 伴侣 ， 当 你 感到 孤独 的 时候 迎接 你 。 我们 会称 他 为 “ Greeter ” 。       Greeter 是 一个 智能 的 数字 实体 ， 它 存在 于 区块 链中 ， 并 能够 根据 其 输入 与 任何 与 之 交互 的 人 进行 交谈 。 它 可能 不是 一个 演讲者 ， 但 它 是 一个 很 好 的 倾听 者 。 这 是 它 的 代码 ：                       contract       mortal       {                       / *   Define   variable   owner   of   the   type   address   * /                       address       owner     ;                         / *   This   function   is   executed   at   initialization   and   sets   the   owner   of   the   contract   * /                       function       mortal     ( )       {       owner       =       msg     .     sender     ;       }                         / *   Function   to   recover   the   funds   on   the   contract   * /                       function       kill     ( )       {       if       (     msg     .     sender       = =       owner     )       selfdestruct     (     owner     ) ;       }               }                 contract       greeter       is       mortal       {                       / *   Define   variable   greeting   of   the   type   string   * /                       string       greeting     ;                         / *   This   runs   when   the   contract   is   executed   * /                       function       greeter     (     string       _ greeting     )       public       {                               greeting       =       _ greeting     ;                       }                         / *   Main   function   * /                       function       greet     ( )       constant       returns       (     string     )       {                               return       greeting     ;                       }               }                 你 会 注意 到 这个 代码 中有 两个 不同 的 合约 ： “ mortal ” 和 “ greeter ” 。 这 是因为 Solidity （ 我们 使用 的 高级 合同 语言 ） 具有 继承性 ， 这 意味着 一个 契约 可以 继承 另 一个 契约 的 特征 。 这 对于 简化 编码 非常 有用 ， 因为 合同 的 常见 特征 不 需要 每次 重写 ， 所有 合同 都 可以 用 更 小 ， 更 易读 的 块 来 编写 。 所以 只要 声明 迎宾 者 是 凡人 ， 你 就 继承 了 “ 凡人 ” 契约 的 所有 特征 ， 并 使 迎宾 者 的 代码 简单 易读 。       继承 特征 “ 凡人 ” 仅仅 意味着 迎宾 合同 可以 被 其 所有者 杀死 ， 清理 区块 链 ， 并 在 不再 需要 合同 时 收回 锁定 的 资金 。 以太 坊 的 契约 默认 为 不 死 的 ， 没有 所有者 ， 这 意味着 一旦 被 部署 ， 作者 就 没有 特殊 的 特权 了 。 在 部署 之前 考虑 这 一点 。       Solc 编译器       在 你 能够 部署 它 之前 ， 你 需要 两件 事情 ： 编译 代码 和 应用程序 二进制 接口 ， 它 是 一个 JavaScript 对象 ， 它 定义 了 如何 与 合约 进行 交互 。       这 两个 事情 你 都 可以 通过 使用 编译器 完成 。 你 可以 使用 solidity 编译器 。       如果 你 还 没有 安装 编译器 ， 那么 你 需要 安装 一个 。 你 可以 在 这里 找到   安装 Solidity 的 说明   。             译者 注       solc 编译器 安装 方法 ： 直接 利用 npm 包 管理器 安装       npm   install   - g   solc             编译 你 的 合同       如果 你 没有 得到 以上 的 固体 ， 那么 你 需要 安装 它 。 你 可以 在 这里 找到 安装 Solidity 的 说明 。       现在 你 已经 安装 了 编译器 ， 你 需要 编译 合约 来 获取 编译 后 的 代码 和 应用程序 二进制 接口 。             solc   - o   target   - - bin   - - abi   Greeter . sol               这 将 创建 两个 文件 ， 一个 文件 包含 已 编译 的 代码 ， 另 一个 文件 在 名为 target 的 目录 中 创建 应用程序 二进制 接口 。             $ tree   .   ├ ─ ─   Greeter . sol   └ ─ ─   target         ├ ─ ─   Greeter . abi         ├ ─ ─   Greeter . bin         ├ ─ ─   Mortal . abi         └ ─ ─   Mortal . bin               你 会 看到 有 为 两个 合同 创建 的 文件 ; 但是 因为 Greeter 包括 Mortal ， 所以 你 不 需要 部署 Mortal 来 部署 Greeter 。       您 可以 使用 这 两个 文件 来 创建 和 部署 合同 。             var   greeterFactory   =   eth . contract ( & amp ; lt ; contents   of   the   file   Greeter . abi & amp ; gt ; )     var   greeterCompiled   =   & quot ; 0x & quot ;   +   & quot ; & amp ; lt ; contents   of   the   file   Greeter . bin & quot ;               你 现在 已经 编译 了 你 的 代码 ， 并 把 它 提供 给 Geth 。 现在 您 需要 准备 好 进行 部署 ， 包括 设置 一些 变量 ， 例如 您 希望 使用 的 问候语 。 将 下面 的 第一行 编辑 为 比 “ Hello     World ！ ” 更 有趣 的 内容 并 执行 这些 命令 ：                       var       _ greeting       =       & quot ; Hello   World ! & quot ;                 var       greeter       =       greeterFactory     .     new     (     _ greeting     , {     from     :     eth     .     accounts     [     0     ] ,     data     :     greeterCompiled     ,     gas     :     47000000     } ,       function     (     e     ,       contract     ) {                       if     (     !     e     )       {                             if     (     !     contract     .     address     )       {                               console     .     log     (     & quot ; Contract   transaction   send :   TransactionHash :   & quot ;       +       contract     .     transactionHash       +       & quot ;   waiting   to   be   mined ...& quot ;     ) ;                             }       else       {                               console     .     log     (     & quot ; Contract   mined !   Address :   & quot ;       +       contract     .     address     ) ;                               console     .     log     (     contract     ) ;                           }                         }               } )                 使用 在线 编译器       如果 你 没有 安装 solC ， 你 可以 简单 地 使用 在线 编译器 。 将 上面 的 源代码 复制到 在线 solidity 编译器 ， 然后 您 的 编译 代码 应该 出现 在 左侧 窗格 中 。 将   greeter   合同 和   mortal   合同 中标 有 Web3     deploy 的 框中 的 代码 复制到 单个 文本文件 中 。 现在 ， 在 该 文件 中 ， 将 第一行 更 改为 您 的 问候语 ：                       var       _ greeting       =       & quot ; Hello   World ! & quot ;                 现在 您 可以 将 结果 文本 粘贴 到 您 的 geth 窗口 中 ， 或者 使用   loadScript ( \" yourFilename . js \" )   导入 文件 。 等待 30 秒 ， 你 会 看到 这样 的 消息 ：             Contract   mined !   address :   0xdaa24d02bad7e9d6a80106db164bad9399a0423e               您 可能 必须 使用 您 在 开始 时 选择 的 密码 来 “ 解锁 ” 发送 交易 的 帐户 ， 因为 您 需要 支付 部署 合同 的 天然气 费用 。       personal . unlockAccount ( web3 . eth . accounts [ 0 ] ,   \" yourPassword \" )   。       该 合同 估计 需要 约 18 万个 气体 部署 （ 根据 在线 固体 编译器 ） ， 在 撰写 本文 时 ， 测试 网上 的 气体 价格 为 20   gwei （ 等于 （ 20000000000     wei ， 或 0.00000002   ether ） 每 单位 有 很多 有用 的 统计数据 ， 包括 网络 统计 页面 的 最新 天然气 价格 。       请 注意 ， 这些 费用 并 不 支付 给 以太 坊 开发者 ， 而是 交给 了 矿工 们 ， 那些 计算机 正在 努力 寻找 新 的 区块 并 保证 网络安全 的 同行 。 天然气 价格 是 由 当前 的 计算 供求 市场 决定 的 。 如果 天然气 价格 太高 ， 你 可以 成为 一名 矿工 ， 降低 你 的 要价 。       在 不到 一分钟 的 时间 内 ， 你 应该 有 一个 合同 地址 的 日志 ， 这 意味着 你 已经 成功 地 部署 了 你 的 合同 。 您 可以 使用 以下 命令 验证 部署 的 代码 （ 将 被 编译 ） ：             eth . getCode ( greeter . address )               如果 它 返回 “ 0x ” 以外 的 任何 内容 ， 那么 恭喜 ！ 你 的 小家伙 活着 ！ 如果 再次 创建 合同 （ 通过 执行 另 一个 eth . sendTransaction ） ， 它 将 被 发布 到 一个 新 的 地址 。       运行 Greeter       为了 打电话 给 你 的 机器人 ， 只 需 在 你 的 终端 上 输入 以下 命令 ：             greeter . greet ( ) ;               由于 这个 调用 在 区块 链上 没有 任何 变化 ， 所以 它 立即 返回 并且 没有 任何 天然气 成本 。 你 应该 看到 它 返回 你 的 问候语 ：             &# 39 ; Hello   World ! &# 39 ;               让 其他人 与 您 的 代码 进行 交互       为了 让 其他人 运行 你 的 合同 ， 他们 需要 两件 事情 ： 合同 所在 的 地址 和 ABI （ 应用程序 二进制 接口 ） ， 这是 一种 用户手册 ， 描述 了 它 的 功能 名称 以及 如何 给 你 的 JavaScript 控制台 。 为了 让 他们 每个 人 都 运行 这些 命令 ：             greeterCompiled . greeter . info . abiDefinition ;   greeter . address ;               如果 使用 基于 浏览器 的 工具 进行 编译 ， 则 可以 从 标有 “ 接口 ” 的   greeter   和   mortal   合同 的 字 段 中 获取 ABI 。       然后 ， 您 可以 实例 化 一个 JavaScript 对象 ， 该 对象 可 用于 在 连接 到 网络 的 任何 计算机 上 调用 合约 。 替换 ' ABI ' （ 一个 数组 ） 和 ' Address ' （ 一个 字符串 ） 在 JavaScript 中 创建 一个 契约 对象 ：             var   greeter   =   eth . contract ( ABI ) . at ( Address ) ;               这个 特殊 的 例子 可以 通过 简单 的 调用 来 实例 化 ：                       var       greeter2       =       eth     .     contract     ( [ {     constant     :     false     ,     inputs     :     [ ] ,     name     :     &# 39 ; kill &# 39 ;     ,     outputs     :     [ ] ,     type     :     &# 39 ; function &# 39 ;     } , {     constant     :     true     ,     inputs     :     [ ] ,     name     :     &# 39 ; greet &# 39 ;     ,     outputs     :     [ {     name     :     &# 39 ; &# 39 ;     ,     type     :     &# 39 ; string &# 39 ;     } ] ,     type     :     &# 39 ; function &# 39 ;     } , {     inputs     :     [ {     name     :     &# 39 ; _ greeting &# 39 ;     ,     type     :     &# 39 ; string &# 39 ;     } ] ,     type     :     &# 39 ; constructor &# 39 ;     } ] ) .     at     (     &# 39 ; greeterAddress &# 39 ;     ) ;                 将 greeterAddress 替换 为 合同 的 地址 。       提示 ： 如果 你 的 机器 上 没有 正确 的 安装 编译器 ， 你 可以 从 在线 编译器 获取 ABI 。 为此 ， 请 使用 下面 的 代码 ， 仔细 地用 编译器 中 的 abi 替换 greeterCompiled . greeter . info . abiDefinition 。       清理 后 自己 ：       你 必须 非常高兴 才能 签下 第一份 合同 ， 但是 当 业主 继续 写下 更 多 的 合同 时 ， 这种 激动 有时 会 消失 ， 导致 在 区块 链上 看到 放弃 的 合同 。 未来 ， 区块 链 租金 可能 会 实施 ， 以 增加 区块 链 的 可扩展性 ， 但 现在 ， 成为 一个 好 公民 ， 并 人 道 地 放弃 你 的 废弃 机器人 。       交易 将 需要 发送到 网络 ， 并 在 下面 的 代码运行 后 支付 块 链 更改 的 费用 。 自毁 是 由 网络 补贴 的 ， 所以 它 的 成本 要 比 平常 的 交易 少得 多 。             greeter . kill . sendTransaction ( { from : eth . accounts [ 0 ] } )               这 只能 由 合同 所有者 发送 的 交易 触发 。 您 可以 验证 该 行为 是否 完成 ， 只要 看看 是否 返回 0 ：             eth . getCode ( greeter . address )               请 注意 ， 每个 合约 都 必须 执行 自己 的 kill 子句 。 在 这种 特殊 情况 下 ， 只有 创建 合同 的 帐户 才能 杀死 它 。       如果 你 不 添加 任何 杀人 条款 ， 它 可能 永远 独立 于 你 和 任何 地球 上 的 边界 永远 活着 ， 所以 在 你 实际 使用 之前 ， 请 检查 你 的 当地 法律 对此 的 看法 ， 包括 对 技术 出口 的 任何 可能 的 限制 ， 任何 有关 数字 众生 公民权利 的 立法 。 对待 你 的 机器人 人 道 。  ", "tags": "blockchain", "url": "/wiki/blockchain/greeter.html"},
      
      
      {"title": "合约", "text": "    Table   of   Contents           什么 是 合同 ？           以太 坊 高级 语言 ¶           Solidity ¶           蛇 ¶           LLL ¶           Mutan （ 已弃 用 ） ¶                   写 一份 合同 ¶           编译 合同 ¶           在 geth 中 设置 solidity 编译器           编译 一个 简单 的 合同 ¶                   创建 和 部署 一个 合同 ¶           与 合同 交互 ¶           合约 元 数据           测试 合同 和 交易 ¶                 什么 是 合同 ？       契约 是 代码 （ 其 功能 ） 和 数据 （ 其 状态 ） 的 集合   位于 以太 坊 区块 链上 的 特定 地址 。 合同 账户 是   能够 在 他们 自己 之间 传递 消息 以及 实际上 图灵     完成 计算 。 合约 在 以太 坊 特有 的 区块 链上 生存   二进制 格式 称为 以太 坊 虚拟机 （ EVM ） 字节 码 。       合同 通常 用 一些 高级 语言 编写 ， 例如 Solidity ， 然后 编译成 字节 码   上 传到 区块 链上 。       也 可以 看看       还有 其他 的 语言 ， 特别 是 Serpent 和 LLL ， 在 本 文档 的 以太 坊 高级 语言 部分 中有 进一步 描述 。       Dapp 开发资源 列出 了 集成 开发 环境 ， 帮助 您 使用 这些 语言 开发 的 开发工具 ， 提供 测试 和 部署 支持 等 功能 。       以太 坊 高级 语言 ¶       以 Ethereum 虚拟机 （ EVM ） 执行 的 特定 于 以太 坊 的 二进制 格式 （ EVM 字节 码 ） 形式 签署 合同 。 然而 ， 契约 通常 是 用 更 高级 别的 语言 编写 的 ， 然后 使用 EVM 编译器 将 其 编译成 字节 代码 以 部署 到 区块 链 。       以下 是 开发人员 可 用于 为 以太 坊 编写 智能 合约 的 不同 高级 语言 。       Solidity ¶       Solidity 是 一种 类似 于 JavaScript 的 语言 ， 它 允许 您 开发 合同 并 编译 为 EVM 字节 码 。 它 目前 是 以太 坊 的 旗舰 语言 ， 也 是 最 受欢迎 的 。           Solidity 文档   -   Solidity 是 用于 编写 合同 的 旗舰 Ethereum 高级 语言 。       Solidity 在线 实时 编译器       标准化 合约 API       有用 的 应用程序 模式   -   对于 应用 程序开发 非常 有用 的 代码 片段 。           蛇 ¶       Serpent 是 一种 类似 于 Python 的 语言 ， 可 用于 开发 契约 并 编译 为 EVM 字节 码 。 它 旨在 最大 限度 地 简化 和 清理 ， 将 低级语言 的 许多 效率 优势 与 易于 使用 的 编程 风格 相结合 ， 同时 为 合约 编程 添加 特定 领域 特定 功能 。 蛇 是 使用 LLL 编译 的 。           蛇 在 ethereum 维基 上       蛇 EVM 编译器           LLL ¶       Lisp   Like   Language （ LLL ） 是 一种 类似 于 Assembly 的 低级语言 。 它 意味着 非常简单 和 简约 ; 基本上 就是 直接 在 EVM 中 进行 编码 的 一个 小包装 。           LIBLLL 在 GitHub       LLL 的 例子           Mutan （ 已弃 用 ） ¶       Mutan 是 由 Jeffrey   Wilcke 设计 和 开发 的 静态 类型 C语言 。 它 不再 被 维护 。       写 一份 合同 ¶       如果 没有 Hello   World 程序 ， 没有 语言 是 完整 的 。 在 内部 操作   以太 坊 环境 下 ， Solidity 没有 明显 的 “ 输出 ” 字符串 的 方式 。     我们 可以 做 的 最 接近 的 是 使用 日志 事件 来 放置 一个 字符串   blockchain ：             contract   HelloWorld   {                   event   Print ( string   out ) ;                   function ( )   {   Print ( & quot ; Hello ,   World ! & quot ; ) ;   }   }               此 合约 将 在 Print 类型 的 区块 链上 创建 一个 日志 条目   参数 “ Hello ， World ！ ” 每次 执行 。       也 可以 看看       Solidity 文档 有 更 多 的 例子 和 指导 编写 Solidity 代码 。       编译 合同 ¶       可靠性 合同 的 汇编 可以 通过 一些   机制 。           通过 命令行 使用 solc 编译器 。       在 JavaScript 控制台 提供 的 web3 . eth . compile . solidity 中 使用   geth 或 eth （ 这 仍然 需要 solc 编译器   安装 ） 。       在线 Solidity 实时 编译器 。       流星 戴斯 科斯 莫 建设 稳固 的 合同 。       混合 IDE 。       以太 坊 钱包 。           注意       关于 solc 和 编译 Solidity 合同 代码 的 更 多 信息 可以 在 这里 找到 。       在 geth 中 设置 solidity 编译器       如果 你 启动 你 的 geth 节点 ， 你 可以 检查 哪些 编译器   可用 。             & amp ; gt ;   web3 . eth . getCompilers ( ) ;   [ & quot ; lll & quot ; ,   & quot ; solidity & quot ; ,   & quot ; serpent & quot ; ]               该 命令 返回 一个 字符串 数组 ， 指示 哪些 编译器 是   目前 可用 。       注意       solc 编译器 与 cpp - ethereum 一起 安装 。 或者 ，   你 可以 自己 构建 它 。       如果 您 的 solc 可执行文件 位于 非标准 位置 ， 则 可以 指定 一个   使用 th   -   solc 标志 的 solc 可执行文件 的 自定义 路径 。             $   geth   - - solc   / usr / local / bin / solc               或者 ， 您 可以 通过 控制台 在 运行 时 设置 此 选项 ：             & amp ; gt ;   admin . setSolc ( & quot ; / usr / local / bin / solc & quot ; )   solc ,   the   solidity   compiler   commandline   interface   Version :   0.2 . 2 - 02bb315d / . - Darwin / appleclang / JIT   linked   to   libethereum - 1.2 . 0 - 8007cef0 / . - Darwin / appleclang / JIT   path :   / usr / local / bin / solc               编译 一个 简单 的 合同 ¶       我们 来编 一个 简单 的 合同 来源 ：             & amp ; gt ;   source   =   & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ;               这个 合同 提供 了 一个 单一 的 方法 乘以 a   正整数 a 并 返回 *   7 。       您 已经 准备 好 使用 geth   JS 控制台 来 编译 solidity 代码   eth . compile . solidity （ ） ：               & amp ;     gt     ;       contract       =       eth     .     compile     .     solidity     (     source     )     .     test       {           code     :       &# 39 ; 605280600c6000396000f3006000357c010000000000000000000000000000000000000000000000000000000090048063c6888fa114602e57005b60376004356041565b8060005260206000f35b6000600782029050604d565b91905056 &# 39 ;     ,           info     :       {               language     :       &# 39 ; Solidity &# 39 ;     ,               languageVersion     :       &# 39 ; 0 &# 39 ;     ,               compilerVersion     :       &# 39 ; 0.9 . 13 &# 39 ;     ,               abiDefinition     :       [     {                   constant     :       false     ,                   inputs     :       [     {                       name     :       &# 39 ; a &# 39 ;     ,                       type     :       &# 39 ; uint256 &# 39 ;                   }       ]     ,                   name     :       &# 39 ; multiply &# 39 ;     ,                   outputs     :       [     {                       name     :       &# 39 ; d &# 39 ;     ,                       type     :       &# 39 ; uint256 &# 39 ;                   }       ]     ,                   type     :       &# 39 ; function &# 39 ;               }       ] ,               userDoc     :       {                   methods     :       {                   }               }     ,               developerDoc     :       {                   methods     :       {                   }               }     ,               source     :       &# 39 ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } &# 39 ;           }       }                 注意       编译器 也 可以 通过 RPC ， 因此 通过   web3 . js 到 任何 浏览器 中 的 Ð app 连接   通过 RPC   /   IPC 进行 通信 。       以下 示例 显示 了 如何 通过 JSON - RPC 将 geth 连接 到   使用 编译器 。                     $   geth   - - datadir   ~ / eth /   - - loglevel   6   - - logtostderr = true   - - rpc   - - rpcport   8100   - - rpccorsdomain   &# 39 ; * &# 39 ;   - - mine   console     2 & gt ; & gt ;   ~ / eth / eth . log           $   curl   - X   POST   - - data   &# 39 ; { & quot ; jsonrpc & quot ; : & quot ; 2.0 & quot ; , & quot ; method & quot ; : & quot ; eth _ compileSolidity & quot ; , & quot ; params & quot ; : [ & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ; ] , & quot ; id & quot ; : 1 } &# 39 ;   http : / / 127.0 . 0.1 : 8100               一个 源 的 编译器 输出 将 为 您 提供 每个 合同 对象   代表 一份 合同 。 实际 的 回报 值   eth . compile . solidity 是 契约 对象 对 的 契约 名称 映射 。     由于 我们 的 合同 名称 是 test ， eth . compile . solidity （ source ） . test   将会 给 你 包含 合同 对象 的 测试 合同   以下 领域 ：       码       信息       资源       语言       languageVersion       compilerVersion       abiDefinition             [ Application   Binary   Interface   Definition ] ( https : / / github . com / ethereum / wiki / wiki / Ethereum - Contract - ABI )               userDoc               [     NatSpec       Doc     ] (     https     :     / / github . com / ethereum / wiki / wiki / Ethereum - Natural - Specification - Format )   for   users .                 developerDoc               [     NatSpec       Doc     ] (     https     :     / / github . com / ethereum / wiki / wiki / Ethereum - Natural - Specification - Format )   for   developers .                 编译器 输出 的 直接 结构 （ 编码 和 信息 ）   反映 了 两种 截然不同 的 部署 路径 。 编译 的 EVM 代码   通过 合同 创建 交易 发送到 区块 链     休息 （ 信息 ） 将 理想 地 生活 在 可 公开 验证 的 分散式 云上   元 数据 补充 区块 链上 的 代码 。       如果 您 的 来源 包含 多个 合同 ， 则 输出 将 包含 一个 条目   对于 每个 合同 ， 相应 的 合同 信息 对象 可以 被 检索   合同 的 名称 作为 属性 名称 。 你 可以 尝试 通过 检查     最新 的 GlobalRegistrar 代码 ：             contracts   =   eth . compile . solidity ( globalRegistrarSrc )               创建 和 部署 一个 合同 ¶       在 开始 本 节 之前 ， 请 确保您 同时 拥有 一个 未 锁定 的 帐户   以及 一些 资金 。       现在 ， 您 将 通过 将 事务处理 以前 一节 中 的 EVM 代码 作为 数据 发送到 空白 地址 来 在 区块 链上 创建 合同 。       注意       使用 在线 Solidity 实时 可以 轻松 完成 此 操作   编译器 或 Mix   IDE 程序 。             var   primaryAddress   =   eth . accounts [ 0 ]   var   abi   =   [ {   constant :   false ,   inputs :   {   name :   &# 39 ; a &# 39 ; ,   type :   &# 39 ; uint256 &# 39 ;   }   } ]   var   MyContract   =   eth . contract ( abi )   var   contract   =   MyContract . new ( arg1 ,   arg2 ,   ... ,   { from :   primaryAddress ,   data :   evmByteCodeFromPreviousSection } )               所有 二进制 数据 都 以 十六进制 格式 进行 序列化 。 十六进制 字符串 总是 有 一个   十六进制 前缀 0x 。       注意       请 注意 ， arg1 ， arg2 ， ... 是 合约 的 参数   构造函数 ， 以防 它 接受 任何 。 如果 合同 不 需要 任何   构造 函数参数 ， 那么 这些 参数 可以 省略 。       值得 指出 的 是 ， 这 一步 需要 您 付费 执行 。 你 的   账户 余额 （ 您 作为 发件人 在 发件人 字段 中 ） 将会 是   一旦 交易 完成 ， 根据 EVM 的 气体 规则 减少     成 块 。 过 了 一段时间 ， 你 的 交易 应该 出现 在 一个   确认 它 所 带来 的 状态 是 一个 共识 。 你 的 合同   现在 居住 在 区块 链上 。       异步 的 做法 是 这样 的 ：             MyContract . new ( [ arg1 ,   arg2 ,   ... , ] { from :   primaryAccount ,   data :   evmCode } ,   function ( err ,   contract )   {       if   ( ! err   & amp ; amp ; & amp ; amp ;   contract . address )           console . log ( contract . address ) ;   } ) ;               与 合同 交互 ¶       与 合约 的 交互 通常 使用 抽象 层来 完成   作为 eth . contract （ ）   函数 ， 它 返回 一个 javascript 对象 与 所有 的 合约 功能     在 JavaScript 中 可 用作 可 调用函数 。       描述 合同 可用 功能 的 标准 方式 是 ABI   定义 。   这个 对象 是 一个 描述 呼叫 签名 和 返回值 的 数组   为 每个 可用 的 合约 功能 。             var   Multiply7   =   eth . contract ( contract . info . abiDefinition ) ;   var   myMultiply7   =   Multiply7 . at ( address ) ;               现在 在 ABI 中 指定 的 所有 函数调用 都 可用   合同 实例 。 您 可以 在 合同 实例 上 调用 这些 方法   以 两种 方式 之一 。             & amp ; gt ;   myMultiply7 . multiply . sendTransaction ( 3 ,   { from :   address } )   & quot ; 0x12345 & quot ;   & amp ; gt ;   myMultiply7 . multiply . call ( 3 )   21               当 使用 sendTransaction 调用 时 ， 函数调用 通过 发送 来 执行   交易 。 这 将 花费 乙醚 发送 和 通话 将 被 记录   永远 在 区块 链上 。 以 这种 方式 进行 的 呼叫 的 返回值 是     交易 的 散列 。       当 使用 调用 进行 调用 时 ， 该 功能 在 EVM 和 本地 执行   函数 的 返回值 与 函数 一起 返回 。 打电话 给 在 这   方式 不 记录 在 区块 链上 ， 因此 不能 修改 内部     合同 的 状态 。 这种 呼叫 方式 被 称为 常数   函数调用 。 以 这种 方式 进行 的 呼叫 不会 花费 任何 代价 。       如果 您 只 对 返回值 和 使用 感兴趣 ， 您 应该 使用 呼叫   sendTransaction 如果 你 只 关心 对 状态 的 副作用   合同 。       在 上面 的 例子 中 ， 没有 副作用 ， 因此 sendTransaction   只 燃烧 气体 ， 增加 宇宙 的 熵 。       合约 元 数据       在 前面 的 章节 中 ， 我们 解释 了 你 如何 创建 合同   blockchain 。 现在 我们 将 处理 其余 的 编译器 输出 结果   合同 元 数据 或 合同 信息 。       与 合同 交互 时 ， 您 并未 创建 您 可能 需要 的 合同   文档 或 查看 源代码 。 合同 作者 受到 鼓励   通过 在 区块 链上 注册 来 提供 这些 信息     通过 第三方 服务 ， 如 EtherChain 。 管理 API 提供   方便 的 方法 来 获取 这个 包 的 任何 合同 ， 选择   寄存器 。             / /   get   the   contract   info   for   contract   address   to   do   manual   verification   var   info   =   admin . getContractInfo ( address )   / /   lookup ,   fetch ,   decode   var   source   =   info . source ;   var   abiDef   =   info . abiDefinition               使 这项 工作 的 基本 机制 是 ：           合同 信息 被 上 传到 可以 通过 URI 识别 的 地方   是 可 公开 访问 的       任何人 都 可以 找出 什么 是 只 知道 合约 的 URI   地址           这些 要求 是 通过 使用 2 步 区块 链 注册表 来 实现 的 。 首先   步骤 在 合同 中将 合同 代码 （ 哈希 ） 与 内容 哈希 注册   称为 HashReg 。 第二步 注册 一个 带有 内容 哈希 的 url     UrlHint 合同 。 这些 注册 合同   是 边疆 释放 的 一部分 ， 并 进入 了 宅基地 。       通过 使用 这种 方案 ， 知道 合同 的 地址 来 查找 URL 并 获取 实际 合同 元 数据 信息 包 就 足够 了 。       所以 如果 你 是 一个 认真 的 合同 创造者 ， 其 步骤 如下 ：           将 合约 本身 部署 到 区块 链       获取 合同 信息 json 文件 。       将 合同 信息 json 文件 部署 到 您 选择 的 任何 网址       注册 codehash   -   & gt ; 内容 哈希   -   & gt ; 网址           JS   API 通过 提供 帮助 程序 使 这个 过程 非常简单 。 呼叫   admin . register 从 合约 中 提取 信息 ， 写出 它 的 json     序列化 在 给定 的 文件 中 ， 计算 文件 的 内容 哈希 值   最后 把 这个 内容 哈希 注册 到 合同 的 代码 哈希 。 一旦 您     部署 该 文件 到 任何 网址 ， 您 可以 使用 admin . registerUrl 进行 注册   在 区块 链上 的 内容 哈希 链接 也 是 如此 。   （ 注意 ， 如果 是     固定 内容 寻址 模型 被 用作 文档 存储 ， url - hint 是 no   需要 更长 的 时间 ）                     source   =   & quot ; contract   test   {   function   multiply ( uint   a )   returns ( uint   d )   {   return   a   *   7 ;   }   } & quot ;   / /   compile   with   solc   contract   =   eth . compile . solidity ( source ) . test   / /   create   contract   object   var   MyContract   =   eth . contract ( contract . info . abiDefinition )   / /   extracts   info   from   contract ,   save   the   json   serialisation   in   the   given   file ,   contenthash   =   admin . saveInfo ( contract . info ,   & quot ; ~ / dapps / shared / contracts / test / info . json & quot ; )   / /   send   off   the   contract   to   the   blockchain   MyContract . new ( { from :   primaryAccount ,   data :   contract . code } ,   function ( error ,   contract ) {       if ( ! error   & amp ; amp ; & amp ; amp ;   contract . address )   {           / /   calculates   the   content   hash   and   registers   it   with   the   code   hash   in   ` HashReg `           / /   it   uses   address   to   send   the   transaction .           / /   returns   the   content   hash   that   we   use   to   register   a   url           admin . register ( primaryAccount ,   contract . address ,   contenthash )           / /   here   you   deploy   ~ / dapps / shared / contracts / test / info . json   to   a   url           admin . registerUrl ( primaryAccount ,   hash ,   url )       }   } ) ;               测试 合同 和 交易 ¶       通常 你 需要 采取 低级 别的 测试 和 调试 策略   合同 和 交易 。 本 节 介绍 一些 调试 工具 和   您 可以 使用 的 做法 。 为了 测试 合同 和 交易 没有     真正 的 结果 ， 你 最好 在 私人 区块 链上 测试 它 。 这 可以   通过 配置 替代 网络 ID （ 选择 唯一 的 整数 ） 来 实现   和 / 或 禁用 对等体 。 建议 练习 ， 为了 测试 你 使用 一个     替代 数据 目录 和 端口 ， 以便 您 甚至 不会 意外 冲突   与 您 的 实时 运行 的 节点 （ 假设 使用 默认 运行   你 的 虚拟机 调试模式 下 的 配置文件 和 最高 的 日志 记录   详细 级别 建议 ：             geth   - - datadir   ~ / dapps / testing / 00 /   - - port   30310   - - rpcport   8110   - - networkid   4567890   - - nodiscover   - - maxpeers   0   - - vmdebug   - - verbosity   6   - - pprof   - - pprofport   6110   console   2 & amp ; gt ; & amp ; gt ;   ~ / dapp / testint / 00 / 00 . log               在 您 提交 任何 交易 之前 ， 您 需要 设置 您 的 私人 测试   链 。 请参阅 测试 网络 。             / /     create       account   .     will       prompt       for       password       personal   .   newAccount   ( ) ;   / /     name       your       primary       account   ,     will       often       use       it       primary     =     eth   .   accounts   [   0   ] ;   / /     check       your       balance     (   denominated       in       ether   )     balance     =     web3   .   fromWei   (   eth   .   getBalance   (   primary   ) ,     & quot ; ether & quot ;   ) ;         / /     assume       an       existing       unlocked       primary       account       primary     =     eth   .   accounts   [   0   ] ;     / /     mine       10       blocks       to       generate       ether       / /     starting       miner       miner   .   start   (   4   ) ;   / /     sleep       for       10       blocks     (   this       can       take       quite       some       time   ) .     admin   .   sleepBlocks   (   10   ) ;   / /     then       stop       mining     (   just       not       to       burn       heat       in       vain   )     miner   .   stop   ( ) ;     balance     =     web3   .   fromWei   (   eth   .   getBalance   (   primary   ) ,     & quot ; ether & quot ;   ) ;               创建 交易 后 ， 您 可以 使用 以下 几行 强制 处理 它们 ：             miner . start ( 1 ) ;   admin . sleepBlocks ( 1 ) ;   miner . stop ( ) ;               您 可以 使用             / /   shows   transaction   pool   txpool . status   / /   number   of   pending   txs   eth . getBlockTransactionCount ( & quot ; pending & quot ; ) ;   / /   print   all   pending   txs   eth . getBlock ( & quot ; pending & quot ; ,   true ) . transactions               如果 您 提交 了 合同 创建 交易 ， 您 可以 检查 所 需 的 代码 是否 实际 插入 到 当前 区块 链中 ：             txhash   =   eth . sendTansaction ( { from : primary ,   data :   code } )   / / ...   mining   contractaddress   =   eth . getTransactionReceipt ( txhash ) ;   eth . getCode ( contractaddress )      ", "tags": "blockchain", "url": "/wiki/blockchain/contracts.html"},
      
      
      {"title": "帐户类型，gas和交易", "text": "    Table   of   Contents           EOA 与 合约 账户           外部 拥有 账户 （ EOAs ）           合同 帐户                   什么 是 交易 ？           什么 是 信息 ？           什么 是 天然气 ？           估算 交易成本           gasUsed           天然气 价格           交易成本 示例                   帐户 互动 示例   -   博彩 合同           离线 签署 交易                 EOA 与 合约 账户       以太 坊 有 两种 类型 的 账户           外部 拥有 的 帐户       合同 账户           在 Serenity 中 可以 消除 这种 区别 。       外部 拥有 账户 （ EOAs ）       一个 外部 控制 的 账户           有 一个   ether   balance ，       可以 发送 交易 （ 以太 汇款 或 触发 合约 代码 ） ，       由 私钥 控制 ，       没有 相关 的 代码 。           合同 帐户       合同           有 一个 ether   balance ，       有 相关 的 代码 ，       代码执行 由 从 其他 合同 收到 的 事务 或 消息 （ 调用 ） 触发 。       当 执行   -   执行 任意 复杂 的 操作 （ 图灵 完备 性 ）   -   操纵 自己 的 永久 存储 ， 即 可以 拥有 自己 的 永久 状态   -   可以 调用 其他 合同           以太 坊 区块 链上 的 所有 操作 都 是 由 外部 拥有 账户 发起 的 交易 启动 的 。 每当 合同 账户 收到 一笔 交易 时 ， 其 代码 就 会 按照 作为 交易 一部分 发送 的 输入 参数 的 指示 执行 。 合约 代码 由 参与 网络 的 每个 节点 上 的 以太 坊 虚拟机 执行 ， 作为 对 新块 进行 验证 的 一部分 。       这种 执行 需要 完全 确定性 ， 唯一 的 上下文 是 区块 链上 的 块 位置 和 所有 可用 的 数据 。     区块 链上 的 区块 表示 时间 单位 ， 区块 链 本身 是 一个 时间 维度 ， 并 表示 由链 上 的 区块 指定 的 离散 时间 点处 的 状态 的 整个 历史 。       所有 乙醚 的 余额 和 价值 都 以 魏为 单位 ： 1 乙醚 是 1e18   wei 。       注意       以太 坊 中 的 “ 契约 ” 不应 被 视为 应该 被 “ 履行 ” 或 “ 被 遵守 ” 的 事物 。 相反 ， 它们 更 像是 以太 坊 执行 环境 中 的 “ 自主 代理 ” ， 当 被 消息 或 事务 “ 戳穿 ” 时 总是 执行 特定 的 代码段 ， 并 直接 控制 自己 的 以太 平衡 和 自己 的 密钥 / 价值 商店 来 存储 他们 的 永久 状态 。       什么 是 交易 ？       在 以太 坊 使用 术语 “ 交易 ” 来 指代 存储 从 外部 拥有 账户 发送到 区块 链上 另 一个 账户 的 消息 的 签名 数据包 。       交易 包含 ：   消息 的 接收者 ，   识别 发送者 的 签名 并 证明 他们 通过 区块 链向 接收者 发送 消息 的 意图 ，   VALUE 字段   -   从 发件人 传送 给 收件人 的 金额 ，     一个 可选 的 数据字 段 ， 可以 包含 发送给 合同 的 消息 ，   表示 允许 执行 交易 执行 的 最大 计算 步骤 数量 的 STARTGAS 值 ，     一个 GASPRICE 值 ， 代表 发送者 愿意 为 天然气 支付 的 费用 。 一个 单位 的 气体 对应 于 一个 原子 指令 的 执行 ， 即 计算 步骤 。       什么 是 信息 ？       合同 能够 将 “ 消息 ” 发送给 其他 合同 。 消息 是从 不 序列化 的 虚拟 对象 ， 只 存在 于 以太 坊 执行 环境 中 。   他们 可以 被 认为 是 函数调用 。       消息 包含 ：   消息 的 发送者 （ 隐含 的 ） 。   消息 的 接收者   VALUE 字段   -   将 消息 一起 传送 到 合同 地址 的 wei 的 数量 ，   一个 可选 的 数据字 段 ， 即 合约 的 实际 输入 数据     一个 STARTGAS 值 ， 它 限制 了 消息 触发 的 代码执行 的 最大 气体 量 。       从 本质 上 讲 ， 消息 就 像 一个 交易 ， 除了 它 是 由 合同 而 不是 由 外部 参与者 产生 的 。 当 一个 正在 执行 代码 的 协议 执行 CALL 或 DELEGATECALL 操作码 时 ， 产生 一条 消息 ， 该 操作码 产生 并 执行 一条 消息 。 就 像 一个 交易 ， 一条 消息 导致 收件人 帐户 运行 其 代码 。 因此 ， 契约 可以 和 其他 契约 的 关系 完全 一样 ， 外部 行为 者 可以 。       什么 是 天然气 ？       以太 坊 在 称为 以太 坊 虚拟机 （ EVM ） 的 区块 链上 实施 了 一个 执行 环境 。 参与 网络 的 每个 节点 都 运行 EVM 作为 块 验证 协议 的 一部分 。 他们 检查 正在 验证 的 块 中 列出 的 交易 ， 并 运行 EVM 内 交易 触发 的 代码 。 网络 中 的 每个 完整 节点 都 执行 相同 的 计算 并 存储 相同 的 值 。 显然 以太 坊 并 不是 要 优化 计算 效率 。 其 并行处理 冗余 并行 。 这是 提供 一个 有效 的 方式 来 达成 一致 的 系统 状态 ， 而 不 需要 信任 的 第三方 ， 神谕 或 暴力 垄断 。 但 重要 的 是 ， 他们 不 在 那里 进行 最佳 计算 。 事实上 ， 合约 执行 是 冗余 复制 跨 节点 ， 自然 使得 它们 昂贵 ， 这 通常 会 产生 一个 激励 ， 不要 使用 区块 链 进行 离线 计算 。       当 您 运行 分散 的 应用程序 （ dapp ） 时 ， 它会 与 区块 链 交互 以 读取 和 修改 其 状态 ， 但 dapps 通常 只会 放置 对于 区块 链上 的 共识 至关重要 的 业务 逻辑 和 状态 。       当 由于 被 消息 或 事务 触发 而 执行 合同 时 ， 每个 指令 都 在 网络 的 每个 节点 上 执行 。 这有 一个 成本 ： 对于 每个 执行 的 操作 都 有 一个 指定 的 成本 ， 用 多个 气体 单位 表示 。       天然气 是 交易 发件人 在 以太 坊 区块 链上 进行 的 每一项 操作 都 需要 支付 的 执行 费用 的 名称 。 这个 名字 的 灵感 来源于 这个 费用 充当 了 加密 燃料 ， 推动 了 智能 合约 的 运动 。 天然气 是从 执行 代码 的 矿工 那里 购买 的 。 由于 天然气 的 单位 与 自然 成本 的 计算 单位 相一致 ， 因此 天然气 和 乙醚 的 故意 脱钩 ， 而 乙醚 的 价格 一般 会因 市场 力量 而 波动 。 二者 是 由 自由市场 调节 的 ： 天然气 的 价格 实际上 是 由 矿工 决定 的 ， 他们 可以 拒绝 以 低于 最低 限度 的 天然气 价格 进行 交易 。 为了 得到 天然气 ， 你 只 需要 添加 乙醚 到 你 的 帐户 。 以太 坊 客户 自动 为 您 的 乙醚 购买 天然气 ， 金额 为 您 指定 的 金额 ， 作为 交易 的 最大 支出 。       以太 坊 协议 收取 合同 或 交易 中 执行 的 每个 计算 步骤 的 费用 ， 以 防止 以太 坊 网络 上 的 故意 攻击 和 滥用 。 每笔 交易 都 需要 包含 一个 天然气 限制 和 一个 愿意 为 每个 天然气 支付 的 费用 。 矿工 可以 选择 包括 交易 和 收取 费用 。 如果 由 交易 产生 的 计算 步骤 （ 包括 原始 消息 和 可能 触发 的 任何 子 消息 ） 使用 的 气体 总量 小于 或 等于 气体 限制 ， 则 处理 交易 。 如果 天然气 总量 超过 天然气 限制 ， 则 所有 的 变化 都 将 被 恢复 ， 除非 交易 仍然 有效 ， 矿工 仍然 可以 收取 费用 。 交易 执行 中未 使用 的 所有 多余 的 天然气 将 作为 Ether 退还给 发货人 。 你 不必 担心 超支 ， 因为 你 只 收取 你 消耗 的 气体 。 这 意味着 发送 高于 估计值 的 气体 限制 的 交易 是 有用 的 ， 也 是 安全 的 。       估算 交易成本       一笔 交易 的 总成本 是 基于 两个 因素 ：       gasUsed 是 交易 消耗 的 总 天然气       天然气 在 交易 中 指定 的 一个 单位 天然气 的 价格 （ 以太 ）       总成本 =   gasUsed   *   gasPrice       gasUsed       EVM 中 的 每个 操作 都 分配 了 一定 数量 的 消耗 气体 。   gasUsed 是 执行 所有 操作 的 所有 气体 的 总和 。 有 一个 电子表格 提供 了 一些 背后 的 分析 一瞥 。       对于 估算 gasUsed ， 有 一个 可以 使用 的 估计 天然气 API ， 但 有 一些 注意事项 。       天然气 价格       用户 构建 并 签署 交易 ， 并且 每个 用户 可以 指定 他们 想要 的 任何 价格 ， 可以 是 零 。 然而 ， 在 边境 发起 的 以太 坊 客户 有 一个 0.05 e12 的 默认 煤气 价格 。 随着 矿工 们 的 收入 得到 优化 ， 如果 大多数 交易 都 是 以 0.05 e12 的 价格 提交 的话 ， 要 说服 矿工 接受 指定 较 低 或 零 的 煤气 价格 的 交易 是 困难 的 。       交易成本 示例       我们 拿 一个 只 增加 2 个 数字 的 合约 。   EVM   OPCODE   ADD 消耗 3 种 气体 。       使用 默认 天然气 价格 （ 截至 2016 年 1 月 ） 的 大致 成本 将 为 ：       3   *   0.05 e12   =   1.5 e11   wei       由于 1 个 乙醚 为 1 个 乙烯 ， 所以 总成本 为 0.00000015 乙醚 。       这是 一种 简化 ， 因为 它 忽略 了 一些 成本 ， 比如 将 两个 数字 合同 在 一起 的 成本 ， 甚至 在 它们 被 添加 之前 。           题       燃气 费       燃气 成本 计算器       以太 坊 天然气 价格                       Operation   Name       Gas   Cost       Remark                       步       1       每个 执行 周期 的 默认 金额               停止       0       自由               自杀       0       自由               沙 三段       20                     SLOAD       20       从 永久 存储 中 获得               sstore       100       永久 保存               平衡       20                     创建       100       合同 创建               呼叫       20       启动 一个 只读 的 调用               记忆       1       扩展 内存 时 每 增加 一个 单词               TXDATA       五       交易 的 每个 字节 的 数据 或 代码               交易       500       基本 费用 交易               合同 创建       53000       从 21000 改为 宅基地                   帐户 互动 示例   -   博彩 合同       如前所述 ， 有 两种 类型 的 帐户 ：           外部 拥有 账户 （ EOAs ） ： 由 私钥 控制 的 账户 ， 如果 您 拥有 与 EOA 关联 的 私钥 ， 则 可以 从 其 发送 以 太和 消息 。       合同 ： 一个 拥有 自己 代码 的 账户 ， 由 代码 控制 。           默认 情况 下 ， 以太 坊 执行 环境 无生气 ， 没有 任何 事情 发生 ， 每个 帐户 的 状态 保持 不变 。 但是 ， 任何 用户 都 可以 通过 从 外部 拥有 的 账户 发送 交易 来 触发 行动 ， 从而 使 以太 坊 的 轮子 运动 。 如果 交易 的 目的地 是 另 一个 EOA ， 那么 该 交易 可能 转移 一些 以太 ， 但 否则 什么 都 不 做 。 但是 ， 如果 目的地 是 合同 ， 合同 又 会 激活 ， 并 自动 运行 其 代码 。       代码 能够 读 / 写 自己 的 内部 存储器 （ 数据库 将 32 字节 的 密钥 映射 为 32 字节 的 值 ） ， 读取 接收 到 的 消息 的 存储 ， 并 将 消息 发送到 其他 契约 ， 并 依次 触发 它们 的 执行 。 一旦 执行 停止 ， 并且 由 合同 发送 的 消息 触发 的 所有 子 执行 都 会 停止 （ 这 一切 都 以 确定性 和 同步 的 顺序 进行 ， 即子 调用 在 父 调用 进一步 之前 完全 完成 ） ， 那么 执行 环境 将 停止 再 一次 ， 直到 下 一次 交易 被 唤醒 。       合同 通常 有 四个 目的 ：           维护 一个 数据 存储 ， 代表 对 其他 合同 或 外部 世界 有用 的 东西 ; 其中 一个 例子 是 模拟 货币 的 合同 ， 另 一个 例子 是 记录 特定 组织 的 成员 资格 的 合同 。       作为 一种 更 复杂 的 访问 策略 的 外部 账户 ; 这 被 称为 “ 转发 合同 ” ， 并且 通常 涉及 只有 在 满足 某些 条件 时才 简单 地 将 传入 消息 重新 发送到 某个 期望 的 目的地 ; 例如 ， 可以 有 一个 转发 合同 ， 等待 直到 给定 的 三个 私钥 中 的 两个 在 重新 发送 它 之前 确认 了 一个 特定 的 消息 （ 即 multisig ） 。 更 复杂 的 转发 合同 根据 发送 的 消息 的 性质 具有 不同 的 条件 。 这个 功能 的 最 简单 的 用例 是 一个 可以 通过 一些 更 复杂 的 访问 过程 覆盖 的 提取 限制 。 钱包 合同 就是 一个 很 好 的 例子 。       管理 多个 用户 之间 正在 进行 的 合同 或 关系 。 这方面 的 例子 包括 一个 金融 合同 ， 一些 特定 的 调解人 的 代管 ， 或者 某种 保险 。 也 可以 有 一方 开放 合同 ， 让 任何一方 随时 参与 ; 其中 一个 例子 就是 一个 合同 ， 它 自动 支付 给 谁 提出 一个 有效 的 解决方案 ， 以 解决 某个 数学 问题 ， 或者 证明 它 提供 了 一些 计算资源 。       为 其他 合同 提供 功能 ， 本质 上 充当 软件 库 。           合同 通过 交替 地 称为 “ 呼叫 ” 或 “ 发送 消息 ” 的 活动 彼此 交互 。     “ 消息 ” 是 一个 包含 一定 数量 的 以太 数据 的 对象 ， 任何 大小 的 数据 的 字节 数组 ， 发送者 和 接收者 的 地址 。 当 合同 收到 一条 消息 时 ， 它 可以 选择 返回 一些 数据 ， 消息 的 原始 发送者 可以 立即 使用 这些 数据 。 这样 ， 发送 消息 就 像 调用 一个 函数 一样 。       因为 合同 可以 扮演 不同 的 角色 ， 所以 我们 期望 合约 会 互相 影响 。 例如 ， 考虑一下 Alice 和 Bob 赌 100     GavCoin 的 情况 ， 即 旧金山 的 温度 在 明年 的 任何 时候 都 不会 超过 35 º C 。 然而 ， Alice 非常 注重 安全性 ， 而且 她 的 主 账户 使用 转发 合同 ， 这个 转发 合同 只能 通过 三个 私人 密钥 中 的 两个 批准 发送 消息 。 鲍勃 对 量子 密码学 是 偏执 的 ， 所以 他 使用 转发 契约 ， 只 传递 与 传统 的 ECDSA 一起 签名 Lamport 签名 的 消息 （ 但 由于 他 是 老式 的 ， 他 倾向 于 使用 基于 SHA256 的 Lamport     sigs 版本 ， 这是 以太 坊 直接 不 支持 ） 。       投注 合约 本身 需要 从 某个 合同 中 提取 有关 旧金山 天气 的 数据 ， 并且 在 需要 将 GavCoin 实际 发送给 Alice 或 Bob （ 或者 更 确切 地说 ， Alice 或 Bob 的 转发 ） 时 还 需要 与 GavCoin 合同 进行 交谈 合同 ） 。 我们 可以 显示 账户 之间 的 关系 ：               当 鲍勃想 完成 下注 时 ， 会 发生 以下 步骤 ：           发送 交易 ， 触发 Bob 的 EOA 到 他 的 转发 合同 的 消息 。       Bob 的 转发 合同 将 消息 的 散列 和 Lamport 签名 发送到 用作 Lamport 签名 验证 库 的 合同 。       Lamport 签名 验证 库 看到 Bob 想要 一个 基于 SHA256 的 Lamport   sig ， 所以 根据 需要 多次 调用 SHA256 库来 验证 签名 。       一旦 Lamport 签名 验证 库 返回 1 ， 表示 签名 已经 过 验证 ， 它会 向 代表 该 合同 的 合同 发送 一条 消息 。       投注 合约 检查 提供 旧金山 温度 的 合约 以 查看 温度 。       投注 合同 认为 对 消息 的 回应 显示 温度 高于 35 º C ， 因此 它 向 GavCoin 合同 发送 一条 消息 ， 将 GavCoin 从 其 账户 转移 到 Bob 的 转发 合同 。           请 注意 ， GavCoin 全部 “ 存储 ” 为 GavCoin 合同 数据库 中 的 条目 ; 在 步骤 6 的 上下文 中 的 “ 帐户 ” 一词 仅仅 意味着 在 GavCoin 合同 存储器 中 存在 具有 用于 投注 合同 地址 的 密钥 和 其 余额 的 值 的 数据 条目 。 收到 此 消息 后 ， GavCoin 合同 将 此值 减少 一定 数量 ， 并 增加 与 Bob 的 转发 合同 地址 对应 的 条目 中 的 值 。 我们 可以 在 下图 中 看到 这些 步骤 ：               离线 签署 交易       [ 也许 把 这个 添加 到 常见问题 中 ， 并 指向 turboethereum 指南 的 ethkey 部分 ？   ]           韧性 原始 交易 广播者      ", "tags": "blockchain", "url": "/wiki/blockchain/account-types-gas-and-transactions.html"},
      
      
      
      
      
        
      
      {"title": "Google软件测试之道", "text": "    Table   of   Contents           第 1 章   Google 软件测试 介绍           第 2 章   软件测试 开发 工程师           -   测试 先行                 第 1 章   Google 软件测试 介绍           当 有人 来 问 我 , google 成功 的 关键 是 什么 , 我 的 第一个 建议 是 , 别 找 太 多 的 测试人员       在 google , 写 代码 的 开发人员 也 承担 测试 的 重任       如果 你 是 名 工程师 , 那么 你 也 是 个 测试人员 。 如果 你 的 职位 有 测试 两个 字 , 你 的 任务 是 怎么 让 那些 没有 测试 头衔 的 人 更好 地 做 测试 。       质量 不是 被测 出来 的       开发人员 要 对 自己 的 代码 负责 , 比 专职 的 测试人员 更 适合 做 测试       在 google , 测试 是 独立 存在 的 部门 , 与 专注 领域 平行 的 部门 , 成为 工程 生产力 团队           第 2 章   软件测试 开发 工程师       -   测试 先行  ", "tags": "book-note", "url": "/wiki/book-note/how-google-test.html"},
      
      
      {"title": "《增长黑客》读书笔记", "text": "    Table   of   Contents           关于                 关于           硅谷 增长 黑客 ( Growth   Hacker )       2012 年   Grwth   Hacker   is   the   new   VP   Marketing       技术 :   A / B   测试 , 搜索引擎 优化 , 电子邮件 召回 , 病毒 营销       数据 驱动 营销 、 以 市场 指导 产品 , 通过 技术手段 贯彻 增长 目标 的 人       AARRR 转化 漏斗 模型 ,   Acquisition ( 获取 用户 ) 、 Activation ( 激发 活跃 ) 、 Retention ( 提高 留存 ) 、 Revenue ( 增加收入 ) 、 Referral ( 传播 推荐 )       代表 人物 :   安迪 - 琼斯   Andy   Johns      ", "tags": "book-note", "url": "/wiki/book-note/growth-hacker.html"},
      
      
      {"title": "《巴菲特致股东的信》投资原则", "text": "    Table   of   Contents           第一章   本书 用意           第二章   复利 增值           第三章   市场 指数 :   坐享其成 的 基本原理                 第一章   本书 用意           本杰明 · 格雷厄姆 的 基本 原则 :   短期 来看 , 市场 确实 会 不是 出现 混乱 , 产生 非理性 的 波动 ; 但 长期 来看 , 证券 的 市场 定价 符合 其 自身 潜在 价值 。       没人能 系统化 地 预测 短期 随机 波动 , 观测 短期 波动 与 投资 根本 无关       长期 来看 , 市场 将 趋于 理性 , 并且 最大 限度 地 将 公司 的 经济 状况 反应 到 其 股价 上       市场 先生 :   证券市场 是 一个 喜怒无常 的 狂躁 抑郁症 患者 , 每天 准备 让 投资者 买卖 自家 公司 一半 的 筹码 , 时而 情绪高涨 , 对 自己 前景 十分 看好 , 这时 只会 吧 高价 筹码 卖 给 你 ;   时而 情绪低落 妄自菲薄 , 这时 会 将 低价 筹码 卖 给 你 ; 而 大部分 情况 下 都 处于 中立 位置 。       必须 独立 于 市场 做出 判断       持有 股票 犹如 坐拥 公司 ,   我们 更 应该 关注 将会 发生 什么 ,   而 不是 什么 时候 发生       投机 行为 既 不能 说 是 非法 的 , 又 不能 说 是 丧失 道德 , 但 这种 行为 并 不会 为 我们 带来 多大 油水           第二章   复利 增值           爱因斯坦 把 复利 称作 世界 第八 大 奇迹 。       最好 的 投资 策略 是 投资 于 自始至终 都 产生 复利 的 项目 , 无一例外           第三章   市场 指数 :   坐享其成 的 基本原理           美国 大多数 基金 连 跟上 大盘 指数 都 很 难       成本 低廉 , 操作 起来 容易      ", "tags": "book-note", "url": "/wiki/book-note/buffett-letter-investing.html"},
      
      
      {"title": "《格鲁夫给经理人的第一课》阅读笔记", "text": "    Table   of   Contents           第一篇   早餐 店 的 生产线           第二篇   打 好 团体 战           管理 杠杆 率           管理 的 必经之路 :   开会           不 挥舞 权杖 的 决策                   第三篇   推动 组织 的 巧手           第四篇   某事 在 人           期末考   最后 的 叮咛                 第一篇   早餐 店 的 生产线       第二篇   打 好 团体 战       管理 杠杆 率           经理人 的 产出   =   他 直接 管辖 部门 的 产出   +   他 间接 影响 所 及 部门 的 产出       直接 产出 指 的 是 经理人 的 分内事 。   经理人 的 直接 产出 包括 领导 的 部属 产出       间接 产出 指 的 是 对 非 直接 领导 的 人 , 其他 部门 的 影响 带来 的 产出 。 问题 :   技术人员 的 间接 产出 具体 有 哪些 ? 如何 衡量 这部分 产出 ?                 分内事 与 结果       你 的 日常 工作 到底 是 什么 ?       活动 是 我们 日常 真正 在 做 的 事 。 产出 则 是 成就               经理人 的 日常 工作       信息 收集 :   不时 地 在 公司 走动 走动 。 设计 一些 正事 让 经理人 必须 在 公司 走动 。       给予 提示 :   没有 真正 地下 指令       决策 :   明确 的 指令       未雨绸缪 型       亡羊补牢 型               事情 永远 做 不 完 , 经理人 永远 有 忙 不 完 的 事       同时 处理 多个 事情 , 并且 要 将 精力 放在 当时 最能 促进 整个 组织 产出 的 活动 上 ( 即 杠杆 率 最高 的 点 )       报告 的 价值 在于 提案人 必须 对 自己 的 问题 或 方案 进行 更 严格 的 审视 。 报告 用来 表示 一个 人 的 自律 , 远 甚于 它 在 传递信息 上 的 作用 。       当 部属 的 表率 :   一天到晚 都 在 做 的 事情 ,   当 别人 学习 的 对象 。 言传 不如 身教 , 要 「 做 」 , 还要 做 得 「 让 人 看 得到 」 !       调配 自己 的 时间       上级 干涉 表示 上级 用 了 太 多 技能 性 的 指令 ( 不管 他 是不是 真的 懂 ) 。 负 杠杆 率 的 产生 来自 于 这样 的 情况 发生 得 太 频繁 时 , 下属 可能 因此 变得 像 锁头 乌龟 一样 畏首畏尾 。       专家 的 影响力 :   个人 的 工作 对 很多 人 都 有 积极 得以 影响 。 比如 程序员 的 工具 开发 , 类库 开发 。       授权 也 有 杠杆 率 。       一个 必要条件 :   两人 必须 有 相同 的 信息 基础 , 以及 在 开展 工作 和 解决问题 上 有 一套 彼此 认同 的 方法       没有 完备 监督 计划 的 授权 等于 渎职 。 因为 监督 熟悉 的 工作 比较 容易 , 所以 如果 有 机会 , 应该 把 自己 熟悉 的 工作 授权 给 他人 。               监督 的 原则       在 产品 价值 最低 时 就 进行 监督 。 比如 交代 部署 写个 报告 , 应该 在 草稿 完成 的 时候 就 拿 来看       检查 频率 。 不定期 抽查 , 并 对 不同 的 部署 进行 不同 的 采样 方法 和 频率 。               质量 管理 的 原则       不定期 深入 了解 细节 , 而且 在 次数 上 应该 「 不 扰民 」       通过 提问 来 检测 部署 是否 真的 仔细 思考 过               提高 生产力 的 方法 :   一件 事 不要 做 两次 ,   站 着 开会       找出 限制 步骤 , 也 就是 没法 改变 的 事情 , 然后 让 可变 的 事情 围绕 他们 来 改变       类似 的 工作 集中 在 一起 做 。       安排 好 日程表 :   先排 没有 弹性 的 事情 , 再排 重要 但 有 弹性 的 事情 。 对 超过 负荷 的 事情 , 要说 「 不 」       建立 指标 :   用 指标 来 规划 工作 ,   比如 负荷量       存货 法 :   有 一些 利于 提高 长期 生产力 但 又 不 着急 完成 的 项目 , 方便 在 时间 空闲 的 时候 , 可以 介入       标准化 :   处理 事情 的 流程 标准化       应该 有 多少 个 部署 ?   一 星期 内 需要 花 半天 时间 在 一个 部署 身上 为 条件 。 6 - 8 个       规律 化                           管理 的 必经之路 :   开会           一对一 开会 :   部署 不同 的 熟练度 和 能力 , 新 项目 和 老 项目 , 挑战 度 是否 比较 高       下属 是 负责 准备 一对一 会议 的 最佳人选       下属 准备 一个 会议 刚 要       讨论 的 主题 :   有 问题 的 指标 应该 特别强调 。 重要 的 是 部署 应 负责 提出 潜在 的 问题       上司 在 一对一 会议 中 扮演 的 角色 ?   协调者 , 让 下属 畅所欲言 工作 的 状况 和 不顺 。 让 部署 将 他们 的 问题 告诉 他 。 秘诀 :   多问 一个 问题 。       更 有效 的 招数 :       上司 和 下属 都 应该 握有 灰机 刚要 , 并且 都 在 会议 中 做 笔记 。 做 笔记 是 为了 在 会议 中 集中 精神 , 通过 强迫 对 信息 分类 来 消化吸收 的 信息       写 下来 。 在 你 提出 建议 后 , 部署 马上 动笔 写 下来 。       建立 存档 。 将 一些 重要 但 不 紧急 的 事项 列入 次 档案 中 , 留待 下次 讨论       鼓励 下属 在 一对一 会议 中 讲些 「 心里话 」 , 了解 平时 不 愿意 讲 的 工作 问题       电话会议 ,   双方 必须 在 会议 之前 拿到 会议 纲要 ,   双方 也 需要 做 笔记       在 每次 会议 结束 前 , 都 应该 计划 下 一次 开会 的 时间 。 借此 可 避免 因为 与 其他 事撞 到 一起 而 必须 取消 会议       一对一 会议 的 杠杆 率 。 可以 影响 下属 后续 两周 的 工作 品质 , 还 能 增进 对 他 工作 的 了解 。 一对一 会议 有 巨大 的 杠杆 率 。 此外 , 还 方便 统一 做事 方式 , 最终 实现 「 成功 授权 」       从 下属 身上 学到 东西 , 纠正 之前 的 认知 偏差                           部门 会议           部门 会议 的 与会 人员 包括 部门 主管 及其 部署       可以 了解 部署 间 的 互动关系 , 同事 之间 加深 认识       让 上司 能够 在 会议 的 冲突 或 交换意见 中 更加 了解 事情 的 真相       部门 会议 应该 谈 涉及 到 两个 以上 的 部署 的 问题 , 如果 只 涉及 到 两个 人 , 则 要 打断 , 私下 沟通 , 应该 讨论 更多人 参与 的 话题       必须 有 计划 地 进行 , 应该 事先 知道 要 讨论 什么 , 这样 在 开会 前 就 能 做好 准备 。 但 也 要 保留 一定 开放 时间 , 让 大家 畅所欲言 。       人员 :       会议 召集人 。 什么 该 讲 、 什么 不该 讲 、 什么 该 强调 、 什么 该 深入 细节       总结 负责人 。 提出 问题 和 意见 , 激发 会议 讨论 , 并 以身作则 地 带动 自由 讨论 。       提案人 。       其他 与会者 。 参与 以及 发问               召开 会议 之前 , 问问 自己 :   有 什么 任务 要 通过 这场 会议 达成 ? 是不是 有 开会 的 必要 ? 如果 有 迟疑 或 有 其他 办法 , 就 不要 开会 。 开会 的 时间 成本 很 高 , 尽量 使用 替代 方案 。                   个人 思考 , 提出 大概 方向           总结 一下 过去 一两周 的 工作 、 进展 和 问题       对 未来 一两周 工作 的 想法       想 做 哪些 事情 , 大约 一两个 月 能 搞定 的 事情                   不 挥舞 权杖 的 决策       第三篇   推动 组织 的 巧手       第四篇   某事 在 人       期末考   最后 的 叮咛  ", "tags": "book-note", "url": "/wiki/book-note/manager-first-leason.html"},
      
      
      {"title": "《激荡十年》读书笔记", "text": "    Table   of   Contents          ", "tags": "book-note", "url": "/wiki/book-note/wxb-jdsn2018.html"},
      
      
      {"title": "《硅谷增长黑客实战笔记》读书笔记", "text": "    Table   of   Contents           第 1 章   我 的 增长 黑客 旅程           第 2 章   指定 增长 作战 计划           第 3 章   用户 获取 :   增长 的 源头活水           第 4 章   用户 激活 : 增长 的 关键 转化 点           第 5 章   用户 留存 : 增长 的 坚实 根基           第 6 章   从零开始 组建 增长 团队           第 7 章   打造 高效 运转 的 增长 引擎                 第 1 章   我 的 增长 黑客 旅程           海盗 指标       用户 获取       用户 激活       用户 留存       用户 推荐       盈利       流失 召回               关键词 :   实验 驱动 增长       增长 黑客网 每周 增长 例会 流程       检查 增长 指标 、 问题 和 机会       回顾 上周 的 增长 试验       讨论 试验 结果       决定 下周 试验       查看 备选 试验 想法               增长 黑客 的 最初 90 天       了解 增长 工具箱 :   渠道 管理工具 、 邮件 、 移动 推送 、 用户 行为 追踪 、 数据 可视化 等 数据分析 工具箱 、 AB 测试工具       深入研究 历史数据 , 查看 关键 指标 ( 比如 海盗 指标 ) , 目的 是 找到 问题       和 直接 经理 / CEO 会谈 , 了解 工作 背景 信息 , 沟通 双方 工作 风格 , 讨论 增长 的 计划 和 优先级 , 了解 如何 和 工程师 、 设计师 合作 , 以便 上线 试验 。 最 关键 的 议题 是 首先 应该 集中 火力 在 哪 方面 进行 实践 , 基于 第 2 步 的 研究 和 分析       和 团队 及 合作者 开会 , 跨部门 沟通 和 做 关系 , 争取 更 多 盟军       倾听 用户 的 声音 , 用户 拜访 、 电话 、 阅读 用户 评论 , 社交 媒体 留言 , 找 机会 参与 一个 用户 研究 访谈 , 或者 是 坐下 来 听听 客服 电话       确定 增长 指标 , 行动 之前 , 套路 清楚 增长 指标 , 日活 ? 什么 是 有效 活跃 ?       设立 增长 试验 流程       选择 试验 记录 系统       组织 增长 会议       倡导 增长 文化               如何 打响 增长 第一炮       对 增长 黑客 了解 的 越 多 , 越 明白 好 的 用户 留存 对 增长 的 重要性 。 因为 留住 一个 老 用户 永远 比拉 一个 新 用户 的 成本 更 低       确定 增长 指标 — — 北极星 指标 , 唯一 重要 的 指标 , 他 应该 反映 产品 给 用户 传输 的 价值 。 对 酒店 就是 间 夜量 , 对 电商 就是 GMV 。 指标 需要 容易 测量 , 具有 及时性 , 甚至 前瞻性       找到 聚焦 领域 , 不 局限 与 传统 的 客户 通信 渠道 里面 , 而是 同时 也 考虑 从 产品 中 寻找机会 , 看 谁 的 性价比 高       寻找 杠杆 : 我 理解 为 寻找 边际 效益 最大 的 点 , 投入 少 产出 大 。 事实上 , 对于 绝大多数 产品 而言 , 改善 留存 , 最 具有 杠杆 效应 的 领域 都 存在 与 产品 之中 。 我 的 理解 是 , 其他 外部 拉取 途径 从 投放 到拉到 产品 中 这个 过程 都 会 有 个 很大 的 折损 , 所以 外部 途径 ( 比如 发短信 、 push 、 地 推 等 ) 在 最初 的 时候 , 都 不如 优化 产品 的 边际 收益 大       所以 优化 增长 的 第一个 要 看 的 是 留存 率 ! ! !               上线 增长 试验 ,   找到 留存 的 所有 用户 行为 路径 , 找到 最大 的 流量 处 , 提升 转化 。 初期 可以 进行 简单 的 「 文本 试验 」 , 仅仅 修改 文案 提高 转化       对 用户 心理 的 理解 , 「 从众 心理 」                       组建 独立 增长 团队       「 全栈 」 团队 , 程序员 , 美工       做好 宣传教育 工作 , 让 大家 了解 并 认可 团队 工作 , 给予 支持       每周 试验 心得 分享 会 , 邀请 产品 、 市场 、 设计 、 客户服务 部门 代表 参加 。 「 猜猜 哪个 版本 赢 」 的 游戏                   第 2 章   指定 增长 作战 计划           实现 增长 的 方式       公关 宣传       在线 广告       想方设法 增加 DAU ,   给 流失 用户 发 push 和 邮件       印度 皮包公司 买 点击 ? ? ?               每 一个 都 试一下 ? 了解 二八 定律 , 增长 的 秘诀 不 在于 同时 做 很多 事 , 而 在于 找到 目前 影响 增长率 最 关键 的 那 一两件 事情       新 用户 引导               作战 计划书       方向标 :   北极星 指标       路线图 :   增长 模型       仪表盘 :   关键 指标 看 板       参考书 :   用户 心理 决策 地图               增长 的 北极星 :       Facebook   vs   MySpace ,   Facebook 选择 日活 用户 数目 作为 指标 , 而 MySpace 选择 了 注册 用户 数目       6 个 标准       产品 核心 价值 能 反应 在 这个 指标 上 吗 ?       能够 反应 用户 活跃度 吗 ?       如果 这个 指标 变 好 , 是不是 能够 说明 整个 公司 是 在 相好 的 方向 发展 ?       Uber 应该 选择 反应 司机 和 乘客 两方 的 指标 , 比如 总 乘车 次数 , 而 不是 选择 司机 活跃 数目               这个 指标 是不是 很 容易 被 整个 团队 理解 和 交流 呢 ?         不能 太 复杂 , 比如 超过 100 元 的 订单 比例               这是 一个 先导 指标 还是 滞后 指标 ?       这个 指标 是不是 一个 可 操作 的 指标 ?               案例 :       sound   cloud : 音乐 分享 ,   总 收听 时间       slack :   聊天 , 总 消息 数目       box : 云 存储 ,   文件 操作 数目       Airbnb :   民宿 ,   预定 天数 / 总间 夜量       quora :   问题 回答 数               TIPS :       不要 苛求 完美 , 不要 试图 一步到位                       构建 增长 模型       用户 增长 、 利润 增长       将 生意 提炼 为 一个 数学公式 , 从而 帮助 你 用 全面 、 简单 和 结构化 的 方式 思考 增长       Google   andense :   广告 网络 收入   =   广告 展示 数目   x   点击率   x   平均 点击 收入       这个 公式 将 成为 一个 重要 的 决策 工具 ,   新 的 项目 影响 的 是 这个 公式 里面 的 哪 一部分 ?       3 个 因素       输出 变量 :   一般 就是 北极星 指标       输入 变量 :   影响 被 北极星 指标 的 主要 变量       方程 :   关系               总 活跃 用户   =   新增 活跃 用户 数目   +   老 用户 活跃 数目       3 个 步骤       定义 北极星 指标       绘制 用户 路线图       组建 增长 模型               案例 :       以 听歌 应用 来说 ,   如果 北极星 指标 是 「 总 听歌 时间 」 , 那么 访客 的 路线图 是 :       下载 应用       注册 账户       浏览 歌曲       首次 听歌       持续 登陆       持续 听歌               增长 模型 为 :   总 活跃 用户   =   新增 活跃 用户 数目   +   老 用户 活跃 数目   =   下载量   x   注册 率   x   首次 浏览 比例   x   首次 听歌 比例   +   已有 用户 数目   x   持续 登陆 比例   x   持续 听歌 比例       电商 网站 销售额   =   新增 活跃 用户 销售额   +   已有 活跃 用户 销售额   =   网站 新 用户 访问量   x   用户注册 率   x   首次 购买率   x   新 客客 单价   +   老客 数目   x   老客 复购 比例   x   老客 客 单价               通过 增长 模型 可以 明显 看出 , 影响 最大 的 因子 是 哪个 ? 找到 最大 的 瓶颈 因素       增长 模型 可以 有效 地 定量分析 和 指定 目标 , 避免 主管 判断               监控 数据       用户 行为 数据 , 用户 行为 追踪 计划       刚 开始 搭建 的 时候 , 先 关注 重点 几个 事件 的 追踪 , 循序渐进 , 更 快速 地 拿到 数据       最终 要 的 三个 一级 事件       电商 :   页面 浏览 ,   加 购物车 ,   购买 成功       聊天 :   注册 ,   加 好友 ,   发 信息       SaaS :   试用 注册 完成 ,   关键 功能 使用 ,   升级 付费                       打造 一个 增长 仪表盘       北极星 指标       增长 模型 的 关键 指标       关键 细分 指标       重要 用户 分组 ,   新 老客 ,   产品 平台                       用户 心理学 及 绘制 用户 决策 心理 地图       访问 阶段 :   用户 会 注意 到 这个 产品 吗 ? 通过 有 冲击力 的 设计 和 文案 吸引 用户 的 眼球 , 引起 用户 的 共鸣       转化 阶段 :   下载 这个 应用 对 用户 有 什么 好处 ?   稀缺性 、 社交 证据 、 紧迫感       激活 阶段 :   用户 要 如何 使用 这个 产品 ?   通过 各种 方式 对 新 用户 引导 , 简化 流程 , 取出 阻碍 行动 的 壁垒 , 适时 提醒 没有 行动 的 用户 , 以及 帮助 用户 设立 一个 向 高级 用户 进发 的 目标 和 计划       留存 阶段 :   引导 用户 形成 使用 习惯 ,   发现 新 的 价值 ?       庆祝 用户 的 进程 和 里程碑       适时 提醒 和 沟通       向 用户 介绍 新 功能               推荐 阶段 :   将 应用 推荐 给 别人 ,   通过 补贴 鼓励 用户 推荐       变 现阶段 :   用户 愿意 为 产品 付费 吗 ? 值得 吗 ? 有 别的 替代 平 吗 ?               个人 思考 :   北极星 指标 对 算法 建模 的 指导意义       给 我们 定义 了 建模 目标 ( 指标 ) ,   比如 可以 直接 预测 每个 人 一段时间 的 听歌 时 长 , 转化率 , 留存 率 , 然后 利用 这些 指标 做 个性化 运营 , 比如 对 听歌 时 长短 的 、 留存 率低 的 提前 做 更 多 的 营销       找到 影响 分解 因素 的 指标 后 , 通过 算法 预测 这些 指标 , 提供 个性化 的 产品 。 比如 对 不同 的 人 推荐 不同 的 商品 和 头 图 , 增加 转化 etc                   第 3 章   用户 获取 :   增长 的 源头活水           增长 为 王       新 用户 获取 的 永恒 公式 :   LTV & gt ; CAC       用户 生命周期 价值 ( Life   Time   Value ) 要 大于 获取 这个 新 用户 所 需要 的 成本 ( Customer   Acquisition   Cost )       盈利模式 的 可持续性 , 与 盈利 能力   LTV / CAC       CAC   =   ( 营销 总费用   +   销售 总费用 ) / 同 时期 新增 用户数       对于 2C 产品 , 按照 当月 市场 渠道 花费 除以 当月 新增 用户数       对于 2B 产品 , 除了 渠道 花费 , 还 包括 举办 会议 活动 的 花费 , 文案 软文 花费 , 销售费用 等等       混合 CAC ,   把 付费 渠道 和 天然 渠道 ( 口口相传 、 社交 媒体 、 天然 搜索 ) 混在 一起 计算 的 CAC 。 这个 会摊 薄 获客 成本       付费 CAC :   付费 渠道 的 平均 顾客 获取 成本 , 排除 了 天然 渠道       满载 CAC :   加上 所有 市场 和 销售 人员 薪酬 、 工具 设备 等等 所有 的 花费 计算出来 的 用户 获取 成本       计算 LTV :       用户 会 使用 你 的 产品 多少 个 月       平均 每个 月 你 能 从 用户 身上 赚 多少 钱       利润 计算 , 生命周期 收入 乘以 毛利率       按照 不同 渠道 的 月 留存 曲线 , 按照 一定 的 折现 率 , 采用 现金流 折现 的 方法 , 加总 未来 几年 的 收入 模拟 得出       电商 产品 , 平均 订单 价值 x 每年 购买 次数 x 平均 生存 年数                       因为 CAC & lt ; LTV , 增长 渠道 受到 LTV 的 限制 , 很多 产品 的 增长 瓶颈 最终 是 由 产品 的 盈利模式 所 决定       如果 产品 是 免费 的 , 那么 产品 本身 获得 的 盈利 不足以 支撑 长期 付费 增长 , 病毒传播 、 搜索引擎 优化 、 UGC 等 免费 渠道 可能 是 唯一 的 选择       市场 、 产品 、 模型 、 渠道 四者 之间 都 要 有 很 好 的 契合       市场 - 产品 契合 :   有 一个 目标 客户群 存在 。 摩拜 目标 是 大城市 的 白领       产品 - 渠道 契合 :   能够 在 特定 渠道 上 , 找到 产品 的 目标 客户群 。 摩拜 地铁站 广告 , 甚至 将 单车 运到 地铁口 本身 就是 一种 广告效应       渠道 - 模型 契合 :   产品 的 盈利 模型 和 用户 生命周期 价值 能够 支持 使用 这些 渠道 的 成本 。 以 每次 1 元 的 收入 为例 , 人均 每年 收入 150 元 , 那么 除去 单车 折旧 分摊 到 每个 人 50 , 运营 成本 分摊 50 , 还有 50 元 的 收入       模型 - 市场 契合 :   目标 客户群 愿意 为 这个 产品 付费 , 支持 产品 的 盈利 模型 。 有 一定 比例 的 用户 愿意               选择 合适 的 用户 获取 渠道       认识 产品 的 特点 ,   2C , 2B , 免费 , 付费 , 高 门槛       了解 你 的 用户 群体       列出 可能 的 备选 渠道       付费 增长 ,   在线 广告       病毒 营销       搜索引擎 优化       销售               筛选 最初 的 获取 渠道       大体 量 的 优先       便宜 或 免费 渠道 优先       可追踪 的 渠道 优先       可 精准 定位 目标 用户群 的 渠道 优先       可 随时 开始 、 随时 结束 的 渠道 优先       初步 测试 的 时候 应该 观测 两个 指标 选择 对 哪些 渠道 加大 投入 。 目的 , 找到 LTV / CAC 最高 的 渠道       CAC , 看 那个 渠道 的 获客 成本 最低       留存 时间 和 LTV , 看 哪个 渠道 的 用户 留存 最长 , LTV 最高 , 即 用户 价值 高                       运营 、 优化 和 拓展 用户 获取 渠道       指定 新 用户 获取 目标       决定 市场 预算 分配 和 渠道 的 日常 运营         通过 广告设计 测试 和 用户 定位 测试 , 优化 已有 渠道 的 表现       不断 发现 和 探索 新 的 渠道 .   过于 依赖 某个 单一 渠道 是 有 风险 的                             增长 黑客 最爱 的 用户 获取 渠道 : 用户 推荐       YC 调研 显示 , 超过 70 % 的 增长 专家 表示 「 用户 推荐 」 是 产品 早期 增长 最为 重要 的 渠道 之一       获取 成本低       用户 质量 好 ,   目标 用户 相似       转化 比例 高 ,   由于 有 好友 做 背书 , 更 容易 让 人 信服       用户 推荐 是 指 公司 利用 系统性 方式 鼓励 老 用户 向 其他人 传播 你 的 产品 的 服务       病毒传播 是 指新 用户 通过 一种 机制 不断 吧 产品 推广 到 他们 的 社交圈       网络 效应 是 指 更 多 的 用户 加入 网络 会 使 产品 变得 更好 , 要 尽量 将 产品 做成 一个多 玩家 游戏       用户 推荐 的 六大类       口口相传       展示 互传 ,   二维码 相传 哈哈       补贴 推荐 ,   各种 分享 领 红包 , 最好 是 送 产品 功能       社交 网络 用户 推荐       病毒传播       产品 内 传播 机制 ,   水印               衡量 用户 推荐 的 万能 公式 :   K 因子 , 平均 一个 老客户 可以 带来 几个 新 客户 , 如果 K & gt ; 1 , 那么 用户数 就 会 持续 的 增长       通过 邀请 加入 的 新 用户 数目 = 潜在 的 推荐人 总数   x   推荐人 的 转化率   x   分支 因子   x   被 推荐人 转化率       推荐人 转化率 是 指 用户 会 推荐 的 概率 , 一般 这个 概率 会 比较 低 , 优化 空间 比较 大       被 推荐人 转化率 一般 也 比较 低 , 它 可以 分解 为 接受 邀请 比例   x   完成 注册 的 比例       静态 分享 的 转化率 一般 比较 低 , 通过 合适 的 实际 插入 「 邀请 触发 」 会 提高 转化率 , 比如 完成 订单 后 邀请 、 完成 注册 后 要求       邀请 页面 , 邀请 落地 页 存在 优化 空间       邀请 奖励 , 给 邀请 双方 的 奖励       邀请 方式 , 社交 媒体 还是 什么 , 微信 ? 可能 被 封杀       邀请 信息 , 文案 和 图片       邀请 结果 , 统计 发出 邀请 数目 , 接受 邀请 数目 , 需要 埋点 跟踪                       个人 思考 :   在 用户 获取 阶段 , 算法 可以 有用 的 点       预测 用户 的 个性化 LTV 价值 , 可以 帮助 我们 为 不同 的 人 提供 不同 的 营销 预算 , 帮助 我们 做 个性化 补贴 , 给 LTV 价值 高 的 用户 提供 补贴       预测 用户 的 个性化 CAC , 也 可以 提供 个性化 补贴                   第 4 章   用户 激活 : 增长 的 关键 转化 点           投入 比 开发新 产品 更 更 多 的 精力 到 新 用户 体验 上       新 用户 激活 往往 是 增长 团队 最 容易 找到 机会 的 地方       新 用户 激活 具有 放大 效应               如何 定义 激活       找到 aha 时刻 , 该 时刻 具体 、 清洗 可 衡量 , 发生 在 用户 的 早期 。 三要素 : 谁 在 多长时间 内 完成 了 一个 里程碑式 的 动作       对于 拍照 应用 是 拍 一张 照片       对 Dropbox 是 里面 有 一个 文件       对 Facebook 是 7 天内加 了 10 个 好友       Twitter 是 关注 了 30 个 用户 , 有 一定 比例 关注 你       LinkedIn 是 一周 内 建立 4 个 联系人               找到 用户 的 Aha 时刻 就 找到 了 激活 用户 的 密码       列出 所有 可能 的 关键 行为       通过 数据分析 , 找到 留存 率正 相关 最强 的 行为       通过 定性 用户 调研 进一步 确认 关键 行为 ( 找到 因果关系 而 不是 相关 干洗 )                       衡量 新 用户 激活 的 常用 指标 和 图案       激活 率                   四大 原则 和 八大 误区           增强 动力       保持 外部 广告 和 新 用户 欢迎 页 的 一致性 , 避免 疑惑       向 用户 解释 开通 权限 的 好处       利用 社会 信任 , 展示 推荐 的 老 用户 名字 和 照片       让 用户 有 参与感               减少 障碍       推迟 注册 的 步骤 , 先 让 用户 使用 产品       移除 多余 步骤       避免 冷启动 , 例如 做 网站 的 产品 给 用户 一 开始 就 做 一个 页面               适时 助推       明确 窗口 机会 , 尽快 完成       灵活 采用 各种 UX 模式 进行 用户 引导       利用 邮件 、 移动 推送 、 短信 等 外部 渠道 提醒 用户 完成 新 用户 激活               私人 定制 ,   让 用户 自己 选择 兴趣 和 偏好 , 根据 用户 消费 历史 推荐 , 根据 用户 目的 给予 不同 的 引导                   个人 思考 :   在 用户 激活 阶段 算法 可以 做 的 事情           通过 回归 分析 确认 关键 行为 的 权重 , 可以 解决 没有 定量分析 的 问题       好友 / 明星 / 大 V / 联系人 / 店铺 推荐       留存 率 预估 , 对 留存 率低 的 用户 增加 更 多 的 营销 行为       私人 定制 这块 可以 使用 到 个性化 推荐                   第 5 章   用户 留存 : 增长 的 坚实 根基           钱 可以 买 了 新 用户 , 却 买 不来 用户 留存       留存 的 强大 之 处 在于 其 复利 效应       用户 留存 要 和 产品 的 关键 行为 挂钩 ,   游戏 可以 使用 登陆 ,   投资 app 可以 使用 完成 投资       留存 周期 , 不同 功能 的 APP 的 使用 频率 不同 , 游戏类 的 至少 每周 一次 , 可以 定义 周 留存 率 ; 而 即时 通信 类 的 更 接近 至少 一天 一次 , 将 留存 定义 为 每天 登陆 更为 妥当       留存 曲线 :   留存 曲线 是 精明 的 风险投资 人 最 爱看 的 一张 图       好 的 产品 留存 曲线 会 变得 越来越 平 ,   如果 曲线 一直 往下走 , 那么 最终 所有 用户 都 会 流失       纵向 对比 不同 时期 的 留存 曲线 ,   可以 看到 留存 率 是否 在 提升 ,   产品 是否 在 改进               留存 的 不同 阶段 :       新 用户 激活 阶段 :   目标 是 帮助 新 用户 快速 发现 产品 价值 达到 Aha 时刻       中期 留存 阶段 :   目标 是 帮组 用户 形成 使用 习惯       长期 留存 阶段 :   目标 是 让 用户 经常 回来 使用 产品 ,   感受 到 产品 的 核心 价值 ,   避免 用户 的 流失       流失 用户 阶段 :   目标 是 让 用户 重新 发现 产品 价值 ,   换回 用户               用户 分组 :   从 留存 曲线 中 找到 增长 的 玄机       可以 比较 不同 流量 来源 用户 的 留存 曲线 , 了解 不同 渠道 引流 的 用户 长期 表现 怎么样       可以 比较 不同 用户 特征 的 用户 留存 曲线 :   性别 、 客户端 、 行为 分 群       行为 分 群 : YouTube 的 例子       视频 观看 者 :   留存 率会 上线 波动 很大       视频 评论者 :   参与度 更高 、 留存 率 也 会 越 高       视频 创造者 :   很小 的 比例 ,   留存 率 最高 ,   驱动 他们 留存 的 因素 是 粉丝 数 、 与 粉丝 的 互动 和 金钱 收入                       留存 之后 , 如何 做到 「 长相 厮守 」       留存 与 参与 的 区别 ,   留存 是 指 关键 行为 , 一次 即可 ;   参与度 则 指 关键 行为 的 数量       留存 率高 、 参与度 高 :   上瘾 型 产品 ,   如抖音 、 微信 等 社交 类 应用       留存 率高 、 参与度 低 :   效用 型 产品 ,   如 各种 工具 类 APP 、 电商 类 APP       留存 率低 、 参与度 高 :   潮流 型 产品 ,   如 跳 一 跳 等 小 程序 游戏       留存 率低 、 参与度 低 :   一次性 产品 ,   如 商场 打 照片 应用               用户 参与度 的 1 、 9 、 90 规则 :   留存 用户 细分       消极 用户 :   没有 按照 最 理想 的 方式 使用 产品 ,   但是 他们 按照 自己 的 方式 以 比较 正常 的 频率 持续 使用 产品 ( 90 % )       核心 用户 :   以 比较 正常 的 频率 和 正确 的 方式 使用 产品 ,   是 活跃 用户 里 的 大多数 ( 9 % )       超级 用户 :   不光 高频 使用 产品 , 还 可能 使用 一些 高级 功能 ,   骨灰级 用户 ( 1 % )               用户 的 参与度 阶梯       可以 将 消极 用户 向 核心 用户 引导 、 将 核心 用户 向 超级 用户 引导               持续 引导 :   留存 永无止境       鼓励 用户 正确 地 使用 产品 的 行为       提高 用户 使用 产品 的 频率 :   游戏 中 的 连击 、 胸章 etc       增加 统一 用户 使用 不同 产品 功能 的 数量       让 用户 使用 多个 客户端       在 合适 的 时刻 向 用户 介绍 新 功能 ,   促使 用户 感知 产品 更 多 的 价值                       打造 独角兽 产品 的 秘诀 :   习惯 养成 和 行为 放大       sarah   taval   10 亿美元 独角兽 级别 产品 路线图 :   增加 参与 用户数 ( 用户 获取 )   - & gt ;   长期 留住 用户 ( 用户 留存 )   - & gt ;   自我 持续   - & gt ;   10 亿 + 美元 独角兽       有 粘性 的 产品   =   用户 用 得 越 多 积累 的 好处 越 多   +   用户 用 的 越 多 离开 后 损失 越大       自我 持续 :   产生 良性循环 ,   参与 闭环       完成 关键 行为 :   BJ   Fogg   的 行为 模型   行为 = 动机   能力   触发   三个 因素 的 影响       行为 :   我们 想 让 用户 采取 的 行为 ,   例如 下单       动机 ( motivation ) :   用户 有 多 想 完成 这个 行动 ,   异地 比 本地 更想订 酒店 。 通过 温馨 提示 、 胸章 、 从众 心理 、 金钱 奖励 、 沉默 成本 等 方式 增强 用户 动机       能力 ( ability ) :   这个 行为 对 用户 来说 有 多 容易 ,   订购 酒店 越 方便 越 好 。 减少 用户 执行 这个 关键 行为 的 困难 。       和 触发 ( trigger ) :   提醒 用户 采取行动 ,   推送 发券 。 在 合适 的 时候 触发 推送 提醒               习惯 养成 的 黄金 公式 《 Hooked :   How   to   build   habit / forming   products 》 nir   eyal       给 用户 单次 行为 提供 一个 「 奖励 」 , 并且 这个 奖励 是 不 固定 的 , 每次 都 不 一样 , 让 用户 期待 这 下次 回来 可能 会 得到 一个 惊喜 。 微信 每周 免费书 , 每周 数目 都 不同       要求 用户 在 产品 中 投入 一点 努力 , 而 这 一点 努力 可以 增加 用户 在 产品 里 储藏 的 价值 。 ( 就是 沉默 成本 )       让 外 在 的 触发 和 用户 内在 的 触发 相结合 。 时间 久 了 就 成 了 习惯       打造 一个 参与 闭环 , 让 用户 的 一个 行为 可以 带来 更 多 的 行为       单人 模式 闭环       Facebook   每年 会 显示 「 查看 你 的 Facebook 回忆 」               多人 模式 闭环       社交 网络 ,   一个 人 的 行为 瞬间 有 了 可以 影响 到 他 所在 的 整个 社交 生态系统 的 能力 。 「 谁 看 了 你 的 履历 」                                           提高 用户 留存 的 8 种 武器 :   产品 改进 、 新 用户 引导 和 教育 、 邮件 、 通知 、 客户服务 、 促销 、 忠诚 用户 计划 、 新 产品               三个 模型 的 关系 :           要 做 一个 独角兽 , 需要 完成 sarah   taval 增长 模型 的 前 3 步       第一步 让 用户 完成 关键 行为 :   利用 BJFogg 的 模型 让 用户 从 新 用户 变成 完成 关键 行为 的 用户       要 实现 用户 长期 留存 和 自我 永续 需要 用到 上瘾 模型 , 让 用户 形成 习惯 。                   个人 思考 :   在 留存 阶段 算法 可以 做 的 事情           留存 率 预估 :   X 天 留存 率 ,   现阶段 运营 是 通过 单一 维度 ( 比如 上次 购买 时间 来 判定 留存 不 留存 ) ,   而 留存 率 预估 可以 考虑 多个 维度 , 通过 精准 预估 留存 率 实现 精准 营销       留存 关键 行为 引导 :   引导 用户 感受 到 产品 的 价值 、 引导 用户 增加 沉没 成本       参与度 阶梯 引导 :   通过 push 内容 、 优惠券 投放 、 功能 推送 等 方式 引导 用户 进入 下 一个 等级       如何 科学 地 评估 用户 的 LTV 价值 ?   如何 建模 LTV 价值                   第 6 章   从零开始 组建 增长 团队           为什么 :   Snapchat 与 Facebook 的 例子       从 增长 黑客 到 增长 团队 ,   扎克 伯格 说 「 增长 团队 是 Facebook 过去 8 年里 比较 重要 的 一个 发明 」       要 想 持续 推动 增长 , 必须 要 转变 为 有 体系 、 有 流程 、 不断 试验 、 以 团队 的 形式 来 推动               增长 团队 的 本质 :   一个 以 用户 和 利润 增长 为 目标 的 产品 团队       增长 团队 更 偏重 指标 为 导向 , 其 方法论 更 偏重 数据 驱动 , 更 注重 试验 , 并且 常常 要求 团队 明确 地 在 商业 指标 和 用户 体验 之间 做 权衡       传统 的 营销 更多地 关注 新 用户 获取 , 而 增长 团队 则 着眼 与 整个 生命周期       互联网 产品 最 有效 的 增长 机制 往往 都 存在 与 产品 内部 ,   增长 团队 所用 的 工具 是 免费 、 可 规模化 、 可 衡量 的       增长 和 运营       《 运营 之光 》 产品 团队 负责 界定 和 提供 长期 用户 价值 ; 而 运营 团队 负责 创造 短期 用户 价值 和 协助 产品 完善 长期 价值 。 运营 增加 用户 的 参与度       运营 流行 于 中国 , 因为 钱多人 便宜 , 增长 源自 美国 , 因为 人贵 以及 工程师 文化                       增长 团队 的 职能       战略 :   建立 并 维护 公司 增长 模型 , 针对 北极星 指标 找到 现阶段 性价比 最高 的 增长 杠杆 。       执行 :   在 整个 生命周期 内 积极 寻找 增长 机会       流程 :   高效 和 系统化 的 AB 测试 体系 , 跨部门 合作       文化 :   数据 和 实验 驱动 的 文化 ,   不过 分 依赖 直觉               增长 团队 的 构成 ,   不同 产品 不是 一模一样 的 ,   取决于 那 部分 人 对 增长 更 重要       增长 产品 经理 ,   第一 成员 负责 增长 机制 的 开发 和 改进       增长 工程师 ,   某些 公司 增长 就是 由 增长 工程师 主导       增长 数据 科学家 / 分析师 ,   熟悉 试验 设计 、 有 良好 的 统计学 基础 ,   可以 正确 设置 AB 测试 并 分析 结果 。 对 商业模式 和 用户 行为 有 深入 的 理解 ,   能够 主动 提出 好 的 问题 , 并 通过 数据分析 找到 答案       用户 研究 ,   定性 的 用户 研究 , 了解 用户 心理 , 调查 问卷 , 访谈       增长 设计师 ,   提供 不同 版本 的 产品 或 创意设计 以供 AB 测试       了解 并 认同 最小 可行性 测试 ,   初期 出 一个 初版 并 快速 迭代       在 试验 结果 中 学习 ,   在 设计 中 注重 影响力 ,   而 不是 过分 强调 设计 的 艺术性               市场 渠道 专家 ,   搜索引擎 、 付费 广告 、 病毒传播 等 有 积累 的 专家               增长 团队 的 组织 架构       独立 特区 模式       Facebook ,   直接 向 CEO 汇报 ,   是 一个 全栈 团队       适合 在 公司 成立 的 早期 形成 , 后期 形成 比较 困难               功能 主导 模式       增长 团队 隶属于 产品 团队 ( 也 可能 是 其他 团队 , 但 以 产品 团队 居多 ) 的 模式       LinkedIn ,   Uber ,   Airbnb       产品 和 增长 向 同一个 人 汇报 ,   避免 增长 团队 与 产品 团队 掐架               内部 咨询师 模式       独立 团队 ,   内部 咨询师 ,       Mozilla ,                       从零开始 组建 增长 团队       先 做好 产品 ,   通过 「 核心 价值 测试 」 ( 看 留存 率 ) ,   再 做 增长                   第 7 章   打造 高效 运转 的 增长 引擎  ", "tags": "book-note", "url": "/wiki/book-note/growth-hacker-in-action.html"},
      
      
      {"title": "《赢》阅读笔记", "text": "    Table   of   Contents           前言 「 每天 都 有 一个 新 问题 」           第一 部分   基础 篇           第 1 章   使命 和 价值观           第 2 章   坦诚           第 3 章   考评           第 4 章   发言权 和 尊严                   第二 部分   公司 如何 才能 赢           第 5 章   领导力           第 6 章   招聘           第 7 章   人员 管理           第 8 章   分手           第 9 章   变革           第 10 章   危机 管理                   第三 部分   如何 赢得 竞争           第 11 章   战略           第 12 章   预算           第 13 章   有机 的 成长           第 14 章   企业 并购           第 15 章   六西格玛                   第四 部分   个人 职业生涯 如何 才能 赢           第 16 章   合适 的 工作           第 17 章   晋升           第 18 章   糟糕 的 老板           第 19 章   工作 与 生活 的 平衡                   第五 部分   有关 赢得 其他 问题           第 20 章   问题 无处不在                         前言 「 每天 都 有 一个 新 问题 」           作者 收到 很多 问题 ,   所有 问题 都 可以 归结为 一个 问题   「 怎样才能 赢 ? 」       本书 主要 为 身处 业务 第一线 的 人们 创作 的 ,   他 么 是 小 企业主 、 中层 经理 、 车间主任 、 技术 工人 、 正在 找寻 自己 第一份 工作 的 大学 毕业生 、 考虑 职业生涯 的 MBA 以及 新 公司 的 创立者       反复 看到 的 主题 :       由 最好 的 选手 组成 的 队伍 能 赢 , 因此 你 要 发现 和 留住 最好 的 选手       不要 思虑 过头 , 以致 延误 行动       不管 你 处在 什么 业务部门 , 都 要 不断 与 别人 分享 自己 的 学习 经验 。 积极 分享 是 个 需要 继续 保持 的 事情       保持 积极向上 的 态度 , 并 要 感染 他人       永远 不要 把 自己 当做 受害者       保持 快乐 , 做生意 不过 是 游戏 而已 , 而 赢 的 游戏 就是 最 快乐 的 事 。                   第一 部分   基础 篇       第 1 章   使命 和 价值观             使命 是 目标 ;   价值观 指 的 实际 是 手段 和 行动 。         使命 :   「 我们 的 业务 如何 才能 赢 ? 」 赢 才 是 关键       使命 需要 在 可能 的 目标 和 不 可能 的 目标 之间 建立 一种 平衡 。 现实 ( 赚钱 ) 与 理想 的 结合体       Ben   & amp ;   Jerry 冰激凌 公司 的 使命 有 :   「 盈利 增长 」 「 提高 股东 价值 」 也 有 「 纯天然 的 冰激凌 」 , 「 提高 本 地区 全国 和 全世界 人民 的 生活 质量 」               使命 应该 是 明确 的 ! 而 不是 空头 口号       美团 :   EAT   BETTER   LIVE   BETTER       阿里 :   让 天底下 没有 难 做 的 生意 ， 促进 开放 、 透明 、 分享 、 责任 的 新 商业 文明       貌似 都 没有 说 现实 ,   只 说 理想 ,   原因 是 什么 ?               具体 而 明确 地 阐述 价值观       要 具有 可 实施 性       对于 含糊 的 价值观 需要 做 很多 具体 阐述       在 确立 企业 的 价值观 及其 相关 的 行动 纲领 时 , 多一分 具体 和 细致 是 永远 不为过 的       个人 想法 :   其实 这种 将 价值观 明确 为 可 实施 的 一条 一条 很 像 亨达瑞 的 原则               业界 各 公司 的 价值观       美团 :   以 客户 为 中心 ;   正直 诚信 ;   合作 共赢 ;   追求 卓越 。 个人 点评 :   按照 明确 的 原则 来看 , 这 确实 是 满足 的       阿里 :   客户 第一 ;   团队 合作 ;   拥抱 变化 ;   诚信 ;   激情 ;   敬业 。               实践 中 的 贯彻 :   以 价值观 为 标准 , 赏罚分明 才能 让 价值观 得以 贯彻       使命 和 价值观 的 关系       应该 合二为一 , 互相促进       实际 中 互相冲突 的 例子 很多       在 公司 遇到 某种 危机 的 时候 ( 比如 竞争对手 降价 增加 市场份额 ) , 可能 会 做 一些 违背 价值观 的 事       在 经济 低迷 的 时候 , 缩减 预算 削弱 广告 预算 , 忘记 了 提高 和 传播 品牌 知名度 的 使命       安达信 会计公司 :   本来 是 一个 审计 公司 , 后来 经济繁荣 开始 进军 咨询 行业 , 咨询 部门 一直 补贴 审计 部门 ? 两 部门 出现 了 价值观 和 使命 的 冲突 , 倒闭 ; 安然 公司 是 能源 公司 ?       问题 :   价值观 重要 还是 活着 重要 ?   个人 理解 :   急缺 的 更 重要 ,   缺 实际 例子                           第 2 章   坦诚           缺乏 坦诚 是 商业 生活 中 最 卑劣 的 秘密       缺乏 坦诚 不是 指 恶意 的 欺诈 , 而是 指 不能 真诚地 表达 自己 的 想法       扼杀 创新 , 妨碍 优秀 的 人 贡献 出 自己 的 才华               坦诚 的 作用 ,   三种 途径       坦诚 将 更 多 人 吸引 到 对话 中 ,   显而易见 是 的       提高效率 ,   因为 更多人 参与 , 就 能 更 快 发现 问题 ;   问题 : 参与 人太多 了 实际上 效率 会 下降 , 在 坦诚 和 效率 之间 做 权衡 ?       节约 成本 ,   减少 形式主义 , 所以 节约 了 成本               缺乏 坦诚 的 原因       人们 之所以 不 说出 自己 的 想法 ,   起源 与 自私 ,   是因为 这会 给 自己 带来 更 多 的 便利 ( 哲学家 康德 )       问题 :         难道 给 别人 不是 也 带来 更 多 便利 吗 ?   说 人 不好 会 使 别人 陷入 窘境 ?   还是 说 如果 只 给 自己 带来 便利 的 不 坦诚 是 不好 的 ?         什么 是 坦诚 ?   确切 的 是 指 「 想法 」 这个 概念 的 内涵 是 值 什么 ?         坦诚 但是 要 注意 表达 的 方式                               坦诚 是 能 做到 的 , 虽然 很难       可能 需要 10 年 的 时间       必须 激励 它 、 赞赏 它 、 时刻 谈论 他 。 还 需 自己 夸张 地 展现 它                   真相 和 结果           对 下属 的 批评 也 要 坦诚       一 开始 实践 坦诚 确实 比较 困难 , 但是 真相 是 坦诚 是 可以 帮助 团队 变得 更好 的                   问题 :   坦诚 意味着 需要 更多人 参与 讨论 , 表达 自己 的 真实 想法 , 必然 存在 参与 人数 跟 坦诚 之间 的 矛盾 , 如何 把握 这个 度 ?   有没有 什么 案例 ?               第 3 章   考评           力求 公平 和 有效 :   有 鉴别力 的 考评       简单 来说 ,   优胜劣汰 ,   创造 竞争 。 达尔文主义               定义 :       硬件 :   具体 业务 , 比如 生产线 , 阿里 的 电商 业务 和 地图 业务 ,   决定 资源 向 哪些 业务 倾斜       软件 :   员工 。 将 员工 划分 为 以下 三个 类别 , 最好 的 20 % , 中间 的 70 % 以及 最差 的 10 % 。 然后 采取相应 的 「 行动 」 。 行动 很 关键 , 用 实际行动 激励 最好 的 员工 ;   中间 的 70 % 要 保证 其 能动性 和 工作 激情 , 以 培训 教育 、 积极 反馈 和 有 周全 考虑 的 目标 设定 ; 最差 的 10 % ,   不得不 离开 。 离开 寻找 更 合适 更 擅长 的 地方 。               憎恨 以及 不 憎恨 该 制度 的 原因       批评 :   ( 腐败 的 区别 考评 机制 ) 认为 区别 考评 是 在 看会 不会 拍 老板 马屁 ,   确实 存在 这种 风险 ,   取决于 老大 。 区别 考评 的 先决条件 是 能够 根据 事实 来 进行 考评       解决之道 : 有 清晰 的 期望值 、 目标 和 时间表 , 以及 一个 稳定 可靠 的 评价 流程               批评 :   被 离开 的 是 被 排斥 的 ;   但 实际上 这样 对 每个 人 都 更 公平 !   举 了 个 例子 , 上学 的 时候 百分制 考评 , 是 一个 很 有 价值 的 引导 , 因为 目标 明确       批评 :   为 人 太好了 , 没 办法 推行 20 - 70 - 10 制度 ;   难以 狠心 解雇 那些 好人 。       批评 :   区别 考评 制度 只 奖赏 那些 值得 收到 奖赏 的 团队 成员 。 实际上 也 应该 是 这样 , 不满 的 人 是 哪些 表现 不佳 的 人 。       批评 :   认为 只 在 美国 能够 执行 ,   本地 会 水土不服 。 事实证明 并非如此       批评 :   认为 只 对 首尾 两 部分 人有 好处 , 对 中间 70 % 的 人会 失去 动力 。 消极影响 :   中间 70 % 的 人 中 比较 有 能力 的 人会 因此 离开 公司 。 这 需要 以下 机制 进行 保证 :   1 .   能够 识人 , 知道 员工 之间 的 差别 ;   2 .   提供 始终如一 、 积极 的 反馈 ;   3 .   建立 真正 有效 的 员工 培训中心 。       划分 中间 70 % 的 做法 可能 使 某些 人 变得 消极 ,   但 对于 其他 许多 人 来说 ,   却 增加 了 前进 的 动力 。       偏向 那些 积极向上 和 性格外向 的 人 , 轻视 了 那些 害羞 和 内向 的 人 , 并 忽略 他们 的 其他 才能 。 靠 业绩 说话 , 人人平等 。               问题 : 是否 需要 花费 时间 和 精力 对于 后 10 % 的 人 进行 培养 ?   是否 可以 允许 在 保证 成长 的 前提 下 ,   每个 人有 不同 的 成长 速度 ?   ( 我 之前 的 一个 原则 是 说 :   可以 接受 不同 的 人 有 不同 的 成长 速度 , 但 不能 接受 不 成长 )           第 4 章   发言权 和 尊严           关注 企业 中 的 每 一个 人       一个 信仰 :   世界 上 的 每 一个 人 都 想得到 发言权 和 尊严 ,   而且 也 应当 得到 。 可以 将 之前 提到 的 3 个点 都 串 起来 ,   如何 串 ?       发言权 :   人们 希望 有 机会 说出 他们 的 思想 , 拥有 自己 的 观点 、 看法 , 获得 被 倾听 的 感受 ,   无论 他们 的 国际 、 性别 、 年龄 或者 文化背景 如何 。       尊严 :   人们 本能 地 和 自发 的 希望 由于 自己 的 工作 、 努力 和 个性 而 得到 尊重 。               如何 实现 每 一个 人 的 发言权 和 尊严 ?   个人 思考 :   定期 沟通 与 提问 , 并 不断 给予 一种 信息 , 有 问题 的 时候 要 及时 问 ?       25 年来 , 公司 一直 为 我 的 双手 支付 报酬 , 但 实际上 , 公司 完全 可以 用 上 我 的 头脑 — — 而且 什么 钱 也 不用 花                   第二 部分   公司 如何 才能 赢       第 5 章   领导力           在 你 成为 领导者 之前 ,   成功 只同 自己 的 成长 有关 ;   当 你 成为 领导者 之后 ,   成功 都 同 别人 的 成长 有关 。       什么 是 一个 领导者 真正 需要 做 的 ?   我 刚刚 得到 提吧 , 但 我 以前 从未 做过 管理 。 我 怎样才能 成为 一个 好 的 领导 呢 ?       日常 的 平衡       长期 和 短期 之间 的 矛盾 :   既 要 保证 每个 季度 的 业绩 ,   又 要 去 做 5 年 以后 对 公司 的 事业 有利 的 事情 ,   这样 的 领导 该 怎么 当 呢 ?               领导者 应该 做 什么 ?   个人 理解 : 对外 抗住 风险 、 压力 为 员工 争取 合理 的 利益 ,   内 对 指引 前进 的 方向 、 帮助 团队 、 员工 成长 。 本质 :   平衡 矛盾 , 相信 员工 ( 3 ) 但 又 要 质疑 ( 6 ) ; 树立权威 ( 5 ) 又 要 承认错误 ( 7 ) 。 在 担任 领导者 的 时候 , 你 无论如何 都 不 可能 做到 尽善尽美 , 而 只能 争取 做 得 更好 。       坚持不懈 地 提升 自己 的 团队 , 把 同 员工 的 每 一次 会面 都 作为 评估 、 指导 和 帮助 他们 树立 自信心 的 机会 。       很大 精力 花 在 一下 三类 活动 上       必须 做 评估 — — 让 合适 的 人 去 做 合适 的 工作 , 支持 和 提拔 哪些 表现出色 的 人 , 把 那些 不 适合 的 人员 调 开       必须 提供 指导 — — 引导 、 批评 和 帮助 下属 ,   提高 他们 在 各 方面 的 能力       必须 树立 员工 的 自信心 — — 向 员工 表达 你 的 激励 、 关系 和 赏识               每天 ! !       即使 对 那些 已经 有 一定 信息 的 人 , 也 要 把握住 每个 时机 , 持续 不断 的 鼓励 他们 。 要 毫不 吝啬 地 加以 表扬 , 越 具体 越 好 。 「 比如 , 这个 问题 提 得到 ! 」 「 但是 问题 还 可以 有 深度 点 , 建议 下次 可以 提前 了解 一下 」               让 员工 不但 要 怀有 梦想 , 而且 还要 拥抱 梦想 、 实践 梦想 。       描绘 一个 梦想 ( 画 大饼 ) ,   也 需要 将 梦想 变成 真实 ( 做 大饼 )       方法 :   指引 的 方向 必须 具体 、 旗帜鲜明 。 我们 的 梦想 是 什么 ?   通过 持续 学习 和 在 实际 工作 中实 操 演练 , 让 每个 员工 都 能 成为 某个 领域 的 技术 专家 , 具有 核心 竞争力       你 必须 坚持不懈 地 谈论 自己 的 梦想 — — 脱口而出 , 简直 要说 到 被 别人 当成 废话 的 地步 。       通过 实际 奖励 来 强化 梦想 , 梦想 才 回家 进入 企业 的 生活               深入 员工 中间 , 向 他们 传递 积极 的 活力 和 乐观 精神       领导 的 作风 是 有 传染性 的               以 坦诚 精神 、 透明度 和 声望 ,   建立 别人 对 自己 的 信赖感 。       真诚 坦率 、 言出必行       赏罚分明 、 以身作则       在 艰难 的 时候 , 领导者 需要 对 做 错 的 事情 负责 ; 在 收获 的 时期 , 他们 会 慷慨 地 赞扬 部下       担任 领导 不是 王冠 , 而是 赋予 了 一项 职责 — — 吧 其他人 身上 最好 的 潜质 激发 出来 。 为此 , 需要 让 你 的 员工 信赖 你               有 勇气 , 干预 做出 不 受欢迎 的 决定 、 说出 得罪人 的话 。       过分 强硬 的 要求 会 招致 别人 的 抱怨 和 反抗 。 但是 在 充分 听取意见 , 并 把 自己 的 想法 解释 清楚 的 基础 上 , 你 必须 向前走 。       你 的 目标 不是 赢得 竞选 , 而是 做好 自己 的 工作               以 好奇心 , 甚至 怀疑 精神 来 监督 和 推进 业务 , 要 保证 自己 提出 的 问题 能 激发 员工 的 实际行动 。 通过 问题 引导 员工 ?       提出 各种 问题 , 显得 是 部门 中 最 无用 的 人       只会 提问 还 远远不够 , 必须 保证 自己 的 问题 能 引发 争论 , 并 让 大家 采取相应 的 行动 。 否则 只会 是 事后诸葛亮               勇于 承担风险 、 勤奋学习 、 成为 表率       成功 的 公司 都 信奉 冒险 和 学习 , 但 老大 需要 主动 承担风险       可以 公开 谈论 自己 的 错误 和 教训 , 从 中学 到 东西       向 优秀 的 人 包括 下属 学习               学会 庆祝       在 过去 的 一年 里 , 团队 是否 为 自己 所 取得 的 业绩 举行 过 或大或小 的 庆祝 活动                           第 6 章   招聘           赢家 是 这样 炼成 的 :   4E1P 计划       怎么 招聘 领导者 ?       在 招聘 程序 开始 之前 就 进行       正直 :   说 真话 、 守信 , 要 对 做 过 的 事情 负责 , 勇于 承认错误 并 改正       智慧 :   有 强烈 的 求知欲 , 有 宽广 的 知识面       成熟 :   能够 控制 怒火 、 承受 压力 和 挫折 , 或者 反过来 , 在 自己 功成名就 的 时刻 , 能够 喜悦 但 不是 谦逊地 享受 成功 的 乐趣 。               4E ( 和 1P ) 计划 。 个人 思考 :   这些 原则 跟 最近 的 一些 思考 比较 像 , 做 事情 主动 、 积极 成长 的 人 比 基础 好 但 不够 主动 的 人 更 值得 招聘       Energy   活力 :   积极向上 的 活力 ,   渴望 行动 、 喜欢 变革 、 外向 、 乐观       Energize   能力 :   激励 别人 的 能力 ,   对 工作 乐观       Edge   决断力 :   对 麻烦 的 是非 问题 作出 决定 的 勇气 。       Execute   执行力 :   落实工作 任务 的 能力 。 有 执行力 的 人 非常 明白 , 「 赢 」 才 是 结果       Passion   激情 :   对 工作 有 一种 衷心 的 、 强烈 的 、 真正 的 兴奋 感 。 热爱 学习 、 最求 进步 , 当 周围 的 人 跟 他们 一样 时 , 他们 会 感到 极大 的 兴奋 。               招聘 高层人士 ,   额外 的 4 个 特征           真诚 ,   不要 虚伪       对 变化 来临 的 敏感性 。 每个 领导者 都 应该 有 远大 的 目标 以及 预知 未来 的 能力 , 不过 优秀 的 领导者 还 必须 有 一种 预见 意外 变化 的 特殊 才能 。 总能 提前 知道 对手 在 思考 什么       爱 才 — — 领导者 希望 周围 的 人 比 自己 更 优秀 、 更 聪明 。 争论       强大 的 韧性 。 三起三落               有关 招聘 的 常见问题       在 招聘 时 , 您 是 如何 进行 面试 的 ?       永远 不要 完全 依赖 一次 面试 !   某些 人 具有 识别 明星 和 假货 的 特殊 才能 , 那么 招聘 时 就要 更多地 依赖 他们               只 需要 招聘 有 技术 专长 的 人 , 对方 是否 具备 4E 的 特征 重要 吗 ?       此时 只 需 具备 部分 特质 就 足够 了               如果 有人 不 具备 上述 的 一个 或 两个 E , 那该 怎么办 ? 是否 可以 通过培训 来 弥补       至少 应该 具备 前 两个 E ,   因为 他们 是 个人 本性 , 很难 通过培训 来 弥补 ;   最好 不要 雇佣 哪些 缺乏 积极 活力 的 人 , 没有 活力 的 人 将 削弱 整个 组织 的 动力 。 决断力 和 执行力 可以 靠 经验 累积 和 管理 培训 来 提高               不 具备 4E 特征 和 激情 的 人 能够 在 事业 中 获得成功 吗 ?       当然 可以 。 有 的 个人 只是 依靠 自己 的 决定 聪明 , 或者 不顾一切 一意孤行 的 作风 , 就 可以 达到 了不起 的 高度 。 但是 在 组织 中 , 缺少 4E 和 1P 的 人 , 尤其 是 领导者 , 能够 持续 取得成功 的 却 不多见               我 喜欢 雇佣 那些 工作 马上 就 能 上 手 的 人 。 你 认为 这是 一个 决定性 的 因素 吗 ?       需要 权衡 。 个人 建议 选择 第二种 , 早期 的 时候 自己 也 并 不 这样 认为 。 在 招聘员工 时 , 不要 给 他们 提供 职业生涯 的 「 终极 职位 」 , 除非 是 部门 负责人 或 CEO 。 努力 寻找 那些 有 极大 潜力 的 、 能够 与 业务 共同 成长 , 能 在 公司 其他 部门 得到 更高 职位 的 人才 是 合算 的 。               需要 多长时间 才能 知道 你 选择 的 人 是否 胜任 ?       通常 是 一年 以内 , 最多 不 超过 两年 。               在 面试 中 , 您 可以 提 一个 什么 问题 , 以 帮助 自己 决定 到底 雇佣 谁 呢 ?       上 一次 离职 的 原因 。 是 环境 ? 是 老板 ? 还是 团队 ?                           第 7 章   人员 管理               你 已经 得到 了 出色 的 选手 , 接下来 该 怎么办 ?               把 人力 资源管理 提升 到 重要 的 位置 , 提升 到 组织 管理 的 首位           认为 人力 资源管理 负责人 至少 应当 与 CFO 平起平坐 。 ( 思考 : 貌似 这里 说 的 人力 资源管理 指 的 是 直接 对 员工 进行 管理 的 经理 ) 但 实际上 人力 资源管理 并 没有 得到 应有 的 重视 。 原因 :   工作 影响 难以 量化 ;   常备 归入 员工福利 的 类别 ;   被 内部 阴谋 扭曲 了 。       人力 资源管理 , 既 是 牧师 ( 倾听 抱怨 , 而 不 加以 反驳 ) , 又 是 父母 ( 给予 关爱 和 教育 , 提供 必要 的 帮助 )                   采用 一套 严格 的 、 费 官僚化 的 业绩 评价 体系           简单明了 , 摒除 哪些 耗费 时间 、 言之无物 的 官样文章 。       评价 的 标准 应该 是 一致 的 , 与 员工 个人 的 行为 直接 相关 。 可 量化       经理人 对 员工 的 考评 每年 至少 有 一次 , 最好 是 两次 , 采取 正式 的 、 面对面 会谈 的 形式       好 的 评价 体系 应当 包括 职业 发展 的 内容 。 谈谈 未来 的 职业 发展 规划 , 谈谈 能够 替代 他们 的 员工               建立 有效 的 激励机制 — — 通过 奖金 、 认同 和 培训 机会 激励 和 留住 员工       物质 和 精神 上 实现 双丰收       培训 :   优秀 的 人 永远 不会 认为 自己 已经 到达 比赛 的 终点 , 而 总是 渴望 继续前进 !               积极 对待 与 周围 群体 的 关系 。       工会 :   提前 处理 好 关系       明星 :   让 他们 意识 到 公司 没有 人 是 不可 缺少 的 , 能 找到 替代者 , 有 充足 的 替补队员       边缘 分子 :   业绩 一般 但是 为 人 很 好 ( 老 白兔 )   先礼后兵 , 先拉 , 无效 就 解聘       捣乱分子 :   优秀 业绩 但 制造 麻烦 。 明确要求 改正 , 否则 远离 正常 业务               与 惰性 抗争 , 不要 忽略 70 % 的 群体 , 他们 是 组织 的 心脏 和 灵魂 。 给予 锻炼 的 机会 , 未来 的 明显 也 可能 在 这 70 % 的 人 中 靠 前 的 部分       尽可能 设计 扁平化 组织 结构 , 清晰 地 展示 出 各种 关系 和 责任 。 明确责任 ! ! !   扁平化 ! ! !                   第 8 章   分手           解雇 别人 不是 件 容易 的 事       三种 原因 :         违背 价值观 ;   不要 犹豫       经济 低迷 裁员 ;   竟 可能 收集 公司 业务 信息 ,   及时 和 清晰 地 传达 给 员工       个人 业绩 不佳 ;   直觉 ( WTF )               解雇 的 时候 容易 出现 的 三个 严重错误       行动 太 匆忙 ,   要 提前 警告 那些 有 问题 的 员工       不够 坦诚 ,   在 之前 的 评估 中 就要 明确 且 坦诚 的 告诉 员工 , 而 不要 装 老好人       拖 得 太久 ,   不要 制造 大 的 意外 ; 将 耻辱感 减 到 最小 , 指导 他 在 其他 地方 找到 更好 的 机会 ;                   第 9 章   变革           即使 是 大山 也 要 去 撼动 , 拥抱 变化       如果 不能 影响 , 要么 接受 现实 , 要么 离开 吧       如果 能 影响 一些 变革       每次 发动 变革 的 时候 , 明确 一个 清晰 的 目的 或 指标       不能 为 变革 而 变革 , 不是 为了 跟踪 什么 管理 潮流       明确 变革 的 目的 ,   为什么 要 变革 ? 变革 的 目的 是 为了 解决 什么 问题 ? 这种 变革 手段 能 解决 这个 问题 吗 ?               招募 支持 的 人 已经 能 适应 变革 的 人       找到 这样 一些 人 : 经常 问 为什么 我们 不 … … ?       频繁 地想 别人 发文 , 推动 大家 前进 , 永不 停息       家得宝 公司 案例 , 初创 员工 能力 跟不上 公司 的 发展 。 引入 外部 空降 者 和 内部 接受 变革 的 人               清理 反对者 , 即使 他们 有 不错 的 业绩 。 要 痛下决心       利用 意外 机会       抓住 对手 失败 的 时刻 , 抓住 科技 机遇       亚洲 经融 危机 的 时候 , GE 低价 收购 泰国 汽车贷款 公司       抓住 经济危机 收购 廉价 资产 、 抓住 公司 破产 收购 有 价值 的 业务       怎么 感觉 跟 我 炒股 的 手法 很 像 啊 , 不 就是 抄底 吗 ?                           第 10 章   危机 管理               千万 不可 坐以待毙               寻求 免疫           危机 会 避免 错误 重新 犯 两次       预防 方法 :   严格控制 ;   建立 良好 的 内部 管理 流程 ; 公开 离职 原因               对 危机 的 剖析       在 讨论 没 中 假设 之前 , 先 简单 看 下 危机 是 怎么 萌芽 、 发展 以及 结束 的               行动计划       处理 危机 需要 考虑一下 五种 假设       假设 问题 本身 要 比 表现 出来 的 更 糟糕       先 将 责任 揽 到 自己 头上 ,   立即行动 起来 。 如果 是 完全 清白 的 , 就 通过 斗争 来 为 自己 澄清               假设 世界 上 不 存在 秘密 , 每个 人 最终 总会 知道 事情 的 真相       很多 人想 封闭 消息 , 减少 危机 影响 的 速度 ; 但 很多 时候 会 导致 以讹传讹 , 反而 会 使得 结果 更加 严重 。 正确 评估 二者 的 利害关系       即使 公开 透明 的 公布 危机 的 应对 措施 , 对 解决 手段 谈论 得 越 多 , 就 越 能 获得 组织 内外 关注 你 的 人 的 信任 。               假设 处理 危机 的 动作 会 被 敌对势力 恶意 丑化       不要 介意 恶意 丑化 , 及时 公开 自己 的 看法       要 确保 自己 在 行动 , 不要 祈祷 外部 有人 来 妥善处理       要 靠 自己 的 行动 争取 , 而 不是 靠 外部 施舍               假设 危机 过程 中 , 人和事 都 会 发生变化       新 秩序 下 , 人和事 都 可能 重新安排               假设 你 的 组织 将 从 危机 中挺 过来 , 而且 会 因为 经历 了 考研 而 变得 更 强大 。       要 有 长远 的 目光       个人 想法 :   可以 将 每次 危机 的 教训 总结 , 写成 原则 , 警告 后世                                   第三 部分   如何 赢得 竞争       第 11 章   战略           奥秘 都 在 「 调料 」 里       如果 你 想 赢 , 那么 在 涉及 战略 的 时候 , 就要 少 点 沉思 , 而敏于 行动       问题 :   什么 是 战略 ?   就是 一个 行动 纲领 , 要 根据 行动 结果 不断 审视 和 修订       制定 战略 的 三个 步骤 :       定制 一个 大 的 方向 规划 — — 找到 聪明 、 使用 、 快速 、 能够 获得 持续 竞争 收拾 的 办法       将 合适 的 人 放到 合适 的 位置 , 来 落实 这个 大 的 规划 。 将 战略 跟 人 匹配 起来       不断 探索 能 实时 你 的 规划 的 最佳 实践经验 。 持续 学习 , 找到 最佳 实践 , 持续 改进 你 的 战略               战略 是 什么 ?         战略 其实 就是 对 如何 开展 竞争 的 问题 作出 清洗 的 选择 。 不管 你 的 生意 有 多 大 , 资金 有 多 雄厚 , 你 也 不 可能 满足 所有人 的 所有 需求 。       小店 生存 最 重要 的 是 一个 战略 位置       战略 不是 指 目标 ( 比如 成为 XX 领域 主导者 ) , 而是 更加 明确 的 行动 。 比如 放弃 XX 领域 转向 XX 行业 , 提升 人才 培训 和 发展       大方向 正确 , 又 有 一定 的 宽度 , 那么 战略 并不需要 经常 改变       把 精力 房子 啊 创新 、 技术 、 内部 流程 、 附加 服务 等 能 与 你 与众不同 的 艺术 上               让 战略 切实可行       今天 的 竞技场 是 什么样 的 ?         竞争对手 是 谁 ?       对手 的 各 市场份额 ?       行业 特征 ?   大众化 ? 高附加值 ? 长 周期 ? 决定 利润 的 主要 因素 ?       每个 竞争对手 的 优势 和 劣势 ?       行业 的 主要 客户 是 谁 ?               最近 竞争 形势 如何 ?       近 一年 对手 都 有 哪些 可能 改变 市场 格局 的 举动 ?       是否 有人 引入 了 可能 改变 游戏 局面 的 新 产品 、 新 技术 或 新 的 销售 渠道 ?       是否 有 新 的 进入 者 , 过去 一年 的 业绩 如何 ?               你 的 近况 如何 ?       你 的 表现 在 过去 一年 对 竞争 格局 的 影响 ?       是否 首购 了 企业 , 引进 了 新 产品 , 挖走 了 对手 的 主要 销售 人员 , 或者 又 新 技术 的 特许权 ?       是否 失去 了 过去 的 某些 竞争 优势 — — 销售 人员 , 产品 , 或 技术               有 哪些 潜在 变量 ?       下 一年 最 担心 什么 ?       竞对 可能 有 什么 新 产品 和 技术 , 甚至 改变 游戏规则       会 不会 有 针对 你 的 兼并 收购 ?       在 预测 未来 的 时候 , 再 极端 偏执 的 想法 都 不过 分               你 有 什么 胜招 ?       能 做 什么 来 改变 竞争 格局 ?       怎么 让 顾客 保持 粘性                       找 对 人 :   让 人 和 工作 匹配 起来       最佳 实践经验 及其 他 :   模仿 并 改进 。 借鉴 别人 的 成功经验 , 然后 加以改进 。 GE 优化 存货 周转率 的 案例           第 12 章   预算           不要 让 预算 指定 程序 缺乏 效率       错误 的 预算 办法       谈判 式 解决 :   总部 和 业务 之间 的 矛盾 , 民主 撕 逼 。       虚伪 的 笑容 :   友好 的 会谈 , 但是 就是 达 不成 什么 实质性 的 结果 , 完全 是 由 总部 来定 , 集权 坑人 。               更好 的 办法       问题 :       如何 超越 去年 的 业绩 ?       竞争对手 在 做 什么 ,   如何 战胜 他们 ?               奖金 根据 与 过去 业绩 的 对比 ( 纵向 对比 ) 和 与 竞对 的 对比 ( 横向 对比 ) 来定 的               运转 起来       3M 中国区 的 例子                   第 13 章   有机 的 成长           开 创新 事物 是 企业 成长 最 有效 的 途径       启动 新 业务 时 三个 常见 的 错误       没有 给 新 业务 足够 的 投资 , 特别 是 对 业务 第一线 的 人员       对 新 项目 的 前景 和 重要性 宣传 的 太少       限制 了 新 项目 的 自主权               原则       做 大笔 投入 , 将 最好 的 人 去 做 新 业务 领导       夸大 宣传 新 项目 的 潜力 和 重要性 。 新 项目 的 报告 层级 至少 应该 高 两级 , 如果 可能 应该 直接 由 CEO 负责       给予 自由度 , 允许 犯错误 ; 让 新 项目 自己 成熟 起来               通常 情况 是 没有 足够 的 资金 和 最好 的 人手 , 需要 竭尽所能 去 战斗       完美 风暴 ,   3 个 原则 都 满足 了 :   福克斯 打败 MSNBC , 超过 CNN 的 例子           第 14 章   企业 并购           警惕 交易 狂热 等 致命 陷阱       并购 的 7 个 陷阱       相信 真的 可能 发生 「 平等 合并 」       过分 关注 经营 战略 上 的 匹配 , 而 忽略 了 企业 文化 的 融合       反 被 别人 当 「 人质 」 , 让步 太 多 , 让 被 收购方 操纵 了 全局       整合 行动 显得 太 保守 了 , 并购 行动 应该 在 90 天内 完成 。 新 荷兰 公司 对 凯斯 公司 收购 的 例子       征服者 综合征 。 收购 后 在 各个 位置 上 安排 自己 的 经理       代价 太高 , 高到 你 转 不 回来 。 美团 收购 摩拜 ?       被 收购方 从上到下 都 体会 到 痛苦 并 予以 抵制 。 接受 现实 , 往前 看 。 组织 架构 调整                   第 15 章   六西格玛           六西格玛 是 一个 品质 改善 计划 , 它 的 宣传 和 执行 可以 改善 顾客 的 产品 体验 , 降低 你 的 成本 , 培养 更好 的 企业 领导       什么 是 六西格玛 ?   以 六西格玛 ( 百 万分之 3.4 ,   5 个 9 ) 对 产品质量 的 某个 指标 进行 要求       要 让 顾客 保持 粘性 , 就要 满足 和 超越 他们 的 期望 。 改进 ( 减少 ) 方差 ( 不确定性 )           第四 部分   个人 职业生涯 如何 才能 赢       第 16 章   合适 的 工作           找 一份 好 工作 , 此后 的 人生 不再 是 劳作       钱 :   世界 上 最 糟糕 的 事情 莫过于 哪些 讨厌 的 家伙 一边 挣着 大钱 , 一边 大放厥词 , 说 前 不 重要 ( 马云 ? )       认为 重要 或 不 重要 都 没错 , 对 自己 的 感受 不诚实 , 今后 或许 会 追悔莫及               人 :   如果 不 欣赏 哪些 与 自己 朝夕相处 的 同事 , 那么 工作 仍然 可能 是 一种 折磨 。 在 职业生涯 中 , 你 也 需要 找到 与 自己 志趣相投 的 人 , 而且 越早 越 好       机遇 :   工作 能 让 你 获得 多少 成长 和 进步       未来 :   你 所 从事 的 没 中 职业 都 是 一场 赌博 , 他 可能 开拓 你 未来 的 发展 空间 , 也 有 可能 缩小 你 的 选择 范围 。 行业 的 问题 ? ?       主导权 :   要 对 自己 诚实 , 明白 自己 为 那些 人 工作       工作 内容 :   至少 热爱 其中 的 某些 部分 , 工作 能 让 你 感到 兴奋       特殊 情况 :   真诚           第 17 章   晋升           没有 捷径       首先 是 运气       令 别人 叹服 的 力量       更 有效 的 办法 是 拓展 你 的 工作 范围 , 采取 大胆 和 超出 期望 的 行动 。 不要 只 做 哪些 期望 之内 的 事               最大 的 敌人 是 自己 , 不要 在 自己 的 组织 里面 当 刺儿头       其他 需要 动用 政治 资本 的 情况       四个 要 和 一个 不要 :       要 管理 好 下述       要 争取 收到 关注 , 取得 出色 的 业绩 , 新 项目 的 时候 , 率先 把手 举 起来       要 寻找 导师       要 保持 积极向上 , 并 感染 他人 。 技能 固然 重要 , 但 理想 更为 关键       不要 惧怕 挫折               要 获得 晋升 , 首先 要 有 渴望 晋升 的 欲望 ! !           第 18 章   糟糕 的 老板           不能 让 自己 表现 为 一名 受害者       未必 非 要 留下 不可 , 优势 确实 需要 另谋高就       问 自己 一系列 问题       为 甚 我 的 老板 为 人 如此 古怪 ?   可能 是 自己 的 问题       坏 老板 会 有 什么样 的 结局 ? 走 或 留 , 业绩 初中 的 坏 老板       我 为什么 还要 在 这里 工作 呢 ?                   第 19 章   工作 与 生活 的 平衡           找 我 说 的 那样 做 , 但 不要 学 我       好好 工作 , 好好 享受 。 作者 显然 花 在 工作 的 时间 更 多       了解 这些 现实 :       你 老板 最 关心 的 是 竞争力       出色 的 业绩 前提 下 , 还是 愿意 让 你 协调 工作 与 生活 的 矛盾 的 , 关键词 是 「 如果 」       平衡 只是 为了 招聘 需要 , 每个 人 都 不 太 一样       公司 讨厌 认为 公开 为 平衡 斗争 的 人 , 认为 他们 是 不愿 承担义务 、 无能       平衡 是 需要 自己 解决 的 问题 。 现在 也 有 经理人 帮助 下属 设定 优先 次序 , 进行 权衡 取舍               管理 的 优先 秩序 。 平衡 是 一种 交易 , 在 所得 和 所失 之间 进行 交易       老板 想要 的 是 赢 !       生活 的 平衡 是 需要 之前 的 业绩 来 赚 积分 的       经验 :       无论 参与 什么游戏 , 都 要 尽可能 的 投入 。 工作 时 就 全心投入 , 回到 家 就 全心 生活       有 勇气 对 之外 的 需求 说 不 。 通常 只 需 拒绝 一些 小事 , 学会 拒绝       不要 将 自己 排除 在外 。 为 他人 牺牲 自己 , 知道 自己 的 天平 更 倾向 工作 , 还是 生活                   第五 部分   有关 赢得 其他 问题       第 20 章   问题 无处不在           中国 竞争 的 问题       历史 上 也 都 发生 过 , 大 公司 需要 变革               未完待续      ", "tags": "book-note", "url": "/wiki/book-note/win.html"},
      
      
      {"title": "《金字塔原理》", "text": "    Table   of   Contents           中文版 序           前言           第 1 篇   表达 的 逻辑           第 1 章   为什么 选择 金字塔 结构           第 2 章   金字塔 内部 的 结构           第 3 章   如何 构建 金字塔           第 4 章   序言 的 具体 写法           第 5 章   演绎推理 与 归纳推理                   第 2 篇   思考 的 逻辑           第 6 章   应用逻辑 顺序                   第 3 篇   解决问题 的 逻辑           第 4 篇   演示 的 逻辑                 中文版 序           当 我们 与 人 沟通 时 , 需要 想 清楚 3 件 事 :   谁 是 我们 的 听众 ? 他们 想 听 什么 ? 他们 想 怎么 听 ?       金子 塔 原理 基本 结构 是 :   结论 先行 ,   以上 统下 ,   归类 分组 ,   逻辑 递进 。 先 重要 后 次要 ,   现 总结 后 具体 ,   先 框架 后 细节 , 先 结论 后 原因 , 先 结果 后 过程 , 先 论点 后 论据       表达 着 :   关注 、 挖掘 受众 的 意图 、 需求 点 、 利益 点 、 关注点 和 兴趣 点 , 想 清楚 说 什么 , 怎么 说 , 掌握 沟通 的 标准 结构 、 规范 动作       能 达到 的 沟通 效果 :   观点鲜明 、 重点 突出 、 思路清晰 、 层次分明 、 简单 易懂 , 让 受众 有 兴趣 , 能 理解 , 记得住       具体做法 :   自上而下 表达 , 自下而上 思考 , 纵向 总结 概括 , 横向 归类 分组 , 序言 讲故事 , 标题 提炼 思想 精华       金字塔 原理 能够 帮助 解决 的 问题       思考 :   提高 结构化 思考 能力       书面 表达 、 公文 写作 :   会 挖掘 读者 的 关注点 、 兴趣 点 、 需求 点 、 利益 点 。 MECE 原则       口头 表达 :   回答 听众 最 常有 的 4 类 疑问 :   是 什么 ?   为什么 ?   如何 做 ?   好不好 ?       管理 下属 :   考虑 全面 、 周到 、 严禁 , 分配任务 、 设计 流程 不 重叠 无 遗漏       培训师 :   重点 突出 、 逻辑 清晰 、 通俗易懂                   前言           条理清晰 的 文章 都 具有 清晰 的 金字塔 结构       要 想 写出 条理清晰 的 文章 , 关键 是 在 开始 写作 之前 , 先 将 作者 的 思想 组织 成 金字塔 结构 , 并 按照 逻辑关系 的 有关 规则 进行 检查和 修改           第 1 篇   表达 的 逻辑           对 受众 来说 , 最 容易 理解 的 顺序 是 :   先 了解 主要 的 、 抽象 的 思想 ,   然后 了解 次要 的 、 为 主要 思想 提供 支持 的 思想           第 1 章   为什么 选择 金字塔 结构               人类 思维 的 基本规律 :           大脑 自动 将 信息 归纳到 金字塔 结构 的 各组 中 , 一边 理解 和 记忆       预先 归纳到 金字塔 结构 中 的 沟通 内容 , 都 更 容易 被 人 理解 和 记忆       应该 有意识 地 将 内容 组织 成 金字塔 结构                   归类 分组 , 将 思想 组织 成 金字塔           大脑 会 自动 将 发现 的 所有 事物 以 某种 秩序 组织 起来 :   看到 星座 图案 , 而 不是 散乱 的 星星       将 具有 「 共性 」 的 任何事物 组织 在 一起       大脑 的 两个 需求       一次 记忆 不 超过 7 个 思想 、 概念 或 项目       找出 逻辑关系               奇妙 的 数字 \" 7 \"       《 奇妙 的 数字 7 ± 2 》 大脑 的 短期 记忆 无法 一次 容纳 约 7 个 以上 的 记忆 项目 , 有 的 人 是 7 个 , 有 的 人 则 只有 5 个 。 当然 最 容易 记住 的 是 1 个 项目 。       当 大脑 发现 有 4 - 5 个 项目 时 , 会 开始 归类 到 不同 的 逻辑 范畴 , 便于 记忆               归类 分组 搭建 金字塔 , 就是 分组       找出 逻辑关系 , 抽象 概括 。 分组 是 不够 的 , 要 构建 每组 的 逻辑关系 。 上面 的 概念 能够 提示 下 一个 层次 的 思想       最 有效 的 表达方法 是 :   先 提出 总 的 概念 ,   在 列出 具体 项目 ,   即 要 自上而下 地 表达思想               自上而下 表达 , 结论 先行       预先 告诉 读者 句子 、 概念 间 的 逻辑联系 , 否则 受众 会 自动 归纳 , 但 由于 背景 知识 缺失 , 很难 get 到 点       通过 有效 的 方法 表达思想 , 减少 读者 用于 这两项 活动 上 的 时间       识别 和 解读 读到 的 词语       找出 各种 思想 之间 的 关系 ,   理解 所 表达思想 的 含义                       自下而上 思考 , 总结 概括       金字塔 中 的 思想 以 3 中 方式 互相 关联       向上 ,   向上 抽象 总结       向下 ,   向下 具体       横向 ,   横向 关联 相似               文章 的 思想 必须 符合 以下 规则       纵向 :   文章 汇总 任一 层次 上 的 思想 必须 是 其 下 一 层次 思想 的 概括       横向 :   每组 中 的 思想 必须 属于   同一 逻辑 范畴   ;         具有 共同点       检查 分组 是否 正确 的 方法 :   能否 用单 一个 名词 表示 该组 的 所有 思想       建议       原因       问题       需要 作出 的 改变                       横向 :   每组 中 的 思想 必须 按照   逻辑 顺序   组织       必须 有 明确 的 理由 说明 为什么 第二个 思想 要 放在 第二位       4 中 逻辑 顺序       演绎 顺序 :   大前提 、 小前提 、 结论 。   三段论 。   演绎推理       时间 ( 步骤 ) 顺序 :   第一 、 第二 、 第三 。 发现 因果关系       结构 ( 空间 ) 顺序 :   波士顿 、 纽约 、 华盛顿 。 化整为零       程度 ( 重要性 ) 顺序 :   最 重要 、 次 重要 等等 。 归纳 总结                                       个人 思考       金字塔 原理 是 符合 人 的 思维 方式 的 原理       要 从 受众 的 角度 来 表达 问题       理清 每个 点 的 逻辑关系                   第 2 章   金字塔 内部 的 结构           金字塔 原理 虽好 , 但是 不要 幻想 一 开始 就 可以 将 思想 组织 呈 金字塔 。 首先 要 梳理 你 想 表达 的 思想 是 什么       金字塔 中 的 子结构       主题 与子 主体 之间 的 纵向 关系       纵向 关系 能够 很 好 地 吸引 读者 的 注意力       引导 一种 疑问 / 回答 式 的 对话 , 勾起 读者 的 兴趣       什么 是 思想 ?   向 受众 发出 新 信息 并 引发 受众 疑问 的 语句 。 就是 要 有 信息量       在 表达 的 下 一个 层次 上 横向 地 回答 读者 的 疑问       不断 引起 读者 疑问 并 回答 疑问 , 知道 读者 不会 有 新 的 任何 疑问 为止       关键 句 ( keyline ) ,   思想 的 凝结 句子       吸引 读者 全部 注意力       在 做好 回答 之前 , 避免 引起 读者 的 疑问 ?       在 引起 疑问 之前 , 避免 先 给出 对 该 问题 的 答案 。   这 不 就是 设置 悬念 吗               只 在 读者 需要 的 时候 才 提供 相应 的 信息               各子 主题 的 横向 关系       表述 必须 具有 明确 的 归纳 或 演绎 关系 , 但 不可 同时 既 具有 管 关系 , 又 具有 演绎 关系       需要 回答 上层 的 问题               序言 的 叙述 方式       除了 要 有 逻辑 , 还 得 跟 用户 的 头脑 中 的 某 一 问题 相关 , 即 写 得 东西 是 用户 需要 的       序言 用于 追溯 问题 的 起源 和 发展       背景 ,   冲突 ,   疑问 ,   回答       序言 以 讲故事 的 形式 告诉 读者 ,   你 已经 了解 或 将要 了解 你 正在 讨论 主题 的 一些 信息 , 从而 使 读者 想起 他 本人 的 疑问                           第 3 章   如何 构建 金字塔           自上而下 法       画出 主题 方框 。 顶端 是 讨论 的 主题 , 主语 / 谓语       设想 主要 疑问 。 确定 文章 的 读者 , 进而 确定 读者 的 主要 疑问 , 请 写 出来       写出 对 该 疑问 的 回答       说明 「 背景 」 。 将 讨论 的 主题 与 「 背景 」 相结合 , 做出 关于 该 主题 的 第一个 不会 引起争议 的 表述       指出 「 冲突 」 。 背景 中 的 哪些 点能 使 读者 产生 疑问 的 「 冲突 」       检查 「 主要 疑问 」 和 「 答案 」               自上而下 法 的 步骤       提出 主要 思想       设想 受众 的 主要 疑问       写序言 :   背景 — — 冲突 — — 疑问 — — 回答       与 受众 进行 疑问 / 回答 式 对话       对 受众 的 新 疑问 , 重复 进行 疑问 / 回答 式 对话               自下而上 法 :         无法 构建 金子 塔 结构 的 顶部 的 时候 ,   还 没想 清楚       从 关键 句 层次 上 着手       步骤       列出 你 想 表达 的 所有 思想 要点       找出 各 要点 之间 的 逻辑关系       得出结论               小标题 :   标题 应 呈现 某种 思想 、 观点 或 论点 , 而 不是 只 说明 要 讨论 问题 的 类别 。 不要 用 \" 我们 的 发现 \" 或 \" 结论 \" 这类 的 标题               初学者 注意事项       一定 先 搭 结构 , 先 尝试 自上而下 法       序言 先写 背景 , 将 背景 作为 序言 的 起点       先多花 时间 思考 序言 , 不要 省略       将 历史背景 放在 序言 中       序言 仅 涉及 读者 不会 对 其 真实性 提出 质疑 的 内容 。 不要 在 序言 中 涉及 任何 读者 不 知道 的 信息 , 也 不要 在 金字塔 中 涉及 任何 读者 已经 知道 的 信息       在 关键 句 层次 上 , 更宜 选择 归纳法 而 非 演绎法                   第 4 章   序言 的 具体 写法           文章 :   序言 ;   演讲 :   开场白       序言 必须 用 讲故事 的 形式 ,   也就是说 必须 介绍 读者 熟悉 的 某些 「 背景 」 , 说明 发生 的 「 冲突 」 , 并 由此 引发 读者 的 「 疑问 」 , 然后 针对 该 疑问 给出 答案 。       序言 的 讲故事 结构 :   目的 是 引导 读者 进入 你 的 思想       为什么 ?   为了 让 读者 抛开 其他 存在 于 脑海中 的 思想 ,   专注 文章 的 内容 。       简单 办法 ,   利用 为 讲完 的 故事 所 产生 的 悬念 效果 。       每 一个 好 的 故事 都 有 开头 、 中间 和 结尾 , 相当于 引入 某种 「 背景 」 , 说明 发生 的 「 冲突 」 , 并 提出 解决方案       世界 上 最好 的 故事 实际上 就是 孩子 们 已经 听过 的 故事 。 也 就是 要 从 熟悉 的 引入 到 不 熟悉 的 点               何时 引入 「 背景 」       如果 你 发现自己 在 文章 开头 没有 说明 与 文章主题 有关 的 背景 , 就 说明 你 或者 选错 了 主题 , 或者 开始 讨论 主题 的 时机 不 对               什么 是 「 冲突 」       这里 的 「 冲突 」 类似 于 讲故事 推动 情节 发展 的 因素 , 能够 促使 读者 提出 「 疑问 」               为什么 要 用 这种 顺序       基本 结构 :   背景 ( S ) - 冲突 ( C ) - 疑问 ( Q ) - 回答 ( A )       标准 式 :   背景 - 冲突 - 答案       开门见山 式 :   答案 - 背景 - 冲突       突出 忧虑 式 :   冲突 - 背景 - 答案       突出 信心 式 :   疑问 - 背景 - 冲突 - 答案               什么 是 「 关键 句 要点 」       不仅 要 回答 由 文章主题 思想 引起 的 受众 的 新 问题 , 还要 呈现 文章 的 框架结构               序言 应当 写 多长       序言 的 长度 应当 确保 在 你 引导 读者 按照 你 的 思路 思考 之前 , 读者 和 你 「 站 在 同一 位置 上 」       序言 实际上 并 不是 一个 单独 的 章节 , 而是 作为 一个 引子 , 将 读者 引入 你 想 说 的 话题 上来               关键 句 要点 是否 需要 用 引言       关键 句 的 每 一个 要点 , 都 应该 按照 与 全文 的 序言 相似 的 背景 - 冲突 - 疑问 结构 逐个 引出 。       在 第一个 关键 句 要点 的 引言 , 你 的 目的 是 提示 读者 这一 主题 为什么 与 全文 的 主题 相关       在 其他 关键 句 要点 的 引言 , 你 的 目的 是 向 读者 说明 将要 讨论 的 主题 与 前面 已 讨论 的 主题 相关               想 写出 好 的 序言 , 必须 遵循 以下 原则 :       序言 的 目的 是 「 提示 」 读者 而 不是 「 告诉 」 读者 某些 信息 。 应该 是 读者 已知 的 , 不要 加 图表       序言 必须 包含 讲故事 的 3 个 要素 , 即 「 背景 」 「 冲突 」 「 答案 」       序言 的 长度 取决于 读者 和 主题 的 需要 。 全文 的 中心 都 依赖 与 读者 提出 的 第一个 疑问 , 或称 初始 疑问 。 这种 疑问 全篇 只能 有 一个 。 如果 存在 两个 , 那么 这 两个 应该 是 互相 关联 的               序言 的 常见 模式       你 写文章 的 目的 通常 是 回答 一下 4 类 问题 之一       我们 应该 做 什么 ?       我们 应该 如何 做 ?       我们 是否 应该 这样 做 ?       为什么 会 发生 这种 情况 ?               商务 文章 中 常见 的 4 种 模式 :       发出 指示 式       请求 支持 式       解释 做法 式       比较 选择 式                       序言 的 常见 模式 — — 以 咨询 为例       项目 建议书       背景 ( S )   =   你 遇到 一个 问题       冲突 ( C )   =   你 已 决定 请 第三方 帮助 解决 此 问题       疑问 ( Q )   =   你 是 我们 应该 聘请 来 解决 该 问题 的 第三方 吗               项目 进度 小结       背景 ( S )   =   我们 一直 在 处理 X 问题       冲突 ( C )   =   我们 告诉 过 你 , 分析 的 第一步 是 确认 Y 是否 成立 。 我们 已经 完成 了 这 一步       疑问 ( Q )   =   你们 发现 了 什么 ?                           第 5 章   演绎推理 与 归纳推理           同 一组 中 的 思想 之间 存在 逻辑 顺序 ,   具体 的 顺序 取决于 该组 思想 之间 的 逻辑关系 是 演绎推理 关系 还是 归纳推理 关系       演绎 是 一种 线性 的 推理 方式 , 最终 是 为了 得出 一个 有 逻辑 词 「 因此 」 引出 的 结论 。 即 三段论       归纳推理 是 将 一组 具有 共同 事实 、 思想 或 观点 归类 分组 , 并 概括 其 共同性       先 说明 行动 , 后 说明 原因 , 因为 要 采取 哪些 行动 才 是 读者 最 关心 的 。 但是 在 行动 跟 用户 直觉 相反 的 时候 , 原因 更 重要           第 2 篇   思考 的 逻辑           两种 常见 错误       仅仅 因为 可以 用 同一个 名词 概括 , 而 将 关联性 很小 的 思想 排列 在 一起       金字塔 结构 顶端 的 中心思想 , 使用 的 是 「 缺乏 思想 」 的 句子                   第 6 章   应用逻辑 顺序           放到 一组 中 的 思想 必须 具有 某种 逻辑 顺序       大脑 的 归纳 分组 分析 活动 只有 一下 3 种       确定 前因后果 关系 , 时间 ( 步骤 ) 顺序       将 整体 分割 为 部分 , 或 将 部分 组成 整体 。 结构 ( 空间 ) 顺序       将 类似 事务 按照 重要性 顺序 归为 一组 , 程度 ( 重要性 ) 顺序               根据 结果 寻找 原因 ,   常见问题 之一 就是 无法 区分 原因 和 结果       揭示 隐含 的 逻辑 思路 ,   需要 明确 说明 真实 的 逻辑 思想           第 3 篇   解决问题 的 逻辑       第 4 篇   演示 的 逻辑  ", "tags": "book-note", "url": "/wiki/book-note/pyramid-principle.html"},
      
      
      {"title": "关于读书笔记", "text": "    Table   of   Contents               总 觉得 要 记录 一些 东西 , 或者 写下 一些 思考 , 才能 说 看 完 了 一 本书 , 于是 有 了 这个 读书笔记 专栏 , 用于 记录 主 路径 之外 的 其他 书籍 的 读书笔记 。  ", "tags": "book-note", "url": "/wiki/book-note/about.html"},
      
      
      {"title": "原则", "text": "    Table   of   Contents           关于           基本 观点                 关于       瑞 · 达利 欧 的 《 原则 》 很多 人 推荐 , 看 了 一下       基本 观点           书面 记录 自己 的 原则       书面 记录 错误 日志 , 并 讨论 避免 这种 错误 的 解决 制度       这 不 就是 我们 的   case   study   吗 ? 哈哈               预测 是否 对 了 没有 意义 , 有 意义 的 是 在 这种 预测 下 你 是 怎么 操作 的 , 过于 相信 自己 的 预测 就 跟 赌博 一样 , 只要 一次 赌错 了 , 挣得 次数 再 多 也 会 在 这 一次 全部 赔 进去       意思 是 要 分析 风险 , 压力 测试 ? 具体 怎么 操作 呢 ?               研究 历史 的 价值 , 世界 上 没有 新鲜事 , 绝大多数 的 事 不过 是 历史 重演       所以 要 多读 历史 , 研究 经济史 ? ?               经常 和 自己 观点 不 一样 的 聪明人 交流 和 讨论 , 可以 帮助 不要 限于 过度 偏激 的 错误 之中       策略 回测 的 重要性       不 进化 就 死亡       不进则退 ?               把 你 的 进化 最大化 。 这种 持续 的 追求 学习 和 改进 的 动力 , 让 人类 天生 对 进步 感到 快乐 , 对 快速 进步 感到 兴奋 。       重要 的 是 进化 本身 , 而 不是 回报 本身       边际 收益 递减 的 自然规律 , 导致 以 赚钱 的 目标 的 追求 , 无法 持续               没有 痛苦 就 没有 收获       个人 评注 :   每次 学习 新 的 东西 的 时候 , 都 是 一件 痛苦 的 事情 , 当学通 了 之后 , 又 是 一件 极大 满足 和 兴奋 的 事情 , 所以 准确 的 说 是 先 有 痛苦 而后 得到 收获 和 快乐               痛苦   +   反思   =   进步       努力 突破 极限 , 从 错误 中 总结 和 反思       要 不断 脱离 舒适 区 ,   突破 自己 的 极限       记得 之前 说 , 每年 都 至少 要 做 一件 之前 没有 做过 的 事情 , 哈哈 , 跟 这个 思想 不谋而合               保持 头脑 开放 ,   吸收 别人 的 思想 , 和 反对 意见       接受 外部 信息 , 降低 自己 的 熵 , 哈哈       接受 自己 的 思维 盲区               冥想       关闭 眼睛 接受 外界 的 东西 , 进入 冥想 , 小时候 倒 是 经常 做 这个               估算 的 能力       其实 就是 把握 主要 因素 , 忽略 次要 因素 , 得到 一种 快速 的 直觉 和 结论       “ 当 你 问 一个 东西 对 不 对 而 对方 告诉 你 并 不 完全 对时 ， 那 它 大致 是 对 的 。               层次 综合 能力       主线 和 细节               综合 考虑 各种 事情 的 概率 , 而 不是 可能       感觉 应该 是 概念 的 问题 , 本质 上 是 重要 的 是 期望值 , 而 不是 某个 情况 下 的 值               简化       傻子 会 把 事情 搞 复杂 , 而 聪明 的 人 把 问题 变 简单               使用 原则       因为 几乎 所有 「 眼前 的 情况 」 都 是 「 类似 情况 」 的 再现 , 识别 「 类似 情况 」 是 什么 , 然后 通过 深思熟虑 的 原则 加以 应对       让 你 的 思维 慢下来 , 以 注意 到 你 正在 引用 的 决策 标准       把 这个 标准 作为 一个 原则 写 下来       当 结果 出现 时 , 评估 结果 , 思考 标准 , 并 在 下 一个 \" 类似 情景 \" 出现 之前 改进 标准               对 你 的 决策 进行 可信度 加权 ,   如果 和 其他人 存在 分歧 , 决策 的 时候 需要 加权 考虑            ", "tags": "book-note", "url": "/wiki/book-note/principles.html"},
      
      
      
      
      
        
      
      {"title": "alibaba", "text": "    Table   of   Contents          ", "tags": "company-data", "url": "/wiki/company-data/alibaba.html"},
      
      
      {"title": "jd.com", "text": "    Table   of   Contents           2016 年                 2016 年  ", "tags": "company-data", "url": "/wiki/company-data/jd.html"},
      
      
      {"title": "华为", "text": "    Table   of   Contents          ", "tags": "company-data", "url": "/wiki/company-data/huawei.html"},
      
      
      
      
      
        
      
        
        
      
      {"title": "在线教育行业", "text": "    Table   of   Contents           参考           第一 部分   了解 行业 的 过去           行业 界定 与 分类           行业 的 演变           行业 周期                   第二 部分   分析 一个 行业 的 现在           产业链 分析                   市场 情况 与 竞争 分析           市场 概况                         参考             https : / / mp . weixin . qq . com / s / LezQBMD7C47DdE5df2362g             第一 部分   了解 行业 的 过去       行业 界定 与 分类       根据 国民经济 行业 分类 ， 在线教育 所处 的 行业 ： 互联网 信息 服务 , 教育 培训       行业 的 演变           起步 阶段 :   MOOC           行业 周期           增长 型 行业           第二 部分   分析 一个 行业 的 现在       产业链 分析       上游 是 教师 ,   下游 是 终端 消费者 ( 学生 和 家长 )       优质 的 教师 资源 是 竞争 的 关键       市场 情况 与 竞争 分析       市场 概况  ", "tags": "economics/company", "url": "/wiki/economics/company/online-edu.html"},
      
      
      
        
        
      
      {"title": "从联邦基准利率读懂美国经济历史", "text": "    Table   of   Contents           关于           美联储 的 起源           经济 政策 和 环境           参考资料                 关于       从 联邦 基准利率 看 美国 经济 历史 , 数据 来源     https : / / www . macrotrends . net / 2015 / fed - funds - rate - historical - chart         持续 更新 汇总               美联储 的 起源       20 世纪 以前 , 美国 多 中央集权 的 恐惧 导致 从未 成功 地 建立 过 集中 的 中央银行 。 美国 第一 银行 在 1811 年 解散 , 美国 第二 银行 在 1836 年 停业 。 两次 尝试 都 以 失败 告终 , 而且 第二 国民 银行 的 停业 给 金融市场 带来 了 一个 严重 的 问题 , 银行 体系 没有 了 最后 贷款人 了 , 一旦 发生 挤兑 导致 的 银行 危机 , 那么 将 导致 全国性 的 银行 恐慌 。 果不其然 , 从 19 世纪 到 20 世纪 初 , 每隔 20 年 左右 就 发生 一次 全国性 的 银行 恐慌 , 累积 的 恐慌 在 1907 年 达到 顶峰 。 1907 年 的 银行 恐慌 导致 了 银行 的 大面积 破产 和 存款人 的 巨大损失 。 这 导致 美国 不得不 成立 一个 中央银行 来 防止 今后 再次出现 类似 的 恐慌 。       1913 年 , 由 时任 美国 总统 的 伍德罗 · 威尔逊 签署 了 《 联邦 储备 法 》 ，   成立 联邦 储备 体系 ， 统一 管理 和 发行 货币 ， 维护 金融 稳定 。 由于 美国 的 分权 思想 根深蒂固 ,   虽然 联邦 储备 体系 是 由 政府 控制 的 ， 但 在 这个 机构 中 赋予 私人 银行家 们 一定 话语权 ， 允许 他们 就 货币 管理 问题 提出 建议 。       经济 政策 和 环境           1913 年 - 1935 年 :   \" 被动 适应 型 \" 货币政策 , 美联储 没有 独立 的 中央银行 地位       1919 - 1920 年 , 由于 一战 末期 美联储 通过 再贴现 和 维持 低利率 帮助 美国政府 战争 筹款 , 带来 了 14 % 的 高 通货膨胀       1920 年 , 美联储 两次 上调 贴现 利率 , 在 1 月 从 4.75 % 提升 到 6 % , 6 月 的 时候 再次 上调 到 7 % , 这 导致 了 1920 - 1921 年间 的 经济 萧条 , 但 在 物价水平 开始 下降 之后 , 开启 了 繁荣 的 20 年 的 高速 发展期       20 年代 , 美联储 发现 了 新 的 货币 供给 — — 公开市场 操作 , 并 在 20 年代 末期 成为 最 重要 的 武器       1928 - 1929 股市 繁荣 ,   导致 美联储 在 1929 年 8 月 提高 贴现率 , 但 为时已晚 , 导致 了 30 年代 初期 的 大 萧条       1930 - 1933 年 大 萧条 , 银行 恐慌 , 美联储 放任不管 , 没有 承担 最后 贷款人 角色 , 导致 央行 持有 大量 超额 准备金       1936 - 1937 美联储 为了 加强 货币 控制 , 3 次 上调 法定 准备金 , 这 进一步 导致 了 更 严重 的 经济 萧条       1937 - 1938 经济 继续 萎缩       《 1933 银行法 》 《 1935 银行法 》 设立 了 公开市场 委员会 , 美联储 开始 独立 于 财政部       1934 存款 保险公司 成立               1942 - 1951 ,   为 二战 和 朝鲜战争 筹 军费 ,   通过 公开市场 操作 将 短期 国债 利率 维持 在 0.375 % , 长期 利率 2.5 % , 带来 了 8 % 的 高 通胀       1952 , 艾森豪威尔 当选 总统 , 并 赋予 了 美联储 独立 自由 地 追求 货币 目标 的 权利       20 世纪 五六十年代 , 以 联邦 基准利率 作为 货币政策 的 目标       1979 - 1982 转为 以 货币 供应量 为 目标       1982 - 90 年代 初期 ,   不再 强调 货币 总量       90 年代 - 2007 年 , 泰勒 规则 成为 理论依据 , 实际 利率 成为 中介 指标       2007 - 2015   次贷 危机 后 的 非常 为 货币政策 , 导致 了 0 利率 时代       2015 - 2019   美联储 为 应对 未来 可能 出现 的 衰退 , 前瞻性 地 加息           参考资料           米 什金 《 货币 金融学 》       ICBC 《 “ 百年 美联储 ” 的 货币政策 演变 历程 及其 启示 》       数据 来源   https : / / www . macrotrends . net / 2015 / fed - funds - rate - historical - chart        ", "tags": "economics/economics-data", "url": "/wiki/economics/economics-data/america-interest.html"},
      
      
      {"title": "通货", "text": "    Table   of   Contents           基本概念           M1 和 M2 经济指标 解读           参考资料                 基本概念             货币 当局   包括 人民银行 自己 和 人民银行 底下 的 国家外汇管理局 。       除了 这 两个 机构 之外 , 还有 一系列 的 机构 ( 商业银行 ) , 这些 机构 被 称为   其他 存款 性 公司           货币 当局   和   其他 存款 性 公司   统称 为   存款 性 公司   。       M0 = 流通 中 现金       流通 中 的 现金 是 指 流通 于 银行 体系 外 的 现钞 ,   你 手中 的 钞票 就是 M0 ,   不 包括 你 手中 可以 直接 支付 的 非 现钞 形式 存在 的 其他 现金 ,   比如 不 包括 借记卡 中 的 钱 、 也 不 包括 支付宝 的 余额 、 余额 宝中 的 钱 、 微信 中 的 钱 都 不 包括 。 M0 是 狭义 的 货币 供应 。       在 无 现金支付 的 潮流 下 , M0 意义 会 越来越 小 。       根据   央行 资产 负载 表   可以 查到 201811 的 「 货币 发行 」 项 的 值 是 76425.21 亿 RMB , 发行 的 货币 并 没有 都 变成 现钞 , 还有 一部分 作为 其他 存款 性 公司 的 「 库存现金 」 存在 。 根据   其他 存款 性 公司 资产 负债表   可以 查到 201811 的 值 为 5861.91 亿 RMB , 所以 现钞 也 就是 M0 只有   76425.21   -   5861.91   =   70563.3 亿 RMB 。 跟 央行 的   货币 供应量   表中 的 值 相同 。                   M1 = M0 ＋ 可 开支票 进行 支付 的 单位 活期存款 ;   M1 是 现实 的 货币 供应 ， 可 直接 用于 支付           中国 的 银行 把 单位 和 个人 分开 叫 的 ， 单位 （ 包括 企业 ） 的 存款 就 称为 存款 ， 比如 活期存款 、 定期存款 。 而 对于 个人 客户 的 存款 称为 储蓄存款 。 比如 定期 储蓄存款 ， 活期储蓄 存款 ， 这 是 指 个人 的 存款 。       美国 的 活期存款 跟 我们 中国 的 定义 是 不同 的 。 美国 M1 里面 的 活期存款 是 指 那种 不 支付 利息 的 支票 账户 。 老美 在 70 年代 以前 ， 只有 商业银行 能 开设 支票 账户 ， 而且 支票 账户 不 给 利息 ， 非常 的 变态 ， 后来 放开 了 ， 非 商业银行 的 一些 金融机构 也 可以 开设 支票 账户 了 ， 而且 也 有 给 利息 的 支票 账户 了 。 所以 美国 M1 统计 里面 的 活期存款 特指 那种 不 给 利息 的 支票 账户 。 中国 的 活期存款 就是 没有 固定 期限 ， 你 想 取款 就 随时 可以 取款 的 那种 账户 。       对于 个人 活期储蓄 存款 而言 ， 这 里面 分成 两个 部分 ， 一个 是 银行卡 里面 的 钱 ， 还有 一部分 不是 银行卡 里面 的 钱 。 对于 银行卡 里面 的 个人 储蓄存款 叫做 “ 银行卡 项下 的 个人 人民币 活期储蓄 存款 ” 。       可 开支票 进行 支付 的 单位 活期存款 是 指 企业 单位 的 活期 账户 中 的 钱 , 这些 钱 可以 开支票 进行 支付 。       根据   中国 的 M1 统计 中 为什么 不 计入 居民 活期存款 ？   -   及时 晴 的 回答   -   知乎   「 银行卡 项下 的 个人 人民币 活期储蓄 存款 」 是 算 到 M1 中 的 ,   因为 刷卡 支付 很 方便 。 但是 根据 央行 资产 负载 表 , 并 没有 单列 这 一部分 。       微信 中 的 钱 、 支付宝 中 的 钱 、 信用卡 等等 也 没算到 M1 中 , 虽然 现在 它们 的 支付 能力 比 现金 并 没有 差 。       从 「 其他 存款 性 公司 资产 负债表 」 中 可以 查 到 「 单位 活期存款   」 在 201811 的 值 为 472935.36 亿 RMB , 远 多于 M0 ,   加上 这 一部分 后 , 可以 算出 M1 = 543498.66 亿 RMB ,   跟 「 货币 供应量 」 中 的 数据 一致 。       可以 看到 M1 中 的 钱 绝大多数 以 单位 活期存款 形式 存在 ,   现钞 形式 只 占 了 13 %                   M2 = M1 ＋ 居民 储蓄存款 ＋ 单位 定期存款 ＋ 单位 其他 存款 ＋ 证券公司 客户 保证金 , M2 是 潜在 的 购买力 , 也 叫 广义 货币           居民 储蓄存款 既 包括 活期存款 、 也 包括 定期存款 。 可以 查到 201811 的 个人 存款额 为   710236.37   亿 RMB , 和 单位 活期 + 定期存款 额度 一样 多 ! !       余额 宝 、 货币基金 、 第三方 支付 余额 等 也 是 计入 M2 , 但是 没有 计入 M1 , 虽然 在 现在 , 他们 大多 可以 直接 用于 支付 。 这个 估计 是 历史 遗留问题 了 。       信用卡 的 额度 貌似 并 没有 在 央行 的 统计 范围 之内       商业银行 在 央行 的 「 存款 准备金 」 不算 广义 货币 , 所以 才 有 降准 是 在 放水       根据   深入 解读 M0 、 M1 、 M2   的 方法 计算出来 的 201811 的 M2 = 1783098.87 亿 RMB , 跟 央行 给出 的 181 万亿 的 数据 相差 3 万亿 ! ? ? ? 问题 在 哪                   根据   无现金 社会 的 冷思考 ： 宜 循序渐进 ， 需谨慎 待 之   文章 , 个人 活期存款 是 计入 M1 的           根据   金融 知识 小 课堂 ： M0 、 M1 、 M2 、 M3 、 M4   貌似 专家 是 支持 将 信用卡 额度 、 承兑汇票 、 借记卡 活期存款 都 算作 M1 的       微信 、 支付宝 等 可以 直接 支付 的 是不是 算 在 「 可 开支票 进行 支付 的 单位 活期存款 」 中 , 因为 那些 钱 本质 上 还是 以 活期 存在 银行 , 只不过 主体 是 企业 , 而 不是 我们 用户 ?           M1 和 M2 经济指标 解读                             相关 文章             货币 供应 与 股票价格 关系 的 实证 分析           M1 对 股市 、 房地产 市场 的 影响           上证指数 与 M1 、 M2 同比 增速 差 之间 的 关联 关系                     M1 主要 是 单位 活期存款 ,   占 了 87 %           M2 ≈ M1 + 居民 存款 + 单位 定期       对于 上证指数 ,   M1 的 联系 更加 紧密 ,   基本 是 同 向 的 。 股市 向 好 ， 会 吸引 更 多 的 资金 通过 证券 经纪商 进入 股市 ， 也 即 证券公司 、 基金 管理 公司 的 活期存款 会 增多 ， Ml 增速 加快 。       M1 同比 增速 与 上证指数 接近 于 同步 变化       房价 相比 于 M1 增速 大约 有 3 个 月 的 滞后       但是 , 他们 的 数据 是 基于 2010 年 之前 的 数据 , 而且 只 在 98 年 和 08 年 附近 是 同步 变动 的 , 但是 在 01 年 - 06 年 的 长熊 期间 , 并 不 同步       房价 指数 就 更 不用说 了       对比 10 年 以后 的 M1 同步 增速 和 上证指数 发现 ,   M1 增速 超过 10 % 是 在 2015 年 9 月 到 2018 年 1 月 ,   而 上证指数 的 峰值 是 在 12 年 6 月 ! !   根本 不 同步 !       M1 增速 比较 快 ,   应该 就是 单位 活期存款 增速 快 ,   这 说明 投资 比较 旺盛       M2 增速 比较 快 , 就要 看 具体 是 哪 一部分 增速 快 了 ,   如果 M1 增速 并 没有 加快 , 而是 居民 存款 加快           参考资料             深入 解读 M0 、 M1 、 M2        ", "tags": "economics/economics-data", "url": "/wiki/economics/economics-data/currency.html"},
      
      
      
        
        
      
      {"title": "第1部分 简介", "text": "    Table   of   Contents           关于           第 1 章   投资 环境                 关于       博迪 的 《 投资 学 》 读书笔记 ,   因为 找 不到 中文 电子书 , 只 找到 英文 电子书 , 所以 只能 看 英文 的 了 , 刚好 熟悉 一下 引文 专业 词汇 。 看 英文 书籍 的 功底 就 靠 之前 看 计算机 英文 书籍 和 论文 的 功底 了 , 不要 慌 , 哈哈 !       第 1 章   投资 环境           一些 术语       Portfolio :   本意 公事包 ,   指 投资 组合 , 即 一组 资产 的 组合       proceeds :   本意 继续 ,   指 收益                    ", "tags": "economics/investment", "url": "/wiki/economics/investment/introduction.html"},
      
      
      
        
        
      
      {"title": "杰拉尔德 E.平托《投资学》", "text": "    Table   of   Contents           关于           第 1 章   市场 组织 结构           第 2 章   证券市场 指数           第 3 章   市场 有效性           第 4 章   投资 组合 管理 : 概述           第 5 章   投资 组合 的 风险 和 收益 I           第 6 章   投资 组合 的 风险 和 收益 II           资本 资产 定价 模型                   第 7 章   证券 投资 组合 规划 与 构建 的 基础           第 8 章   权益 证券 概述           第 9 章   行业 与 公司 分析 简介           第 10 章   股权 价值           第 11 章   股票市场 估值                 关于       【 美 】 杰拉尔 德   E . 平托 《 投资 学 》 读书笔记 ,   结合 博迪 的 一起 看 可能 效果 更好 。       第 1 章   市场 组织 结构           资产 和 市场 的 分类       资产       证券       债券       股票       公募 :   公开 交易       私募 :   比如 风险投资               货币       合约       商品               市场       资本 市场 :   期限 长 的 工具 ,   包括 长期 债券 和 权益       货币 市场 :   一年 以内 期限 的 债务 投资 工具 ,   回购 协议 ,   可 转让 存单 ,   政府 债券 和 商业 票据                       证券       固定 收益 证券       权益       普通股       优先股 ,   金融 分析师 将 优先股 当做 固定 收益 证券               集合 投资       共同 基金       信托       存托 凭证       对冲 基金                           第 2 章   证券市场 指数           指数 权重       价格 加权 ,   即 价格 的 算术 平均值 作为 指数 的 值 , 当然 还 得 除以 基期 。 代表 :   道琼斯 工业 指数       优点 : 简单 , 投资 该 指数 只 需要 每 只 股票 买 相同 的 数目 即可       缺点 : 拆股 导致 权重 需要 调整 , 且 权重 过于 随意 , 因为 每股 价格 的 随意性               等 权重 加权 ,   即 每 只 股票 都 投 相同 的 钱 去 购买 , 算 出来 的 市值 相对 基期 的 比值 。       市值 加权 ,   即用 每 只 股票 的 总 市值 之 和 作为 指数 的 值 , 再 除以 基期 。 代表 :   上证综指       流通 市值 加权 ,   将 总 市值 改为 流通 市值 ,   代表 : SP500 , 沪 深 300       优点 :   公司 市值 高 的 占 比 更 高       缺点 :   市值 上涨 高 的 公司 权重 越来越 大               基本面 加权 ,   按照 某种 基本面 计算 的 权重 , 账面 价值 、 现金流 etc       MSCI 指数 在 国家 内部 按照 市值 加权 , 不同 国家 之间 按照 GDP 加权                       再 平衡       要 定期 调整 成分 股权 重 , 因为 随着 股价 涨跌 、 分红 、 拆股 、 增发 等 因素 影响 , 权重 会变 。       调整 的 要求 是 , 调整 前后 指数 保持 不变       实现 方法 ,   通过 调整 除数 实现               在 构造       在 成分股 发生变化 的 时候 , 需要 重新 构造 指数 , 并 通过 调整 除数 实现 调整 前后 指数 保持 不变                   第 3 章   市场 有效性           有效 市场 假说 :   略 , 参看 《 货币 金融学 》 笔记           第 4 章   投资 组合 管理 : 概述           投资 组合 的 重要性 :   避免 单一 资产 的 大灾难       安然 公司 的 退休 员工 布鲁斯 的 200 万美元 的 资产 全买 了 安然 公司股票       投资 组合 可以 使得 不 需要 减少 期望 收益 , 而 降低 风险 的 方法               减少 波动 风险       多元化 比率 :   计算 等 权重 组合 的 标准差 与 随机 选择 的 标准差 的 比值       投资 组合 对 风险 的 影响 大于 收益       组合 的 权重 ,   收益率 与 风险 的 权衡       当 市场 奔溃 , 金融危机 出现 的 时候 , 投资 组合 也 是 无效 的       现代 资产 组合 理论 :   投资者 不但 要 持有 组合 , 还要 关注 组合 中 个体 证券 之间 的 相关性       投资 客户       个人 投资者 ,   养老 计划       机构 投资者 :   延期 收益 养老金 计划 , 大学 捐赠 基金 , 慈善 基金 , 银行 , 保险公司 , 投资 公司 , 主权 财富 基金 ( 政府 持有 )               共同 基金       开放式 基金 ,   基金 的 股份 随时 可以 增减 的 , 购买 基金 就 相当于 增发 , 卖出 基金 相当于 回购       封闭式 基金 ,   基金 的 股份 不变 , 因而 表现 得 像 股票 , 只能 通过 二级 市场 购买 基金 股份               略 , 参考 书籍 和 《 货币 金融学 》 笔记           第 5 章   投资 组合 的 风险 和 收益 I           资产 的 投资 属性       收益       利息 、 股息 等 定期 收益       指数 点数 没涨 不 一定 没有 收益 , 因为 可能 有 股利 分红 收益               资本 利得 或 损失                       算术平均 收益 ,   直接 算术 平均值       几何平均 收益 ,   考虑 复利 的 平均 收益       资金 加权 收益   IRR ,   每年 的 投入 金额 不同 需要 加权 考虑 ,   将 每年 的 现金流 ( 第 T 期用 最后 的 余额 作为 正 的 现金流 ) 折现 到 现在 等于 0 , 建立 方程 计算 加权 收益率     $ $     \ \ sum _ { i = 0 } ^ T   \ \ frac { CF _ i } { ( 1 + IRR ) ^ i }   =   0     $ $       年化 收益率 , 期限 短于 1 年 需要 将 收益率 年化 , 考虑 重复 投资 的 复利 效应 的 年 化率   $ ( r _ { 年 }   =   ( 1   +   r _ { 时期 } ) ^ c   -   1 ) $       投资 组合 的 收益率 , 是 各 资产 的 加权 收益率 。 假设 总 的 资金 是 1 , 组合 包含 了 个 资产 的 份额 分别 是   w1 , w2 , ... , wn , 各 资产 收益率 为 R1 , r2 , ... , Rn , 那么 总 的 收益 当然 是   w1R1 + w2R2 +...+ wnRn       实际 收益率 , 可以 分解 为 市场 的 无风险 利率 , 通胀率 , 风险 溢价       杠杆 收益率       收益率 的 方差 和 协方差       一个 资产 的 收益率 方差 由 收益率 在 时间 维度 的 样本 方差 刻画 , 分母 是   T - 1       投资 组合 的 方差 跟 各 资产 之间 收益率 的 协方差 有关 , 两个 资产 构成 的 组合 收益率 方差 为     $ $     w _ 1 ^ 2   \ \ sigma _ 1 ^ 2   +   w _ 2 ^ 2   \ \ sigma _ 2 ^ 2   +   2w _ 1w _ 2   \ \ rho ( R _ 1 ,   R _ 2 )   \ \ sigma _ 1 \ \ sigma _ 2     $ $       如果 相关 为 - 1 , 表明 两种 资产 收益 互相 对冲 , + 1 表示 两种 资产 。 考虑 两种 资产 各自 的 方差 相同 , 那么 组合 之后 的 方差 是 小于 等于 单个 资产 的 方差 的               书上 有 一个 例子 , 将 标普 500 指数 和 MSCI 指数 组合 , 获得 了 比 单独 投资 标普 500 ,   其 收益 增加 的 同时 降低 了 风险 ! ! 牛 逼 ! !       一些 收益 和 风险 历史数据       过去 109 年 , 主要 国家 的 股票指数 平均 收益率 8.4 % , 债券 4.8 % ;   排除 美国 后 ,   股票 7.9 % ,   债券 4.2 %       1926 - 2008 ,   美国 小 公司股票 风险 33 % , 大 公司股票 20.6 % , 长期 政府 债券 9.4 % , 长期 公司债券 8.4 % , 短期 国债 3.1 %       1990 - 2008 ,   世界 股票 风险 17.3 % ,   债券 8.6 % ,   排除 美国 股票 风险 20.1 % ,   债券 13.0 %               风险 - 收益 权衡 ,   在 长期 的 有效 市场 中 ,   风险 和 收益 是 正 相关 的 。 风险 溢价       其他 投资 属性       收益率 的 分布 特征 :       偏度 :   中位数 与 均值 不 相等       峰度 :   峰度 低 将 增加 小 概率 事件 的 发生 概率 , 增加 风险               市场 特征 :       流动性 ,   交易量 多 的 股票价格 波动 更 小 , 流动性 更好                       风险 厌恶       风险 追求 ,   投资者 会 因为 赌博 带来 的 风险 , 选择 更 低 的 期望 收益率 , 比如 买 彩票       风险 中性 ,   只 关心 收益 , 不 关心 风险 , 当 投资 只是 财富 的 一小部分 时 , 很多 人 表现 出 风险 中性       风险 厌恶 ,   更 倾向 于 选择 风险 低 的 , 确定性 的 结果 。 风险 - 收益 的 相关性 显示 市场 上 大多数 人 是 风险 厌恶 的 , 这 也 是 投资 里面 的 基本 假设       风险 容忍度 ,   投资者 为 实现目标 愿意 忍受 的 风险 大小               效用 理论 ,   $ ( U   =   E ( r )   -   \ \ frac { 1 } { 2 } A \ \ sigma ^ 2 ) $   E ( r ) 是 预期 收益率 , A 是 风险 厌恶 系数       在   $ ( E ( r ) - \ \ sigma ) $   图上 是 一系列 的 等高线               资本 配置 曲线 , 当 市场 只有 一种 无风险 资产 和 一种 有 风险 资产 , 不同 的 组合 将 得到 不同 的 期望 收益率 和 标准差 , 那么 期望 收益   $ ( R _ p   =   R _ f   +   k   \ \ sigma ) $   是 一条 直线 , 截距 是 无风险 利率 , 而 要 想 获得 更 高 的 收益 , 就 得 承受 更 高 的 风险 ,   该 直线 以下 的 区域 是 可 达到 的 配置 组合       效用 曲线 和 资本 配置 曲线 联合 求解 得到 最优 投资 组合 ,   配置 曲线 以下 区域 相当于 可行 域 , 效用函数 的 最大值 在 其 等高线 与 可行 域 相切 的 地方 取得       应用 :   以 自己 为例 , 进行 估算 , 目前 无风险 利率 相当于 货基 的 利率 3 % ,   波动 率 可以 忽略 ;   股票 基金 ( 中美 两国 综合 下来 ) 的 预期 收益率 10 % ,   波动 率 ( 中美 两国 指数 的 投资 组合 之后 ) 预计 在 20 % 吧 ( 参考 上面 的 数据 ) 。 个人 风险 容忍度 估计 ,   因为 我 认为 无风险 利率 3 % 和 20 % 的 风险 + 20 % 的 收益率 是 可以 同时 接受 的 , 估算 得到   $ ( 0.03   =   0.2   -   0.5   A   0.04 ) $ , 所以 A = 8.5 , 是 一个 有 一定 风险 容忍度 的 值 。 因此 , 按照 资本 配置 曲线 和 效用 曲线 的 相切 计算 我 对 这 两种 资产 的 配置 比例 应该 是 ( 假设 货基 配置 比例 为 w ) :         $ ( U   =   0.1   -   0.07 w   -   0.17 ( 1 - w ) ^ 2 ) $ ,   在 w = 0.794 处 取得 最大值 Umax = - 0.0372       这 表明 我 应该 将 80 % 的 资金 放到 货基 ,   20 % 的 资金 放到 股票指数 基金       按照 这个 配置 ,   期望 收益率 为 4.4 % , 风险 为 4 % , 按照 这个 估计 , 最大 回撤 ( 95 % 置信区间 ) 为   - 3.6 % , 最高 收益 12.4 %       这里 估计 的 是 长期 收益 , 以及 长期 的 资产 配置 , 短期 可以 根据 股票价值 的 低估 与 高估 调整       股票 资产 比例 :   10 % ,   预期 收益 :   3.7 % ,   风险 :   2 %       股票 资产 比例 :   20 % ,   预期 收益 :   4.4 % ,   风险 :   4 %       股票 资产 比例 :   30 % ,   预期 收益 :   5.1 % ,   风险 :   6 %       股票 资产 比例 :   40 % ,   预期 收益 :   5.8 % ,   风险 :   8 %       股票 资产 比例 :   50 % ,   预期 收益 :   6.5 % ,   风险 :   10 %               两种 风险 资产 组合 的 风险       当 两种 资产 的 风险 相关系数 小于 1 时 , 组合 后 的 风险 是 小于 风险 的 加权 平均 的       当 两种 资产 的 风险 相关系数 接近 于 0 时 ( 即 基本 无关 ) , 组合 后 的 风险 是 其 加权 平方 平均 , 当然 小于 加权 平均       当 两种 资产 的 风险 相关系数 小于 0 时 , 组合 后 的 风险 小于 加权 平方 平均 , 存在 一定 的 对冲       当 风险系数 等于 - 1 时 , 组合 后 的 风险 是 风险 之差 , 将 权重 小 的 资产 风险 完全 对 冲掉 了 , 如果 权重 相同 , 那么 组合 完全 没有 风险 !       从 下述 曲线 可以 看出 , 如果 两种 风险 资产 的 强正 相关 , 那么 在 相同 收益率 下 , 组合 的 风险 跟 加权 风险 ( 相关系数 = 1 的 直线 ) 差不不大 , 但是 仍然 可以 减少 一定 的 风险 。 也就是说 通过 在 低 风险 资产 外 配置 一些 高风险 资产 , 收益率 和 风险 都 会 同步 提高 , 但是 比 加权 风险 是 要 小 一些       如果 两种 资产 只有 较弱 的 的 正 相关 , 甚至 负相关 , 那么 可以 通过 在 低 风险 资产 外 配置 小 部分 高风险 资产 , 实现 收益率 提高 而 风险 却 下降 ! ! 而且 , 负 相关性 越强 , 这个 双赢 的 区间 也 越 宽 !                               A股 与 其他 市场 的 相关性 数据 , 数据 来源 :     https : / / news . futunn . com / market / 69264 ? src = 3       统计 发现 ， 最近 三年 A股 市场 （ 以沪深 300 指数 基金 表征 ） （ ASHR ） 与 原油价格 （ BNO ） （ USO ） 高度 相关 ， 相关性 超过 85 % ， 与 人民币 汇率 （ CYB ） 、 中国 在岸 投资 级 债券 （ CBON ） 、 美国 高 收益 债券 （ HYG ） 相关性 在 50 % - 70 % 之间 ， 与 其他 大盘 指数 MSCI 中国 （ MCHI ） 、 MSCI 欧澳 远东 （ EFA ） 的 相关性 接近 50 % ， 而 与 MSCI 香港 （ EWH ） 、 MSCI 新兴 市场 （ EEM ） 的 相关性 则 低于 40 % 。       在 热门 主题 上 ， A股 市场 与 境外 股票市场 的 中国 科技股 （ CQQQ ） 相关性 低于 10 % ， 与 美国 股 市场 基准 标普 500 （ SPY ） 、 美股 科技 （ QQQ ） 、 半导体 （ SMH ） 最近 三年 为 10 % ~ 20 % 的 弱 负相关 。       在 「 避险 资产 」 方面 ， A股 市场 与 中国 十年 期 国债 （ 160123 ） 相关性 接近 - 80 % ， 与 国际 金价 （ GLD ） 、 美国 市政 债 （ MUB ） 的 相关性 接近 - 50 % ， 与 美国 20 年 以上 长期 国债 （ TLT ） 相关性 约 为 - 25 % 。       从中 可以 发现 ， 最近 三年 ， 如果 A股 投资者 将 部分 资产 分配 到 境外 股市 ， 可以 起到 分散 风险 的 作用 。                   多种 风险 资产 的 组合       假设 各 资产 风险 相同 , 且 协方差 系数 相同 , 多种 风险 资产 的 组合 风险 为   $ ( \ \ sqrt { \ \ frac { \ \ sigma ^ 2 } { N }   +   \ \ frac { N - 1 } { N } \ \ rho \ \ sigma ^ 2 } ) $       如果 N 很大 , 那么 组合 的 风险 为   $ ( \ \ sqrt { \ \ rho } \ \ sigma ) $ , 这 表明 如果 资产 之间 的 相关性 极低 , 那么 组合 基本上 没有 风险 ! !               多元化 的 力量       要 选择 相关性 弱 的 或者 相关性 为 负 的 资产 进行 组合       分散 风险 的 关键 是 找到 相关性 远 小于 1 的 资产 ,   低于 0.3 的 相关性 认为 是 有利于 分散 风险 的       同一个 市场 的 股票 相关系数 很 高 ,   美股 大 公司股票 和 小 公司股票 相关系数 高达 70 % , 而 美国 大 公司 和 海外 股票 的 相关系数 为 66 % , 美国 小 公司 和 海外 股票 相关系数 为 49 % 。 相关系数 最低 的 是 股票 和 债券 , 甚至 为 负相关                               多元化 的 途径       不同 的 资产 类别 :   股票 和 债券 , 不同 市场 , 不同 行业 和 部门 股票 之间 的 相关性 不 大 , 都 可以 用来 减少 风险       通过 指数 基金 实现 多元化 , 比较 适合 小 资金量       跨国 投资 ,   还有 货币 收益 ( 汇兑 损益 ) 和 股票 收益 无关       通过 不 持有 本 公司股票 实现 多元化 ,   你 对 公司 的 贡献 也 是 一种 投资 ,   如果 公司 挂 了 , 你 既 没 工作 也 亏了 股票       在 添加 某个 资产 到 组合 之前 进行 评估 。 夏普 比率 :   风险 溢价 / 风险 。 加入 后 的 夏普比 应该 比旧 的 夏普比 乘以 新旧 相关系数 要 大 , 才能 加入       为 风险 组合 买 保险 ,   因为 保险 也 是 一种 资产 , 收益 跟 你 的 投保 对象 收益 是 负相关 的 ! !     $ $     \ \ frac { R _ { new }   -   R _ f } { \ \ sigma _ { new } }   & gt ;   \ \ frac { R _ { p }   -   R _ f } { \ \ sigma _ { p } }   \ \ times   \ \ rho _ { new ,   p }     $ $               黄金 和 股票 具有 负 相关性 , 但是 黄金 的 期望 收益率 通常 很小 甚至 有时 为 负       买入 看跌 期权 ,   也 是 负相关 , 但是 看跌 期权 的 收益率 通常 为 0 甚至 是 负 的       有效 边界 与 投资者 的 最优 组合       增加 投资 机会 , 知道 不能 改善 收益 - 风险 曲线       最小 方差 组合 :   即 在 给定 期望 收益 下 , 最小化 风险       在 所有 可行 的 组合 中 , 在 相同 收益率 下 最小 的 风险 点 构成 的 集合 就是 最小 方差 边界       全局 最小 方差 组合 , 最小 方差 曲线 最 左边 的 点 , 全局 最小 方差 组合 右 下边 的 点 都 不 应该 选择 ; 而 右上方 的 曲线 被 称为 马科 维兹 有效 边界 , 在 这个 边界 上 , 曲线 斜率 是 递减 的 , 也就是说 通过 增加 风险 来 获得 额外 的 收益 的 边际 效益 会 递减                                           加入 无风险 资产 , 相当于 连接 边界 上 的 点 和 无风险 利率 点 的 直线 , 当该 直线 与 最小 方差 边界 相切 的 时候 , 得到 最优 的 资本 配置 线 。 这 条 直线 的 斜率 是 所有 连接 可行 点 与 无风险 点 的 直线 中 斜率 最大 , 可以 解释 为 通过 增加 风险 资产 带来 的 期望 收益率 的 边际 效益 最大 !                     两 基金 分离 定里 :   所有 的 投资者 都 会 持有 两个 投资 组合 或 基金 :   无风险 资产 和 最优 风险 组合 资产 的 一个 组合 。 这 有利于 将 投资 分为 两步 , 第一步 确定 最优 风险 组合 的 最小 方差 曲线 上 的 点 P , 第二步 根据 个人 风险 偏好 在 无风险 利率 点 和 P 点 之间 ( 或 延长线 上 ) 找到 一个 最大化 个人 效用 的 点               第 6 章   投资 组合 的 风险 和 收益 II           资本 市场 理论       唯一 最优 风险 组合 是否 存在 ,   当 不同 投资者 预期 不 一致 时 , 不 存在       资本 市场 线 , 把 市场 组合 作为 最优 风险 资产 组合 , 连接 无风险 利率 点 的 那条 资本 配置 线       消极 投资者 :   践行 有效 市场 假说 , 认为 市场 价格 就是 公司 内在 价值 ( 未来 现金流 的 折 现值 ) , 被动 跟踪 指数       积极 投资者 :   不 信 市场 , 信 自己 的 估值 方法       市场 , 狭义 的 一般 指 本国 或者 一个 区域 内 的 指数 作为 代表 , 广义 的 指 全世界 所有 可 投资 的 资产       杠杆 的 成本 一般 会 比 无风险 利率 高 , 所以 配置 直线 的 外延 将会 是 一条 斜率 更 低 的 直线               风险 的 定价 和 期望 收益率 的 计算       系统 风险 , 不可避免 的 市场 内部 风险 , 如 利率 、 通胀率 、 经济周期 、 政治 不确定性 、 自然灾害 etc       非 系统 风险 , 公司 的 风险 , 研发 失败 、 大量 石油 发现 、 空难 etc 。 可以 通过 投资 组合 分散 这种 风险       总 方差   =   系统 方差   +   非 系统 方差       由于 非 系统 风险 可以 被 分散 , 所以 在 一个 有效 市场 中 , 非 系统 风险 没有 额外 的 风险 溢价 , 只有 系统 风险 才 有 额外 的 风险 溢价       思考题       国债 是 无风险 资产 , 既 没有 系统 风险 也 没有 非 系统 风险 , 呵呵 ?       标普 500 是 市场 组合 , 不 存在 非 系统 风险 , 因为 已经 被 充分 分散 掉 了 , 所以 只有 系统 风险       总 风险 相同 的 两种 资产 , 一种 包含 非 系统 风险 , 另 一种 不 包含 , 那么 因为 非 系统 风险 不 存在 溢价 , 所以 不 包含 非 系统 风险 的 资产 系统 风险 更高 , 具有 跟 高 的 风险 溢价 , 因而 价格 更高                       $ ( \ \ beta ) $ 值 的 计算 和 解释       收益 生成 模型 :   将 相对 于 无风险 利率 的 超额 收益 表示 为 多个 风险 因子 的 线性组合 ,   所以 模型 都 把 市场 组合 收益 作为 关键 因子       单 因子 模型   $ ( E [ R _ i ]   -   R _ f   =   \ \ beta _ i ( E [ R _ m ]   -   R _ f ) ) $ , 即 只 认为 是 市场 组合 收益 的 线性 函数       利用 资本 市场 线 可知 斜率 $ ( \ \ beta _ i   =   \ \ frac { \ \ sigma _ i } { \ \ sigma _ m } ) $ , 即 斜率 是 风险 之 比       上述 的 beta 值 是 充分 分散 的 市场 模型               对于 一般 的 资产 ,   beta 值 的 计算 可以 同时 估计 两边 的 方差 得到   $ ( \ \ beta _ i   =   \ \ rho _ { im }   \ \ frac { \ \ sigma _ i } { \ \ sigma _ m } ) $ , 即 多 了 一个 和 市场 的 相关系数       大多数 股票 和 市场 相关性 很强 , 在 0.7 左右 , 但是 风险 要 比 市场 大 很多       beta 值越 大 , 说明 相比 市场 的 风险 越大 , 需要 获得 更大 的 期望 收益 , 即 风险 溢价       单 因子 模型 可以 通过 市场 数据 反推 期望 收益 ?   但是 方差 是 通过 历史数据 估计 出来 的 , 不 一定 对 未来 是 不变 的 , 因而 具有 局限性                   资本 资产 定价 模型           单 因素 模型       $ ( E [ R _ i ]   =   R _ f   +   \ \ beta _ i ( E [ R _ m ]   -   R _ f ) ) $       即 资产 的 预期 收益率 完全 由 beta 刻画 , 由 系统 风险 ( 即 括号 内 的 项 ) 决定 ?       $ ( E [ R _ m ]   -   R _ f ) $ 实际上 刻画 了 系统 风险 的 收益 , 因为 市场 的 组合 将 非 系统 风险 已经 分散 掉 了 , 剩下 收益 全部 来自 于 系统 风险       beta 刻画 的 是 该 资产 的 系统 风险 相对 于 市场 系统 风险 的 比值 , 而 不是 之前 人们 理解 的 总 风险                       基本 假设       投资者 是 风险 厌恶 的 , 在 非理性 投资者 交易 互相 抵消 、 或者 理性 投资者 占优 的 情况 下 , 是 可以 成立 的 。 A股 估计 不 成立 , 早年 的 美国股市 估计 也 不 成立       市场 无 摩擦 , 无 税收 和 交易成本       投资者 有 相同 的 投资 持有期       投资者 同质 预期       所有 投资 工具 无限 可 分       投资者 是 价格 接受者 , 没法 影响 价格 , A股 一堆 的 庄家 , 这个 假设 太强 了               计算       计算 单只 股票 的 预期 收益率       计算 投资 组合 的 预期 收益率 , 投资 组合 的 beta 值 可以 作为 各 资产 beta 值 的 加权 和       注意 : 预期 收益率 和 实际 收益率 往往 相差 很 远       通过 实际 收益 做 最小 二乘 得到               应用       预期 收益率 的 估计       股票 估值 的 时候 需要 用到 必要 收益率 , 必要 收益率 跟 风险 有关 , 可以 用 预期 收益率 来 替换 , 因为 它 衡量 了 风险       资本 预算 中 的 应用       计算 项目 的 未来 现金流 的 折 现值 是否 为 正       将 项目 的 预期 收益率 作为 折现 率       项目 的 预期 收益率 通过 beta 值 计算       beta 值 怎么 来 ? ?                       投资 组合 实际 收益率 的 比较       夏普比 ,   实际 的 风险 溢价 / 总 风险 ,   $ ( \ \ frac { R _ p   -   R _ m } { \ \ sigma _ p } ) $ ,   表示 单位 总 风险 带来 的 风险 溢价       特 雷诺 比率 ,   实际 的 风险 溢价 / beta ,   $ ( \ \ frac { R _ p   -   R _ m } { \ \ beta _ p } ) $ , 表示 单位 系统 风险 带来 的 风险 溢价       M2 , 将 资产 的 收益 按照 市场 风险 等 比例 折算 后 的 超额 收益 ,   $ ( ( R _ p   -   R _ f ) \ \ frac { \ \ sigma _ m } { \ \ sigma _ p }   -   ( R _ m   -   R _ f ) $ , 表示 按照 市场 风险 折算 的 超额 收益       詹森 $ ( \ \ alpha ) $ , 相对 预期 收益 的 超额 收益 ,   $ ( R _ p   -   [ R _ f   +   \ \ beta ( R _ m   -   R _ f ) ] ) $               基金 经理 业绩 评估       其他 收益率 估计 分析       证券 选择               资本 资产 定价 模型 之外       理论 局限       单 因素 模型 , 易于 理解       单期 局限                       这 一部分 更加 偏向 与 实践           第 7 章   证券 投资 组合 规划 与 构建 的 基础           投资 组合 规划 / 投资 说明书 IPS       客户 投资 目标 :   风险 容忍度 和 收益率 需求       客户 投资 组合 的 限制 条件       构成       简介 , 描述 客户       目标 声明       义务 与 责任       程序 , 使 投资 说明书 保持 最新 的 程序 和 突发事件       投资 目标       投资 限制       投资 指引 ,   政策 如何 被 执行 , 是否 允许 加 杠杆 , 投资 衍生品 , 以及 不能 投资 的 资产 类别       评估 和 审查       附录 ,   资产 配置 策略 , 再 平衡 策略                                   第 8 章   权益 证券 概述           略           第 9 章   行业 与 公司 分析 简介           行业 分析 的 重要 价值       了解 公司 的 业务 和 商业 环境       识别 活跃 的 股权 投资 机会       投资 组合 的 业绩 归因               识别 类似 公司 的 方法       产品 或 服务 提供             提供 类似 产品 和 服务 的 公司 归一 一类       部门 :   一组 相关 的 行业               商业 周期 敏感性       根据 对 商业 周期 的 敏感性 分为 周期性 和 非周期性 行业       周期性 性 产品 和 服务 跟 经济周期 相关 , 通常 是 比较 昂贵 或者 必要 时 可以 推迟 购买 的 。 例如 : 房产 、 汽车 、 工业 股票       非周期性 公司 表现 高度 独立 于 经济周期 。 食品 和 饮料 、 家庭 和 个人 护理产品 、 医疗保健 和 公共事业       防御型 , 周期 对 公司 影响 较 小 。 如 大宗 消费品 ( 面包 ? ) 、 提供 基本 服务 ( 快餐 、 杂货铺 、 药房 )       成长型 , 有 强烈 的 特定 需求 的 行业 , 无论 经济 环境 如何 , 也 能 为 公司 带来 增长                       统计 相似性       根据 过去 证券 回报率 之间 的 相关性 为 基础 , 从 相关性 高到 低 排序                       行业 分类 系统       全球 行业 分类 标准 GICS : 标普 和 摩根斯坦利       罗素 全球 行业 RGS       行业 分类 基准 ICB : 富时 指数 和 道琼斯               代表性 行业       基础 材料 和 加工 ,   基础 材料 、 化工产品 、 纸 、 容器 、 包装 、 金属 、 矿产品       非 必须 消费品 , 来自 消费者 产品 的 公司 和 服务 , 需求 高度 经济周期 敏感性 , 汽车 、 服装 和 酒店 饭店 业务       必需 消费品 , 食品 制造 、 饮料 、 烟草 和 个人 护理       能源 , 生产 、 勘探 、 销售 能源 相关 公司       金融服务 , 银行 、 金融 、 保险 、 房地产 、 资产 管理 或 经纪 服务 的 公司       医疗保健       工业 / 耐用品 制造业       科技 , 电脑 、 通信 设备 、 软件 、 半导体 、 电子 娱乐 、 网络服务 等       通信 , 提供 通信 服务 的 公司       公共事业 , 发电 、 天然气               政府 行业 分类 系统       描述 和 分析 行业       行业 生命周期       导入期       成长期       动荡 期       成熟期       衰退期               行业 的 集中度       进入 壁垒       行业 产能       市场份额 稳定性       价格竞争               行业 发展 、 利润率 和 风险 的 外部 影响       宏观经济       技术 因素 影响       人口 因素 影响       政府 因素 影响               公司 分析 的 要素       提供 公司简介 , 对 业务 投资 活动 公司 治理 感知 到 的 优势 和 劣势 的 基本 理解       解释 相关 的 产业 特点       分析 公司 产品 和 服务 的 需求       分析 公司 产品 的 供应 , 包括 成本 分析       解释 公司 的 定价 环境       提供 和 解释 相关 的 财务 比率 , 包括 自身 不同 时期 和 竞对 的 对比       财务报表 的 预测               具体 参考 9.6 . 1 中 的 表 9 - 5           第 10 章   股权 价值           三种 主要 的 估值 模型       现值 模型 ,   未来 现金流 的 折 现值       乘数 模型 ,   市盈率 , 市销率       基于 资产 的 估值 模型 ,   用 资产 价值 减去 债务 和 优先股 价值               DDM 模型 ,   将 未来 的 现金流 折现 , 计算 股票 档期 价值       股权 自由 现金流 估值 模型 FCFE   =   营业 现金流 CFO   -   固定资本 投资 FCInv   +   净 借入 金额 。 将 未来 自由 现金流 折现       必要 收益 估计 采用 资本 资产 定价 模型 CAPM , 股票 i 的 必要 收益率 = 现在 预期 无风险 利率   +   beta _ i   市场 风险 溢价       优先股 股票 估值       不可 赎回 的 优先股 相当于 永续 债券 , 用 折现 的 现金流 估值 即可       对于 可 赎回 的 优先股               戈登 增长 模型 , 略 , 参考 《 货币 金融学 》       多 阶段 股利 贴现 模型       对 成长期 、 过渡期 、 成熟期 等 不同 周期 给予 不同 的 增长率 进行 估值 , 每 一段 都 采用 现金流 折现               价格 乘数 模型 , 和 行业 平均值 对比       市盈率 ,   低 市盈率 有 回报 优势       市净率 ,   市净率 跟 未来 收益率 成反比               比较法           第 11 章   股票市场 估值           理论 市盈率 的 估计       柯布 道格拉斯 增长 函数   $ ( Y   =   AK ^ { \ \ alpha } L ^ { \ \ beta } ) $ ,   K 代表 资本 存量 ,   L 代表 劳动力 ,   A 是 全 要素 生产率       规模 报酬 不变 ,   $ ( \ \ alpha   =   1 - \ \ beta ) $       $ ( \ \ frac { \ \ Delta   Y } { Y }   =   \ \ frac { \ \ Delta   A } { A }   +   \ \ alpha   \ \ frac { \ \ Delta   K } { K }   +   \ \ beta   \ \ frac { \ \ Delta   L } { L } ) $       A 的 变化率 叫做 索洛 剩余 , 来自 于 科技 的 进步 和 创新               中国 GDP 的 增长 经历 3 个 阶段 — — 《 中国 全 要素 生产率 以及 经济 增长 因素 分解 》 2017 , 建信 期货 研究 中心       1952 - 1977 , 这一 阶段 资本 存量 / 就业人数 / 全 要素 生产率 平均 年 对数 增长率 分别 为   11.05 % / 2.57 % / - 3.52 % ， 对 经济 增长 的 贡献率 分别 为   151.79 % / 9.11 % / - 60.9 % 。 可以 看出 虽然 在 建国 后 投资   建设 热情高涨 ， 资本 存量 快速 累积 ， 同时 就业人口 增长 也 比较 快 ， 但 由于 我国 从 半殖民地 半封建社会 快速   进入 社会主义 社会 ( 部分 地区 进入 共产主义 社会 ) ， 社会 生产关系 发生 了 重大 变化 ， 再 加上 三年 大跃进 活动   以及 十年 文革 的 影响 ， 导致 所 投入 的 要素 并未 得到 有效 充分 使用 ， 全 要素 生产率 平均 年 对数 下滑   3.52 % ，   真实 产出 平均 年 对数 增长 也 仅 为   5.79 % 。       第二个 阶段 是   1977 - 2007   年 ， 这一 阶段 资本 存量 / 就业人数 / 全 要素 生产率 平均 年 对数 增长率 分别 为   9.68 % / 2.16 % / 1.39 % ， 对 经济 增长 的 贡献率 分别 为   80.78 % / 4.66 % / 14.56 % 。 可以 看出 这 段时间 投资 增速 依然   较 高 ， 由于 计划生育 的 原因 就业人数 增速 已经 明显 下降 ， 要素 投入 对 经济 增长 的 贡献率 有所 下降 ; 但 在 中   国 改革 开放政策 的 影响 下 ， 一方面 通过 改革 破除 经济 政治体制 中 不 符合 经济 增长 规律 的 条条框框 ， 一方面   通过 开放 引入 外资 与 国外   先进 生产 / 管理 技   术 ， 更 重要 的 一   点 是 通过 加入 世界   贸易组织 开拓 广阔   市场 ， 使得 这 段时间 全 要素 生产率 实现 年 平均 对数 增长   1.39 % ， 推动 真实 产出 年 平均 对数 增速 达到   9.52 % 。       第三个 阶段 是   2007 - 2015   年 ， 这一 阶段 资本 存量 / 就业人数 / 全 要素 生产率 平均 年 对数 增长率 分别 为   12.83 % / 0.35 % / - 1.99 % ， 对 经济 增长 的 贡献率 分别 为   80.78 % / 4.66 % / 14.56 % 。 这个 阶段 首先 是 遭遇 由 美国 次   贷 危机 引发 的 全球 金融 海啸 ， 由于 美国 等 消费国 居民 部门 加 杠杆 能力 在 危机 中 消失 殆尽 ， 中国 的 外贸 红利 大幅 消退 ; 为了 维持 合意 的 经济 增速 ， 当时 中央政府 推出 了 四 万亿 大规模 经济 刺激 计划 ， 经由 缺乏 财政 约   束 的 地方 政府 数倍 放大 ， 造成 了 另 一轮 大跃进 ， 资本 存量 年 平均 对数 增速 达到   12.83 % ; 但 由于 海外 市场 的   不景气 ， 巨大 产能 得不到 充分利用 ， 反而 形成 了 严重 产能 过剩 问题 与非 金融 企业 部门 债务 杠杆 率高企 问题 ，   全 要素 生产率 年 平均 对数 增长率 又 回落 到 - 1.99 % 。              ", "tags": "economics/investment2", "url": "/wiki/economics/investment2/intro.html"},
      
      
      
        
        
      
      {"title": "第1篇 导言", "text": "    Table   of   Contents           关于           第 1 章   世界 之旅           第 2 章   本书 概览                 关于           书名 : 《 宏观 经济学 》 原书 第五版       作者 : 【 美 】 奥利维 尔 · 布兰 查德       译者 : 楼永 　 孔 爱国           第 1 章   世界 之旅           在 研究 某个 经济体 的 时候 , 宏观 经济学家 首先 考虑 3 个 变量       产出 ,   整个 经济体 的 产出 水平 ( GDP ) 和 增长率       失业率       通货膨胀率               担心 两个 问题       生产率       贸易赤字               欧洲 在 20 世纪 70 年代 之前 ,   失业率 一直 低于 美国 , 然而 在 这 之后 开始 高于 美国       1999 年 , 欧盟 开始 推广 欧元 , 统一 的 货币 好处 是 , 消除 了 汇率 风险 ,   坏处 是 各国 都 使用 了 统一 的 货币政策           第 2 章   本书 概览           国民收入 和 产出 账户       GDP :   生产 和 收入       GDP 是 某 时期 经济 中 生产 的 最终 产品 和 服务 的 价值       「 最终 」 表示 不 包括 中间 产品 ,   比如 企业 1 生产 了 100 美元 钢材 卖 给 企业 2 , 企业 2 用来 生产 了 200 美元 汽车 又 卖 给 个人 了 , 中间 的 100 美元 钢材 不算 GDP 中 , 只算 最终 的 200 美元               GDP 是 某 时期 经济 中 增加值 的 总和       「 增加值 」 即 产品 的 价值 减去 中间 产品 的 价值 , 比如 上述 例子 汽车 的 增加值 是 200 - 100 = 100 美元               GDP 是 某个 时期 经济 中 的 收入 之 和       「 收入 」 并 不是 指 产品 卖出 的 价值 , 而是 只 增加值 的 那 一部分 , 包含 「 劳动收入 」 和 「 资本 收入 」 , 钢材 企业 的 100 元全 是 增加值 , 假设 企业 员工工资 是 80 美元 , 那么 「 劳动收入 」 就是 80 美元 , 剩下 的 20 美元 就是 企业 利润 , 也 就是 「 资本 收入 」 ; 汽车 企业 的 增加值 也 是 100 美元       间接税 / 增值税                       名义 GDP ,   最终 产品数量 乘以 当期 价格       实际 GDP ,   最终 产品数量 乘以   不变价格   ,   一般 报告 中 的 数字 都 是 指 实际 GDP ,   扣除 了 通胀 后 的 GDP       GDP 增长率 一般 都 是 指 实际 GDP 增长率       实际 GDP 的 计算       电脑 和 汽车 的 价格               失业率       20 世纪 40 年代 以前 , 都 是 采用 登记 失业率 ,   现在 都 是 通过 调查 失业率       失业 定义 是 指 , 没有 工作 并且 在 找 工作       没有 工作 也 不 找 工作 的 叫 「 非 劳动 人口 」               通货膨胀率       GDP 平减 指数       消费者 价格指数               西班牙 失业率 高达 24 % 的 秘密 , 太懒 了       以 货币 衡量 的 GDP   =   GDP 平减 指数   *   实际 GDP , GDP 平减 指数       GDP 平减 指数 可以 衡量 通货膨胀率 , 不过 衡量 的   生产   的 商品 通货膨胀率       CPI 也 可以 衡量 通货膨胀率 , 它 衡量 的 是   消费   的 商品 通货膨胀率       通常 情况 下 , 两者 差异 不 大 ,   但是 在 进出口 商品价格 变动 较大 的 时候 , 两者 差异 就会 变大       GDP 的 构成       消费 C       投资 I       政府 支出 G       净 出口 X - IM               GDP 构成 的 理解 :   从 GDP 的 定义 来看 , 它 是 经济 中 生产 的 最终 产品 和 服务 的 价值 , 从 最终 产品 和 服务 的 去向 来看 , 可以 分为 被 普通 消费者 消费 掉 了 ( 即 消费 ) , 被 企业 买 走 了 ( 投资 ) , 被 政府 买 走 了 ( 政府 支出 ) , 被 歪 果仁 买 走 了 ( 出口 ) , 由于 消费者 、 企业 、 政府 还 通过 进口 消费 掉 了 一些 价值 , 所以 也 要 减掉 , 从而 可以 把 进口 变成 净 出口      ", "tags": "economics/macroeconomics", "url": "/wiki/economics/macroeconomics/intro.html"},
      
      
      
        
        
      
      {"title": "第2篇 金融市场", "text": "    Table   of   Contents           理解 利率           现值 ( present   value )           四种 信用 市场 工具           到期 收益率           回报率           名义 利率 与 实际 利率                   利率 行为           利率 的 风险 和 期限 结构           风险 和 期限 结构           三种 理论                   股票市场 、 理性 预期 理论 和 有效 市场 假说           股票市场           理性 预期 理论           有效 市场 假说                         理解 利率       现值 ( present   value )       现值 ( 折 现值 )   是 为了 未来 的 现金流量 折算 成 当前 的 现金 价值 , 从而 可以 进行 比较 。       第 n 期 到期 的 现金流 CF 折现 的 现值 PV 为       $ $     PV   =   \ \ frac { CF } { ( 1 + i ) ^ n }     $ $       i 是 利率 / 收益率 等 。       直观 解释 :   n 年 后 给 你 CF 元 ,   市场 利率 为 i , 那么 这笔 钱 的 在 现在 的 价值 就是 现值 PV 。       四种 信用 市场 工具           普通 贷款 ,   到期 偿还 本息 :   普通 一次性 贷款       固定 支付 贷款 ,   定期 偿付 部分 本金 和 利息 :   例如 房贷 ,   分期 贷       息票 债券 ,   每期 偿还 利息 ,   到期 偿还 票 债券 面值 的 债券 :   国债       贴现 发行 的 债券 ,   没有 利息 ,   到期 偿还 票 债券 面值 的 债券 。 所以 一般 要 以 低于 票面价格 卖出 ,   一般 的 公司债券 都 是 这样           到期 收益率       将 债务 工具 未来 所有 的 现金流 CF 按照 收益率   i   折算 的 现值 如果 等于 价格 , 那么 i 就是 到期 收益率 。           一次性 借款 N 元 , 一年 后 偿付 本息 ,   假设 利息 为 i ,   那么 收益率 就是 i       如果 分 两年 ,   方案 一 , 每年 都 按照 利率 i 偿还 利息 , 到期 后 偿还 本金 ;   方案 二 , 到期 后 一次性 安 年利率 i 还本付息 ,   这 两种 收益率 还是 i 吗 ?   显然 第一年 的 利息 如果 存 银行 或 搞 理财 ,   第二年 还 可以 获得 额外 的 收益 ,   但是 第二年 的 利息 并 没有 这个 优势 , 这 说明 虽然 每年 的 利率 是 一样 的 ,   但是 第一种 方案 收益率 应该 高 一些 ,   这 正是 因为 现金 的 时间 价值 不 一样 。 在 这个 例子 中 ,   假设 每年 的 收益率 为 r ( 与 利率 i 区分 ) ,   对于 方案 一 , 第一年 的 利息 现值 为   $ (   i   N   /   ( 1 + r )   ) $ ,   第二年 的 利息 现值 为   $ (   i   N   /   ( 1 + r ) ^ 2   ) $ , 确实 不 一样 ,   第二年 的 本金 的 现值 为   $ (   N / ( 1 + r ) ^ 2   ) $ ,   根据 等式 , 未来 全部 的 现金流 的 现值 等于 价格 ( 也 就是 本金 ) ,   那么 有           $ $             N   =   \ \ frac { i   N   } { ( 1 + r ) }   +   \ \ frac { i   N }   { ( 1 + r ) ^ 2 }   +   \ \ frac { N } { ( 1 + r ) ^ 2 }           $ $       所以 到期 收益率 满足           $ $           1   =   \ \ frac { i }   {   1 + r   }   +   \ \ frac { 1 + i } { ( 1 + r ) ^ 2   }         $ $         可以 通过 解 这个 方程解 出 到期 收益率   $ ( r   =   i ) $ ,   也就是说     到期 收益率 是 把 债务 等效于 按照 每年 付息 ,   到期 偿还 本金 , 这种 还款 方式 的 利率   ! ! 再 来看 第二种 方案 ,   很 容易 列出 方程         $ $         N   =   \ \ frac { 2i   N }   { ( 1 + r ) ^ 2 }   +   \ \ frac { N } { ( 1 + r ) ^ 2 }           $ $         解 出         $ $         r   =   \ \ sqrt { 1   +   2i }   -   1   \ \ approx   i   -   i ^ 2   & lt ;   i         $ $         实际 收益率 是 比 年利率 i 小 的 。     -   等额 本息 ,   到期 收益率 案例 。 以 房贷 为例 ,   100 万元 本金 LV 借款 ,   假设 月利率 为 i ,   按照 每月 等额 本息 方式 还款 n 年 ,   每月 还 钱 金额 FP 计算 。         $ $         LV   =   \ \ frac { FP } { 1 + i }   +   \ \ frac { FP } { ( 1 + i ) ^ 2 }   +   \ \ frac { FP } { ( 1 + i ) ^ 3 }   +   ...   +   \ \ frac { FP } { ( 1 + i ) ^ { 12n } }         $ $         所以         $ $         FP   =   LV   /   ( \ \ frac { 1 } { 1 + i }   +   \ \ frac { 1 } { ( 1 + i ) ^ 2 }   +   \ \ frac { 1 } { ( 1 + i ) ^ 3 }   +   ...   +   \ \ frac { 1 } { ( 1 + i ) ^ { 12n } } )         $ $         以 LV = 1000000 元为例 , 年利率 为 4.9 % , 那么 月利率 i = 0.40833 % , 那么 可以 就算 出   FP = 5307 元               def       calc _ fp     (     lv     ,       i     ,       n     ) :           r       =       sum     (     1     /     pow     (     1       +       i     ,       k     )       for       k       in       range     (     1     ,       n     +     1     ) )           return       lv       /       r         if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :           print       calc _ fp     (     726000     ,       0.0325     /     12     ,       12     *     30     )       #   实际 值   3159.60           print       calc _ fp     (     940000     ,       0.0539     /     12     ,       12     *     30     )       #   实际 值   5272.48                 假设 以 等额 本金 方式 还款 ,   那么 每月 还款 金额 很 好 算 ,   就是 等额 本金 + 当月 的 剩余 借款 产生 的 利息 。         房贷 计算 参考   loan _ calc . py   。     -   息票 债券 ,   用 P 表示 息票 现期 价格 ,   C 表示 每期 支付 利息 ,   F 为 债券 面值 , 也 就是 到期 后 支付 的 金额 , 那么 有     $ $     P   =   \ \ frac { C } { 1 + i }   +   \ \ frac { C } { ( 1 + i ) ^ 2 }   +   \ \ frac { C } { ( 1 + i ) ^ 3 }   +   ...   +   \ \ frac { C } { ( 1 + i ) ^ { n } }   +   \ \ frac { F } { ( 1 + i ) ^ { n } }     $ $     C 与 F 的 比值 也 是 息票 利率 ,   如果 息票 现期 价格 P 等于 面值 F , 那么 息票 相当于 每期 还息 到期 还本 的 借款 ,   到期 收益率 就是 票面 利率 。 实际 中 , 价格 P 往往 和 面值 F 不 相等 , 如果 价格 P 低于 面值 , 那么 到期 收益率 显然 高于 票面 利率 。 这 说明 ,     息票 价格 P 与 到期 收益率 负相关   ! !     -   永续 债券 ,   就是指 n 等于 无穷的 息票 债券 ,   类似 于 固定 股息 的 股票 。 有       $ $     P   =   \ \ frac { C } { i }     $ $     可以 看到 ,   价格 跟 利率 成反比 ! !   这 也 相当于 一个 简单 的 股票价格 模型 ,   当 市场 无风险 利率 升高 了 ,   股东 要求 的 到期 收益率 也 会 相应 提高 ,   也 就是 i 会 提高 ( 这种 情况 下 容易 验证 到期 收益率 就 等于 票面 利率 , 你 可以 认为 F = P , 实际上 在 n 无限大 时 , 只要 F 为 有限 值 , 都 没有 影响 ) ,   那么 股票价格 就 会 下跌 ,   这样 才能 让 当下 的 买家 获得 相应 的 收益率 , 这样 交易 才能 完成 !     -   当 利率 下降 到 很 低 的 时候 ,   国库券 / 银行利率 可能 出现 负 利率 ! ! 因为 大额 的 资金 持有 现金 不 方便 。       回报率           对 任何 证券 来说 ,   回报率 ( R ) 既 包含 了 利率 ( ic ) 也 包括 价格 变动率 g ( 也 叫 资本 利得率 ) !     $ $     R   =   i _ c   +   g     $ $     不 分红 的 股票 就 只有 资本 利得率 g ,   价格 不变 的 债券 就 只有 利率 。 由于 价格 波动 可能 为 负 的 , 这会 导致 回报率 为 负值 ! !       利率 风险 :   债券 期限 越长 , 价格 变动 幅度 受 利率 影响 会越 大 , 这会 加大 长期 债券 的 投资 风险 !   股票 可以 认为 就是 一种 无限期 的 债券 !       距离 到期日 期限   和   持有 期限   相同 的 债券 是 没有 利率 风险 的 。 因为 它 的 回报 只有 利息 , 到期 时 的 价格 就是 约定 的 票面价格 , 也 就是 当初 买 的 价格 !           名义 利率 与 实际 利率       实际 利率 是 指 从 名义 利率 ( 票面 利率 ) 扣除 通货膨胀 之后 的 利率 。 费雪 方程 ( Fisher   Equation )       $ $     i   ( \ \ text { 名义 利率 } )   =   i _ r   ( \ \ text { 实际 利率 } )   +   \ \ pi ^ e   ( 通货膨胀 )     $ $       据此 , 我们 来 测算 一下 2008 年 - 2018 年 10 年 期间 ,   5 年期 定期存款 的 实际 利率 ,   2008 年时 3 年 以上 定期存款 的 基准利率 为 4.14 % ,   10 年间 年均 通货膨胀率 估计值 约 为 7 % ( 用 M2 增长率   -   GDP 增长率 ) , 这 表明 我们 的 名义 利率 为   - 2.86 % ! ! !   这 跟 我们 直观 感受 一致 ,   「 把 钱存 银行 ,   越存 越 没 钱 ! 」                   时间       M2 同比       国内 生产总值 - 同比       通货膨胀率                       2017 年       8.20 %       6.90 %       1.30 %               2016 年       11.30 %       6.70 %       4.60 %               2015 年       13.30 %       6.90 %       6.40 %               2014 年       12.20 %       7.30 %       4.90 %               2013 年       13.60 %       7.80 %       5.80 %               2012 年       13.80 %       7.90 %       5.90 %               2011 年       13.60 %       9.50 %       4.10 %               2010 年       19.70 %       10.60 %       9.10 %               2009 年       27.68 %       9.40 %       18.28 %               2008 年       17.82 %       9.70 %       8.12 %               平均       15.12 %       8.27 %       6.85 %                   数据 来源 :     http : / / data . stats . gov . cn / easyquery . htm ? cn = C01       注意事项 :   GDP 同比 增速 是 指 GDP 平减 指数 ,   如果 你 根据 GDP 实际 值算会 高 不少 ,   要 用 国家 公布 的 GDP 增长 数据 ,   那个 数据 是 已经 扣除 了 物价 之后 的 。     参考     中国 近 10 年 的 通货膨胀率 大概 在 多少 ？   -   江枫 渔火 的 回答   -   知乎       问题 :   为什么 算 通货膨胀 也 要 用 扣除 物价指数 之后 的 GDP 增长率 ?       名义 利率 无法 直接 观测 到 , 1997 年 , 美国财政部 发行 了   指数化 债券 TIPS ( 全称 通货膨胀 保值 债券 )   , 债券 的 利率 按照 物价水平 变化 进行 调整 。       利率 行为           由于 债券 价格 跟 利率 存在 负相关 ,   所以 可以 通过 债券市场 和 货币 市场 的 供求 分析 来 研究 利率 变动 机制 。 所以 关键 是 导出 资产 的 需求 曲线 和 影响 资产 需求 的 因素 。 在 本章 中 ,     只 考虑 一种 证券 和 一种 利率   ,   在 下 一章 中 会 考虑 多种 利率 之间 的 关系 ,   也 即 是 利率 的 期限 结构 。       资产 需求 的 决定 因素 :       财富       财富 增加 会 让 我们 拥有 更 多 资源 购买 更 多 的 资产 ,   所以 财富 的 增长 会 带来 资产 需求量 的 上升               预期 收益率       显然 在 其他 因素 不变 的 情况 下 ,   本 资产 预期 收益率 的 上升 会 导致 资产 需求 的 上升       而 其他 可 替代 资产 的 预期 收益率 的 上升 会 导致 对本 资产 的 需求 的 下降               风险       风险 越大 , 对 该 资产 的 需求 就 会 越 低               流动性       资产 变现 的 速度 叫 流动性 ,   其他 因素 相同 的 情况 下 ,   流动性 越高 ,   需求 越高                       资产 需求 变动       财富 的 变动       在 经济周期 的 扩张 阶段 , 经济 迅速 增长 , 财富 会 不断 增加 , 从而 导致 对 债券 的 需求 上升 。               预期 收益率 的 变动       如果 预期 收益率 会 提高 ,   会 降低 对 当前 债券 的 需求 。 注意 区分 预期 收益率 「 高 」 和 「 将会 变高 」 , 一个 说 的 是 值 , 一个 说 得 是 变化 , 因为 未来 会变 高 , 说明 当前 的 收益率 是 偏低 了 , 所以 会 降低 当前 对 债券 的 需求 。 预期 收益率 的 变化 影响 的 是 长期 债券 的 的 需求 , 而 短期 债券 ( 一年 内 的 ) 不 受 影响 , 因为 预期 收益率 的 变化 对 它们 的 收益率 没有 影响 , 对 长期 债券 来说 , 预期 收益率 的 提高 会 降低 未来 长期 债券 的 价格 , 所以 会 影响 它 的 资本 利 得 。               风险 变动       如果 债券市场 的 价格 波动 幅度 加大 , 那么 债券 投资 的 风险 水平 就 会 提高 , 因此 对 债券 的 需求 就 会 降低       相反 , 如果 股票市场 等 其他 资产 的 风险 增加 , 那么 债券 的 风向 相当于 降低 , 会 增加 对 债券 的 需求               流动性 变动       流动性 增加 会 增加 对 债券 的 需求       其他 资产 的 流动性 增加 会 降低 对 债券 的 需求       1975 年 , 美国 废除 了 股票 佣金 规则 , 大幅 降低 了 股票交易 过程 中得 劲 交易 费用 , 相当于 增加 了 股票 的 流动性 , 导致 了 债券 需求 的 降低                       债券 供给 的 变动       投资 的 预期 盈利 能力       在 经济 扩张 周期 中 , 经济 高速 增长 , 投资 的 预期 盈利 能力 也 会 相应 提高 , 会 增加 债券 的 供给               预期 通货膨胀率       预期 通货膨胀率 提高 , 利率 却 没变 , 那么 真实 的 借款 成本 会 下降 , 会 增加 债券 的 供给       费雪 效应 :   预期 通货膨胀率 的 变化 影响 名义 利率 。 预期 通货膨胀率 的 提高 , 增加 了 借款 成本 , 从而 增加 了 债券 的 供给 ,   在 需求 不变 的 情况 下 ,   导致 了 债券 的 均衡 利率 上升 。   因为 只有 给 更 多 的 利率 , 才能 增加 需求量 , 债券 才 发 的 出去 。       经济 扩张 和 收入 提高 会 导致 利率 的 上升 ?       日本 低利率 的 解释 :   90 年代 末 21 世纪 初 ,   日本 出现 了 极 低利率 , 在 1998 年 11 月 , 出现 了 负 利率 。 这 是因为 经济衰退 ,   出现 了 通货紧缩 和 负 利率 。 通货紧缩 会 增加 对 债券 的 需求 , 因为 投资收益 降低 , 债券 收益 稳定 , 这 导致 债券 需求 的 增加 ;   另一方面 ,   通货紧缩 增加 了 实际 利率 , 导致 债券 的 供给 降低 。 需求 增加 , 供给 降低 , 从而 导致 债券 价格 的 提升 , 利率 的 降低 。       过低 的 利率 甚至 负 利率 是 经济 出现 严重 问题 的 标志 , 并 不是 好事 。               政府 预算       财政赤字 导致 政府 债券 的 供给 增加                       凯恩斯 流动性 偏好 理论       人们 利用 储藏 财富 的 资产 主要 包括 货币 和 债券 。 经济运行 的 过程 中 , 财富 总量 等于 二者 之 和 。 一种 资产 的 需求量 降低 , 必然 提高 另 一种 资产 的 需求量 。 货币 不 包括 不 支付 利息 的 现金 。       利率 上升 会 增加 货币 的 需求 , 导致 债券 需求 降低 , 债券 价格 就 会 降低 。       货币 需求 的 变化       收入 效应 :   经济 扩张 和 收入 增加 , 人们 愿意 持有 更 多 的 货币       ? ? ? ?               物价水平 效应 :   物价水平 上涨 , 会 导致 人们 持有 更 多 的 货币               货币 供给 的 变动       央行 货币 供应量 的 提高 将 增加 货币 供给               收入水平 的 变动 : 经济 扩张 周期 , 收入水平 提高 ,   相同 利率 下 ,   作为 价值 储藏 的 货币 需求 也 会 提高 , 从而 导致 利率 上升 ?       物价水平 的 变动 : 物价水平 提高 导致 相同 利率 下 的 货币 需求 的 提高 , 从而 利率 上升       货币 供应量 的 提高 将会 导致 货币 供应 的 提高 , 从而 利率 下降       货币 与 利率 的 负相关 ?   诺贝尔经济学奖 得 住 弗里德曼 提出 了 重要 的 批判 ,   货币 供应量 的 提高 , 会 降低利率 , 这 确实 没错 , 他 将 这种 效应 叫做 流动性 效应 。 然而 , 他 认为 流动性 效应 只 反映 了 货币 和 利率 关系 的 一部分 , 货币 供应量 的 提高 还会 改变 其他 变量 , 具有 额外 的 效应 。       收入 效应 :   货币 供应量 的 提高 会 对 经济 产生 扩展 的 影响 ,   从而 增加收入 和 财富 水平 ,   这会 导致 利率 的 上升       物价 效应 :   货币 供应量 的 提高 会 导致 物价水平 的 上升 ,   从而 增加 相同 利率 下 的 货币 的 需求 ,   提高 利率 ,   物价 效应 并 不会 马上 生效       预期 通货膨胀 效应 :   货币 供应 的 提高 会 导致 预期 通货膨胀率 的 上升 ,   从而 提高 名义 利率 。   预期 通货膨胀 效应 会 马上 生效       实际 情况 是 ,   流动性 效应 很 弱 ,   物价 效应 和 通货膨胀 效应 占 主导               我国 币 供应量 与期 国债 3 月 收益率 曲线 如下 图 ,   可以 看到 09 年 大 放水 , 导致 利率 的 大幅 降低 , 从 3.5 降低 到 1 。 ( M2 看 左轴 , 收益率 看右 轴 )                         利率 的 风险 和 期限 结构       风险 和 期限 结构           利率 的 风险 结构 :   违约 风险 ,   流动性 ,   所得税       相同 期限 ,   风险 越高 的 债券 , 利率 越高 , 额外 的 利差 也 叫 风险 溢价   risk   premium       信用 评级 机构 提供 债券 的 评级 ,   在 美国 , 低于 Baaa 级别 的 债券 也 叫 垃圾 债券 , 利率 高       流动性 :   交易量 越多 的 债券 流动性 越 好 ,   国债 的 交易量 最 多 , 流动性 最好 。       公司债券 和 国债 的 利差 不仅仅 反映 风险 , 还 反映 了 流动性 。 所以 风险 溢价 更 确切 的 是 「 风险 和 流动性 溢价 」       所得税 因素 , 美国 投资 市政 债券 获得 的 利息收入 可以 免交 联邦 所得税                         三种 理论               利率 的 期限 结构           将 那些 具有 相同 风险 、 流动性 和 税收 特性 而 期限 不同 的 债券 收益率 连成 一条 曲线 , 成为 收益率 曲线 ( 如下 图 所示 的 2019 - 01 - 04 日 的 国债 收益率 曲线 )             如果 短期 利率 较 低 , 收益率 曲线 通常 向上倾斜 ; 如果 短期 利率 较 高 , 那么 收益率 曲线 更 多 是 向下倾斜 , 即 反转 的 收益率 曲线 。       反转 的 收益率 曲线 比较 少见 , 可以 从 不同 期限 的 债券 利率 随 时间 变化 图中 看到 , 短期 债券 利率 高于 长期 债券 利率 只有 少数 的 几天 。             不同 债券 的 利率 会 随着 时间 的 推进 , 出现 相同 的 变动 趋势 ( 见 下图 ) 。                         预期 理论 , 市场 分割 理论 , 流动性 溢价 理论 是 用来 解释 上述 期限 结构 的 三重 理论 。           预期 理论 基本 假设 :   长期 债券 的 利率 等于 在 长期 债券 到期 期限内 短期 利率 的 平均 预期 值 。       所以 , 如果 短期 利率 过低 , 那么 短期 利率 很 有 可能 低于 未来 一段时间 的 短期 利率 平均值 ,   所以 短期 债券 利率 低于 长期 债券 利率       默认 假设 , 债券 购买者 对 不同 期限 的 债券 没有 任何 偏好 差异 ,   相当于 不同 期限 的 债券 可以 完全 替代 ,   这 与 实际 不符 。       假设 未来 5 年内 该 债券 的 利率 期望值 为   5 % , 6 % , 7 % , 8 % , 9 % ,   那么 5 年期 的 该 债券 利率 为 平均值   7 % 。       这个 理论 的 一个 推论 是 , 只有 在 短期 利率 的 预期 值 上升 的 条件 下 , 现期 的 长期 利率 才 会 高于 短期 利率 , 而 实际上 , 长期 利率 几乎 总是 高于 短期 利率               市场 分割 理论 :   认为 市场 上 不同 期限 的 债券 之间 完全 不可 替代 , 相当于 另外 一个 极端       由于 通常 情况 下 , 长期 债券 的 需求量 较 小 , 其 价格 较 低 而 利率 较 高 , 所以 收益率 曲线 通常 是 向上倾斜 的 。 它 解释 了 预期 理论 无法解释 的 这个 问题       但是 无法解释 短期 利率 和 长期 利率 具有 相同 的 变动 趋势       无法解释 短期 利率 的 变动 对 长期 债券 供求关系 所 产生 的 影响 ,   也 就是 在 短期 利率 很 高 的 时候 , 收益率 曲线 会 出现 反转               流动性 溢价 理论 :   结合 预期 理论 和 市场 分割 理论 ,   长期 债券 的 利率 等于 长期 债券 到期 期限内 的 短期 利率 平均值 加上 随着 该 债券 供求 状态 变动 而 改变 的 流动性 溢价 ( 也 较 期限 溢价 ) 。       它 假设 不同 期限 的 债券 可以 互相 替代 ( 跟 预期 理论 相同 ) ,   因此 短期 利率 的 变动 会 影响 长期 利率 。 但是 该 理论 允许 投资者 对 不同 期限 的 债券 存在 偏好 , 比如 更 偏向 于 短期 债券 , 也 就是 流动性 高 的 债券 , 而 长期 债券 的 低 流动性 需要 一个 流动性 溢价 , 牺牲 流动性 的 补偿 。       因为 流动性 溢价 的 存在 , 导致 长期 债券 通常 都 是 高于 短期 债券 的       如果 短期 债券 利率 突然 特别 高 ,   比如 在 2013 年 06 月底 、 2013 年 年底 和 2017 年底 有 少数 几天 , 短期 利率 突然 暴涨 , 长期 债券 即使 有 流动性 溢价 , 也 没能 超过 短期 利率 。               利率 期限 结构 的 实证 研究       由于 收益率 曲线 包含 了 未来 利率 的 预期 值 的 信息 ,   所以 可以 使用 收益率 曲线 来 预测 通货膨胀率 和 经济周期 的 未来 情况       经济繁荣 时期 , 利率 随之 上升 , 而 在 经济衰退 时期 , 利率 随机 下降 。 平坦 或者 向下倾斜 的 收益率 曲线 意味着 未来 短期 利率 预期 值 将 会 下降 , 经济 往往 处于 衰退 阶段       陡峭 的 收益率 曲线 意味着 货币政策 处于 宽松 状态 , 而 平摊 或者 向下 的 收益率 曲线 意味着 货币政策 处于 紧缩 状态 。       下图 计算 了 1 年期 以内 的 短期 国债 利率 平均值 与 1 年期 以上 的 长期 国债 利率 平均值 之间 的 差值 ,   不妨 把 这个 差值 叫做 期限 溢价 ( 图中 的 利率 先 按 月 平均 了 )             可以 看到 2009 年 , 溢价 达到 2 个点 , 正是 四 万亿 大 放水 的 货币 宽松 ;   类似 的 还有 2015 年 年 中 和 2007 年 年 中 , 分别 对应 A股 的 两次 大牛市 ;   而 在 2017 年 期间 , 溢价 处于 很 低 的 值 ,   正是 收紧 信贷 降杆 杆 的 期间 。       大 放水 的 时候 ,   短期 收益率 会 很 低 , 长期 收益率 一般 变化 比较 缓慢 , 所以 产生 了 很大 的 期限 溢价 ;   而 在 牛市 期间 , 国债 的 收益 过低 , 大家 都 拿 钱 炒股 , 所以 收益率 曲线 就会 很平 , 甚至 如果 短期 流动性 较 低时 出现 反转 , 此时 的 期限 溢价 就 很 低 。 可以 参考     基础知识 讲堂 ： 收益率 曲线 ， 如何 用 它 预测 经济 大势 ？         但是 收益率 曲线 貌似 是 政策 和 市场 行为 的 结果 反映 , 用来 预测 是不是 不太 现实 , 预测 牛市 逃顶 ? 哈哈                   股票市场 、 理性 预期 理论 和 有效 市场 假说       股票市场           普通股 股票价格 的 计算       股票 的 两个 重要 权利 :   投票权   和   对 流入 公司 全部 资金 的 剩余 求偿 权 。       任何 投资 的 价值 都 可以 通过 计算 该项 投资 在 其 生命周期 内 产生 的 全部 现金流 的 现值 来 衡量       单期 估值 模型 :     $ $     P _ 0   =   \ \ frac { Div _ 1 } { 1   +   k _ e }   +   \ \ frac { P _ 1 } { 1   +   k _ e }     $ $     其中 P0 是 当期 的 估值 , P1 是 预期 到期 后 的 价格 ,   Div1   是 该期 的 分红 ,   ke 是 要求 的 收益率 。 如果 当期 股价   不 高于   你 的 估值 P0 , 那么 股票 就 有 买入 价值 。       n 期 估值 模型     $ $     P _ 0   =   \ \ frac { D _ 1 } { 1   +   k _ e }   +   \ \ frac { D _ 2 } { ( 1   +   k _ e ) ^ 2 }   +   ...   + \ \ frac { D _ n } { ( 1   +   k _ e ) ^ n }   +   \ \ frac { P _ 1 } { ( 1   +   k _ e ) ^ n }     $ $     如果 n 很大 的 时候 , 最后 一项 趋近 于 0 , 可以 忽略 ,   那么     $ $     P _ 0   =   \ \ sum _ { i = 1 } ^ { \ \ infty } \ \ frac { D _ i } { ( 1   +   k _ e ) ^ i }       $ $     因为 在 这个 无穷 级数 中 ,   Di   未知 难以 直接 求和 , 所以 有 一些 根据 对 Di 的 一些 假设 的 计算 模型       戈登 增长 模型 ( Gordon   growth   Model ) :   假设 红利 具有 固定 不变 的 增长率       红利 增长率 一直 很定 不变       红利 增长率 必须 低于 要求 收益率 ke       $ (   D _ i   =   D _ 0 ( 1 + g ) ^ i   ) $ ,   g 是 红利 增长率       $ (   P _ 0   =   \ \ frac { D _ 1 } { k _ e   -   g } ) $                       市场 中 股票价格 的 确定       价格 是 由 愿意 出具 最高 价格 的 购买者 来 确定 的       价格 是 由 该 资产 使用 效率 最高 的 购买者 来 确定 的 ,   因为 使用 效率高 的 人 对 资产 的 定价 会 更 高       信息 在 资产 定价 中 发挥 重要 的 作用 ,   如果 你 有 一些 别人 不 知道 的 信息 ,   那么 你 更 有 可能 出 更 合理 的 价格 。 对于 不 了解 公司 的 人 来说 , 对 公司 的 风险 具有 更 不确定性 , 所以 他们 要求 的 收益率 ke 会 比较 高 。 相反 , 如果 你 知道 公司 的 一些 运营 信息 , 你 要求 的 收益率 ke 会 比较 低 , 所以 你 的 出价 会 更 高 。       新 的 信息 会 导致 对 该 股票 的 红利 水平 和 风险 水平 被 重新 评估 ,   股价 也 被 重新 评估       货币政策 对 股价 的 影响       央行 利率 的 降低 , 导致 投资人 对 ke 要求 降低 ,   股价 自然 会涨 ;   相反 加息 会 降低 股价       次贷 危机 增加 了 股票 的 风险 , 导致 投资人 对 ke 要求 提高 ,   股价 也 因而 降低                           理性 预期 理论           适应性 预期 认为 某个 指标 ( 比如 通货膨胀率 ) 的 预期 值 被 认为 是 过去 该 指标 的 平均值 ,   这种 方法 被 上个世纪 五六十年代 的 经济学家 使用 , 但 后来 被 认为 是 错 的 。       约翰 · 穆斯 发展 出 理性 预期 理论 ,   该 理论 认为 :   预期 结果 将会 等于 使用 全部 信息 获得 的 最优 的 预测 结果 , 不仅仅 是 历史 的 信息       理性 预期 的 解释 :   实际上 就是 我们 所说 的 模型 的 上限 。       如果 存在 一个 十分 重要 的 额外 因素 , 但是 无法 活动 相关 信息 , 那么 没有 考虑 这一 因素 的 预期 结果 依然 是 理性 的 。 比如 所有人 都 不 知道 下个月 央行 加息 , 那么 现有 的 价格 预期 仍然 是 理性 预期 。 因为 这个 事情 在 当下 , 任何人 都 不 知道 。       理性 预期 理论 的 理论 基础 ,   为什么 人们 会 使用 他们 知道 的 所有 信息 预测 得到 最好 的 结果 ,   因为 如果 不 这么 做 , 就 会 付出 很 高 的 代价 。 比如 遭受 经济损失 ! 因而 , 在 金融市场 中 , 用 全部 信息 预测 最优 预测 结果 动机 十分 强烈 , 其 结果 是 「 有效 市场 假说 」 。           有效 市场 假说           其实 只是 理性 预期 理论 在 金融市场 中 证券 定价 问题 中 的 应用 结果 而已       前提 :   金融市场 汇总 的 证券 价格 能够 完全 反应 全部 可用 信息 这一 迁移 假设 基础 之上       理论 基础       套利 :   指 市场 参与者 消除 未 使用 的 盈利 机会 ( 信息 ) 的 过程 。       纯 套利 :   盈利 机会 不 包含 风险       普通 逃离 :   存在 一定 风险               有效 市场 假说 也 可以 表达 为 :   在 有效 市场 中 ,   所有 未 利用 的 盈利 机会 都 会 被 消除               强势 版本 :   在 有效 市场 中 , 所有 的 价格 都 是 正确 的 , 并且 反应 了 市场 基本面 ( 即 能够 对 证券 未来 收益 数额 产生 直接 影响 的 要素 ) 。       有效 市场 假说 对 投资 的 应用 :       《 旧金山 纪事报 》 将 猿猴 选 的 股票 和 股票 专家 选 的 股票 对比 他们 的 绩效 表现 , 发现 难 分 胜负 。       如果 专家 公布 了 是 有 短缺 将 不断 加剧 ,   你 应该 用 全部 积蓄 来 投资 石油 股票 吗 ?       根据 有效 市场 假说 , 我们 不能 期望 超过 正常 水平 的 高 收益率 , 已经 公布 的 信息 的 结果 已经 反映 在 当前 股价 中 了 , 所以 我们 无法 获得 额外 的 收益 。       绝大多数 市政 经验 表明 ,   投资 分析 专家 提供 的 建议 不能 保证 我们 获得 超过 市场 正常 水平 的 投资 绩效 。 ( 在 有效 市场 中 ,   A股 要 除外 吧 ,   各种 内幕 )               一个 幻想 致富 的 人 发明 一种 高明 的 骗术 , 每周 都 会 写 两份 信 ,   在 信 A 中 , 他 预测 A 对能 在 足球比赛 中 获胜 ,   在 信 B 中 , 他 预测 B 对能 获胜 。 然后 他 把 用户 的 地址 分为 两组 , 一组 发 信件 A , 一组 发 信件 B 。 在 下 一周 中 , 他 只会 对 上周 预测 正确 中 的 用户 中 , 重复 上述 步骤 。 在 进行 10 场 比赛 之后 , 就 会 有 一部分 10 场 都 预测 正确 的 人 。 随后 他 向 这些 人发 最后 一封信 , 声称 他 是 足球 预测 转件 , 除非 他们 支付 大笔 费用 , 他 才 会 继续 提供 预测 结果 。 这 说明 , 即使 我们 无法 精确 预测 市场 结果 , 市场 中 依然 会 有 一些 持续 成功 的 人 。       你 应该 信 一些 小道消息 吗 ?   有效 市场 假说 告诉 我们 , 小道消息 也 已经 反映 在 价格 中 了 。 除非 你 是 最早 得知 这一 消息 的 人 , 你 才能 获得 超过 正常 水平 的 高额 收益 。       好消息 总能 促使 股价 上涨 吗 ?   当 市场 已经 预期 到 这种 好消息 了 , 那么 股价 不会 发生变化 ,   只有 公布 未知 的 信息 才 会 导致 股票价格 发生变化 。 比如 不及 预期 , 或 高于 预期 。 和 预期 一致 不会 使 股票 发生 任何 变化       有效 市场 假说 为 投资者 提供 的 建议 ,   投资人 ( 我们 中 的 绝大多数 人 都 属于 此类 ) 不 应该 视图 通过 频繁 地 买卖 证券 来 获得 超过 市场 正常 水平 的 收益 ,   除了 能够 增加 交易 佣金 外 没有 任何 意义 !         对 小型 投资者 而言 , 他们 相对 于 投资规模 的 投资 组合 管理 成本 较 高 , 所以 购买 某种 共同 基金 投资 工具 而 不是 购买 单只 股票 是 一种 合理 的 策略 。 并且 , 有效 市场 假说 告诉 我们 , 没有 任何 基金 能够 长久 地 获得 超过 市场 正常 水平 的 收益 , 所以 应该 买 低 佣金 费用 的 共同 基金 产品 。       经济 基本面 之外 的 某些 因素 也 可能 对 股价 产生 影响               行为 金融 :       如果 股票价格 上涨 到 非理性 水平 时 , 要 将 他们 拉 回 基本面 的 水平 , 需要 有 做 空 机制       实际上 人们 很少 做空 ,   损失 厌恶              ", "tags": "economics/money-banking-and-financial-market-book", "url": "/wiki/economics/money-banking-and-financial-market-book/financial-market.html"},
      
      
      {"title": "第3篇 金融机构", "text": "    Table   of   Contents           金融 结构 的 经济学 分析           8 个 谜团           交易成本           信息 不 对称 : 逆向 选择 和 道德风险           次品 车 问题 :   逆向 选择 影响 金融 结构           道德风险           道德风险 如何 影响 债务 市场 的 金融 结构           其他                   金融危机 与 次贷 崩溃           导致 金融危机 的 因素           历史 上 美国 金融危机 发生 机制                   银行业 与 金融机构 管理           商业银行 资产 负债表           银行 的 基本 业务           银行 管理 基本 原则           信用风险 管理           利率 风险管理           表外 业务 活动                   银行 监管 的 经济学 分析           信息 不 对称 与 银行 监管           20 世纪 80 年代 美国 初代 协会 危机 和 银行 危机           世界 范围 内 的 银行 危机                   银行业 :   结构 与 竞争           银行 体系 的 发展 历史           金融 创新 和 \" 影子 银行 体系 \" 的 发展           银行 并购 与 全美 范围 的 银行业           储蓄机构 :   监管 和 结构           国际 银行业务                   非 银行业 金融机构           保险业           证券市场 机构           共同 基金           对冲 基金           私募 股权 投资 基金 和 风险投资 基金           政府 金融机构           思考题                   衍生 金融工具           避险           金融 期货 合约 和 期货市场           期权           思考题                   金融 行业 内 的 利益冲突           利益冲突 及其 重要性                         金融 结构 的 经济学 分析       8 个 谜团           对 工商企业 来说 ,   股票 不是 最 重要 的 外部 融资 渠道 ,   1970 - 2000 年 美国 企业 的 外部 融资 中 , 股票市场 的 资金 只 占 了 11 %       企业 融资 活动 中 ,   发行 可 流通 债务 工具 和 股权 证券 不是 最 主要 的 融资 方式 ,   在 美国 , 债券 是 比 股票 更加 重要 的 融资 来源 , 占 了 32 % 。 在 其他 国家 , 外部 融资 ( 股票 + 债券 ) 比 美国 低 很多 ,   即使 在 美国 , 也 没 超过 50 %       与 直接 融资 ( 从 贷款 这 那 直接 融资 ) 相比 , 间接 融资 , 即 有 金融 中介机构 的 融资 , 其 重要性 要 大得多 。       金融 中介机构 ,   企业 外部 融资 的 首要 来源 是 有 银行 以及 诸如 保险公司 、 养老 基金 和 金融公司 等 非银行 金融 中介机构 所 提供 的 贷款 。       在 各种 经济 部门 中 , 金融体系 是 收到 最为 严格 监管 的 部门 之一 。 主要 是 为了 促进 信息 披露       只有 那些 规模 庞大 、 信用 卓著 的 公司 才能 易于 从 证券市场 上 获得 融资 , 而 其他 公司 和 个人 只能 找 银行贷款       对 家庭 和 企业 而言 , 抵押品 是 债务 合约 的 一个 普遍 特征       典型 的 债务 合约 是 极其 复杂 的 法律 文件 , 他 对 借款者 的 行为 施加 了 严格 的 限制           交易成本           小额 资金 的 交易成本 高 , 甚至 达 不到 认购 门槛       资金 太 少 , 无法 做到 投资 组合 以 分散 风险 ,   从而 投资 风险 更高       利用 金融 中介机构 , 可以 将 很多 人 的 少量 资金 汇总 , 从而 降低 交易成本       规模 经济 ( 共同 基金 ) , 比如 余额 宝 , 货币基金 , 债券 基金 , 股票 基金 etc   将 个人 的 小钱 汇总 成 大钱 , 货币基金 可以 投资银行 间 拆借 市场 , 但是 个人 的 少量 资金 门 都 进不了 , 也 无法 获得 拆借 市场 的 高 收益 ;   此外 通过 IT 技术 , 可以 将 单笔 交易成本 进一步 降低       专门 技术 ,   比如 余额 宝 可以 让 你 投资 到 货币基金 的 流动性 增强 , 随时 可取                   信息 不 对称 : 逆向 选择 和 道德风险           信息 不 对称 是 指 交易 一方 对 另外 一方 缺乏 充分 的 了解 , 从而 其 在 交易过程 中 难以 做出 准确 决策 。 例如 , 一个 未上市 的 公司 或者 陌生人 找 你 借钱 , 由于 你 缺乏 对 其 的 了解 和 信任 , 你 不 知道 借出去 是否 有 对应 的 收益 , 风险 多大 , 从而 为了 规避 风险 , 你 会 选择 不借 。       信息 不 对称 会 导致 逆向 选择 和 道德风险 。       逆向 选择 是 交易 之前 的 信息 不 对称 :       越 劣质 的 贷款人 越 愿意 承诺 更 高 的 收益率 , 从而 导致 正真 优质 的 借款人 无法 通过 低利率 借到 钱 , 使得 市场 上 都 是 高风险 的 借款人 , 也 就是 常说 的 「 劣币逐 良币 」 , 也 就是 这里 所说 的 「 逆向 选择 」 。 这种 时候 , 对 出借 人 来说 , 由于 市场 都 是 劣质 借款人 , 所以 会 拒绝 借钱 , 从而 导致 借贷 市场 无法 有效 地 运转 起来 。               道德风险 是 交易 之后 的 信息 不 对称       一旦 你 将 钱 借给 借款人 后 , 那么 他们 会 由于 使用 别人 的 钱 , 从而 可能 从事 高风险 的 活动 。                   次品 车 问题 :   逆向 选择 影响 金融 结构           二手车 市场 中 , 买家 由于 对 车子 信息 了解 不够 充分 , 只会出 一个 次品 车 和 好车 之间 的 一个 价格 。 而 对 卖家 来说 , 对 自己 的 车 了解 得 非常 充分 , 所以 如果 他 知道 自己 的 车 是 次品 车 , 那么 会 非常 乐意 以 这个 价格 卖掉 ; 如果 是 好车 , 他 可能 不 愿意 折价 卖掉 。 这种 逆向 选择 导致 市场 上 很少 会 有 良好 的 二手车 出售 。 由于 市场 上 二手车 平均 质量 低下 , 几乎 没有 人 愿意 买 次品 车 , 该 市场 的 成交量 会 很小 。       股票市场 和 债券市场 中 的 「 次品 车 」       垃圾 公司 的 股票 和 垃圾 债券 就是 股票 和 债券 的 次品 车 , 由于 逆向 选择 , 那些 真正 好 公司 的 股票 和 债券 就 很 难以 合理 的 价格 出售       因为 逆向 选择 的 问题 , 所以 才 有 问题 1 - 2 , 股票 和 债券市场 并 不是 主要 融资 渠道               逆向 选择 解决办法       通过 中介机构 解决 , 二手车 中介 可以 对车 进行 评估 , 从而 让 所有人 都 能 辨别 好车 和 坏车 , 消除 信息 不 对称       借款 方向 资金 供应方 提供 足够 的 详细情况 , 已 消除 信息 不 对称 的 影响 。 设立 私人 公司 , 专门 负责 收集 和 生产 能够 区别 优质 公司 和 劣质 公司 的 信息 , 然后 卖 给 证券 的 购买者 。 美国公司 代表 :   标准普尔公司 ,   穆迪 公司 ,   价值 线   etc       搭便车 问题 :   私人 生产 和 销售 的 信息 体系 会 导致 那些 没有 购买 的 人 可以 搭便车 。 因为 , 他们 可以 跟着 那些 购买 了 信息 的 人 买 。 这 使得 , 最终 不会 有人 买 这些 信息 了 。       政府 监管 :   由 政府 通过 严刑峻法 强制 要求 公司 公布 真实 信息 。 监管 机构 :   证券交易 委员会 。 要求 公司 按照 标准 会计准则 披露 公司 的 销售 、 资产 和 收益 等 信息 。 然而 , 这种 披露 还是 存在 问题 , 各种 内部 交易 等 丑闻 表明 , 这种 监管 在 巨大 的 利益 面前 , 还是 有 一定 的 局限性 。 ( 问题 5 )       金融 中介机构 ( 比如 银行 ) 可以 生产 公司 信息 , 分辨 公司 的 优劣 , 从而 可以 选择 只 将 资金 贷给 优质 公司 。 银行 之所以 可以 避免 搭便车 问题 , 因为 他们 是 通过 发放 私人 贷款 , 而 不是 购买 公开市场 上 交易 的 证券 。       随着 法律法规 的 完善 , 获取 企业 信息 难度 的 降低 , 银行 的 作用 就 会 被 削弱 。       大 公司 通常 比小 公司 在 市场 上 的 信息 越 多 , 加上 信誉 也 更好 , 所以 发行 证券 融资 也 越 容易 。 所以 越大 的 公司 , 就会 越少 地 使用 中介 结构 间接 融资 ( 问题 6 )       抵押品 和 净值 ,   通过 抵押品 , 借款者 更 愿意 提供 借款 , 以 解决 信息 不 对称 的 问题 ( 谜团 7 ) 。 公司 净值 越高 , 违约 风险 就 越 小 , 所以 高 净值 公司 越 容易 借到 钱 。 「 只有 不 需要 钱 的 人才 能 借到 钱 ! 」                   道德风险           道德风险 是 交易 完成 之后 的 信息 不 对称 问题 。 它 是 指 证券 的 销售者 具有 一种 隐瞒 信息 , 并 从事 那些 违背 证券 购买者 意愿 的 活动 的 动力 。       委托 - 代理 问题 :   如果 经理人 只有 公司 的 一小部分 股权 ,   而 拥有 大部分 股权 的 股东 不是 公司 的 管理者 。 这种 所有权 和 控制权 的 分离 包含 了 道德风险 :   由于 经理 们 没有 股东 那样 追求 利润 最大化 的 动力 , 所以 经理人 可能 会 按照 自己 的 利益 而 不是 股东 利益 行事 。 比如 从事 更 高风险 的 活动 。 比如 , 本来 你 借钱 给 对方 办 公司 扩大 生产 , 而 对方 却 拿 去 炒股 投机 了 , 从而 提高 了 违约 风险 。 简单 地说 , 就是 经理人 挪用 公司 资金 干 自己 的 事情 。 比如 将 公司 的 钱 投资 , 投资收益 归 自己 , 等等 。 之所以 出现 这种 问题 , 是因为 管理者 拥有 公司 更 多 的 信息 , 而 大 股东 处于 信息 不 对称 的 信息 缺乏 方 。       委托 - 代理 问题 解决 方法 :         信息 生产 : 监督   ,   经常 对 公司 进行 审计 !   不过 同样 会 存在 搭便车 问题 ,   所以 私人 股东 对 公司 审计 会 导致 被 人 搭便车 , 从而 没有 人 愿意 出钱 对 公司 审计 。       政府 监督 :   会计 审计 ,   上市公司 都 需要 通过 独立 审计 !       金融 中介机构 :   风险投资 公司 (   v   enture     c   aptital   firm ) ;   VC   在 投资 公司 后 会 持有 股权 , 通常 会 坚持 派 其 成员 进入 新 企业 的 管理层 ,   以便 严厉 地 监管 公司 的 活动       债务 合约 :   规定 公司 定期 支付 固定 金额 的 契约型 合约 。 这样 , 管理者 隐藏 利润 只要 不 影响 还 钱 , 问题 都 不大 。 这种 不 需要 经常 监督 公司 活动 , 从而 导致 核实 成本低 的 有点 , 所以 筹资 活动 中 更 多 地 选择 债务 合约 而 不是 股权 合约 。 也 解释 了 第 1 个 问题 , 更 多 的 发行 债券 而 不是 股权 。                   道德风险 如何 影响 债务 市场 的 金融 结构           债务 合约 中 解决 方法       净值 和 抵押品 ,   比如 住房 抵押 贷款       限制性 条款 的 监督 和 强制执行 :   限制性 条款 , 不能 将 借 去 的 钱 拿 去 干 别的 事情 , 如果 违背 , 就 强制执行 条款 中 约定 的 内容 。 规定 借款 公司 必须 维持 与 公司 规模 相对 应 的 某种 资产 最低 持有 量       抵押品 保值 条款 :   汽车贷款 要求 车主 必须 购买 最低 金额 的 保险 , 从而 保证 在 偿还 贷款 之前 被 转卖       提供 信息 的 条款 :   要求 公司 按照 季度 会计报表 和 收入 报表 的 形式 来 定期 提供 与其 活动 相关 的 信息         债务 合约 需要 复杂 的 限制性 条款 来 降低 道德风险                     其他           欠发达 的 国家 金融体系 处于 欠发达 状态 ( 金融 抑制 ) , 导致 公司 更 多 采用 银行 等 中介 结构 筹资 , 而 直接 通过 证券 融资 比例 比 发达国家 小 很多 。 这个 指标 是否 可以 用来 衡量 一个 国家 的 金融体系 完善 程度 ?           由 落后 的 法律 体系 、 不 完善 的 会计准则 、 政府 监管 的 缺失 、 通过 指导性 信贷 计划 实现 的 政府 干预 以及 银行 的 国家 所有制 等 因素 形成 的 制度 环境 有助于 揭示 国家 之间 存在 贫富 差异 的 原因 。                 http : / / www . csrc . gov . cn / pub / newsite / yjzx / sjdjt / zbsczdjcyj / 201505 / t20150514 _ 276935 . html                 金融危机 与 次贷 崩溃               标准 普尔 指数 近 20 年 的 走势图 , 可以 看到 有 两个 较大 下跌 的 区间 , 第一个 是 2000 - 2002 年 附近 的 互联网 泡沫 , 另 一个 就是 2008 年 的 美国 次贷 危机 。               国内 市场 也 有 类似 的 趋势 ,   第一个 是 2008 年 受 美国 次贷 危机 导致 的 大跌 ,   第二个 是 2015 年 的 股灾 。       导致 金融危机 的 因素           在 金融危机 中 ,   由于 金融 系统 的 瓦解 , 导致 信息 不 对称 问题 加剧 ,   产生 了 十分 严重 的 逆向 选择 和 道德风险 问题 ,   从而 使得 金融市场 无法 向 资金 从 储蓄 者 转移 到 哪些 具有 生产 投资 机会 的 居民 和 企业 手中 ,   经济 活力 将会 急剧 紧缩 。       资产 负载 表 的 资产 市场 效应       股票市场 下跌 ,   导致 借款 公司 资产 负载 表 急剧 恶化 原因 之一 。 因为 股价 下跌 表示 公司 的 净值 下降 了 ,   这会 导致 贷款 者 的 放贷 意愿 下降 。       物价水平 的 意外 下降 ,   将 导致 公司 的 实际 负债 提高 , 因为 公司 的 借款 合同 是 钱 , 而 钱 在 物价水平 降低 的 时候 更 值钱 了 。 从而 导致 公司 的 净值 降低 。       本币 的 贬值 ,   在 一些 发展中国家 , 本币 计价 的 债券 难以 卖出 , 所以 会以 外币 计价 发行 债券 。 但是 公司 的 资产 大多 以 本币 计价 , 所以 本币 的 贬值 将 导致 公司 的 净值 降低 。       资产 贬值               金融机构 ( 特别 是 银行 ) 资产 负载 表 恶化 将 导致 贷款 的 供给 测 减少       银行 危机 , 银行 破产 , 将 导致 挤兑 , 信贷 规模 也 将 降低 , 从而 提高 利率 水平 。       不确定性 的 增加 , 金融市场 中 的 不确定性 增加 , 是 贷款 者 难以 准确 判别 贷款 的 品质       利率 的 提高       政府 财政收支 失衡           历史 上 美国 金融危机 发生 机制           第一阶段 : 金融危机 的 发端       在 一个 国家 推进 金融 自由化 的 阶段 , 即 削减 对 金融市场 和 金融机构 的 管制 , 或者 是 在 市场 中 产生 重要 金融 创新 的 阶段 , 往往 会 播下 金融危机 的 种子 。 比如 : 次贷 危机 中 的 次级 贷款 ,   本次 金融危机 中 的 P2P , 比特 币   etc 。       政府 安全 体系 实际上 增加 了 银行 从事 道德风险 行为 的 冬季 , 时期 可能 承担 与 没有 政府 安全 体系 的 情况 相比 更 多 的 风险 。 因为 银行 在 进行 \" 如果 盈利 , 银行 获利 ; 如果 输 了 , 纳税人 赔偿 \" 的 游戏 。 在 缺乏 监管 的 情况 下 , 过度 承担风险 的 活动 将 难以 控制 。 资本金 的 减少 将会 博士 银行 收缩 信贷 , 即 所谓 的 「 去 杠杆 」 ( deleveraging )       资产 价格 泡沫 及其 破灭       不确定性 的 增加 , 金融危机 一个 最为 显著 的 特征 是 , 某个 大型 金融机构 的 破产 是 造成 不确定性 增加 的 最为 重要 的 原因 。 2008 年 雷曼 兄弟 破产               第二阶段 :   银行 危机       由于 不断 恶化 的 商业 环境 和 银行 的 不确定性 , 导致 存款 者 撤资 , 引发 银行 危机 。 银行 危机 发生 后 , 会 导致 企业破产 清算 , 从而 将 哪些 差 的 公司 和 好 公司 区分 开来 , 一旦 完成 后 , 不确定性 就 会 降低 ,   利率 也 开始 下降 ,   然后 金融危机 消退 。               第三阶段 :   债务 紧缩       美国 大 萧条               2007 ~ 2008 次贷 危机       在 2000 年 之前 , 只有 信用 评级 最高 的 借款者 才能 得到 住房 抵押 贷款 。 但是 后来 , 出现 了 资产 证券化 , 将 小规模 高风险 的 次级 抵押 贷款 打包 成 标准 的 债权 证券 。       在 经纪人 帮助 贷款人 发起 抵押 贷款 时 , 会 存在 委托 - 代理 问题 , 经纪人 成交 后 拿到 佣金 就 没有 责任 了 。 这 导致 了 逆向 选择 , 风险 偏好 越高 的 人 越 容易 去 贷款 。 并且 在 承销 银行 和 评级 机构 中 也 存在 这种 问题 , 他们 在 成交 后 不在乎 贷款人 的 偿付能力 。       由于 担保 债务 凭证 , 担保 债务 凭证 2 , 担保 债务 凭证 3 等 结构化 信贷 产品 过于 复杂 , 以至于 难以 衡量 证券 的 基础 资产 所 产生 的 现金流 , 美联储 主席 伯南克 在 2007 年 的 一次 讲话 中 就 说 「 他 很 想 知道 那些 该死 的 东西 到底 值 多少 钱 」 , 负载 的 信贷 产品 实际上 破坏 了 信息 , 从而 导致 了 信息 不 对称 问题 更加 严重 。       房地产 价格 泡沫 破裂       危机 在 全球 范围 扩散       银行 资产 负载 表 恶化       知名 公司 破产       援助 计划 ,   2008 年 美众议院 否决 了 布什总统 提出 的 7000 亿美元 的 援助 计划               新兴 市场 的 金融危机       政府 腐败       政治 不 稳定       货币 危机 ,   汇率 失守       其他 和 美国 类似               韩国 财阀 ,   绑架 政府 ,   政府 提供 担保 , 导致 了 道德风险 的 加剧       2008 年 金融危机 爆发 后 , 国内 采取 了 4 万亿 的 救援 计划 ,   2009 年 M2 同步 增速 最高 达到 了 29.74 % 。 而 此时 银行 间 的 隔夜 拆借 利率 降到 低点 , 只有 不到 1 % 。 而 在 2008 年 危机 爆发 时 , 最高 可达 3 % 以上 。   参考   https : / / tracholar . github . io / economics - data / shibor / data . html             银行业 与 金融机构 管理       商业银行 资产 负债表           负债       可 开支票 存款 , 不 计息   6 % , 不断 萎缩       非 交易 性 存款 ,   53 % ,   利率 较 高 ,   包括 储蓄存款 和 定期存款 ( 或 定期存单 )       借款 ,   从 联邦 储备 体系 , 贷款 银行 , 公司 借款 ,   隔夜 准备金       银行资本               资产       准备金       应收 现金 ,   另一家 银行 的 支票 ,   资金 尚未 转入 的 时候       银行 同业 存款 ,   小 银行 将 资金 存在 大型 银行       证券 ,   对 商业银行 来说 都 是 债券 , 因为 商业银行 不让 持有 股票 ,   包括 政府 证券 和 其他 证券       贷款 , 61 %       其他 资产               案例 研究 ,   招商银行 资产 负债表       负债 中 70 % 是 客户 存款       60 % 左右 的 资产 是 发放 的 贷款 , 剩下 的 主要 是 存在 央行 的 存款 , 债务 投资                                 银行 的 基本 业务           出售 负债 筹资 , 然后 用 这些 资金 购买 资产 来 获取 利润 , 这个 过程 称作 资产 转换 。 其实 就是 赚 利差       借短 贷长 :   将 储蓄存款 转换 为 抵押 贷款       借款者 评估 5C 原则 :   道德品质 ,   还款 能力 ,   抵押品 ,   经营 环境 , 资产 净值       服务费 收入                   银行 管理 基本 原则           流动性 管理 ,   在 出现 存款 外流 是 有 足够 的 现金支付 存款 者       资产 管理 ,   获得 风险 较 低 的 资产 以及 实现 资产 的 多样化 , 将 风险 降 至 一个 可以 接受 的 较 低水平       负债 管理 ,   低成本 获取 资金       资本 充足 性 管理       流动性 管理 和 准备金 的 作用       银行 需要 持有 超额 准备金 , 在 存款 外流 时 , 不用 调整 资产 负债表 的 其他 项       流动性 较 高 的 证券 也 可以 作为 二级 准备金               资产 管理 :   目标 是 最大化 利润 , 必须 寻找 能够 提供 最高 回报率 的 贷款 和 债券 的 同时 , 降低 风险 , 并且 通过 持有 流动性 资产 来 保持足够 的 流动性 。 这 不是 跟 我们 个人 理财 一样 吗 。 嘿嘿       找到 支付 高利率 的 低 风险 的 借款者 。 通过 信用 审核 降低 逆向 选择 的 问题 , 并持 保守 的 态度 , 贷款 违约 率 一般 低于 1 % , 但 也 不能 过于 保守       购买 回报率 较 高 而 风险 较 低 的 证券       通过 资产 多样化 降低 风险 水平 。 在 20 实际 80 年代 , 美国银行 将 贷款 过于 集中 与 能源 、 房地产 、 农户 中 , 导致 在 这 三个 产品价格 暴跌 的 时候 , 银行 遭受 了 巨大 的 损失 。       银行 必须 对 流动性 进行 管理 , 以 可 接受 的 成本 满足 法定 准备金 的 要求 。 比如 持有 超额 准备金 、 流动性 好 的 政府 债券               负债 管理 :   20 世纪 60 年代 以前 , 主要 是 通过 没有 利息 的 支票 存款 ( 占 了 60 % ) 。 之后 , 由于 新 的 金融工具 , 比如 可 转让 定期存单 , 隔夜 拆解 市场 的 完善 , 可 转让 定期存单 和 银行借款 ( 拆借 ) 陈维 主要 的 融资 工具 。       资本 充足 性 管理 , 为了 防范 破产 风险 , 资本金 高 的 , 更能 抗 坏账 。       银行 盈利 能力 指标 :   资产 回报率 ( ROA )   =   税后 近 利润 / 资产       股权 回报率 :   ROE   =   税后 净利润 / 股权 资本       股本 乘数   EM   =   资产 / 股权 资本 ,   其实 就是 杠杆 倍数 。       ROE   =   ROA   *   EM       因为 资产 回报率 取决于 经营 能力 , 不会 相差太多 。 所以 , 杠杆 越大 , 单位 股权 资本 获得 的 股权 回报率 就 越 高 , 所以 银行 的 所有者 不 愿意 持有 太 多 股权 资本 。       风险 和 收益 的 平衡 , 正是 资本 充足 性 管理 的 目标 。 在 贷款风险 小 的 时候 , 加 杠杆 , 在 贷款风险 大 的 时候 , 去 杠杆 。       银行 监管 机构 指定 了 规定 , 银行 必须 要 持有 一定 的 资本       提高 EM 的 方法 :       回购 股票 , 降低 股权 资本       派发 股利 , 降低 留存 收益 , 从而 降低 股权 资本       发行 新 的 融资 工具 , 增加 负债 的 方式 增加 资产               降低 EM 的 方法 :       发行 新股 , 增加 股权 资本 。 股票 增发       减少 红利 , 增加 留存 收益 , 从而 增加 股权 资本       出售 证券 , 通过 减少 负债 的 方式 降低 资产 规模               2007 年 金融危机 , 导致 银行 「 信贷 紧缩 」 。 一部分 原因 是 银行 的 资本 紧缩 , 降低 了 信贷 增长速度       房地产 崩盘 , 导致 银行 坏账 增加 , 银行 的 资产 贬值 , 从而 减少 了 银行 的 资本 。 所以 银行 需要 筹集 新 的 资本 , 但是 在 经济 环境 不好 的 时候 , 这是 十分困难 的 。 因此 , 银行 选择 了 提高 贷款 标准 , 减少 负债 , 降低 信贷 规模 。                           信用风险 管理           在 信贷 市场 , 最 需要 借钱 的 恰恰 是 那些 具有 较 高风险 的 人 。 并且 他们 具有 利用 贷款 , 从事 高风险 活动 的 动机 。       甄别 和 监督       对于 个人 贷款 , 需要 信用 评估 , 收集 工资 流水 , 财务状况 , 工作 状况 etc       对于 企业 贷款 , 需要 盈利 情况 , 借款 用途 , 信用 品质 , 销售 数据 etc       贷款 专业化 ,   损失 了 多样性 , 但是 增加 了 信息 收集 完善 , 减少 信息 不 对称       限制性 条款 的 监督 和 执行 :   限制 借款 中 从事 高风险 活动 , 如果 违反 , 则 可以 强制执行 。 银行 和 其他 金融机构 花费 大量 金钱 进行 审计 并且 收集 信息 , 其 原因 就 在于 对 贷款 实行 甄别 和 监督               长期 客户 联系       可以 根据 客户 历史 行为 进行 风险 判断 , 减少 信息 不 对称 。 长期 客户 联系 可以 减少 收集 信息 的 成本 。       长期 客户 还 能 让 银行 发现 更 多 的 可能 高风险 活动 。       长期 客户 也 能 获得 更 多 好处 , 以 更 低 的 利率 取得 借款 。 而且 为了 在 未来 持续 获得 这种 好处 , 客户 会 有意识 地 避免 合同 中 没有 的 高风险 活动 。               贷款 承诺 :   承诺 与 市场 利率 相关 的 利率 向 企业 发放 既定 额度 之内 的 贷款       抵押品 和 补偿性 余额       对 企业 来说 , 补偿性 余额 是 指 必须 在 这家 银行 的 存款 账户 上 保留 某 一个 最低 的 资金 限额 。       此外 补偿性 余额 可以 帮组 银行 对 借款者 进行 监控 。 获得 客户 的 财务 信息 。               信贷 配给       即使 借款者 提供 更 高 的 利率 , 也 拒绝 发放 任何 贷款 。 防止 逆向 选择       愿意 发放 借款 , 但是 借款 数量 低于 借款者 的 要求 。 贷款 规模 的 减小 , 可以 减小 客户 从事 高风险 活动 的 收益 。 比如 , 给 你 发 1000 元 , 你 不会 为了 这点 小钱 破坏 自己 的 信用 ; 但是 如果 是 1000 万 , 可能 你会 卷款 潜逃 。                   利率 风险管理           利率 敏感性 资产 和 利率 敏感性 负债 可以 互相 对冲 利率 变动 的 风险 。       缺口 指 的 是 敏感性 资产 与 敏感性 负债 之间 的 差额 , 缺口 与 利率 变动 的 乘积 就是 银行 利润 受 利率 变化 的 影响 。 例如 缺口 是   - 1000 万元 , 利率 提高 1 % , 那么 利润 就 会 变动   - 1000W * 1 % = - 10W 。       固定 利率 的 长期 资产 和 负债 的 期限 并 不 一致 带来 了 新 的 问题 。 久期 分析 ,   证券 价值 变动 百分比   =   利率 变动 的 百分比   *   久期 。 即用 平均 生命周期 乘以 利率 变动 作为 价值 变动 。 比如 上述 问题 , 如果 敏感性 资产 平均 生命周期 是 3 年 , 资产 价值 1000W , 而 负债 平均 周期 只有 2 年 , 负债 价值 2000W 。 那么 资产 变动 为 3 % 为 30W , 负债 变动 为 2 % 为 40W 。 所以 利润 会 变动   30W   -   40W   =   10W       久期 的 管理 , 通过 资产负债 重新 调整 比较 困难 , 现在 可以 通过 金融 远期 合约 、 期货 、 期权 等 金融 衍生 工具 来 实现 , 而 无需 调整 资产 和 负债           表外 业务 活动           贷款 销售 ,   将 贷款 现金流 以 合约 出售 给 下家 , 通常 价格 会 高于 贷款 初始 金额 。 从而 赚取 差价 利润 。       手续费 ,   担保费 , 银行 承兑 担保费 ,       表外 交易 活动 风险 很大 , 巴林 银行 破产案       黄铜 交易 事件 ?       主观 交易 活动 和 主管 记账 业务 的 人 分离                   银行 监管 的 经济学 分析       信息 不 对称 与 银行 监管           政府 安全网       银行 恐慌 和 建立 存款 保险制度 , 由于 无法 获知 银行 经理 是否 拿 着 我们 的 钱 从事 高风险 业务 , 或者 他 就是 个 彻头彻尾 的 骗子 , 存款人 在 将 资金 存入 银行 时 就 会 有所 顾虑 , 从而 削弱 银行 的 生存能力       如果 存款人 缺乏 银行 资产 质量 信息 , 就 可能 导致 银行 恐慌 出现 。 比如 , 5 % 的 银行 在 贷款 中 遭受 了 巨额 损失 , 如果 存款人 不 缺乏 银行 资产 质量 信息 , 那么 久 无法 区分 自己 存 的 银行 是否 也 有 问题 , 就 会 导致 挤兑 ! 在 19 世纪 和 20 世纪 初期 , 美国 的 银行 恐慌 是 常见 的 事情 , 平均 每 20 年 一次 大规模 的 银行 恐慌 。       为 存款人 建立 的 政府 安全网 ,   存款 保险 给 了 存款人 信心 , 即使 对 银行 担心 , 也 不会 去 银行 提取 存款 , 从而 造成 挤兑 现象 。       存款 保险公司 主要 采用 两种 方法 来 处理 破产 的 银行       偿付 法 :   联邦 存款 保险公司 允许 银行 破产 , 并且 在 10W 以内 都 可以 全额 偿付       收购 与 接管 法 :   找 其他 公司 收购 该 银行 , 也 可以 提供 补贴 贷款 etc       同一 存款人 在 同 一家 存款 机构 所有 被 保险 存款 账户 的 存款 本金 和 利息 合并 计算 的 资金 数额 在 50 万元 以内 的 ， 实行 全额 偿付 ； 超出 50 万元 的 部分 ， 依法 从 投保 机构 清算 财产 中 受偿 。     - - - - 中国 存款 保险制度                       存款 保险制度 的 副作用 , 会 提高 银行 从事 高风险 活动 的 动力 , 所以 需要 严刑峻法 进行 监督 !       政府 救济 贷款 ,   央行 向 破产 银行 注入 资金 并 进行 接管       「 太大而 不能 倒闭 」 提高 了 银行 从事 高风险 活动 的 动力 ,   「 挣 了 我 受益 , 输 了 纳税人 受损 」               对 持有 资产 的 限制 ,   目的 是 为了 解决 政府 安全网 和 银行 固有 的 「 道德风险 」 问题       限制 银行 持有 股票 , 限制 某种 贷款 的 金额 , 限制 发放 某种 贷款               银行 资本金 的 要求       杠杆 比率 ,   我国 要求 杠杆 比率 必须 高于 4 %       《 办法 》 规定 ， 杠杆 率 是 指 商业银行 持有 的 、 符合 有关 规定 的 一级 资本 与 商业银行 调整 后 的 表 内外 资产 余额 的 比率 ； 商业银行 并表 和 未 并表 的 杠杆 率均 不得 低于 4 % ； 银监会 对 商业银行 的 杠杆 率 及其 管理状况 实施 监督 检查 ， 对 银行业 的 整体 杠杆 率 情况 进行 持续 监测 ， 加强 对 银行业 系统性 风险 的 分析 与 防范 。       http : / / www . cbrc . gov . cn / chinese / home / docView / 201107076FDBE8AD253598EEFFD81124DE87BF00 . html                 分 基于 风险 的 资本 要求 。 《 巴塞尔 协议 》 要求 银行 持有 的 资本 至少 占 其 风险 加权 资产 的 8 % 。 既 包括 表内 业务 , 也 包括 表外 业务 。 但是 , 风险 权重 与 实际 风险 不 匹配 , 导致 风险 套利 的 出现 。               及时 纠正 行动 , 对于 「 资本 不足 」 的 银行 ,   限制 银行 扩大 资产 规模 ,   限制 银行 扩张 办 分公司       银行 监管 :   注册 与 检查 。 由于 骗子 和 冒险家 可以 利用 金融机构 从事 高风险 投机 活动 , 所以 有 这种 一算 的 人 对 从事 金融机构 管理 有 更 高 积极性 。 金融机构 注册 机制 是 防止 这 类 逆向 选择 问题 产生 的 一种 方法 ;   通过 定期 对 金融机构 进行 实地 检查 , 监管 机构 可以 核查 金融机构 是否 符合 资本 要求 已经 满足 持有 资产 的 规定 ,   这 有助于 一直 到 的 风险 。       骆驼 评级 ,   评估 过程 主要 包含 了 六个 方面 的 内容 :   资本 充足率 ,   资产 质量 ,   管理 ,   盈利 ,   流动性   以及   对 市场 风险 的 敏感性 。       如果 评级 过低 ,   监管部门 可以 停业 和 整顿 , 甚至 关闭 银行       银行 需要 按照 规定 定期 提交 报告 ,   纰漏 资产 负债表 , 收入 红利 , 所有权 归属 etc       机构 :   银监会               风险 评估 管理 :   某一 时刻 的 资产 负债表 没 问题 , 不 代表 时间 点 之间 没 问题 , 1995 巴林 银行 倒闭 ,   尤其 是 金融机构 环境 的 改变 。       由 董事会 和 高级 管理层 实时 的 监督 活动 的 质量       对于 全部 具有 重大 风险 的 业务 活动 所 采取 的 政策 和 限制 措施 是否 足够       风险 测量 和 监控 体系 的 质量       为 于方 雇员 从事 欺诈 和 未 授权 活动 的 内部 控制措施 是否 足够       压力 测试 :   计算 最坏 情况 下 所 遭受 的 损失 ,   或者 进行 风险 价值 计算               信息 披露 要求 ,   向 公众 提供 财务 信息       市场 价值 法       对 消费者 保护 ,   所有 贷款 者 必须 向 消费者 提供 有关 借款 成本 的 信息 , 包括 标准化 的 利率 以及 贷款 的 融资 总费用 等 。           对于 竞争 的 限制 ,   竞争 会 加大 金融机构 从事 过 高风险 活动 的 动力 ,   通过 规章制度 保护 金融机构 免于 ( 过度 ) 竞争 。 但是 会 降低 银行 机构 的 效率 。               美国 主要 金融 立法                       20 世纪 80 年代 美国 初代 协会 危机 和 银行 危机           70 年代 和 80 年代 的 金融 创新 浪潮 开始 ,   传统 业务 盈利 能力 降低 ,   银行 面临 货币基金 等 金融机构 的 竞争 ,   银行 开始 高风险 业务 , 加上 存款 保险制度 的 推动 ,   银行 的 风险 过高 导致 破产 数量 多 了 很多       美国银行 破产 数据     https : / / www . fdic . gov / bank / historical / bank /             世界 范围 内 的 银行 危机       略         监管 手段 总结 :   控 风险 、 缩 规模 、 降 杠杆 、 纰 信息         信用 评级 机构 的 监管       银行业 :   结构 与 竞争       在 大多数 国家 中 , 通常 会 有 四五家 银行 控制 整个 银行业 , 而 在 美国 确有 7000 加 商业 银商 、 1200 家 储贷 协会 、 400 加 互助 储蓄银行 以及 8000 家 信用社 。       银行 体系 的 发展 历史           北美 银行 1782 年 在 费城 成立 , 标志 着 美国 现代 商业银行 的 开始 。 ( 中国 历史 清朝 乾隆年间 )                       1863 年前 , 所有 商业银行 都 是 由 其 所在 州 的 银行 委员会 批准 注册 成立 的 。 那 时候 没有 国家 货币 , 银行 主要 通过 发行 银行券 来 筹集资金 。 由于 监管 松懈 , 银行 经常 由于 欺诈 和 资本 短缺 而 倒闭 , 发行 的 银行券 也 一文不值       《 1863 年 国民 银行 法案 》 创建 了 那些 在 联邦 注册 的 银行 所 组成 的 银行 体系 , 该 体系 由 美国财政部 下属 的 通货 监理 署 负责 监管 。 州 银行 仍然 存在 , 从而 形成 了 双重 银行 体系 。       《 1933 年 银行法 》 即 《 格拉斯 - 斯蒂格 尔法 》 将 商业 存款 业务 和 证券 业务 分开 , 规定 商业银行 必须 出售 他们 的 投资 银行业务 。 摩根 大通 将 投资 业务 该组 为 摩根 斯丹利 投资银行 。       美国 的 银行 受 多个 机构 监管 :         通货 监理 署 :   1850 家 国民 银行       联邦 储备 体系 和 州 银行 监管局 :   作为 联邦 储备 体系 成员 的 900 家州 银行       联邦储备委员会 : 拥有 一家 或者 多家 银行 的 公司 ( 银行 持股 公司 )       联邦 存款 保险公司 和 州 银行 监管 当局 :   4800 家州 银行 ,   这些 银行 在 联邦 存款 保险公司 投保 , 但 不是 联邦 储备 体系 的 成员       州 银行 监管局 : 没有 参加 联邦 存款 保险公司 的 500 家州 银行                   金融 创新 和 \" 影子 银行 体系 \" 的 发展           适应 需求 变化 的 金融 创新 :   利率 波动性       可变 利率 抵押 贷款 ,   贷款 的 利率 随着 某种 市场 利率 ( 通常 为 国库券 利率 ) 的 变化 而 变化       金融 衍生 工具 :   预防 或者 对冲 利率 风险 的 产品 , 比如 基于 金融工具 的 期货 合约 ,   之前 是 基于 实物 的 期货 合约 。 1975 年 金融 衍生 工具 诞生 了 。               适应 供给 变化 的 金融 创新 :   信息技术       银行 信用卡 和 借记卡       电子 银行业务 ,   虚拟 银行       垃圾 债券 :   由于 信息技术 的 发展 ,   人们 获取 公司 的 信息 变得 容易 , 所以 一些 评级 Baa 级别 的 公司 发行 的 长期 债券 也 有人 买 了       商业 票据 市场 :   公司 通过 商业 票据 市场 发 发型 短期 借款 工具 。 货币基金 、 养老 基金 也 需要 这种 优质 的 短期 资产       证券化 :   将 不 具备 流动性 的 金融资产 ( 诸如 住房 抵押 贷款 、 汽车贷款 、 信用卡 应付账款 等 ) 转换成 可 流通 的 资本 市场 证券 的 过程 。 通过 证券化 , 金融机构 可以 将 本金 和 利息 的 索取权 作为 证券 销售 给 第三方 。 并且 可以 通过 资产 组合 分散 风险 。               规避 现存 管制 的 金融 创新       两类 规章制度 严格 限制 了 银行 的 盈利 能力 :   法定 准备金 制度 ,   存款 利率 的 限制       法定 准备金 由于 没有 利息 ,   导致 法定 准备金 实际上 是 一种 对 存款 征收 的 税金 , 金额 就是 这部分 准备金 的 市场 利息       支票 存款 账户 没有 利息 ,   市场 利率 的 提高 导致 银行 资金 的 流出 , 减少 可贷 资金 的 规模       货币 市场 共同 基金 :   没有 准备金 的 要求 ,   也 不受 利率 的 约束       流动性 账户 ,   每个 工作日 结束 时 , 超过 规定 金额 的 余额 会 自动 购买 支付 利息 的 证券 投资 ( 这 不 就是 支付宝 余额 自动 转入 余额 宝 吗 , 啊 哈哈 )               金融 创新 和 传统 银行业务 的 衰落       资金 获取 成本 优势 的 下降       资金运用 收入 优势 的 下降       银行 反应 :       增加 高风险 资产       扩大 表外 业务                       美国 银行业 结构       分散 的 原因 是 「 限制 设立 分支机构 」 的 政策       规避 方法 :       银行 持股 公司 ,   虽然 有 很多 小 银行 , 但是 他们 的 所有权 都 属于 某个 大 的 银行 持股 公司 。 相当于 间接 实现 了 很多 分支机构       自动 提款机 , 用 别的 银行 的 自动 提款机                           银行 并购 与 全美 范围 的 银行业           花旗银行       摩根 大通       美洲 银行           现在 , 大型 独立 投资银行 企业 已经 成为 历史 。               世界 范围 内 银行业 与 证券业 有 三种 基本模式           全能 银行 :   德国 、 荷兰 、 瑞士 , 银行业 和 证券业 之间 没有 任何 分割       英格兰 是 全能 银行 :   英国 、 加拿大 、 澳大利亚 、 美国 ,   不同 业务 由 法律 认定 的 不同 附属 公司 开展 ; 银行 不 持有 商业银行 股份 ; 银行 与 保险公司 的 联系 不 甚 普遍       银行 与 其他 金融服务 机构 间 有 隔离 :   日本                   储蓄机构 :   监管 和 结构           储蓄机构 :   储贷 协会 、 互助 储蓄银行 、 信用社           国际 银行业务           国际贸易 的 增长 ,   带来 了 融资 的 需要       全球性 投资 银行业务 , 承销 外国 证券       欧洲 美元 市场       发源 于 \" 冷战 \" 时期 , 苏联 担心 美国 冻结 它 的 美元 资产 , 所以 将 存款 转移 至 欧洲 , 以 避免 没收 。 但是 仍然 想以 美元 的 形式 保存 , 以便 用于 国际 交易 。               美国 国内 还有 国际 银行 , 可以 像 外国人 发放贷款 和 吸收 存款 。               美联储 跟 中国 央行 的 区别 ?     美联储 是 私人 公司 ?           非 银行业 金融机构       保险业           人寿保险 公司       人寿保险 保单 是 保险 , 没有 储蓄 功能 ,   养老 基金 才 有 ,   所以 现在 养老 基金 比 人寿保险 保单 更 受欢迎               财产 和 意外 伤害 保险公司       对 债务 工具 的 保险 CDS       保险 管理 :   目的 是 为了 减少 逆向 选择 和 道德风险       甄别 , 例如 根据 历史 驾驶 记录 确定 车险 的 风险 等级       基于 风险 的 保费       限制性 条款 ,   比如 人寿保险 规定 你 跟 如果 被保险人 在 保单 生效 的 头 两年 自杀 , 将 取消 死亡 给付       预防 欺诈       保险 中止 条款 , 例如 车险 公司 规定 超速 罚单 过多 就 中止 合同       免 赔额 ,   医疗保险 的 免 赔额 可以 防止 滥用 保险 的 行为       共同 保险 ,   和 承包人 共同 承担 损失       对 保险金额 的 限制 ,   例如 汽车 被盗 险 保险金额 不会 高于 车辆 真实 价值 , 否则 客户 就 不会 采取措施 防止 汽车 被盗               养老 基金 ,   养老 基金 可以 炒股 的       私人 养老 计划       公共 养老 计划       财务 公司 :   借大 放小 ,   主要 经营 小额 消费 贷       销售 财务 公司 , 零售 或 制造业 企业 向 购买 自己 商品 的 客户 发放贷款       消费者 财务 公司 :   小额 消费 贷       工商企业 财务 公司 :   以 一定 折扣 购买 应收 账款 ( 债权 ) 的 方式 , 为 企业 提供 贷款 。                   证券市场 机构           投资银行 :   证券 销售 的 一级 市场       需要 担保 证券 以 某 一 价格 发行 ( 即常 说 的 包销 ) , 因此 高风险 高 收益               证券 交易所 :   证券交易 的 二级 市场       收 中介费 手续费 ,   没有 任何 风险 ,   但 有 可能 被 打                   共同 基金           主要 有 居民 投资 ,       美国 的 机构 投资者 交易额 占 股票市场 的 70 % 以上       分为 开放型 基金 ( 可以 随时 赎回 )   和   封闭型 基金 ( 不可 赎回 , 但是 可以 交易 )       货币 市场 共同 基金 :   余额 宝 这种 , 现在 已近 熟悉 得 不能 再 熟悉 了           对冲 基金           一种 特殊 的 共同 基金       10W - 2000W 不 等 的 最低 投资 要求 , 通常 是 100W   美元       联邦 法律 要求 对冲 基金 投资者 不得 超过 99 人 ( 有限 合伙人 制度 )       高 杠杆 20 倍 甚至 50 倍           私募 股权 投资 基金 和 风险投资 基金           简称 私募 和 风 投       风 投是 私募 的 一种 , 主要 投资 于 未上市 公司 ;   还有 一种 投资 于 上市公司 首购 的 私募 叫 并购 基金       私募 可以 避免 搭便车 问题 , 因为 他们 要么 投资 于 未上市 公司 , 要么 全资 并购       有限 合伙人       加 杠杆 , 高风险 高 收益           政府 金融机构           联邦 信贷 机构       住房 抵押 :   房利美 和 房地 美       存在 政府 隐性 担保 带来 的 道德风险 问题                   思考题           死亡率 难以预测 , 人寿保险 公司 将 如何 调整 其 资产 类型 ?       由于 死亡率 容易 精准 预测 , 人寿保险 公司 可以 精确 预测 未来 一段时间 需要 赔付 的 金额 , 所以 可以 投资 大量 的 长期 资产 ;   相反 , 如果 这个 前提 发生 了 改变 , 那么 就 应该 减少 长期 资产 的 配置 。       对于 个人 的 资产 配置 也 一样 ,   如果 用钱 金额 和 收入 波动 比较 小 , 可以 买 一些 长期 资产 比如 定存 、 股票 、 债券 等 , 相反 应该 多 配置 一些 短期 资产 :   短债 、 货基 etc               为什么 财产 和 意外 伤害 保险公司 持有 大量 市政 债券 , 而 人寿保险 公司 没有 ?       两个 原因 ,   市政 债券 收益 免税 ,   财产 和 意外 伤害 保险公司 买 其他 资产 的 收益 是 不能 免税 的 ,   人寿 公司 有 免税 政策 ?   市政 债券 流动性 好 ( 愿意 买 的 人 多 流动性 好 ) ,   意外险 公司 无法 向 预测 死亡率 那样 预测 意外 赔付 金额 , 所以 需要 持有 更 多 的 流动性 好 的 市政 债券               养老金 计划 的 税收 优惠政策 如何 鼓励 储蓄 ?       保单 中 为何 会 广泛 使用 免 赔额 条款 ?       为了 减少 道德风险 ,   让 被 保险 者 也 承担 损失 来 增加 责任               为什么 货币基金 能够 按照 固定价格 赎回 份额 ,   其他 他 基金 不能 ?       因为 波动 小 , 且 收益 都 为 正 的 , 收益 被 当做 利息               对冲 基金 和 共同 基金 有何 不同 ?       门槛 高       有限 合伙人       很大 程度 上不受 监管       要求 投资者 在 长时间 ( 通常 为 几年 ) 放弃 资产 使用权       费用 高昂       高 杠杆               私募 股权 基金 的 四点 优势 ?       不 受 监管       没有 当前 利润 的 压力 , 可以 眼光 更 长远       给 基金 管理者 更 多 的 公司 股权       可以 克服 搭便车 问题                   衍生 金融工具           原因 :   利率 波动 大 , 利率 风险 太高       目标 :   通过 衍生 工具 , 控制 风险       工具 :   远期 合同 、 金融 期货 、 期权 和 互换           避险           远期 合约 ,   交易 日期 在 签订 日期 之后 的 某个 时间 点 的 协议 。       远期 合约 可以 锁定 收益 和 利率 风险 , 但是 存在 违约 风险 和 缺乏 流动性 的 缺点 。           金融 期货 合约 和 期货市场           金融工具 在 未来 进行 交割 ,   但是       期货 合约 的 几个 特点 规避 了 远期 合约 的 缺点       交割 数量 和 日期 是 标准化 的 ,   匹配 购买 双方 更加 容易 , 增强 了 流动性       期货 合约 可以 再 交易       允许 交割 满足条件 的 多种 标的物 。 满足 15 年 以上 且 15 年内 不能 被 赎回 的 国债 都 可以 交割 。 也 可以 提高 囤积者 的 囤积 成本 , 打击 囤积者 , 因为 需要 囤积 更 多 的 交割 物       期货 合约 不是 交易 双方 签订 合约 , 而是 与 交易所 内部 的 清算 所 签约 , 可以 避免 违约 风险       为 保证 清算 所 财务状况 良好 , 要求 买卖双方 都 必须 存入 一笔 初始 存款 , 成为 保证金 要求 。 保证金 账户 每天 会 根据 期货 合约 市场 价值 变动 而 变动 。   保证金 账户 如果 过少 , 会 被 要求 增加 保证金 。       大部分 期货 合约 无需 到期日 交割 标的 资产 ,   而 远期 合约 则 必须 如此 。 为了 避免 到期日 交割 , 卖出 合约 的 交易者 做 一笔 反向 交易 , 即 买入 相同 的 期货 合约 。 同时 持有 多头 和 空头 , 期货 交易所 允许 交易者 低效 期货 合约 。 从而 降低 交易成本 。                   期权           已 约定 价格 购买 标的物 的 权利 ,   当然 也 可以 选择 不买       由于 期权 可能 获利 , 因此 买方 愿意 支付 期权 费       美式 期权 :   到期 之前 随时 行权       欧式 期权 : 智能 在 到期日 当天 执行       股票 期权 、 金融 期货 期权       期权 具有 非线性 收益 函数 , 损失 最大 不 超过 期权 费 , 收益 无 上限       较长 的 到期 期限 将 增加 期权 合约 的 价值       标的物 价格 波动 越大 , 看涨 期权 和 看跌 期权 的 期权 费 都 很 高           思考题           如果 你 管理 的 养老 基金 在 6 个 月 后 预计 会 有 1.2 亿美元 的 资金 流入 , 你 应当 签订 什么样 的 远期 合约 来讲 未来 利率 锁定 在 目前 的 水平 ?       签订 以 当前 利率 收益 的 6 个 月 后 卖出 的 远期 合约                   金融 行业 内 的 利益冲突       利益冲突 及其 重要性  ", "tags": "economics/money-banking-and-financial-market-book", "url": "/wiki/economics/money-banking-and-financial-market-book/banking-markets.html"},
      
      
      {"title": "第4篇 中央银行和货币政策的实施", "text": "    Table   of   Contents           第 16 章   中央银行 和 联邦 储备 体系           物价 稳定 目标 和 名义 锚           美联储 的 起源           思考题                   第 17 章   货币 供给 过程           联邦 储备 体系 的 资产 负债表           货币 乘数                   第 18 章   货币政策 工具           准备金 市场 与 联邦 基准利率           公开市场 操作           贴现 政策           法定 存款 准备金 要求           欧洲央行           中国 央行 ( 自己 补 的 )           资料           思考题                   第 19 章   货币政策 的 实施 : 战略 与 策略           货币 目标 制           通货膨胀 目标 制           带有 隐形 锚 的 货币政策           策略 : 政策 工具 的 选择           资产 泡沫           联邦 储备 体系 的 政策 演化           思考题                         第 16 章   中央银行 和 联邦 储备 体系       物价 稳定 目标 和 名义 锚           物价 稳定 ,   稳定 的 低 通胀       名义 锚 :   指有 足浴 控制 价格水平 进而 实现 物价 稳定 的 , 类似 于 通货膨胀率 或 货币 供应量 的 一个 名义 变量 。       名义 锚 可以 限制 时间 不 一致 问题       5 个 其他 货币政策 目标 :       高 就业率       经济 增长       金融市场 稳定       利率 稳定       外汇市场 稳定               高 就业率 :       经济 处于 充分就业 时 失业率 不为 零 的 原因 :   摩擦性 失业 , 员工 找 更好 工作 期间 的 失业 ;   结构性 失业 ,   工作 需求 与 本地 工人 技能 或 可用性 不 匹配 。       高 就业率 是 维持 较 低 的 失业率 水平 , 而 不是 零 失业率 ,   存在 一个 最低 的 失业率 , 叫 自然 失业率       合理 的 目标 是 4.5 %   -   6 %               经济 增长       通过 减税 , 促使 企业 增加 投资 , 增加 纳税人 的 储蓄 , 进而 推动 经济 增长               金融市场 稳定       促进 更加 稳健 的 金融体系 的 形成 , 进而 避免 金融危机 的 发生               利率 稳定       利率 波动 会 增加 不确定性 , 增加 决策 的 困难 , 也 就是 增加 利率 风险                   外汇市场 稳定           汇率 的 波动 会 增加 跨国 贸易 的 汇率 风险                   长期 来看 , 物价 稳定 会 促进 经济 增长 和 利率 稳定 , 但是 短期内 , 物价 和 这 两个 目标 并 不 一致 。 在 经济 扩张 周期 中 , 过热 的 经济 会 导致 高 的 通胀 , 为了 维持 物价 稳定 , 央行 提高 利率 , 但是 这会 导致 失业率 提高 , 增加 利率 风险               层级 使命 模式 :           物价 稳定 应该 是 央行 首要 的 长期 目标       在 稳 物价 基础 上 , 再 去 考虑 失业率 和 经济 增长       这是 英格兰 、 加拿大 、 新西兰 、 欧洲央行 的 官方 指导性 原则               双重 使命 模式 :       美联储 采用 的 模式       同等对待 物价 稳定 和 充分就业               因为 存在 经济周期 , 物价 稳定 应该 作为 一个 长期 目标 , 而 不是 短期 目标           美联储 的 起源           美国 不 信任 中央集权 ,   对 银行 也 不 信任       但是 在 1907 年 的 银行 危机 之后 , 成立 了 由 12 家 地区 联邦 储备 银行 的 联邦 储备 体系 。 而 这 12 加 地区 联邦 储备 银行 是 由 当地 的 私人 董事 们 监督       美联储 体系 ( 设计 目的 : 分权 ) :       联邦 储备 银行       12 个 区域 各有 一家       纽约 、 芝加哥 、 旧金山 联邦 储备 银行 的 资产 占 了 12 家 的 50 % 以上       纽约联邦储备银行 资产 占 了 12 家 的 1 / 4       每 一家 联邦 储备 银行 是 准 公共 机构 ( 部分 私有 , 部分 政府 所有 )       地区 的 会员 银行 需要 购买 联邦 储备 银行 的 股票 , 法律 规定 每年 的 股 利率 不 超过 6 %       纽约联邦储备银行 行长 是 联邦 公开市场 理事会 永久 成员 , 始终 拥有 1 票 表决权 , 其他 4 票则 轮流 由 其他 11 家 银行行长               联邦 储备 理事会       7 名 成员 ,   由 总统 提名 ,   参议院 同意 后 任命       任期 可达 14 年 , 避免 受 总统 控制 ,   任期 满后 不能 连续 任命       对 公开市场 操作 具有 投票权       可以 决定 利率 ,   行长 任命 ,   行长 工资 ,   保证金 比率 ,   银行 合并 ,   银行 新 业务 审批       雇佣 一批 经济学家       重要 资料 《 联邦 储备 银行 评论 》       理事会 主席 才 是 真正 的 导演 ,   是 美联储 的 发言人 :   格林斯潘 、 伯南克               联邦 公开市场 理事会       美联储 的 核心 ,   所以 该 理事会 直接 被称作 「 美联储 」       每年 8 次 公开市场 操作 会议 ,   直接 影响 货币 供应 和 利率       12 名对 公开市场 具有 投票权 :   7 名 联邦 储备 理事会 成员 + 5 名 行长 ,   但是 其他 7 家 银行行长 也 参与 讨论       联邦 公开市场 委员会 会议       外汇 、 国内 公开市场 其他 相关 主题 报告       国民经济 预测 报告 ,   即 「 绿皮书 」       对 当前 货币政策 和 国内 方针政策 的 讨论       向 公众 发布 会议 成果                       联邦 咨询 委员会       2900 余家 会员 商业银行       1980 年 以前 , 只有 成员 银行 需要 在 联邦 储备 系统 中 保有 准备金 , 在 联邦 储备 理事会 的 游说 下 , 到 1987 年 , 国会 要求 所有 商业银行 都 必须 缴存 存款 准备金                       联邦 储备 体系 的 独立性 是 通过 法律 规定 实现 的 权力 制衡 实现 的 ,   更 重要 的 是 控制 了 钱袋子 。 联储 每年 的 净 收益 大部分 是 要 上交 财政部 ,   使得 联储 没有 从中 获利 的 可能 , 但是 使 联储 可以 超越 其他 机构 ,   不 受 国会 拨款 程序 的 限制 。       格林斯潘   vs   伯南克 ,   能力 强 + 集权   vs   民主       欧洲央行       独立性 最高       目标 就是 「 物价 稳定 」               其他 国家 中央银行           思考题           美联储 为什么 要 建立 拥有 12 家 地区 联邦 银行 的 联邦 储备 体系 , 而 不是 像 其他 国家 只 设立 一家 中央银行 ?       分权 和 制衡               国会 对 联邦 储备 体系 施加影响 的 工具 有 哪些 ?       规范 联邦 储备 体系 的 法律 是 由 国会 颁布 的 ,   并且 随时 可以 调整 。   如果 国会议员 对 联储 的 货币政策 不满 , 通常 会 威胁 要 控制 联储 的 财务 来源 , 迫使 他 像 其他 政府 机构 一样 提交 预算 申请 。       总统 通过 影响 国会 ,   提名 理事会 成员 来 影响 联储 ,   但 影响 有限 , 无法 完全 控制                   第 17 章   货币 供给 过程       联邦 储备 体系 的 资产 负债表           控制 基础 货币 的 方法 :   央行 可以 精确 控制 基础 货币 ,   但 无法 精确 控制 商业银行 的 准备金       公开市场 操作 ,   可以 严格控制 ,   也 叫 非 借入 基础 货币 ( MBn )       对 商业银行 的 贴息贷款 , 也 叫 借入 准备金 ( BR ) , 这部分 不能 严格控制 ,   因为 浮款 的 存在       基础 货币   MB   =   MBn   +   BR               多倍 存款 创造 ,   银行 支票 存款   D   =   R   /   r ,   R 是 存款 准备金 ,   r 是 存款 准备金率       持有 现金 的 增加 会 减少 存款 创造 ,   只有 存款 的 增加 才 会 带来 存款 的 多倍 创造       银行 超额 准备金 也 会 减少 存款 创造 ,   银行 持有 的 超额 准备金 受 货币 供给 变动           货币 乘数           货币 供给 ( M1 ) 等于 现金 加 支票 存款 ,   M   =   C   +   D   =   ( 1 + c ) D         基础 货币 包括 现金 、 法定 准备金 、 超额 准备金 , 假定 他们 都 跟 银行 中 的 支票 存款 成 比例 关系 , 那么   MB   =   C   +   RR   +   ER   =   cD   +   rD   +   eD   =   ( c   + r   + e ) D       M   =   ( 1 + c ) / ( r   +   c   +   e )   MB ,   货币 乘数   m   =   ( 1 + c ) / ( r   +   c   +   e )       经济 稳定 的 情况 下 ,   超额 准备金率 e 很小 ,   现金 持有 系数 c 也 比较稳定 ,   通过 调整 法定 准备金率 可以 方便 地 调节 货币 供给 。 但是 在 经济危机 的 时候 , c 会 上升 ( 挤兑 ) , e 也 会 上升 ( 不敢 放贷 ) , 导致 通过 r 调整 货币 供给 变得 很弱 了           第 18 章   货币政策 工具       准备金 市场 与 联邦 基准利率           联邦 基准利率 相当于 中国 的   shibor ,   即 准备金 同业 拆借 利率       2008 年 开始 , 美联储 开始 给 准备金 支付 利息 ,   并 将 利率 固定 为 联邦 基准利率 目标 减掉 一个 固定 的 值       通过 准备金 利率 Ier 和 央行 贴现 , 可以 将 联邦 基准利率 锁定 在   Ier   和   贴现率   Id   之间 ,   因为 如果 联邦 基准利率 跌到 Ier , 那么 有 准备金 的 银行 宁愿 把 钱 放到 央行 也 不会 借出去 ,   如果 联邦 基准利率 涨 到 贴现率 Id , 那么 缺少 准备金 的 银行 宁愿 找 央行 借钱 也 不会 去 同业 市场 拆借       因此 ,   美联储 可以 通过 对 准备金 利率 和 贴现率 的 调节 实现 对 联邦 基准利率 的 调节 ,   如果 这 两个 利率 相差 很小 , 那么 基准利率 波动 就 很小           公开市场 操作           主动型 操作 :   主动 变动 存款 准备金 和 基础 货币       防御型 操作 :   被动 低效 其他 影响 准备金 和 基础 货币 的 因素 ,   例如 浮款       主要 操作 手段 :   主要 是 对 国债 逆 回购 与 回购       操作 方式 :       短期 回购 协议   1 - 15 天       再 买 回 交易               目前 用 得 最 多 吧 ,   「 缩表 」 「 扩表 」 之类 的 就是指 这个           贴现 政策           一级 信贷 ( 例如 利率 比 联邦 基准利率 目标 高 100 个 基点 ) 、 次级 信贷 ( 比 一级 信贷 高 50 个 基点 , 给 经营 出现 问题 的 银行 ) 、 季节性 信贷       贴现 工具 可以 实现 美联储 最后 贷款人 功能 ,   为了 防止 银行 倒闭 ,   但 由于 道德风险 的 存在 ,   不能 轻易 使用       911 时间 、 次贷 危机 这个 工具 被 大量 采用 ,   以 维护 金融 稳定           法定 存款 准备金 要求           通过 调整 法定 存款 准备金       但是 这个 工具 目前 已经 几乎 不用 了 ?   国内 为啥 经常 用 呢           欧洲央行           和 美国 央行 的 类似 , 通过 存款 便利 和 贷款 便利 调整 准备金 利率 范围           中国 央行 ( 自己 补 的 )           存款 准备金率       存贷款 基准利率 ( 中国 特有 )       公开市场 操作       逆 回购 ,   就是 贷款 给 商业银行 , 到期 后要 还 ,   一般 年底 会 有 大量 的 逆 回购 ,   年底 现金 变多 导致 准备金 不够 用 的 原因 ?       常备 借款 便利 SLF ,   2013 年 开始 ,   期限 1 - 3 个 月 ,   以 有价证券 为 质押 对 商业银行 发放贷款 ,   类似 于 美联储 的 贴现 , 利率 较 高 ， 被 认为 是 利率 走廊 的 上限 。       中期 借款 便利 MLF ,   3 个 月 , 可以 展期 ,   MLF 相当于 重点 照顾 ， 定向 滴灌 ,   可以 定向 这个 牛 逼                   资料             https : / / www . federalreserve . gov / releases / h15 /             思考题       第 19 章   货币政策 的 实施 : 战略 与 策略       货币 目标 制           央行 一般 会 宣布 货币 供应量 每年 以 一个 确定 的 数值 ( 即 目标 ) 增长 , 比如 M1 增长 5 % 或者 M2 增长 6 % 等       美国       1970 ,   阿瑟 · 伯恩斯 ,   货币 目标 制 ,         1993 ,   艾伦 · 格林斯潘 ,   不再 使用 货币 目标 制               货币 目标 制 并 不是 一个 可靠 的 货币政策 执行 向导       德国 经验 值得 借鉴       短期内 货币 可以 偏离 目标值 , 但是 长期 内 , 货币 目标 还是 可以 抑制 通货膨胀       优点 :   可以 方便 测算 ,   一般 几周 就 能算出 ,   现在 估计 更 快       缺点 :   货币 总量 跟 目标 变量 关系 很弱 时 ,   就 没 啥 用           通货膨胀 目标 制           1990 年 , 新西兰 最先 使用 这个 目标 作为 名义 锚       内容       公布 中期 通货膨胀率 的 目标 数值       将 物价 稳定 作为 首要 和 长期 的 货币政策 ,   使 之 成为 一种 制度 承诺       使用 信息 综合 方法 , 即 在 指定 货币政策 中 使用 许多 变量       与 公众 和 市场 交流 , 增加 透明度       提高 央行 责任               新西兰 、 加拿大 、 英国 ,   采用 这种 目标 制后 ,   通货膨胀率 都 明显 下降 , 经济 都 取得 了 不错 的 增长       优点 :       可以 使用 所有 可 得 信息 ,   不 限于 一种 变量       普通人 容易 理解 , 透明度 高       约束 央行 ,   避免 为了 短期内 扩大 产出 、 提高 就业率 而 实施 过度 扩张 的 货币政策 , 降低 陷入 时间 不 一致 的 可能       增加 央行 的 责任 ,   新西兰 法规 规定 如果 没有 达标 , 可以 开出 央行 行长               通货膨胀 制 的 效果 还是 不错 的       这些 国家 , 通货膨胀率 一旦 下降 , 就 会 稳定 地 维持 在 低维 , 参考书 中原 图       其他 反面 观点 :       信号 有 延迟 ,   但 事实上 货币 总量 目标 也 有 这个 问题       过于 僵化 , 限制 了 政策 制定者 的 灵活性 ,   但 实际上 这种 批评 过于 鲁莽 ,   通货膨胀 制 也 会 考虑 其他 指标 , 在 特殊 情况 下 也 可以 修正 目标       。 。 。                   带有 隐形 锚 的 货币政策           美国 , 没有 明显 的 锚       控制 通胀 高于一切       通过 收集 信息 , 监测 未来 通货膨胀率       采取 \" 先发制人 \" 货币 策略       货币政策 对 通货膨胀率 的 影响 具有 滞后性 , 两年 后 才能 产生 显著 影响       通货膨胀 的 预期 早已 体现 在 工资 和 物价 的 制定 过程 中       格林斯潘 ,   「 直接 行动 」               优点 :   效果 确实 好 ,   可以 先发制人       缺点 :   不 透明 ,   这种 高度 不确定性 会 导致 金融市场 不必要 的 波动       伯南克 推进 向 通货膨胀 目标 制           策略 : 政策 工具 的 选择           美联储 和 其他 中央银行 的 三个 重要 工具 :   公开市场 操作 ,   法定 存款 准备金 ,   贴现率       两种 政策 工具 :   准备金 总量   和   利率       选择 政策 工具 三个 标准 :       可 观测 和 可 计量 ,   联邦 基准利率 比 准备金 更加 及时 , 但是 观测 到 的 是 名义 利率       可控性 ,   央行 要 能 控制 它 ,   联邦 基准利率 可控性 高 , 但是 预期 通货膨胀率 不 可控 , 导致 实际 利率 也 不 可控       对 目标 可 预计 性 ,   利率 和 目标 ( 比如 通货膨胀 ) 之间 联系 的 紧密 程度 高于 总体 准备金 总量 , 所以 全世界 的 央行 都 会 将 短期 利率 作为 政策 工具 。 加息 、 降息               泰勒 规则 :   如何 设定 联邦 基准利率 ?       联邦 基准利率   =   通货膨胀率   +   均衡 的 实际 联邦 基准利率   +   1 / 2 ( 通货膨胀 缺口 )   +   1 / 2 ( 产出 缺口 )       通货膨胀 缺口   =   当前 通货膨胀率   -   目标 通货膨胀率       产出 缺口   =   当前 GDP 与 充分就业 条件 下潜 在 GDP 估值 之间 的 百分比 偏差       举例 :   假定 当前 通货膨胀率 为 3 % ,   目标 是 2 % ,   实际 利率 设定 为 2 % ,   那么 通胀 缺口 为   + 1 % ,   实际 GDP 比 潜在 GDP 高 1 % ,   那么 基准利率 应该 设定 在   =   3 %   +   2 %   +   0.5 %   +   0.5 %   =   6 %       通货膨胀率 每 上升 1 % ,   基准利率 应该 上升 1.5 % ,   货币 当局 应该 讲 名义 利率 提升 至 高于 通胀率 的 升高 幅度 的 水平 ,   否则 将 导致 宽松 的 货币政策 ,   带来 更加 严重 的 通胀                   资产 泡沫           信贷 驱动 的 泡沫 :   次贷 危机 ,   最 严重       过度 乐观 的 预期 驱动 的 泡沫 :   2000 年 科技股 泡沫       格林斯潘 认为 央行 应该 放任 泡沫 ,   因为 无法 知道 那 是否是 泡沫 ,   泡沫 只有 在 破 了 之后 才 知道 是 泡沫       对 非理性 繁荣 不用 管 , 但是 要管 信贷 扩张 带来 的 泡沫       提高 利率 不 一定 能 降低 泡沫 , 反而 会 加剧 泡沫 破裂 后 对 经济 的 恶劣影响 ;   并且 还会 带来 额外 的 不利 影响 。       结论 是 : 不 应该 利用 货币政策 来 打破 泡沫   ( 中国 房地产 ,   非理性 繁荣 ? ? ? )       对 金融 功 机构 的 反 周期性 资本金 要求 ,   在 繁荣 的 时候 提高 资本金       次贷 危机 告诉 我们 , 不能 放任 信贷 驱动 的 泡沫           联邦 储备 体系 的 政策 演化           早期 :   贴现 政策 作为 主要 工具       一战 末期 , 通过 再贴现 的 低利率 为 战争 筹款 , 1919 - 1920 年间 , 通货膨胀率 高达 14 %       1920 年 , 将 贴现率 从 4.75 % 提高 到 6 % , 又 提高 到 7 % , 并 维持 了 一年 , 导致 货币 供应量 急剧下降 , 以及 1920 - 1921 年 的 大 萧条       在 物价 下降 之后 , 通胀 降到 了 零 , 为 「 繁荣 的 20 年代 」 铺平 了 道路       20 年代 初期 , 发现 了 公开市场 操作 。 大 萧条 之后 , 贴现率 太高 导致 贴现 贷款 规模 下降 , 联储 的 收益 全部 来自 于 贴现 利息 , 这 导致 联储 收益 急剧 降低 。 迫使 联储 不得不 在 公开市场 购买 盈利性 证券 , 然后 发现 这种 操作 导致 了 会员 银行 的 准备金 增加 了 , 并且 银行 的 存贷款 发生 了 多倍 扩张 !       美联储 在 大 萧条 时期 , 没有 承担 其 最后 贷款人 的 责任 , 导致 了 1930 - 1933 年 的 银行 危机       30 年代 ,   获得 调节 法定 存款 准备金率 的 权利 !       经历 大 萧条 的 银行 为了 避免 挤兑 , 开始 囤积 了 大量 的 超额 准备金 ,   而 当时 的 美联储 担心 这些 超额 的 准备金 会 流入 市场 , 形成 无法控制 的 信贷 扩张 ,   于是 在 1936 - 1937 年分 三次 上调 法定 存款 准备金率 , 这 带来 了 货币 供应量 的 萎缩 , 以及 1937 - 1938 年 的 经济 萎缩 ,   以及 极高 的 失业率 。 这次 惨痛 经历 , 也 使得 美联储 后来 很少 使用 这个 工具       1942 - 1951 ,   美联储 为了 帮助 美国 二战 的 军费 筹资 ,   将 短期 国债 利率 钉 在 0.375 % , 长期 国债 利率 为 2.5 % 。 无论 何时 , 只要 债券 利率 上升 价格下降 , 美联储 就 通过 公开市场 操作 购买 债券 , 将 利率 降低 到 目标 位置 。 结果 是 , 导致 了 货币 供给 的 快速 扩张       1950 年 的 朝鲜战争 爆发 , 导致 利率 的 高速 增长 , 引发 了 美联储 和 财政部 的 激烈 争论 , 美联储 不愿 货币 供给 继续 快速 扩张 , 而 财政部 要求 美联储 将 利率 继续 钉 在 低位 , 以 实现 军费 筹备 。 最后 达成 共识 是 , 美联储 可以 提高 利率 , 但是 要 慢慢 提升 。 在 1952 年 , 艾森豪威尔 当选 总统 之后 , 赋予 了 美联储 独立 实施 货币政策 的 权利       以 货币 市场 为 目标 ,   20 世纪 五六十年代 。 将 货币 市场 状况 , 尤其 是 利率 , 作为 货币政策 的 目标       这个 目标 是 在 经济 扩张 周期 , 货币 供应 快速增长 ; 而 在 萧条 时期 , 货币 增长 缓慢 。 即 「 顺 周期 调节 」 但是 导致 经济波动 很大       1970 年 , 阿瑟 · 伯恩斯 担任 美联储 主席 , 宣布 将 货币 总量 作为 中介 指标 , 通过 公开市场 操作 进行 。 然而 由于 货币 总量 跟 利率 之间 的 矛盾 , 导致 美联储 实际上 会 为了 利率 的 稳定 , 而 放弃 货币 总量 的 目标 , 从而 导致 了 「 顺 周期 」 的 货币政策 。       1979 - 1982 ,   美联储 将 联邦 基准利率 的 目标 范围 放宽 了 5 倍 多 , 主要 操作 工具 变成 了 非 借入 准备金 , 这 导致 基准利率 大幅 波动 , 货币 供给 增长率 也 波动 很大       1982 - 20 世纪 90 年代 初 , 放弃 货币 总量 目标 , 转向 将 贴现 贷款 作为 操作 工具 , 实现 利率 稳定 。       20 世纪 90 年代 以后 , 再次 以 联邦 基准利率 作为 目标 ,   并且 定期 公布 。 并 采用 先发制人 的 方式 调节           思考题           \" 失业 是 一件 糟糕 的 事 , 政府 应该 尽 一切 努力 消除 失业 \" ,   你 是否 统一 这种 看法 ?       失业 是 糟糕 的 事 , 但是 存在 一个 自然 失业率 ,   所以 达到 充分就业 并 不 意味着 没有 人 失业 。 反而 , 低于 自然 失业率 的 低 失业率 反而 表明 劳动力 比较 缺乏                   下列 哪些 变量 是 操作 工具 , 哪些 是 中介 指标 :   10 年期 国债 利率 ,   基础 货币 ,   M2           操作 工具 指能 直接 控制 的 指标 ,   所以 基础 货币 是 的       中介 指标 是 指标 介 影响 的 容易 观测 的 指标 ,   10 年期 国债 利率 和 M2 都 是                   比较 基础 货币 和 M2 的 可测性 和 可控性 ,   你 选择 什么 做 中介 指标           基础 货币 在 可测性 和 可控性 都 高于 M2 , 但是 M2 更 适合 做 中介 指标 , 因为 M2 跟 目标 的 联系 更 紧密               货币政策 执行 中 使用 名义 锚 的 好处 ?       可以 避免 时间 不 一致 的 问题               将 货币 目标 制作 为 货币政策 的 好处 有 哪些 ?       方便 测量 核算 ,   可以 立即 了解 央行 是否 实现 了 其 目标               货币 目标 制 成功 的 前提 是 什么 ?       货币 总量 跟 最终目标 存在 强 相关               为什么 采用 通货膨胀 目标 制 有利于 中央银行 独立 执行 政策 ?                    ", "tags": "economics/money-banking-and-financial-market-book", "url": "/wiki/economics/money-banking-and-financial-market-book/money-banking.html"},
      
      
      {"title": "第5篇 国际金融和货币政策", "text": "    Table   of   Contents           第 20 章   外汇市场           长期 汇率           问题 与 思考题                   第 21 章   国际 金融体系           外汇市场 干预           中国 的 外汇           汇率 制度           思考题                         第 20 章   外汇市场           外汇交易       即期 交易       远期 交易               本币 升值 :       外国 商品 贬值 , 本国 商品 升值       有利于 进口 , 不利于 出口               汇率 波动 , 即 有人 收益 , 也 有人 损失       外汇市场 是 以 柜台 交易 的 形式 运行 的 , 市场 上 的 交易商 主要 是 银行 , 交易 标的物 是 外币存款       外汇交易 门槛 高 , 通常 是 高达 100W 美元 以上 的 交易       个人 旅行 所购 的 外汇 , 通常 是 通过 外汇 零售商 购买 ( 如 美国 运通 ) 和 银行           长期 汇率           一价 定律 :   如果 两个 国家 生产 的 商品 是 同质 的 ,   且 运输成本 和 贸易壁垒 都 非常低 , 那么 该 商品 在 全世界 范围 内 的 价格 都 应该 是 一样 的       购买力 平价 理论 :   两种 货币 的 汇率 会 随着 两 国 物价水平 的 变动 而 变动       实际汇率 :   实际汇率 是 指 国内 商品价格 与 以 本币 表示 的 国外 商品价格 的 比率 。 例如 一 篮子 商品 在 美国 是 50 美元 , 在 日本 是 7500 日元 , 而 美元 对日元 的 汇率 是 1 : 100 , 那么 在 东京 的 价格 用 美元 表示 为 75 美元 , 那么 对 美国 来说 , 实际汇率 是 50 / 75 = 0.66 , 这 表明 美国 国内 的 这 一 篮子 商品 比 日本 便宜 。 购买力 评价 理论 是 说 实际汇率 恒 等于 1 .       购买力 平价 理论 说明 ,   如果 一国 的 物价水平 相对 另 一国 的 上升 , 那么 该国 货币 就 会 相对 另 一国 的 货币贬值       1973 年 - 1980 年 , 英国 物价水平 相对 美国 物价上涨 了 89 % , 那么 按照 购买力 平价 理论 , 美元 将 对 欧元 升值 , 实际 升值 比例 在 58 % 的 样子 。 但是 波动 很大 , 说明 购买力 平价 理论 对 汇率 的 短期 预测 能力 很差 。           购买力 平价 理论 的 问题 :   商品 同质 的 假设 过 强 , 很多 商品 和 劳务 的 交易 具有 很强 的 地域性 , 不能 跨国 贸易 。 例如 : 房产 、 土地 、 餐饮 etc       长期 汇率 影响 因子 :   提高 可 贸易 的 本国 商品 需求 的 因素 , 将会 让 本币 升值       相对 价格水平 :   如果 本国 价格水平 上升 , 而 外国 价格水平 不变 , 那么 本国 商品 需求 将会 降低 , 除非 本币 贬值 才 会 达到 稳定       贸易壁垒 ,   即 关税 和 配额 。 假设 美国 提高 关税 、 降低 外国 商品 配额 , 那么 会 增加 本国 商品 的 需求 , 从而 是 本币 升值 。 长期 看 ,   增加 贸易壁垒 , 会 使 本币 升值 。   那么 问题 来 了 ,   中美 两国 都 增加 贸易壁垒 , 那么 中美 两国 的 汇率 就 可能 互相 抵消 么 ?       对 本国 商品 和 外国 商品 的 偏好 ,   长期 看 ,   一国 出口商品 的 增加 会 让 本币 升值 ,   而 进口商品 的 增加 会 让 本币 贬值       生产率 ,   一国 的 生产率 的 提高 , 将 降低 本国 商品 的 成本 , 从而 提升 本国 商品 的 需求 , 进而 让 本币 升值                   外汇储备 与 汇率 ,   人民币 贬值 将会 增加出口 , 从而 增加 外汇储备               短期 汇率 :   供求 分析 。   短期 汇率 更 多 的 依赖于 存量 外汇           汇率 的 变动       本币 利率 , 本币 利率 的 上升 , 意味着 本币 的 回报率 更 高 了 , 所以 本国 资产 的 需求 会 提升 , 本币 升值       预期 未来 的 汇率变动 ,   预期 汇率 的 提高 ,   本币 会 变得 更 值钱 ,   这 将 使 本币 升值       利率 变动 如果 是 由 实际 利率 上升 导致 的 , 那么 本币 升值 。 如果 是 有 预期 通货膨胀率 导致 的 , 将会 使 本币 贬值       货币 供给 的 增加 ,   将 导致 本币 贬值       汇率 超调 ,   货币 供给 的 增加 对 短期 汇率 的 贬值 程度 大于 长期 汇率 贬值 程度       本币 跟 实际 利率 强 相关 , 跟 名义 利率 相关性 要弱 一些               次贷 危机 后 美元 迅速 贬值 然后 又 升值 的 解释       次贷 危机 发生 后 , 当时 只有 美联储 将 利率 大幅 降低 接近 0 利率 , 而 其他 国家 央行 暂时 并 没有 调低 利率 , 导致 美元 迅速 贬值 。 随后 , 次贷 危机 开始 波及 全球 , 导致 其他 国家 的 央行 也 调低 利率 , 并且 预期 会 继续 下调 ,   导致 美元 升值 。 进一步 , 由于 避险 的 需要 , 对 美国 国债 的 需求 的 提升 也 进一步 导致 美元 升值 。               利息 平价 条件 :   国内 利率   =   外国 利率   -   预期 本币 的 升值 率 ,   这个 条件 本质 是 本国 资产 和 外国 资产 预期 汇率 报 必须 相等 。 只有 当 这个 条件 成立 的 时候 ,   投资者 才 会 同时 持有 本币 和 外币           问题 与 思考题           如果 欧元 升值 , 你 更 愿意 和 加州 的 酒 还是 欧洲 酒 ?       当然 是 加州 酒 了 , 因为 欧元 升值 导致 欧洲 酒以 美元 计价 的 价格上涨 了               如果 日本 物价水平 相对 美国 上涨 5 % ,   那么 根据 物价 平价 理论 , 以 美元 表示 的 日元 价值 会 发生 怎样 的 变动 ?       日本 物价水平 相对 上涨 5 % ,   为了 物价 保持 不变 , 那么 日元 应该 贬值 5 % , 才能 保证 以 美元 计价 的 物价水平 保持 不变 。               美国 总统 宣布 , 将 启动 新 的 反 通胀 侧 策略 来 降低 通胀率 , 如果 公众 相信 总统 , 那么 美元汇率 会 怎么 变化       预期 通胀率 的 降低 , 将 增加 美元 资产 的 需求 , 导致 美元 升值                   第 21 章   国际 金融体系       外汇市场 干预           外汇 干预       外汇储备 / 国际 储备       卖出 外汇 ( 资产 减少 ) , 买入 本币 ( 负债 减少 )       央行 在 外汇市场 购买 本国货币 的 时候 , 会 导致 基础 货币 和 国际 储备 等额 减少       央行 在 外汇市场 购买 外汇 的 时候 , 会 导致 基础 货币 和 国际 储备 等额 增加       冲销 性 外汇 干预 : 如果 央行 不 希望 在 外汇 操作 的 时候 影响 基础 货币 的 供应 , 就 需要 在 国债 市场 上 进行 反向 对冲 操作               国际收支               经常 账户 :   投资 收入 、   服务 交易 、   单方面 转移 ( 赠与 、 补贴 与 对外 援助 )       资本 账户 :   资本 交易 的 净收入       经常 账户   +   资本 账户   =   政府 国际 储备 的 净 变动       经常 账户 是 现金 和 实物 ?   资本 账户 是 有价证券 ?       经常 账户 的 赤字 ( 即 贸易逆差 ) 会 导致 对 本币 的 需求 减少 , 对 本币 造成 贬值 压力           中国 的 外汇           中国 目前 ( 201901 ) 的 外汇储备 是 21.25 万亿 人民币 , 约 占 央行 的 总资产 的 3 / 3 ,   对 其他 存款 性 公司 的 债权 10.7 万亿 , 约 占 总资产 的 1 / 3 , 而 国债 只有 1.5 万亿 , 很少 。 总 的 资产 负载 表 规模 是 36.32 万亿 人民币 , 按照 当前 汇率 计算 , 对应 5.4 万亿美元       美联储 目前 的 资产 负载 表 规模 在 4 万亿美元 , 比 中国 要少 , 相比 最高点 已 缩减 了 4000 亿美元 。           汇率 制度           固定汇率 、 浮动汇率 、 有 管理 的 浮动 ( 通过 买买 货币 间接 影响 汇率 ,   这个 是 目前 的 主流 )       一战 之前 ,   大多数 国家 货币 是 跟 黄金 挂钩 的 , 即 金本位制 度 ,   所以 货币 之间 的 汇率 也 是 固定 的       二战 之后 ,   战胜国 建立 了 一个 固定 的 汇率 体系 ,   即 布雷顿 森林 体系 ,   持续 到 1971 年       国际货币基金组织 ( IMF ) :   向 遭受 国际收支 困难 的 国家 和 地区 提供 贷款 ,   维护 固定汇率 体系 ,   收集 各国 经济 数据       世界银行 :   为 发展中国家 提供 贷款 ,   其 资金来源 是 在 发达国家 发行 的 债券       世贸组织 WTO       美元 被 作为 储备 货币 ,   其他 国家 的 持有 的 国际 资产 都 是 以 美元 计价 的 。       欧元区 ,   德国马克 被 当做 锚定 货币       汇率 的 稳定 需要 国际 储备 来 维持 ,   本币 升值 的 时候 要 买入 国际 储备 , 发行 本币 ,   本币 贬值 时则 要 卖出 国际 储备 减少 本币 的 供给 , 如果 国际 储备 减少 到 0 了 , 那么 就 出 法定 贬值       在 资本 可以 自由 流动 的 情况 下 ,   固定汇率 会 使 小国 的 货币政策 也 完全 依赖于 大国 ,   大国 加息 效果 也 得 跟着 加息 ,   因为 根据 利息 评价 理论 ,   固定汇率 的 本币 升值 率为 0 ,   所以 两国 的 利率 也 需要 完全一致       特别 提款权 SDR :   IMF 发行 的 代替 黄金 ,   也 叫 纸黄金 ,   凭 它 可以 到 IMF 换 各 成员国 的 货币       欧洲 货币 体系   EMS       各 成员国 约定 汇率 可以 在 一个 小 范围 内 波动 , 一旦 超过 就 必须 干预               投机性 攻击       1992 年 外汇 危机 ,   德国 统一 后 , 面临 高 通胀 的 问题 , 于是 德国 央行 提高 了 本国 利率 , 这 给 其他 国家 货币贬值 压力 , 最后 导致 英国 退出 了   EMS                   兴新 市场 的 外汇 危机               资本 管制           IMF :   国际金融 的 最后 贷款人 ?       道德风险       慢 ,   导致 金融危机 已经 发生 并且 扩散 开 了 。 应急 信贷 额 与 短期 流动性 便利               货币政策 的 汇率 目标 制       优点       控制 通胀       避免 时间 不 一致 , 因为 汇率 实时 可见       简单 易 理解               快速 降低 通胀 的 手段       缺点       没法 应对 国内 突发事件       失去 了 货币政策 的 自主权 ,   法国 盯住 德国马克 , 导致 了 本国 的 高 通胀               新兴 市场 采用 汇率 目标值 优点 更大 ,   持续 通货膨胀 时 非常适合 采用 汇率 目标 制               货币 局 :   以 外汇 作为 本币 发行 的 基础 ,   香港 、 阿根廷 。 有利于 快速 控制 通胀 , 但是 容易 发生 货币 紧缩 ( 无法 抵消 公众 恐惧 导致 的 收缩 性 货币政策 ) ,   在 金融危机 的 时候 , 央行 无法 承担 最后 贷款人 角色 , 导致 最终 阿根廷 比索 崩盘 , 对 美元 贬值 70 %       美元 化 :   采用 美元 ( 其他 国家 的 货币 ) 而 放弃 本币 。 相当于 放弃 铸币 税 ,   因为 央行 用 发行 的 货币 购买 了 有价 资产 , 可以 获得 收益 , 美联储 每年 获得 的 铸币 税有 300 亿刀           思考题           如果 美联储 在 外汇市场 中 购买 美元 ， 但是 实施 冲销 性 的 公开市场 操作 以 抵消 外汇市场 干预 的 影响 ， 这种 操作 对 国际 储备 、 货币 供给 和 汇率 有 什么 影响 ？       国际 储备 减少 了 , 用来 兑换 美元 了 ,   货币 供给 减少 了 , 因为 流通 的 美元 减少 了 ,   汇率 会 上升 , 因为 美元 的 供给 减少 了      ", "tags": "economics/money-banking-and-financial-market-book", "url": "/wiki/economics/money-banking-and-financial-market-book/international-financial.html"},
      
      
      {"title": "第6篇 货币理论", "text": "    Table   of   Contents           第 22 章   货币 需求           思考题                   第 23 章   总需求 和 总共 给 分析           第 24 章   货币政策 传导 机制                 第 22 章   货币 需求           核心 问题 :   货币 需求 是否 或者 多 大程度 上受 利率 影响       货币 数量 论       人物 :   欧文 · 费雪       M   ×   V   =   P   ×   Y ,   货币 总量   ×   流通 速度   =   名义 收入 / GDP   =   价格水平   ×   总 产出       并 不是 决定 M 的 方程 , 因为 流通 速度 V 的 不 确定 因素       而费雪 认为 , 短期内 流通 速度 V 是 稳定 的       数量 论 :   名义 收入 仅 由 货币 供应 变动 来 决定 ,   如果 货币 数量 M 翻一番 , 那么 名义 收入 也 翻一番 ,   Y 短期内 也 保持稳定 , 因此 物价指数 P 翻一番 。 即 物价水平 完全 由 货币 数量 变动 带来 的       数量 论 认为 货币 需求 完全 由 收入 决定 ,   跟 利率 无关 。       持有 货币 完全 是 为了 交易       但是 货币流通 速度 并 不是 一个 常数 , 并且 波动 很大       个人 思考 :   费雪 方程 与 通货膨胀率 估计 实证     根据 费雪 方程 , 并 假定 流通 速度 稳定 , 那么   货币 总量 的 同比 变动   =   价格水平 的 同比 变动 ( 即 通货膨胀率 )   +   总 产出 的 同比 变动 ( 即 扣除 通胀 后 的 实际 GDP 增长率 )     所以 ,   通货膨胀率   =   M2 增速   -   实际 GDP 增速   是 有 一定 道理 的     根据 这个 估计 ,   中国 从 2008 - 2018 年 10 年间 的 通货膨胀率 大约 在 7 %     参考 :     https : / / www . zhihu . com / question / 22923480                         凯恩斯 流动性 偏好 理论 ,   持有 货币 的 3 大 动机       交易 动机 ,   买 东西 ,   跟 收入 正 相关       预防 动机 ,   预防 未知 支出 ,   跟 收入 正 相关       投机 动机 ,   跟 收入 正 相关 , 还 跟 利率 有关 , 利率 高 的 时候 跟 倾向 持有 债券 而 利率 低 的 时候 更 倾向 持有 货币 。 利率 与 货币 需求 负相关       需求 函数   M / P   =   f ( i ,   Y ) ,   f 跟 利率 i 负相关 , 跟 收入 Y 正 相关 。 所以 流通 速度 V 跟 f 成反比 , 跟 i 正 相关 , 所以 和 i 一样 具有 顺 周期性 。       理解 一下 :   凯恩斯 的 理论 是 说 ,   在 收入 Y 不变 ( GDP 不变 的 情况 下 ) , 利率 提高 会 导致 人们 购买 更 多 的 债券 , 减少 对 货币 的 需求 , 所以 货币 需求 会 减少 , 表现 为 M2 降低 或者 通胀 P 增加 ?               凯恩斯 理论 进一步 发展 :   鲍莫尔 · 托宾       交易 需求 也 对 利率 敏感 ,   因为 利率 越高 ,   人们 就 更 愿意 将 尚未 交易 的 钱 购买 债券 ,   而 成本 是 jiao 交易 手续费 和 时间 成本       持有 货币 是 有 机会成本 的 ,   当 利率 上升 的 时候 , 持有 货币 的 机会成本 就 会 上升               凯恩斯 理论 的 问题 ,   投资性 需求 导致 人们 不会 同时 持有 货币 和 债券 , 二十 几种 这种 情况 是 普遍现象       托宾 通过 风险 修复 了 这个 模型 ,   他 认为 多样化 的 资产 配置 可以 降低 风险       但是 托宾 的 理论 并 没有 解释 几乎 无风险 的 美国 国库券 为什么 没有 替代 剩下 的 货币                   弗里德曼 的 现代 货币 数量 论           笼统 的 认为 影响 资产 需求 的 因素 都 会 影响 货币 的 需求       $ ( Y _ p ) $ 是 预期 收入 的 现值 ,   $ ( r _ b ) $ 是 债券 预期 回报率 ,   $ ( r _ m ) $ 是 货币 预期 回报率 , $ ( r _ e ) $ 是 股票 预期 回报率 ,   $ ( \ \ pi _ e ) $ 是 预期 通货膨胀率       弗里德曼 用 永久 收入 作为 货币 需求 因素 , 意味着 货币 需求 不 受 经济周期 影响 ?   所以 还是 GDP 决定 货币 需求 罗 ?       财富 储藏 除了 货币 之外 , 还有 债券 、 股票 和 商品 , 这些 因素 对 货币 需求 的 影响 反映 在 他们 的 预期 回报率 上 , 商品 对应 的 是 通货膨胀率     $ $     \ \ frac { M _ d } { P }   =   f ( Y _ p ,   r _ b   -   r _ m ,   r _ e   -   r _ m ,   \ \ pi ^ e   -   r _ m )     $ $                   弗里德曼 与 凯恩斯 理论 的 区别 :           凯恩斯 认为 只有 一种 利率 影响 , 而 弗里德曼 认为 有 多种       弗里德曼 没有 将 货币 的 回报率 看做 一个 常数       银行 间 充分 竞争 中 ,   债券 回报率   -   货币 回报率   的 差值 保持稳定       哪怕 货币 利率 收到 管制 ( 例如 中国 ) ,   债券 回报率   -   货币 回报率   的 差值 也 将 达到 稳定 , 银行 会 在 其他 方面 竞争 ? ( 现在 银行 的 货币基金 各种 宝宝 ? )       因此 , 从 本质 上 来说 ,   弗里德曼 认为 在 利率 变动 的 情况 下 , 回报率 的 差值 会 保持稳定 , 所以 货币 需求 只是 永久性 收入 的 函数 , 跟 利率 无关 。 因为 利率 上升 会 导致 货币 的 回报率 也 会 上升 , 从而 抵消 利率 上升 带来 的 其他 资产 相对 货币 回报率 的 上升       弗里德曼 认为 货币 需求 是 稳定 的 , 波动 不 大 , 因此 是 可 预测 的 , 流通 速度 也 是 可 预测 的 , 名义 收入 取决于 货币 供给                   实证 分析 :           流动性 陷阱 :   当 利率 降低 到 0 的 时候 ,   货币 需求 曲线 就 变 平坦 了 ,   不管 需求 是 多少 , 利率 都 是 0 了 ! 所以 货币政策 就 很 难 刺激 经济 增长 了       20 世纪 70 年代 以前 , 货币 需求 函数 是 稳定 的 , 但是 金融 创新 的 发展 , 导致 了 需求 函数 出现 极大 的 不稳定性 , 因而 采用 货币 供应量 目标 制 不再 是 央行 的 一个 比较 好 的 目标 了 ,   因为 货币 需求 跟 总 产出 的 关系 ( 比例 系数 ) 波动 很大 了 !                   思考题           为什么 弗里德曼 的 货币 需求 理论 认为 货币流通 速度 是 可 预测 的 ， 而 凯恩斯 理论 的 观点 却 与 之 相反 ？       因为 他 认为 货币 需求 只 跟 长期 总收入 相关 , 而 长期 总收入 是 稳定 的 , 所以 流通 速度 是 可 预测 的 ;   而 凯恩斯 认为 还 跟 利率 有关 , 利率 波动 导致 流通 速度 不 可测性 。                   第 23 章   总需求 和 总共 给 分析           货币 总需求 ( 持有 的 货币 数量 )   Y   =   C   +   I   +   NX   +   G       消费 支出   C :   用于 购买 消费品 和 服务 ,   手机 、 定 外卖 etc       投资 支出   I :   企业 新 机器 、 新建 厂房 、 新建 住宅 ,   购买 的 东西 要 变成 资本       净 出口   NX :   出口   -   进口       政府 支出   G :   政府 购买 商品 和 服务 的 支出               需求 曲线   Y   =   Y ( P ) ,   P 是 价格水平       价格水平 下降   = & gt ;   货币 实际 供给 增加 ( 名义 货币 不变 )   = & gt ;   利率 下降   = & gt ;   投资 增加   = & gt ;   货币 总需求 增加       价格水平 下降   = & gt ;   ...   = & gt ;   利率 下降   = & gt ;   汇率 下降 、 本币 贬值   = & gt ;   出口 增加   = & gt ;   货币 总需求 增加       根据 费雪 方程 ,   名义 货币 不变 、 流通 速度 不变 ,   那么 价格水平 下降 必然 导致 货币 总需求 增加       需求 曲线 的 移动 , 即 P 保持 不变       货币 供给 的 增加 ,   流通 速度 不变 ,   货币 需求 Y 必然 增加 ,   曲线 往右 移动 ;   货币 供给 增加 会 导致 利率 降低 ,   带来 了 投资 和 出口 的 增加 ,   从而 导致 需求 Y 的 增加       政府 支出 增加 或 进出口 增加 , 导致 货币 需求 的 增加       税收 的 减少 , 导致 居民 可 支配 收入 的 增加 , 带来 了 消费 支出 增加 , 货币 需求 增加 ;   但是 税收 减少 会 导致 政府 支出 减少 吗 ?       消费者 和 企业 乐观 情绪 / 悲观 情绪 增加 , 将 导致 消费 和 投资 的 增加 / 减少 ,   「 动物 精神 」               总供给 ( 挣 的 钱 )       长期 总供给 曲线 :   自然 失业率 水平 下 的 自然 产出 水平 , 他 是 长期 经济 在 任意 价格水平 下 都 要 达到 的 产出 水平 , 所以 长期 供给 曲线 是 一条 垂线 ,   即   Y ( P )   ≡   Y 。 理解 :   长期 稳定 的 产出 目标 跟 价格水平 无关 , 而是 经济 固有 的 结果 , 价格水平 高 了 只会 让 名义 产出 变高       短期 总供给 曲线 :   短期 来看 ,   价格水平 的 提高 将 使得 企业 的 收入 增加 , 但是 工资 成本 和 原材料 成本 短期 保持 不变 , 从而 让 企业 的 利润 增加 , 使得 企业 扩大 生产 投资 , 增加 了 总 产出 。 所以 短期 供给 曲线 斜率 是 正 的       短期 总供给 曲线 的 移动 :   生产成本 的 提高 , 将 导致 相同 价格水平 下 , 企业 的 产出 就 会 减少 ,   供给 曲线 向 左移       影响 短期 总供给 曲线 移动 的 因素 ,   即 影响 生产成本 的 因素       劳动力 市场 的 供求关系       通货膨胀 预期 ,   预期 通货膨胀 将 导致 工资 上涨 , 从而 提高 生产成本       工人 增加 实际工资 的 努力 :   罢工 等       与 工资 无关 的 生产成本 变化 :   技术 变革 及 原材料 的 变化                       总供给 与 总需求 的 均衡           第 24 章   货币政策 传导 机制  ", "tags": "economics/money-banking-and-financial-market-book", "url": "/wiki/economics/money-banking-and-financial-market-book/money-theory.html"},
      
      
      
        
        
      
      {"title": "第1部分 简介", "text": "    Table   of   Contents           关于           第 1 章   引言                 关于       《 期权 、 期货 及其 他 衍生品 》 ( 原书 第 9 版 ) 笔记       第 1 章   引言           衍生 产品 的 规模 远远 大于 股票市场 ! ! 甚至 是 全球 经济 总产值 的 诺 干倍       用于 对冲 风险 、 投机 和 套利       衍生 产品   derivative   是 指有 某种 更为 基本 的 变量 派生 出来 的 产品       高频 交易 与 算法 交易      ", "tags": "economics/options-futures", "url": "/wiki/economics/options-futures/intro.html"},
      
      
      
      {"title": "《公司理财》", "text": "    Table   of   Contents           关于           概论                 关于       罗斯 的 《 公司 理财 》 是 商学院 的 经典 教材 , 这里 记录 了 个人 认为 是 重点 , 且 之前 不 知道 的 新 知识 。 部分 知识 在 《 财务会计 教材 》 、 《 货币 金融学 》 、 《 投资 学 》 中 已经 记录 了 的 , 不再 重复 。       概论           杜邦 恒等式 :         内部 增长率 , 没有 任何 外部 融资 的 情况 下 , 公司 可能 实现 的 最大 增长率       可 持续 增长率 , 可以 用来 分析 公司 未来 的 融资 决策 。 是 公司 没有 外部 股权 融资 且 保持 负债 - 权益 比 不变 的 情况 下 可能 实现 的 最高 增长率       如何 评估 公司 的 价值       用 公司 未来 的 净现金 流 折现 得到 的 现值 , 折现 率 可以 用 《 投资 学 》 里 的 风险 分析 的 预期 收益率 ?              ", "tags": "economics", "url": "/wiki/economics/corporate-finance.html"},
      
      
      {"title": "国际清算银行 2018 年度经济报告", "text": "    Table   of   Contents           关于           国际清算银行           年度 经济 报告                         关于       国际清算银行       参考 :     https : / / www . fmprc . gov . cn / web / wjb _ 673085 / zzjg _ 673183 / gjjjs _ 674249 / gjzzyhygk _ 674253 / gjqsyh _ 674479 / gk _ 674481 /                 【 成立 】 成立 于 1930 年 ， 最初 为 处理 第一次世界大战 后 德国 战争 赔款 问题 而 设立 ， 后 演变 为 一家 各国 中央银行 合作 的 国际 金融机构 ， 是 世界 上 历史 最 悠久 的 国际金融组织 。               【 宗旨 】 以 “ 促进 各国 中央银行 之间 的 合作 并 为 国际 金融业务 提供 便利 ” 为 宗旨 ， 是 中央银行 行长 和 官员 的 会晤 场所 。               【 成员 】 目前 共有 60 家 成员 中央银行 或 货币 当局 。 中国人民银行 于 1996 年 9 月 正式 加入 国际清算银行 。 1996 年 11 月 中国 认缴 了 3000 股 的 股本 ， 实缴 金额 为 3879 万美元 。 2005 年 6 月 1 日 ， 经 追加 购买 ， 中国 共有 该行 4285 股   的 股本 。 中国人民银行 是 该行 亚洲 顾问 委员会 的 成员 。 目前 ， 中国人民银行 易纲 行长 系 国际清算银行 董事 。     https : / / www . bis . org / about / member _ cb . htm                 【 总部 】 瑞士 巴塞尔 。 网址 ： http : / / www . bis . org               【 组织 结构 】 股东大会 、 董事会 、 管理 委员会 是 主要 决策 和 管理机构 。               【 股本 和 资金来源 】 2003 年 4 月 1 日起 ， 国际清算银行 使用 国际货币基金组织 特别 提款权 （ SDR ） 计算 股本 ， 共有 面值 相等 的 60 万股 （ 每股 面值 5000SDR ） ， 由 成员国 认缴 。               年度 经济 报告       原始 报告     https : / / www . bis . org / publ / arpdf / ar2018 _ zh . htm    ", "tags": "economics", "url": "/wiki/economics/bis-report-2018.html"},
      
      
      {"title": "经济金融学相关教材", "text": "    Table   of   Contents           关于           证券 投资 学           货币 银行学           国际金融           计量经济学           金融机构 与 金融市场           金融 经济学 基础           微观 经济学 A           精算学 基础           风险管理 与 保险           非 参数 统计           时间 序列 分析 A           金融学 导论           固定 收益 证券           试验 设计 与 方差分析           公司 金融           博弈论 及其 应用           金融 衍生品 导论           金融风险 管理           金融 衍生品           会计学 原理           财务管理           运筹学           宏观 经济学           微观 经济学           鞅 论 与 随机 积分           人工智能 导论           计算机 视觉           自然语言 处理           信号 统计 建模 基础                 关于       收集 自 科大 教务 系统     https : / / mis . teach . ustc . edu . cn / kbcx . do         证券 投资 学           “ 投资 学 ” （ 英文版   原书 第 8 版 ） （ 美 ） 滋维 ? 博迪 ， 亚         《 投资 学 》   赵昌文 ， 俞乔 主编   清华大学出版社         《 证券 投资 学 》 是 一门 应用型 课程 ， 主要 介绍 了 投资 的 概念 与 证券市场 的 相关 知识 。 本 课程 主要 包括 投资 主体 、 投资 环境 、 投资 工具 三个 部分 ， 使 学生 掌握 投资 学 的 一般 理论 ， 并 将 所学 知识 应用 于 实践 。             货币 银行学           黄达 .   货币 银行学 .   中国人民大学出版社 .         易纲   吴有昌 .   货币 银行学 .   上海人民出版社 .         米 什金 .   货币 金融学 .   中国 人民出版社 .         现代 货币 学 教程   胡庆康   复旦大学 出版社         本 课程 主要 介绍 货币 银行学 的 基本 理论 ， 并且 结合 所学 理论 分析 现实 的 金融 问题 。 课程内容 包括 ： 货币 及其 制度 ； 信用 与 利率 ； 金融市场 与 金融机构 体系 ； 商业银行 理论 ； 中央银行 理论 与 货币政策 ； 货币 供给 与 需求 理论 ； 通货膨胀 理论 。             国际金融           《 国际金融 》 ， 陈 雨露 ， 中国人民大学出版社 .         《 国际金融 新编 》 第 4 版   姜波克   复旦大学 出版社         国际 金融学 是 一门 学习 国际 金融学 的 一般 知识 、 经典 理论 和 内外 均衡 分析方法 的 课程 ， 包括 国际收支 和 国际 收支平衡 表 、 汇率 、 内外 均衡 的 短期 调节 和 中长期 调节 、 外汇 管理制度 和 政策 调节 、 金融 全球化 对 内外 均衡 的 冲击 。             计量经济学           D ． N ． 古 扎拉 蒂著   《 计量经济学 基础 》 （ 第 4 版 ） ， 林少 宫译 ， 中国人民大学出版社 ， 2005 ， 北京         《 计量经济学 》 ， 李子奈 ， 潘文卿 ， 高等教育出版社       《 计量经济学 》   庞浩 主编   科学出版社 2010         计量经济学 是 探讨 如何 运用 模型 方法 定量 描述 和 分析 具有 随机性 特征 的 经济 变量 关系 的 经济学 分支 ， 具体内容 包括 ： 一元 与 多元 线性 回归 模型 、 线性 回归 模型 多重 共线性 、 异 方差 性 、 自 相关性 、 虚拟 变量 回归 模型 、 现代 时间 序列 模型 等 相关 内容 。             金融机构 与 金融市场           《 金融市场 与 机构 通论 》 弗兰克   J   法博兹   等 著 ， 康卫华   等 译         《 货币 、 金融市场 与 金融机构 》 大卫   S   基德 韦尔   等 著 ， 李建军 等 译         《 Foundations   of   Financial   Markets   and   Institutions 》 Frank   J   Fabozzi ,   Franco   Modigliani ,   Michael   G   Ferri .           《 货币 、 金融市场 与 机构 》   基德 韦尔   机械 工业 出版社         本 课程 主要 介绍 金融机构 和 市场 的 构成 （ 股票市场 、 抵押 贷款 市场 、 衍生品 市场 和 国际 市场 ） ， 货币政策 对 金融机构 和 金融市场 的 影响 ， 货币政策 对 利率 的 影响 以及 金融机构 如何 管理 利率 变化 带来 的 风险 问题 。             金融 经济学 基础           《 金融 经济学 引论 》 , 宋逢明 ， 高等教育出版社         本 课程 重点 阐释 现代 金融 经济学 的 基础理论 ， 包括 偏好 、 效用 与 风险 厌恶 度量 、 Arrow - Debreu 经济 、 均衡 资产 定价 、 套利 资产 定价 及 M - M 理论 。 课程 也 将 涉及 市场 微观 结构 和 行为 金融学 的 一些 基础理论 。 通过 本 课程 的 学习 ， 学生 应该 能够 清晰 地 理解 在 不 确定 情况 下 个人消费 和 投资 组合 决策 的 主要 理论 成果 ， 以及 对 证券 定价 的 意义 。               微观 经济学 A           Jack   Hirshleifer ,   Amihai   Glazer ,   David   Hirshleifer   .   Price   Theory   and   Applications :   Decisions ,   Markets ,   and   Information   ( 7th   Edition )   Cambridge   University   Press , 2005 .   （ 中译本 ： ( 美 ) 杰克 ? 赫 舒拉 发   ( Jack   Hirshleifer ) ， 阿米 亥 ? 格雷泽 ( Amihai   Glazer )   ， 大卫 ? 赫 舒拉 发 ( David   Hirshleifer )   《 价格 理论 及其 应用 ： 决策 、 市场 与 信息 》 ， 机械 工业 出版社 ， 2009 . ）         《 IntermediateMicroeconomics . A   Modern   Approach ( 8th ) .   》   Hal   Varian   Norton2009         在 学生 掌握 了 经济学 原理 的 基础 上 ， 本 课程 将 介绍 微观 经济学 的 理论 工具 和 分析方法 ， 使得 学生 能够 运用 简单 的 经济学 模型 分析 现实 经济 问题 ， 为 更 深入 地 学习 后续 专业课程 打下基础 。 本 课程 的 主要 内容 包括 边际 与 均衡 分析 ， 选择 与 需求 ， 生产 与 供给 ， 市场 结构 ， 要素 市场 ， 一般 均衡 ， 不确定性 与 信息 ， 外部性 ， 以及 福利 经济学 等 。             精算学 基础           保险 精算 原理 与 实务   王晓军   人大 版 （ 第三版 ）         本 课程 介绍 精算学 的 基础知识 ， 旨在 让 学生 了解 精算学 与 精算 职业 ， 初步 具备 定量分析 风险管理 问题 的 能力 。 课程内容 主要 包括 ： 精算学 与 精算 职业 ， 以及 利息 理论 的 相关 知识 ， 包括 利息 的 度量 、 利息 问题 求解 、 年 金 、 收益率 、 分期 偿还 表与 偿债基金 、 债券 等 内容 。             风险管理 与 保险           1 . 保险 知识 学习 读本 ， 中国保监会 普及 保险 知识 编写组 ， 中国 金融 出版社 ；   2 . 社会保障 学 ， 郑功成   著 ， 商务印书馆 ；   3 . 中国 保险 史 ， 中国 保险 学会 《 中国 保险 史 》 编审 委员会 ， 中国 金融 出版社         本 课程 主要 介绍 风险管理 与 保险 的 概念性 框架 ， 旨在 让 学生 了解 风险管理 和 保险 的 运作 机制 ， 从而 提高 解决 实际 问题 的 能力 。 课程内容 主要 包括 风险 与 保险 的 基本概念 、 保险 发展史 、 风险 与 保险 的 法律 原则 、 个人财产 与 责任 风险 、 企业 财产 与 责任 风险 、 人寿 与 健康 风险 、 保险法 等 。             非 参数 统计           1 . M .   Hollander   and   Douglas   A .   Wolfe   ( 1999 )   Nonparametric   Statistical   Methods ,   2nd   edition ,   ISBN   0 - 471 - 19045 - 4   2 . Gibbons ,   Jean   Dickinson   and   Chakraborti ,   Subhabrata   ( 2003 )   Nonparametric   Statistical   Inference ,   4th   Ed .           非 参数 统计 方法 的 介绍 。 内容 包括 U 统计 量 , 秩 方法 , 置换 检验 ,   非 参数 分布 估计 ,   非 参数 回归 ,   光滑 方法 等 .             时间 序列 分析 A           1 ． Peter   J .   Brodk   well ,   Richard   A .   Davis .   Time   series :   Theory   and   Methods .   2nd   Edition   Springer - Verlag ,   2006 .   2 .   安 鸿志 ， 时间 序列 分析 ， 华东师范大学出版社 ， 1992 .   3 .   王燕 ， 应用 时间 序列 分析 （ 第二 版 ） ， 中国人民大学出版社 ， 2008 .   4 ． 何书元 ， 应用 时间 序列 分析 ， 北京大学出版社 ， 2003 .           《 时间 序列 的 理论 与 方法 》   田铮   译   高等教育出版社         本 课程 主要 介绍 时间 序列 的 一般 理论 方法 以及 应用 。 内容 包括 宽 平稳 和 严 平稳 随机 过程 ， 自 协方差 函数 ， 偏相关 函数 ， 最佳 线性 预报 ， 时间 序列 的 趋势 项 和 季节 项 诊断 ， 残差 的 诊断 和 检验 ， ARMA 过程 和 ARIMA 过程 及 参数估计 、 预报 ， 谱 密度 和 周期 图 。 通过 本 课程 的 学习 ， 使 学生 掌握 时间 序列 的 基本概念 ， 特别 是 ARMA 模型 ， 熟悉 时间 序列 数据 的 分析 步骤 和 建模 预测 方法 ， 并 能够 运用 统计 软件 ITSM （ Pest ） 对 实际 数据 进行 ARMA 模型 建模 和 预报 。             金融学 导论           《 金融市场 与 金融机构 》 ， 米 什金 ， 中国人民大学出版社         《 金融学 》   博迪   人大 版         本 课程 系统 介绍 金融 投资 方面 的 基本 内容 和 基础理论 ， 强调 培养 学生 对 金融市场 的 理论 和 实践 进行 直观 感受 和 缜密 思考 ， 结合 金融市场 案例 解释 金融 方面 的 基本概念 和 理论 。   课程 分 四个 主要 部分 ， 首先 介绍 金融体系 和 金融市场 的 构成 ， 然后 讲授 金融资产 估值 方法 ， 第三 部分 介绍 风险 度量 和 定价 ， 最后 介绍 部分 金融 前沿 理论 ， 将 涵盖 行为 金融 、 云 金融 以及 金融监管 等 内容 。             固定 收益 证券           该 课程 在 系统 介绍 固定 收益 证券 及其 衍生 产品 这 一大 类 金融 产品 的 基本概念 和 主要 原理 的 基础 上 ， 重点 分析 固定 收益 证券 及其 衍生 产品 的 定价 方法 和 涉及 到 风险 因素 ， 以及 处理 这些 问题 的 具体方法 和 技巧 。 主要 内容 包括 ： 固定 收益 证券市场 介绍 ； 各种 固定 收益 证券 及其 衍生 产品 的 定义 和 特性 的 介绍 ， 以及 相关 的 风险 成分 ； 如何 对 它们 进行 估价 ； 固定 收益 证券 分析 及 利率 的 期限 结构 ； 如何 对 其 进行 风险管理 ， 以及 利用 固定 收益 证券 及其 衍生 产品 来 帮助 风险管理 等 方面 。             试验 设计 与 方差分析           《 实验设计 》 （ 第二 版 ） ， 茆诗松 、 周纪 芗 、 陈颖 ， 中国 统计 出版社 。         《 试验 的 设计 与 分析 》   王万中 、 卯 诗松   华师大 出版社         试验 的 设计 与 方差分析 集中 介绍 试验 设计 的 基本原理 ， 如 ： 重复 ， 平衡 法 ， 区组 设计 ， 不 完整 的 区组 设计 ， 混杂 法 ， 随机化 及 交互作用 ， 拉丁 方 设计 ,   固定 效应 与 随机 效应 ， 最优设计 。             公司 金融           1 ． 《 财务管理 精要 》 （ 原书 第 12 版 ） ， 斯科特 ? 贝斯 利 、 尤金   F ? 布里格 姆 著 ， 刘爱娟 、 张燕译 ， 机械 工业 出版社 ， 2003 年   2 ． 《 公司财务 原理 》 （ 原书 第 8 版 ） ， ( 英 ) 理查德 A ? 布雷 利 、 ( 美 ) 斯图尔特 C ? 迈尔斯 、   ( 美 ) 弗兰克 林 ? 艾伦 著 ， 方曙红 等 译 ， 机械 工业 出版社 ， 2007         《 公司 理财 》 （ 原书 第八版 ）   斯蒂芬 A _ 罗斯 、 罗德尔 福 W _ 威斯特 菲尔德 、 杰 弗利 F _ 杰富 著   机械 工业 出版 2009         公司 金融 是 一门 介绍 公司 如何 进行 财务管理 的 课程 。 本 课程 主要 内容 包括 ： 价值 与 资本 预算 、 资本 结构 与 股利 政策 、 融资 决策 、 财务计划 等 。             博弈论 及其 应用           Roger   Myerson .   Game   Theory :   Analysis   of   Conflict .   Harvard   University   Press .         《 A   Premier   in   Game   Theory 》   Robert   Gibbons           本 课程 将 针对 四种 类型 的 博弈 （ 完全 信息 静态 博弈 ， 完全 信息 动态 博弈 ， 不 完全 信息 静态 博弈 ， 不 完全 信息 动态 博弈 ） ， 分别 介绍 相应 的 基本 解 概念 （ 纳什 均衡 ， 子 博弈 精炼 纳什 均衡 ， 贝叶斯 纳什 均衡 ， 精炼 贝叶斯 均衡 ） 。 针对 每 一类 解 概念 ， 重点 介绍 其 基本 思想 及 应用 方法 。 本 课程 目的 在于 使 学生 掌握 博弈论 这一 重要 理论 工具 ， 能够 利用 其 分析 有关 问题 。             金融 衍生品 导论           《 衍生品 市场 基础 》 麦克唐纳   机械 工业 出版社   2009         《 衍生品 市场 基础 》   麦克唐纳   机械 工业 出版社         金融 衍生品 导论 是 一门 关于 期权 、 远期 合约 、 期货 合约 及 互换 合约 等 衍生品 的 课程 。 通过 该 课程 的 学习 ， 学生 能够 了解 金融 衍生品 的 基本知识 ， 包括 金融 衍生品 的 运作 机制 、 交易 策略 、 定价 理论 及 实践 等 。               金融风险 管理           [ 1 ] 施兵 超 ， 杨文泽 ． 金融风险 管理 [ M ] ． 上海 ： 上海财经大学 出版社 ， 2002 ．   [ 2 ] 王春峰 ． 金融市场 风险管理 [ M ] ． 天津 ： 天津大学 出版社 ， 2001 ．         《 金融风险 管理 》   宋 清华 ， 李志辉 主编   中国 金融 出版社 2004         本 课程 以 金融风险 管理 的 理论 为主 ， 重点 介绍 金融风险 管理 领域 中 的 市场 风险 、 信用风险 、 操作 风险 、 流动性 风险 、 利率 风险 以及 外汇 风险 等 的 定义 以及 度量 方法 。 并 介绍 了 金融风险 管理 的 一般 过程 、 金融风险 辨识 、 常见 的 一些 金融风险 度量 方法 、 金融机构 的 风险管理 、 金融 衍生 工具 与 金融风险 管理 的 关系 等 。             金融 衍生品           麦克唐纳 著   任婕茹 等 译   衍生品 市场 基础   机械 工业 出版社         《 衍生品 市场 基础 》   麦克唐纳   机械 工业 出版社         该 课程 是 工商管理 专业 的 一门 专业 选修课 。 该 课程 主要 介绍 ： 衍生品 的 含义 、 种类 与 发展 背景 ； 无 套利 定价 原理 、 远期 、 期货 、 期权 和 互换 的 定价 与 应用 等 ， 并且 结合 所学 理论 分析 现实 的 金融 问题 。             会计学 原理           “ Accounting   Principles / 6E ”   Weygandt   Kieso   Kimmel ,   Citic   publishing   house , 2002.5         本 课程 系统 介绍 会计 的 基本概念 、 会计 的 类别 、 工业 企业 会计核算 的 基本 内容 、 商品流通 企业 核算 的 基本 内容 、 账户 分类 、 会计凭证 、 会计 账簿 、 账务 处理 的 程序 以及 财产清查 、 会计报表 并 简要 介绍 会计 在 企业 管理 中 的 作用 。             财务管理           财务 成本 管理 （ CPA 考试 指导 用书 )   现代 财务 经济 导论 （ 伍 中信 ）         本 课程 主要 阐述 财务管理 的 基本 理论 、 方法 和 操作技能 ， 并 从 筹资 、 投资 、 资金 运营 、 财务 分析 等 资金 运动 环节 分述 其 原理 和 方法 ， 同时 对 财务 预测 和 预算 、 财务 治理 、 利润分配 与 管理 等 财务 活动 也 进行 了 阐述 。 通过 本 课程 的 学习 ， 使 学生 进一步 了解 和 掌握 企业 财务管理 理论 、 方法 和 操作技能 ， 培养 学生 综合 分析 和 解决问题 的 能力 ， 提高 学生 综合 运用 财务 知识 的 能力 。             运筹学           哈维 ． M ． 瓦格纳 著 。 《 运筹学 原理 与 应用 》 ， 国防工业 出版社 。   Gillett   B ． E 著 ， 蔡三宣 等 译 。 《 运筹学 导论 》 ， 机械 工业 出版社 。           运筹学 运用 科学 方法 ， 主要 是 用 数学方法 去 研究 客观 世界 的 多种 运行 系统 中 所 发生 的 复杂 问题 。 其 独特 之 处 在于 为 现实 或 未来 系统 建立 数学模型 ， 并 据此 进行 定量分析 ， 从而 得到 系统 最优 运行 或 最优设计 方案 。 本 课程 主要 讲授 线性规划 、 整体规划 、 非线性 规划 、 动态 规划 等 运筹学 分枝 中 的 基础 模型 和 方法 。             宏观 经济学           Keynes   JM .   The   General   Theory   of   Employment ,   Interest   and   Money .   London :   Macmillan ,   1936 .   Samuelson   PA ,   Nordhaus   WD .   Economics ,   18th   Edition ,   Columbus :   McGraw - Hill ,   2004 .         《 宏观 经济学 》 是 管理 学院 各 专业 学生 必修 的 重要 基础课 ， 也 是 双学位 重要 课程 。 通过 本 课程 的 学习 ， 使 学生 较 系统地 理解 宏观 经济学 的 基本概念 ， 掌握 国民收入 决定论 、 经济 增长 、 失业 、 货币 与 通货膨胀 、 总需求 与 总供给 、 宏观经济 政策 等 理论 ， 熟悉 宏观经济 理论 研究 的 方法 和 体系 ， 为 后继 课 和 进一步 获取 宏观经济 知识 奠定 必要 的 基础 。             微观 经济学           宋承先   著   《 现代 西方 经济学 》   复旦大学 出版社 。   [ 美 ]   James   M .   Henderson   Richard   E .   Quandt   著   苏   通   译   《 中级 微观经济 理论 》   北京大学出版社 。           微观 经济学 的 主要 内容 为 ： 消费者 行为 理论 ， 生产 与 成本 理论 ， 各种 市场 条件 下 厂商 的 决策 、 政府 政策 的 影响 ， 以及 一般 均衡 理论 ， 分配 理论 等 。             鞅 论 与 随机 积分                   陈木法 ,   毛 永华 .   随机 过程 导论 ,   高等教育出版社 ,   2007   2 .   I .   Karatzas ,   S .   Shreve .   Brownian   Motion   and   Stochastic   Calculus   ( Graduate   Texts   in   Mathematics ) ,   second   edition ,   Springer ,   1991   3 .   B .   Oksendal .   Stochastic   Differential   Equations :   An   Introduction   with   Applications   ( Universitext ) ,   6th   edition ,   Springer ,   2010 .                   鞅 论 是 概率论 的 基本 内容 ， 随机 积分 是 现代 概率论 的 重要 内容 。 两者 在 数理 金融 ， 随机 控制 ， 物理 ， 化学 ， 生物 等 诸多 领域 有 重要 应用 。 本 课程 是 概率 统计 系 硕士生 专业 选修 课程 。   主要 内容 有 ： 鞅 论 ， 布朗运动 ， 随机 积分 ， Ito 公式 ， 指数 鞅 ， 鞅 表示 定理 ， 简单 随机 微分方程 介绍 ， 金融 领域 中 的 应用 。               人工智能 导论           《 人工智能 原理 与 方法 》 ， 王永庆 ， 西安交通大学 出版社         《 人工智能 ： 一种 现代 的 方法 》 （ 第 3 版 ）   （ 美国 ） 罗素 （ Stuart   J . Russell ）           本 课程 是 我校 信息 科学技术 学院 人工智能 专业 基础课 之一 。 该 课程 对 人工智能 所 涉及 的 知识 框架 进行 介绍 ， 梳理 不同 内容 之间 的 逻辑关系 ， 目的 是 使 学生 掌握 人工智能 的 基本概念 和 基本 方法 ， 为 专业 方向 课程 的 深入 学习 奠定 基础 。 针对 本 课程 的 学习 ， 要求 学生 熟练掌握 一种 基本 编程 环境 并 能够 进行 编程 实践 ， 同时 具备 基础 的 高等数学 和 线性代数 知识 。             计算机 视觉           Computer   Vision :   A   Modern   Approach .   Forsyth ,   Ponce ( 美 ) 等 著 .   清华大学出版社 .   图像处理 分析 与 机器 视觉 （ 第二 版 ） . Milan   Sonka ( 美 ) 等 著 ， 艾 海舟 等 译 .   北京 ： 人民邮电出版社 .   2003 .         《 图像处理 分析 与 机器 视觉 》   Milan   Sonka   /   Vaclav   Hlavac   /   Roger   Boyle   人民邮电出版社         本 课程 是 人工智能 专业 的 核心 课程 之一 。 设置 本 课程 的 目的 是 使 学生 了解 并 掌握 计算机 视觉 的 基本 内容 、 理论 方法 及 如何 利用计算机 视觉 手段 进行 信息 获取 与 处理 的 方法 和 技巧 ， 为 进一步 深造 和 应用 打下 坚实 的 基础 。 该 课程 主要 讲述 视觉 基础理论 （ 生物 视觉 特性 、 视觉 物理 特性 、 视觉 几何 特性 ） ； 中层 视觉 处理 （ 图像 分割 、 摄像机 标定 、 立体 视觉 、 运动 视觉 ） ； 高层 视觉 处理 （ 目标 检测 、 目标 识别 、 三维重建 ） 。             自然语言 处理           Jurafsky   D ,   Martin   J   H .   Speech   and   language   processing [ M ] .   London : :   Pearson ,   2014 .   Goldberg   Y .   Neural   network   methods   for   natural   language   processing [ J ] .   Synthesis   Lectures   on   Human   Language   Technologies ,   2017 ,   10 ( 1 ) :   1 - 309 .         《 统计 自然语言 处理 （ 第 2 版 ） 》   清华大学出版社         本 课程 是 一门 关于 自然语言 处理 的 基本概念 、 理论 方法 与 最新 研究进展 的 课程 ， 重点 介绍 基于 统计 机器 学习 方法 的 自然语言 处理 技术 。 课程内容 包括 单词 表征 与 词 嵌入 、 语言 模型 、 自动 分词 、 命名 实体 识别 与 词性 标注 、 文本 分类 与 情感 分析 、 机器翻译 、 信息检索 与 问答 、 口语 信息处理 与 人机对话 等 。         本 课程 是 人工智能 方向 的 本科 专业课 ， 要求 学生 通过 本 课程 的 学习 了解 自然语言 处理 的 基本概念 、 理论 方法 与 最新 研究进展 ， 重点 掌握 基于 深度 学习 等 统计 机器 学习 方法 的 单词 表征 与 词 嵌入 、 语言 模型 、 自动 分词 、 命名 实体 识别 与 词性 标注 、 文本 分类 与 情感 分析 、 机器翻译 、 信息检索 与 问答 、 口语 信息处理 与 人机对话 等 自然语言 处理 技术 。             信号 统计 建模 基础           [ 1 ]   Christopher   M .   Bishop ,   “ Pattern   Recognition   and   Machine   Learning ” ,   Springer ,   2006 .   [ 2 ]   R .   O .   Duda ,   P .   Hart   and   D .   Stork ,   Pattern   Classification   ( 2nd   Edition ) ,   John   Wiley   & amp ;   Sons .         《 Pattern   Recognition   and   Machine   Learning 》   Christopher   M . Bishop , Springer           在 语音 及 图像 等 智能 信息处理 领域 ， 用以 表征 信号 特征 的 数学 统计 模型 及 在 此基础 上 机器 学习 方法 得到 了 广泛 的 应用 。 本 课程 的 目的 在于 ： 使 学生 掌握 在 智能 信息处理 中 ， 用以 表征 信号 特征 的 几种 常用 的 数学 统计 模型 ， 包括 其 基本概念 、 参数估计 方法 、 面向 的 问题 等 ； 将 概率论 、 随机 过程 课程 中 学习 的 基础 数学知识 映射 到 实际 应用 中 ， 加深 学生 对于 理论知识 的 理解 ； 课程 首先 对 相关 的 概率 理论知识 进行 回顾 ， 并 介绍 统计 模型 参数估计 的 基础知识 ； 然后 ， 依次 对 混合 高斯 模型 、 隐 马尔科夫 模型 、 n - gram 模型 、 支持 向量 机和 神经网络 进行 介绍 ； 最后 通过 结合 对 语音 识别 ， 文本 分类 ,   机器翻译 ,   自然语言 处理 等 实际 应用 样例 中 的 信号 建模 算法 和 实现 过程 的 讲解 ， 培养 学生 面向 工程 问题 的 思考 能力 。        ", "tags": "economics", "url": "/wiki/economics/book.html"},
      
      
      {"title": "股票、指数、基金基础知识", "text": "    Table   of   Contents           指数 基金           被动 、 主动型 基金 、 增强型 基金 、 QDII           上证 50           深证 100           沪 深 300 指数 基金           中证 500 指数 基金           中证 红利           主要 消费品 指数           标普 500 指数 基金   S & amp ; P           纳斯达克 100                         指数 基金       被动 、 主动型 基金 、 增强型 基金 、 QDII           被动 就是指 无脑 跟踪 指数       主动 指 增加 更 多 的 投资 组合 , 获取 比 被动 指数 基金 超额 回报 。   也 就是 所谓 的 增强型 基金           充分 市场 假说 否定 这种 主动 基金 的 超额 收益 ,   但是 A股 市场 不 充分 , 所以 很多 主动型 基金 都 能 获得 超额 收益           QDII :   境内 投资 机构 投资 境外 证券 的 基金           上证 50           上海 股市 规模 大 、 流动性 的 50 家 企业 , 蓝筹股 。       行业 分布 :   金融 占 了 一半 ! ! !       主要 公司 :   平安 、 茅台 、 招商银行 、 兴业银行 、 民生银行 、 交通银行 。 。 。           深证 100           深圳 市场 A股 流通 市值 最大 、 成交 最 活跃 的 100 只 成份股 。 深证 100 指数 的 成份股 代表 了 深圳 A股 市场 的 核心 优质 资产 ,   占 了 深圳 交易所 上市公司 市值 的 一半 以上 。       行业 分布 :   制造业 占 了 一半 , 金融业 、 和 信息 服务业 、 房地产 占 比 也 不小       主要 公司 :   格力 、 美的 、 五粮液 、 海康 、 万科 、 平安 、 京东方   etc           沪 深 300 指数 基金           选取 沪 深 两市 中 规模 大 、 流动性 好 的 市值 前 300 的 公司股票 ， 覆盖 了 沪深股市 60 % 左右 的 市值 。       行业 分布 :   分布 比较 分散       主要 公司 :   上证 50 中 的 公司 ,   格力电器 、 美的 、 etc           中证 500 指数 基金           首先 它 把 沪 深 300 指数 所有 已经 包含 的 成分股 剔除 掉 ， 这 保证 了 与 沪 深 300 指数 没有 一点 关系 了 ； 第二 ， 它 再 把 除了 沪 深 300 指数 的 成分股 之外 ， 市值 排名 前 300 名 的 股票 也 要 剔除 掉 ， 这 基本 保证 所有 大 中盘股 被 剔除 掉 了 ； 第三 ， 剩下 的 小盘股 中 ， 又 按照 日均 成交 金额 由高到 低 排名 ， 把 排名 最后 的 交易 不 活跃 的 20 % 股票 剔除 ； 最后 ， 把 剩余 的 股票 按照 市值 从 高到 低 排序 ， 选取 排名 前 500 名 的 股票 作为 中证 500 指数 的 成分股 。       行业 分布 :   制造业 占 了 一半多       主要 公司 :   公司 比较 分散 ,   长春 高新 , 中科 曙光   etc           中证 红利           选择 上海 、 深圳 两个 交易所 中 股息 率高 、 分红 比较稳定 、 具有 一定 规模 及 流动性 的 100 只 股票 作为 样本 。       行业 分布 :   制造业 和 金融业 , 电力行业 为主 ,   行业 分布 比 上证 50 分散 , 但 比 沪 深 300 密集       主要 公司 :   农业银行 、 建行 、 双汇 、 中信 、 上汽 、 招商 、 格力   etc           主要 消费品 指数           日常生活 中 最 基本 的 、 必要 的 消费 ， 主要 涵盖 食品 、 饮料 等       具有 抗 经济 周期性       中美 两国 这种 主要 消费品 指数 年化 收益 都 很 高 ,   长期 收益 超过 大盘 ( 沪 深 500 、 标普 500 )           标普 500 指数 基金   S & amp ; P           选择 纽交所 和 纳斯达克 500 只 ,   占美股 总 市值 的 80 % ! !       行业 分布 :   非常 分散 ,   几乎 所有 行业 ,   差不多 是 A股 的 沪 深 300 这种         主要 公司   :   苹果 , 微软 , google ,   通用 ,   沃尔玛 ,   麦当劳 ,   强生 ,   波音 ,   耐克   etc           纳斯达克 100           选择 纳斯达克 前 100 只 非金融 性 公司股票     https : / / en . wikipedia . org / wiki / NASDAQ - 100         行业 分布 :   以 科技股 为主         主要 公司   :   亚马逊 , Intel , 微软 , 苹果 , Google , Facebook   等 科技 大佬      ", "tags": "economics", "url": "/wiki/economics/stock-index.html"},
      
      
      {"title": "认识商业", "text": "    Table   of   Contents           关于                 关于       师兄 推荐 了 《 认识 商业 》 这 本书 ， 据说 很 不错 ， 还 没 来得及 看 ， 先占个 坑 ， 看 了 之后 来 写 。  ", "tags": "economics", "url": "/wiki/economics/intro-business.html"},
      
      
      {"title": "财务会计教程", "text": "    Table   of   Contents           关于           会计 ： 一门 商业 语言           一些 基本概念           会计 恒等式           经济实体 所有制 类型                   计量 收益 评价 业绩           基本概念           一些 基本 假设 和 原则           权责 发生 制 和 收付 实现 制           配比 原则           股利 ( dividend ) 和 存留 收益 的 会计 处理           四个 财务 比率                   记录 交易           权责 发生 制 与 财务报表           权责 发生 制及 会计 调整           财务报表           分类 资产 负债表 ( classified   balance   sheet )           利润表 ( income   sheet )                           现金流量 表           销售 的 会计 处理           存货 与 销货成本           存货 计价 方法                   长期 资产           负债 与 利息           债务 比率 与 利息 保障 倍数                   股东权益           公司 间 投资 与 合并 财务报表           合并 财务报表           收购                   财务报表 分析           TIPS                         关于       空余 时间 ， 看 了 一下 查尔斯   T .   亨格瑞 的 《 财务会计 教程 》 一书 ，     不得不 说 老外 的 经管类 书写 得 确实 好 ， 通俗易懂 ， 举例 丰富 。     下面 是 记录下来 的 一些 重点 信息 。       会计 ： 一门 商业 语言       一些 基本概念               会计 ( accounting ) 分为 财务会计 ( financial   accounting ) 和 管理         会计 。 财务会计 关注 外部 决策者 的 需要 ， 比如 股东 、 供应商 、 银         行 、 投资者 或者 政府 机构 。 而 管理 会计 侧重于 管理者 对 公司 管理         的 需要 。               道琼斯 工业 平均 指数 ( Dow   Jones   Industrial   Average ,   DJIA ) 从         12 家 公司 增加 到 30 .   其他 指数 有 伦敦 经融 时报 指数 ( FTSE ) 、 日经指数 ( Nikkei ) 等 。         当然 还有 我们 的 上证指数 。               财务数据 来源 有 公司 年报 ( annual   report ) 、 公司 向 美国 证券交易 委员会 ( SEC )         报送 的 10 - K 表 ( form   10 - K ) 。               资产 负载 表 ( balance   sheet ) 也 称 财务报表 ( statement   of   financial   position )         反应 公司 在   某个 特定 时间 点上   的 资产 、 负载 和 有所 者 权益 的 状况 。         联系 两个 不同 时间 点 （ 通常 是 一个 财务 周期 ） 的 表示 现金流 表 、 利润表 和 所有者         权益 变动表 这 三个 表 。               会计 主体 ( entity ) 是 指 作为 一个 独立 的 经济 单位 。 公司 或者 一个 车间 都 可以 ， 不同于         法律 主体 。               交易 ( transaction ) 是 指 任何 影响 一个 会计 主体 的 财务状况 且 会计师 能以 货币 可靠 计量 、         记录 的 经济 活动 。               复式 记账法 是 指 对 每 一次 交易 至少 会 影响 两个 账户 的 一种 记账 方法 ， 这种 方法 的 理论依据         是 会计 恒等式 。               账户 ( account ) 是 指 核算 一项 资产 、 负债 或 所有者 权益 变动 的 汇总 记录 。               存货 ( inventory ) 指 公司 持有 的 以备 出售 的 商品 和 材料 。 商品 未 完成 的 也 算 存货 。               记账 交易会 产生 应付账款 ( account   payable ) 。               公认 会计准则 ( GAAP ) 包括 国际 财务报告 准则 ( IFRS ) 和 美国 公认 会计准则 ( U . S . GAAP ) 。               审计师 ( auditor ) 通过 审计 对 公司 的 财务 信息 的 可信性 进行 监督 ， 通常 需要 独立 的 公共 会计师         或 注册 会计师 进行 设计 署名 ， 公司 的 财报 才能 被 监管 机构 接受 。               会计师 职业 ： 注册 会计师 事务所 、 CFO 、 CEO 等 。               会计 恒等式           资产   =   负债   +   所有者 权益                   资产 ( assets )             公司 目前 具有 所有权 且 能 帮助 创造 未来 现金 流入 或者 减少 未来 现金 流出 的         经济 资源 。 如 现金 、 存货 和 设备 等 。               负债 ( liabilities )         定义 ： 一个 组织 对外 所 承担 的 经济 义务 或 外部 主体 对 其 资产 的 要求 权 。 例如 银行         债务 常 以 应付 票据 存在 。               所有者 权益 ( owner ' s   equity )         定义 ： 指 所有者 对 组织 的 资产 的 要求 权 ， 它 的 优先级 低于 负债 。 确切的说 是 对 净值 产         的 要求 权 。               实缴 资本 = 面值 + 溢缴 资本 。               经济实体 所有制 类型               独资企业 ( sole   proprietorship ) 是 指 只有 一位 所有者 的 企业 。               合伙 企业 ( partnership ) 指有 两个 或 两个 以上 个人 联合 作为 共同 所有者 的 一种         组织 形式 。 例如 会计 事务所 一般 都 是 合伙人 的 合伙 企业 。               公司 ( corporation ) 依 法律 成立 的 承担 有限责任 ( limited   liability ) 的 一种 经济         组织 。 公司 债权人 只 对 公司 资产 有 追索权 ， 而 对 所有 这个 人 的 财产 没有 追索权 。         公众 公司 ( publicly   owned ) 是 指 公司 向 公众 出售 所有权 股票 的 企业 。         与 之 相对 的 事 私营 公司 ( privately   owned ) 。 \" Co . \" 、 \" Corp . \" 或 \" Inc . \" 都 代表 公司 。         其他 国家 也 有 其他 标识 。         只有 20 % 的 美国 企业 采用 公司制 ， 却 从事 着 90 % 的 经济 活动 。 其中 72 % 独资企业 只有 5 %         的 经济 活动 。               董事会 ( board   of   directors ) 是 由 股东 选举 出来 代表 股东 执行 任命 和 监督 权利 的 人 。               首席 执行官 ( chief   executive   officer ,   CEO ) 有时 也 担任 董事长 。               计量 收益 评价 业绩       基本概念               经营 周期 ( operating   cycle ) 是 指 公司 用 现金 采购 - & gt ; 商品 存货 - & gt ; 客户 欠款 - & gt ; 回收 现金 整个 过程 。               会计 期间 通常 以年 为 单位 ， 也 有 中期 、 季度 等 会计 周期 。 会计 期间 导致 了 不同 确认 制度 — —         全责 发生 制 ( 会计 要求 ) 、 收付 实现 制 ( 纳税 要求 ) 。               收入 ( revenue ) 是 指 销售 产品 或 提供 劳务 而 形成 的 净资产 的 增加 （ 不用 扣除 成本 ） ，         收入 增 所有者 权益 ， 也 称 销售收入 ( sales ,   sales   revenue ) 。 费用 ( expenses ) 是 指 由于         在 向 客户 销售 产品 或 提供 劳务 的 过程 中 消耗 或者 让渡 资源 而 形成 的 净资产 的 减少 。         收入 不 包括 非 日常行为 产生 的 收益 ， 例如 变卖 资产 、 投资收益 等 。         另外 请 注意 后面 所说 的 权责 发生 制对 收入 时间 上 的 约束 。               收益 ( income ,   earnings ) 指 收入 超出 费用 的 部分 ， 也 称 利润 ( profits ) 。 如果 费用 超过         收入 ， 就 成 超过 部分 为 损失 。 由 利润 累计 形成 的 所有者 权益 总额 成为 存留 收益 ( retained   earnings ，         retained   income ) 。 收益 常 影响 股价 ， 如果 收益 出乎意料 之外 ， 股价 一般 会 做出 反应 ，         正向 意外 上涨 ， 负 向 意外 下跌 。               记账 交易 同时 产生 应收 账款 ( accounts   receivable ,   trade   receivables ) ， 对于 应收 账款 ， 后面         会 提到 由于 坏账 的 影响 形成 的 坏账 准备 。               销货成本 ( cost   of   goods   sold ,   cost   of   sales ,   cost   of   revenue ) 是 指 报告 期间 公司 向 客户 销售 的         存货 的 原始 采购 成本 。 对于 提供 劳务 服务 的 企业 ， 这个 成本 应该 怎么 算 ？ 比如 腾讯               利润表 汇集 了 一个 会计 周期 内 所有者 权益 的 所有 变动 ， 反应 一定 期间 的 所有 收入 和 费用 。               净 收益 ( net   income ,   net   earnings ) 是 扣除 所有 费用 后 的 剩余 部分 ， 如果 为 负 的 就是 净 损失 ( net   loss ) 。               一些 基本 假设 和 原则               会计 主体 假设 ： 一个 独立 的 经济 单位 。               可靠性 ( reliability ) 要求 会计师 只 确认 能 可靠 记录 的 特定 类型 的 事项 。 比如 高管 遇害 等 无法 可靠         计量 的 事件 就 不 记录 。               持续 经营 假设 ( going   concern ,   continuity )   假定 会计 主体 将会 无限期 地 持续 经营 。 长期 资产 采用         历史 成本 计价 的 合理性 来自 于 这里 。               重要性 原则 使得 许多 应 被 记作 资产 的 采购 项目 因为 无足轻重 而 被 计入 了 费用 ， 比如 衣架 等 。         通常 有个 最低 限额 。 应以 不 影响 财务报表 使用者 的 决策 为 前提 。               成本 效益 原则 对 会计 系统 变革 提供 了 知道 ， 要求 变革 的 收益 要 大于 成本 。               货币 计量 假设 ， 所有 交易 都 能 货币 计量 ， 通常 以 本币 计量 ， 美国 的 美元 ， 中国 的 人民币 。               相关性 、 可比性 、 可 理解 性 、 可 核实 性 、 可 理解 性 等 。 。 。 。         这些 貌似 是 国内 会计师 证要 考 的 ， 但 感觉 不怎么 太 重要 。               权责 发生 制 和 收付 实现 制               全责 发生 制 ( accrual   basis ) 要求 收入 在 获得 时 予以 确认 、 费用 在 发生 时 予以 确认 ， 这 导致 收入 和 费用 确认 的 时间     未必 与 现金 收付 的 时间 一致 。 而 收付 实现 制 ( cash   baasis ) 要求 公司 在 收到 现金 时 确认 收入 、 在 支付现金 时 确认 费用 。               公司 安装 全责 发生 制 制备 财务报表 ， 但是 税收 按照 收付 实现 制 ， 此外 公司 还 编制 现金流量 表附 在 年报 中 。               配比 原则               配比 原则 ( mathing ) 要求 在 确认 相关 收入 的 同一 会计 周期 确认 和 记录 费用 。               租赁费 管理费 等 费用 会用 在 一定 期间 的 经营 活动 ， 这些 成本 被 称为 期间 成本 ( period   costs ) ，         而 与 收入 本身 相关联 的 费用 被 称为 产品成本 ( product   costs ) ， 如 销货成本 、 销售 佣金 。               按照 配比 原则 ， 需要 对 长期 资产 进行 折旧 ( depreciation ) ， 而 预付 租金 需要 平摊 到 每 一个 会计 周期 。               股利 ( dividend ) 和 存留 收益 的 会计 处理               股利 在 宣布 日 成为 公司 负债 ， 在 支付 日 负债 和 现金 同时 减少 。               存留 收益 是 剩余 要求 权 ， 而 不是 真 金白银 。 破产 时 所有者 清偿 负债 后 的 剩余 数量 ， 可能 比         现金 多 也 可能 少 。 这是 由于 市场 和 物价水平 变动 等 因素 的 作用 。               股东权益 变动表 ( statement   of   stockholder ' s   equity ) 变动 原因 ：           净 收益 或净 损失       与 股东 之间 的 交易 ， 比如 宣布 股利       其他 全面 收益 ( other   comprehensive   income ) 上述 两者 之外 的 其他 收益 。                   四个 财务 比率               每股 收益 ( earning   per   share ) 是 要求 在 财务报表 中 列报 的 唯一 一个 财务 比率 。           每股 收益   =   净 收益   /   平均 发行 在外 的 普通股 股数                   稀释 每股 收益 是 指 因为 期权 行权 或 债权人 等 有权 以 低于 市场价 购买 普通股 ， 在 其行 权后 ，         每股 收益 将 下降 。               市盈率 ( price - earnings   radio ,   P - E   raio ) 也 称 收益 乘数 ， 是 当前 收益 回收 当前 股价 的 年数 。           市盈率   =   普通股 每股 市场 价值 / 普通股 每股 收益                   股息 率 ( dividend - yield   ratio ) 。 因为 股利 和 每股 收益 同场 不同 ，         这个 指标 体现 了 公司 支付现金 股利 占 每股 收益 的 比例 ， 可能 小于 1 也 可能 大于 1 ， 甚至 可能 为 0 ，         因为 有些 公司 根本 就 不 支付 股利 。           股利 支付 率   =   普通股 每股 股利   /   每股 收益                   记录 交易           记账 方式 ： 复式 记账法 ( double - entry   system )       借 ( debit ) 表示 左边 ， 贷 ( credit ) 表示 右边       会计 恒等式 也 等价 于   借方 = 贷方       数据处理 ： ERP 系统 取代 人工 处理       计算机 处理 记账 语言 — —   XBRL             这章 涉及 具体 细节 ， 我 不大 关心 ， 所以 如 有 需要 请 参看 原文 或者 相关 书籍 。       权责 发生 制 与 财务报表       权责 发生 制及 会计 调整           显性 交易 和 隐性 交易 ： 是否是 实际 发生 。 火灾 导致 的 损失 是 显性 交易 而 折旧 算 隐性 交易       会计 调整 也 称 调整 分录 ， 是 指 在 期末 将 隐性 交易 的 财务 影响 分配 给 正确 的 会计 期间 。 它         从不 影响 现金 。       应计 ( accrue ) 事项 是 指 在 一定 会计 期间 ， 及时 未 发生 显性 交易 ， 也 归集 应收 项目 （ 资产 ）         或 应付 项目 （ 负债 ） 。       应付 员工工资       借款 到期 之前 的 应付 利息               预付 成本 耗用 ， 过去 的 显性 交易 形成 一项 资产 ， 日后 的 隐性 交易 确认 这项 资产 的 耗费 。       折旧 ， 累计 折旧       预付 租金               加盟 的 会计 处理 ： 加盟 属于 特许 经营 ， 公司制 将 特许 经营 费 确认 为 收入 ， 而 加盟商 的 产品         销售收入 不 计入 收入 总额 。       快递 行业 的 加盟 方式 以及 诸多 连锁店 加盟店 ， 主 公司 的 收入 不 应 包括 这些 加盟店 的 收入               应计 未计 费用       应付 工资       应计 利息       应计 所得税               应计 未计 收入       应收 利息       尚未 出具 账单 的 服务费 — — 律师 、 会计 等               确认 原则 ： 稳健性 原则           财务报表       分类 资产 负债表 ( classified   balance   sheet )               流动资产 ( current   assets ) ： 一年 或 一个 正常 经营 周期 内能 变现 的 资产           预付 租金       应收 账款       应收 票据       应收 利息       商品 存货                   长期 资产           设备       房产       土地                   流动 负债 ( current   liabilities ) ： 一年 内 或 一个 正常 经营 周期 到期 的 债务           应付账款       未 获得 租赁 收入       应付 工资       应付 利息       应交 所得 说       应付 票据                   营运 资本 ( net   working   capital ,   net   current   assets )   =   流动资产   -   流动 负债               流动比率 ( current   ratio ) 衡量 流动性           流动比率   =   流动资产   /   流动 负债         太高 ： 持有 过多 流动资产 没有 使用 ， 相当于 浪费           太低 ： 可能 难以 履行 短期 债务                   速动 比率 ( quick   ratio ) ， 也 称 酸性 测试 比率 ， 去除 流动性 差 的 资产           速动 比率   =   ( 流动资产   -   流动性 差 的 资产 如 存货 )   /   流动 负债                       公司 知应 持有 满足 支出 需要 所 必须 的 现金 ， 而 将 任何 临时性 冗余 现金 都 进行 投资 ，     以 创造 额外 收益 。           利润表 ( income   sheet )           分类 ： 单步 式 ( 没有 小计 ) 和 多步式 ( 有小计 ， 大多 公司 都 采用 这种 )       毛利 ( gross   profit ) 或 毛 边际 ( gross   margin ) ， 是 销售收入 超出 销售 成本 的 部分 。       我 的 理解 ： 可以 理解 为 没多 卖出 单位 产品 而 增加 的 利润 ， 因此 可以 忽略 固有 成本     运营 成本 等 间接成本 。               经营 费用 ( operating   expenses )       经营 费用 低 不 一定 是 好事 ， 比如 研发 费用 低 了 会 影响 未来 的 竞争力               经营 收益 ( operating   profit )       盈利 能力 指标       毛利率   =   毛利   ÷   销售收入       销售 利润率   =   净 收益   ÷   销售收入       净资产 收益率 ( return   on   common   stockholder ' s   equity   ratio ,   ROE )         =   净 收益   ÷   普通股 股东权益 平均 余额       资产 收益率   =   净 收益   ÷   总资产 平均 余额                   现金流量 表       现金流量 表 ( statement   of   cash   flow ,   cash   flow   statement ) 按 经营 活动 现 就 流量 、     投资 活动 现金流量 和 筹资 活动 现金流量 分类 报告 一个 会计 主体 在 一定 期间 的 现金 收入     和 现金 支出 。           如果 损失 的 现金 太 多 ， 公司 就 必须 宣布 破产 。 依照 联邦 法律 ， 破产 意味着 公司 需要     寻求 法院 的 破产 保护 。                   现金 等价物 ( cash   equivalents ) 是 指 公司 能够 快捷 地 转化 为 现金 的 流动性 极高 的 短         期 投资 。 例如 货币 市场 基金 和 国库券 。 一般 把 现金 和 现金 等价物 统称 为 现金 。               经营 活动           日常 产品 采购 、 生产 和 销售       利息       股利 ( ? 见 后面 的 说明 ， 存在 分歧 )                   筹资 活动           从 银行借款       从 其他 贷款 机构 借款       偿还 借款       回购 股票       发布 股利                   投资 活动           购置 和 处置 厂房 、 设备 和 其他 生产性 长期 资产       作为 有价证券 的 债权人 或 所有者 提供 或 回收 资金 ： 例如 以 投资 的 方式 持股 另一家         公司 的 股票 （ 注意 区别 子公司 要求 51 % 以上 的 股权 ）                   汇率 对 现金 的 影响 也 会 放到 现金流量 表 下面 ， 作为 会计 调整               分歧           对于 股利 ， 美国 公认 会计准则 规定 股利 既 可以 作为 经营 活动 现金流量 也 可以 作为         筹资 活动 现金流量 。                   非现金 投资 和 筹资 活动 不用 列 在 现金流量 表中 ， 单 必须 单独 列报 。               经营 活动 现金流量 的 计算           计算方法 分为 直接 法 和 间接 法       间接 法 需要 提供 净 收益 与 经营 活动 现金流量 调节 表                   现金流量 表与 会计 恒等式 的 关系           $ ( \ \ Delta ) $ 现金   =   $ ( \ \ Delta ) $ 负债   +   $ ( \ \ Delta ) $ 股东权益   -   $ ( \ \ Delta ) $ 非现金 类 资产       直接 发是 列报 现金 的 所有 变动 （ 左边 ） ， 而 间接 法 列报 变动 的 原因 （ 右边 ）                   现金流量 表 分析           同一 期间 投资 活动 现金流量 为 负 的 公司 ， 即 使用 资金 扩大 固定资产 投资 的 公司 ，         往往 都 是 健康 的 、 成长型 的 公司       经营 活动 现金流量 为 负而 投资 活动 现金流量 为 正 的 公司 都 是 要 跑 路 了                   自由 现金流量 ( free   cash   flow ) 定义 为 经营 活动 现金流量 减去 资本 支出 。               销售 的 会计 处理               双重 测试 确认 收入 ： 公司 必须 以向 客户 交付 商品 或 提供 劳务 ， 即 收入 “ 已 获得 ” ；         他 必须 受到 了 现金 或者 一项 实际上 能 转换 为 现金 的 资产 ， 及 收入 必须 “ 以 实现 ” 。               完工 百分比 法 ： 在 生产 发生 之 时 确认 长期 合同 收入 ， 按照 配比 原则 ， 还 确认 相关         费用 。               销售收入 总额 ( gross   sales ) 是 指 扣除 销售 退回 及 折让 、 销售 折扣 之前 的 销售收入 总额 。         销售收入 净额 ( net   sales ) 是 扣除 这些 折扣 之后 的 净额 。               折扣           商业 折扣       现金 折扣                   补偿性 余额 ( compensating   balance ) ， 银行 常 要求 公司 维持 一个 最低 现金余额 。 它         增加 了 借款人 所 支付 的 实际 利率 。 年 报上 必须 披露 显著 的 补偿性 余额 。               现金管理           分权       收入支出 分 不同 人 负责       收入 和 记账 也 分 不同 人 负责       ....                   坏账 ( doubleful   accounts )           直接 冲销 法 ： 对于 坏账 极少 的 公司       坏账 准备金 ： 采用 历史数据 估计       销售收入 百分比 法       应收 账款 余额 百分比 法       应收 账款 账龄 分析法 ： 不同 时间 赋予 不同 的 百分比                           应收 账款 周转 次数 ( accounts   receivable   turnover ) ： 本期 赊销 销售收入   ÷   本期 应收 账款 平均 余额               应收 账款 天数 ： 365   ÷   应收 账款 周转 次数               内部 控制 ： 保证 记录 的 真实性           管理控制       会计 控制                       案例 ： 某 程序员 将 会计 系统 数值 的 分 后面 一位 存 到 自己 账户 ， 如果 交易量 很大 ， 收入 非常 可观 。               内部 控制 方法       人员 可靠 、 责任 明确       分工       适当 审批       账证 齐全       程序 完善       物理 安防       休假 与 轮岗       独立 检查       成本 效益 分析 ： 是否 投资 财务 系统                   存货 与 销货成本           永续 盘存 制       定期 盘存 制 ： 商品 被盗 也 算入 销货成本               存货 盘亏 控制 专家 一般 认为 ， 最佳 威慑 盗窃 行为 是 在 销售点 安排     一名 警戒 员工 。               采购 成本 ， 一般 计入 期间 成本 ， 但 也 有 计入 销货成本       运输成本 ， 由于 太 困难 分配 成本 ， 所以 一般 也 是 直接 计入 费用       折让 和 折扣               研发 成本 被 视作 期间 成本 ， 而 不是 产品成本               存货 周转 次数 ( inventory   turnover )   =   销货成本   ÷   平均 存货 余额       存货 周转 天数   =   365   ÷   存货 周转 次数               降低 毛利率 提高 毛利 的 方法 长期 来看 是 有利 的 吗 ？ 如果 旺销 不是 因为     基本 需求 的 增加 ， 不过 是 将 未来 的 采购 转为 现在 的 采购 ， 那么 就 不是 。     他 有 可能 造成 未来 产品 的 滞销 。           存货 计价 方法           个别 认定 法 ： 一个 一个 跟踪 ， 以前 这种 方法 成本 很 高 ， 没人用 ， 但是 随着         计算机 发展 ， 这个 方法 成本 已经 降低 很多 了 。 主要 针对 单个 价值 高 的 商品 。       先进先出 法 ： 使 前期 成本 低于 个别 认定 法 ， 电子产品 可以 采用 这个 方法 拖延 所得税       后进先出 法 ： 使 前期 成本 高于 个别 认定 法 ， 可以 合理 拖延 所得税 ， 记住 是 拖延 ，         不是 避税 。 《 国际 财务报告 准则 》 禁止 这种 方法 。 缺点 是 可能 减少 管理者 的 奖金 ，         股价 、 贷款 等 。       加权 平均法       存货 持 有利 得 ， 因为 存货 成本 的 上升 导致 的 利得       原材料 、 在 产品 和 产成品 都 是 一种 存货 形式 ， 在 资产 负债表 中 列报 为 流动资产 。       公司 选择 计价 方法 必须 保持 一致性 ， 但 也 有 例外       存货 价值 采用 成本 与 市价 孰 低法       国际 准则 的 市场 价格 指可 实现 净值 。 而 美国 准则 指 现行 重置成本 ， 即 公司 今天 买 一件 存货     的 成本 。               截止期 差错 ， 将 当前 成本 记录 到 下 一个 会计 期间           长期 资产               定义 ： 公司 必须 在 日常 经营 中 使用 这些 资产 ， 而 不是 为了 在 销售 或者 投资 而 持有 的 。 例如         公司 应该 将 为了 投资 目的 闲置 的 土地 作为 投资 ， 而 不是 作为 厂房 及 设备 。               有形资产 是 看得见 摸得着 的 资产 ， 如 土地 、 建筑 和 设备 等 。               无形资产 也 算是 长期 资产 ， 包括 合同 权利 、 法律 权利 或者 经济 利益 ， 如 专利权 、 商标权 和 版权 。               折旧 ： 有形资产 在 收益 期 的 成本 分配               摊销 ： 对于 无形资产 的 摊销               折耗 ( depletion ) 指对 自然资源 的 摊销               对于 一些 短期 经济 利益 ， 如 广告费 、 预付 租金 等 ， 在 计入 费用 之前 被 当做 流动资产 ；         而 对于 不 只 在 一个 款及 年度 收益 的 资产 ， 将 其 资本化 ， 然后 在 每个 会计 期间 确认 一部分         费用 。               土地 的 计价 ： 美国 会计准则 要求 公司 将 土地 按照 预案 是 历史 成本 计价 ， 除非 土地 公允 价值         跌倒 原始 成本 以下 。               建筑物 和 设备 ： 一项 资产 在 第一次 使用 之前 的 维修 成本 ， 计入 购置 成本 ； 而 之后 的 维修 成本         计入 日常 维护费用 。               采用 非现金 支付 购买 的 资产 ， 按照 公允 价格 计入 成本 。               在 特定 情况 下 ， 公司 可以 减计 资产 价值 ， 但是 永远 不能 增加 其 价值 。               总成本 = 应计 折旧 额   +   残值               使用寿命 取 物理 寿命 和 经济 寿命 最短 的               折旧 方法 ：           直线 法 ， 均摊 到 使用寿命       工作量 法 ： 例如 卡车 按照 行驶 路程       加速 折旧 法 ： 余额 递减 法 ， 双倍 余额 递减 法 。 如果 残值 较大 ， 双倍 余额 递减 法会 使得             价值 减少 的 残值 以下 ， 为了 防止 这种 情况 ， 会 提前 修改 折旧 方法 。             加速 折旧 会 使得 纳税 时间 推迟 ， 从而 使 公司 从中 获益 。                   公司 在 纳税 申报 一般 采用 加速 法 ， 而 给 股东 的 报告 采用 直线 法               估计 使用寿命 或 残值 的 变更 ： 一个 原则 ， 不 追溯到 以前 ， 只 修改 以后               折旧 不 影响 税前 现金余额 ， 但 影响 所得税               资本 改良 ： 以 增加 现有 固定资产 所 提供 的 未来 收益 而 发生 的 支出 。 例如 改良 设备 延长 使用寿命 ，         这个 改良 成本 应 在 剩余时间 进行 分摊               有形资产 出售 利得 和 损失 常 计入 其他 收益 ( 损失 ) ， 但是 不 计入 经营 活动 现金流               有形资产 的 重新 计价 ： 只有 国际 财务报告 准则 才 有 可能 上调 资产 价值 ， 但 都 要求 在 厂房设备 发生         减值 的 时候 重新 计价 ， 下调 资产 价值 。               国际 财务报告 准则 要求 不论 账面 价值 增加 或者 减少 ， 都 要 按照 公允 价值 反映 固定资产 。               公司 对 任何 资产 进行 重新 计价 ， 也 必须 对 同类 资产 中 的 其他 资产 进行 重新 计价 。         采用 重新 计价法 的 资产 不 计提 折旧 。 由于 一旦 公司 采用 重新 计价法 调整 ， 就 必须         一直 进行 调整 ， 实际 中 很多 公司 很少 应用 。 （ 国际 财务报告 准则 ）               公允 价值 常 采用 评估师 基于 市场 的 证据 来 确定 。               重新 计价 利得 和 损失 作为 其他 全面 收益 直接 加 在 股东权益 中 。               资产 减值 记录 条件 ： 资产 市场 价格 显著 下降 ； 资产 使用 方式 显著 变化 ； 法律 或 经济 环境 发生 不利 变化 ；         有 迹象 表明 资产 报废 或者 发生 了 物理 损毁 ； 预测 表明 公司 将 因 使用 资产 而 持续 发生 损失 。               美国 公认 会计准则 要求 一旦 减值 就 不能 恢复 。               减值 损失   =   账面 价值   -   可回收 额 ； 可回收 额是 未来 金 现金流 的 现值 和 公允 价值 减 出售 成本 后 的 净值         中 最少 的 。               无形资产 ： 只有 公司 从 外部 购买 了 资产 的 所有权 ， 公司 才能 在 资产 负债表 中 列报 无形资产 ， 而 公司         自己 创造 的 则 不算 。 公司 内部 的 研发 活动 计入 费用 ， 经管 它 能 为 未来 创造 经济效益 。               美国 公认 会计准则 规定 了 一个 例外 ， 允许 计算机软件 公司 不 自动 地 将 内部 研发 成本费用 化 。 这些 公司         可以 将 旨在 销售 或 租赁 的 软件产品 研发 成本 资本化 。               对于 认定 具有 无限 使用寿命 的 无形资产 ， 公司 一般 不 予以 摊销 ， 二是 定期 评估 这些 资产 是否 减值 。         而 有限 寿命 的 需要 摊销 到 寿命 期内 。               收购 的 未 完工 研发 支出 也 要 资本化 ！ ？               无形资产           专利权       版权       商标       特许权 （ 许可证 ） ： 政府 颁发 的 许可证 ，       租赁 权 ： 土地 租赁                   对于 有限 寿命 的 无形资产 减值 与 有形 长期 资产 一样               对于 无限 使用寿命 的 无形资产 ， 如 商标 ， 要求 比较 账面 价值 与 公允 价值 ， 确认 减值 。               商誉 ： 是 一项 无形资产 ， 不能 出售 或 转让 （ ？ 商标 可以 ） ， 公司 只能 在 收购 另一家 公司         是 确认 商誉 。 商誉 是 向 被 收购 公司 支付 的 收购价格 超出 其 可辨认 净资产 公允 价值 的 部分 。               公司 不 分摊 商誉 ， 但是 要求 每年 审核 一次 商誉 是否 减值 。               自然资源 折耗 ： 未来 恢复 生态 的 费用 需要 计入 计提 折耗 的 总成本               负债 与 利息               负债 分为 流动 负债 和 长期负债 ， 一般 以 1 年期 为限 。               流动 负债           应付账款       应付 票据 ： 期票 ， 大多 是 向 银行 签发 的 定期 贷款 ； 还有 一些 短期 商业 票据 也 算       应计 员工工资       应交 所得税       一年 内 到期 的 长期负债 ： 如果 长期负债 一年 内要 到期 了 ， 就 变成 了 流动 负债       销售税       可 退还 保证金       尚未 获得 收入 ： 预付款 收入 ； 有些 公司 称为 递延 收入       产品 保修 ： 售后服务 也 是 一项 负债 ， 需 定期 提供 保修 “ 准备金 ”                   长期负债           公司债券 ( bonds )       各种 债券                   债券 特点 ： 清算 优先级 为   抵押 债券   & gt ;   信用 债券 （ 应付账款 ）   & gt ;   次级 信用 债券   & gt ;   优先股   & gt ;   股票           债券 保护性 条款 ： 限制 支付 股利 ， 限制 再 借款 ， 保证 一定 的 财务 比率       债权 会计 处理 ： 过于 琐碎 ， 跳过           债券 对 现金流量 的 影响 ： 影响 筹资 活动 现金流量 。 US . GAAP 要求 每年 一次 支付 的 利息 计入 经营 活动 现金 流出量 ；         而 国际 准则 都 可以               债券 的 折价 溢价 不 影响 现金 ， 对 其 的 摊销 就 像 折旧 一样 ， 是 一项 非现金 费用               租赁 ： 资本 租赁 ( 列报 为 资产 和 负债 ) 和 经营 租赁 ( 列报 为 费用 )               资本 租赁 列报 一项 资产 和 一项 负债 ， 金额 为 未来 支付 的 租赁费 的   现值                 其他 长期负债 ：           养老金 ： 退休 后 福利 基金       递延 所得税 ： 公认 会计准则 的 财务报告 要求 与 税法 之间 的 差异 ， 使得 公司 记录 的 税费 ( 直线 法 )         比 实际 缴纳 ( 加速 折旧 ) 的 时间 早 ， 就 形成   递延 所得税 负债         重组 产生 的 负债       或 有 负债 ： 诉讼 导致 的 赔偿金                   债务 比率 与 利息 保障 倍数           净 资产负债率   =   负债 总额   ÷   股东权益 总额       长期负债 与 资本 总额 比   =   负债 总额   ÷   股东权益 总额       资产负债率   =   负债 总额   ÷   资产 总额       利息 保障 倍数   =   ( 税前 收益   +   利息费用 )   ÷   利息费用           股东权益           股东权益 背景       A 类股票 （ 不 公开 交易 ， 员工 持股 ， 表决权 大 ） 和 B 类股票 （ 普通股 ）       库藏 股 ： 公司 回购 用于 未来 分配 的 股票       溢缴 资本       存留 收益               普通股       公司 授权 委托书 ： 股东 委托 代表 执行 投票权               2002 联邦政府 通过 《 奥 半死 - 奥克斯 利 法案 》 加强 监管 ， 董事长 不 兼任 管理 职务 以 保持 独立性               现金 股利 ， 宣布 日 成为 负债               支付 股利 一方面 可能 表明 公司 在 经营 之外 还有 多余 现金支付 股利 ， 表明 公司 运作 良好 ，     另一方面 也 可能 表明 公司 没有 其他 投资 项目 可以 吸收 其 创造 的 全部 现金 。               优先股 不具 表决权 ， 但是 拥有 对 资产 的 优先 要求 权           累计 股利 ： 对 优先股 未付 的 股利 ， 以后 还是 要 付               股票 增发           股票 期权 ： 在 一定 期间 ， 按照 一定 价格 购买 一定 数量 公司股票 的 权利       限制性 股票 ： 作为 溢缴 资本                   股票 分割 ， 降低 每股 价格           股票 股利 ： 是 公司 向 股东 无偿 增发 的 新股 * ， 为什么 可以 算作 股利           不足 一股 ， 发现 金 补齐               股票 回购           原因 ： 管理层 认为 自己 股票 被 低估 ， 相当于 投资 自己 ； 需要 作为 股票 期权 分配 给 员工       会 导致 现有 股票 的 稀释 ， 因为 回购 价格 高 ？                   普通股 其他 发行 方式 ：           非现金 交换 土地 等 资产       债转股                   存留 收益 ： 也 叫 “ 资本 公积 ”               净资产 收益率   =   ( 净 收益   -   优先股 股利 )   ÷   普通股 股东权益 平均 余额           普通股 每股 账面 价值   =   ( 股东权益 总额   -   优先股 账面 价值 )   ÷   发行 在外 普通股 股数           公司 间 投资 与 合并 财务报表           合并 财务报表 是 指 两个 或 两个 以上 的 独立 法律 实体 的 财务 记录 合并 在 一起 列报       在 非 控制 但 能 显著 影响 其 经营 和 财务 政策 的 经济实体 的 投 在 采用 权益法                       控股 比例       试用 法则                       & lt ;   20 %       市场 法               20 %   ~   50 %       权益法               & gt ;   50 %       子公司 ， 合并 财务报表                   合并 财务报表           公司 间 交易 抵消       非 控股 股东权益 ： 反应 费 多数 股东 对 合并 入 多数 股东 报告 中 的 公司 资产 和 收益 的 要求 权 。         合并 时 ， 所有 收益 均 予以 合并 ， 然后 减去 非 控股 股东 持有 的 收益 份额 。           收购           收购价格 高于 收购 净资产 的 公允 价值 的 部分 ， 被 称为 商誉 （ goodwill ） 。       商誉 减值 处理           财务报表 分析           目标 ： 用 财务报表 了解 公司 过去 的 业绩 。       年报 的 附注 非常 重要       重要 表 ： 10 - K ， 10 - Q ， 8 - K 表       银行 或 其他 提供 大额 贷款 的 债权人 可能 要求 提供 一份 预测 财务报表 或 对 预测 结果 的 其他 估计 。           TIPS           制造业 企业 对 折旧 方法 的 选择 会 影响 所 报告 的 毛 边际 ， 因为 他 影响 销货成本      ", "tags": "economics", "url": "/wiki/economics/financial-accounting.html"},
      
      
      {"title": "货币金融学", "text": "    Table   of   Contents           关于           为什么 要 研究 货币 、 银行 和 金融市场           基本概念                   金融体系 概览           金融市场 的 结构           金融市场 工具           金融市场 的 国际化           金融 中介机构 的 功能 ： 间接 融资           金融 中介机构 的 类型           金融体系 监管           网络 练习题                   什么 是 货币           网络 练习                         关于       在 看 完 了 财务会计 教程 之后 ， 继续 看 一些 货币 金融学 方面 的 内容 。     之前 看过 昆曼 的 经济学 原理 ， 收获 很多 ， 对 一些 经济 现象 也 能 在     一定 程度 上 进行 推演 ， 理解 其 背后 的 经济学 原理 。 但是 当 设计 到     金融 方面 的 内容 的 时候 ， 就 一脸 懵 逼 了 。 而且 由于 每天 接触 的 新闻     和 其他 媒体 内容 ， 使得 我 对 金融 方面 的 疑惑 越来越 多 ， 所以 是 时候     看看 这方面 的 东西 。 这 一次 我 选择 了 弗雷德里克   S .   米 什金 的     《 货币 金融学 》 一书 ， 多 看 上 花 了 50RMB 多 买 的 商学院 版本 中译本 。     至于 为什么 要学 这门 课 ， 可以 用 这 本书 中 的 一句 话 来 回答           对 货币 、 银行 和 金融市场 的 学习 和 研究 ， 可以 帮助 你 了解 那些 在 政治 领域     中 激烈 争辩 的 货币政策 执行 议题 的 含义 ， 帮助 你 更加 清晰 的 理解 媒体报道 中     涉及 的 各种 经济 现象 。 由此 获得 的 知识 可以 使 你 获益 终身 。           为什么 要 研究 货币 、 银行 和 金融市场       目标 ： 通过考察 金融市场 （ 诸如 债券市场 、 股票市场 和 外汇市场 等 ）     和 金融机构 （ 诸如 银行 、 保险公司 、 共同 基金 以及 其他 金融机构 等 ）     的 运行 过程 ， 探求 货币 在 经济 循环 中 所 发挥 的 作用 。           观点 ：   健全 的 金融市场 是 实现 高速 经济 增长 的 关键 要素 。           基本概念               证券 ( security ) 又称 金融工具 ， 是 对 发行者 未来 收益 或者 资产 的 一种 求偿 权 。               债券 ( bond ) 是 神诺 在 约定 期限内 定期 偿付 的 债务 证券 。               普通股 ( common   stock ) 代表 了 其 持有人 对于 公司 具有 一定 份额 的 所有权 ， 代表         对 公司 的 资产 和 收益 具有 的 求偿 权 。               不同 利率 具有 一致 的 变动 趋势 ， 经济学家 通常 将 不同 的 利率 集合 在 一起 ，         统称 为 利率 。           高利率 可能 会 使 公司 推迟 新 厂房 的 建设 计划 ， 这 原本 可以 提供 更 多 的 就业机会 。                   金融体系 有 各种各样 的 私人 金融机构 组成 ， 具体 包括 ： 银行 、 保险公司 、 共同 基金 、         财务 公司 以及 投资银行 等 。 自然人 一般 通过 金融 中介机构 ( financial   intermediaries )         间接 向 公司 提供 贷款 。               金融危机               经济周期               通货膨胀               外汇市场 ( foreign   exchange   market ) 是 完成 货币 兑换 的 场所 。               研究 方法 ：           资产 需求 分析 的 简化 方法       均衡 的 概念       。 。 。                   GDP 平减 指数               金融体系 概览           金融市场 的 重要 意义 在于 他 能 有效 地 配置 资本 ( captial ) ， 从而 提高 整体 经济 的 产出 水平 和 效率 。           金融市场 的 结构               债务 市场 ： 债券 或 抵押 票据 等 抵押 债务 工具 。 通常 远大于 股票市场           短期 债务 工具   短于 1 年       中期 债务 工具   1 - 10 年       长期债务 工具   10 年 或 10 年 以上                   股权 市场 ： 发行 普通股 股票 ， 特点 是 剩余 求偿 权 ， 优先级 比 债务 低 。               一级 市场 ( primary   market ) 是 筹集资金 的 公司 发行 的 债券 或 股票 等 出售 给 最初 购买者 的 经融 市场 。           通常 不是 公开 的       投资银行 ( investment   bank ) 进行 证券 承销                   二级 市场 是 将 投资人 交易 证券 的 市场 *           纽交所 、 纳斯达克 、 外汇市场 、 期货市场 和 期权 市场 等       使 金融工具 的 变现 活动 更加 迅速 和 便捷 ， 提高 流动性 ( liquid ) ， 流动性 越高 ， 越 受欢迎       圈定 一级 市场 发行 价格       组织 形式       交易所   ( exchanges )       场外 交易市场 ( OTC   market )   如 美国政府 债券市场 、 可 转让 定期存单 、 联邦 基金 、 外汇市场                           货币 市场 ( money   market )   是 短期 债务 工具 进行 交易 的 金融市场 ， 资本 市场 ( capital   market ) 是 长期         债务 工具 和 股权 工具 交易 的 市场 。 短期 证券 流动性 好 ， 价格 波动 小 ， 更为 安全 。               金融市场 工具               货币 市场 工具 ： 短期 债务 工具 ， 其 原始 期限 通常 短于 1 年               美国 国库券 ， 通过 折价 发行 ( 贴现 ) 来 支付 利息 ， 流动性 最强 ， 几乎 不 存在 违约 风险 ( default ) 。         因为 联邦政府 可以 通过 增加 税收 或者 增发 货币 来 偿还债务 。 银行 是 国库券 的 主要 持有人 。               可 转让 银行 定期存单 ： 定期存单 是 银行 向 存款人 发行 的 ， 定期 支付 利息 到期 后以 初始 购置 价格 还本 。         可以 在 二级 市场 出售 的 定期存单 就是 可 转让 银行 定期存单 。               商业 票据 ： 有 大型 银行 和 诸如 微软 等 民企 发行 的 短期 债务 工具 。               回购 协议 ( repurchase   agreement ,   repos ) ： 是 一种 有效 地 短期贷款 （ 低于 2 年 ） ， 以 国库券 作 抵押 ，         如果 借款者 无法 偿还 就 可以 将 国库券 收归 己 有 。 回购 协议 通常 借款者 是 银行 ， 它 成为 银行 的 主要 资金来源 ，         而 大 公司 是 主要 贷款 者 。               联邦 基金 ： 美联储 中 拥有 存款 的 银行 间 的 隔夜 贷款 。 对应 的 利率 叫做 联邦 基金 利率 。 国内 当然 是         商业银行 间 的 隔夜 拆借 啦 。 银行 在 央行 要 有 足够 的 准备 经 。 该 利率 高 说明 银行 资金紧张 。                       资本 市场 工具 ： 长期债务 工具 ， 其 原始 期限 通常 为 1 年 或者 大于 1 年               股票 ， 股票 市值 占 比大 ， 但是 每年 新 发 股票价值 小 。 大约 一半 股票 由 个人 持有 ， 其余 由 养老金 、         共同 基金 和 保险公司 等 持有 。               抵押 贷款 ， 向 购买 房屋 、 土地 等 不动产 的 个人 或者 企业 提供 的 ， 以 不动产 作 抵押 的 贷款 。 它 是 美国 最大 的         债务 市场 ， 而 住房 抵押 贷款 是 所有 其他 抵押 贷款 的 4 倍 。           住房 抵押 贷款 主要 贷款 者 是 储贷 协会 和 互助 储蓄银行 ， 还有 商业银行 。       商业 农业 抵押 贷款 主要 是 有 商业银行 和 人寿保险 公司 发放 的 。       联邦政府 通过 房利美 ( FNMA ) 、 房地 美 ( FHLMC ) 、 政府 国民 抵押 协会 ( GNMA ) 发行 债券 筹集资金 ， 使用 这些 资金 购买 抵押 贷款 ， 从而 向 市场         提供 融资 。                   公司债券 ， 信用 评级 优异 的 公司 发行 的 长期 债券 。           某些 债券 可 转换成 规定 数量 的 股票 ， 成为 可 转换 债券 。 这种 债券 流动性 更好 。 虽然 市场 规模 比 股票市场 小 ， 但是 每年 发行 的 新 债券 比新 股票 多得多 。       主要 买方 是 人寿保险 公司 、 养老 基金 和 居民 。                   美国政府 证券 ， 美国财政部 发行 的 为 联邦政府 财政赤字 提供 融资 的 长期债务 工具 。           交易量 最大 、 流动性 最强       主要 持有者 包括 联邦 储备 体系 、 银行 、 居民 和 外国人                   美国政府 机构 证券 ， 就是 其他 一些 政府 机构 发行 的 为 政府 项目 提供 融资 的 证券 。 得到 联邦政府 担保 ， 与 政府 债券 类似 。               州 和 地方 政府 债券 ， 市政府 债券 ， 为 当地政府 建 学校 、 修 公路 等 大型项目 提供 融资 的 长期债务 工具 。           免税       商业银行 是 最大 购买者 ， 其他 还有 富人 和 保险公司                   消费 贷款 和 银行 商业贷款 ， 就是 想 消费者 提供 的 普通 商业贷款 啦                       金融市场 的 国际化               外国 债券 ( foreign   bonds ) 是 在 境外 发行 、 以 发行 国 货币 计价 的 债券 。           德国 汽车 公司 在 美国 发行 的 美元 债券       19 世纪 美国 建设 铁路 在 英国 发售 的 外国 债券                   欧洲 债券 ， 是 以 发行 国 以外 的 货币 计价 的 债券 。           伦敦 发行 的 以 美元 计价 的 债券       占 国际 债券 比例 80 %       大多数 欧洲 债券 不是 以 欧元 计价 的 ， 而是 以 美元 计价 的                   欧洲 货币 ， 存放 在 货币 发行 国 境外 银行 的 外汇 存款 。           欧洲 美元 ： 存放 在 美国 境外 的 外国 银行 或者 美国银行 境外 分行 的 美元 存款       不 一定 跟 欧洲 有 关系                   股票市场               金融 中介机构 的 功能 ： 间接 融资           向 贷款 者 借入 资金 ， 向 借款者 发放贷款 来 实现 资金 转移       是 公司 主要 的 融资 来源       存在 的 价值 ： 可 降低 金融交易 中 的 交易成本 、 风险 分担 和 信息 不对称性           交易成本 ( transaction   cost ) 是 指 在 金融交易 过程 中小 号 的 时间 和 金钱 。           是 计划 借出 富余 资金 的 人 所 面临 的 主要 问题 。       金融机构 可以 通过 规模 效应 摊薄 交易成本 ： 例如 一份 法律合同 可以 用 在 上万份 贷款 业务 中       低廉 的 交易成本 可以 使 中介机构 提供 流动性 服务 ： 支票 、 银联 支付 等                   风险 分担 ： 中介机构 低廉 的 交易成本 分散 风险 ， 通过 多样化 的 投资 分散 风险               信息 不对称性           逆向 选择 ( adverse   selection ) ： 交易 前 ， 因为 高风险 的 借款者 对 借款 更 有 积极性 ， 由于 信息 部队 称 导致 他们 更能 借到 钱 ，         但是 偿还 能力差 使得 整个 市场 风险 提高 ， 最终 导致 借款者 不愿 借款       道德风险 ( moral   hazard ) ： 交易 后 ， 借款者 借到 钱 后 可能 会 挪用 到 更 高风险 业务 当中 ， 导致 整个 市场 风险 提高 ，         由于 信息 不 对称 导致 借款者 不愿 借款       金融机构 具有 专业 的 能力 解决 信息 不 对称 问题 ， 比如 可以 对 贷款 企业 和 个人 做 详尽 的 调查                   金融 中介机构 的 类型               存款 机构 ( 银行 ) ： 货币 供给 的 主要 组成部分           商业银行       储贷 协会 和 互助 银行 ， 越来越 像 商业银行       信用社 ： 通过 股份 的 存款 来 募集 资金 ， 主要 用于 发放 消费 贷款                   契约型 储蓄机构 ： 能够 精确 预测 未来 年度 需要 向 受益人 支付 的 金额 ， 主要 投资 公司债券 、 股票 和 抵押 贷款 等 长期 证券           人寿保险 公司 ： 出售 年 金 募集 资金 ， 购买 长期 证券 获取 投资收益 ， 股票投资 数目 受限       火灾 意外 伤害 保险公司 ： 就是 各种 保险公司 啦       养老 基金 和 政府 退休 基金                   投资 中介机构           财务 公司 ： 出售 商业 票据 、 发行股票 和 债券 来 融资 ， 资金 借给 消费者 和 小型企业 ， 有些 财务 公司 是 母公司 建立 的 ，         帮助 出售 母公司 商品 ， 例如 汽车 公司 下 的 信贷 公司       共同 基金 ： 通过 股份 汇集 资金 ， 然后 批量 购买 股票 和 债券 获取 更 低 的 交易成本 ， 更 多样 的 投资 组合       货币 市场 共同 基金 ： 简称 货币基金 ， 和 共同 基金 融资 方式 类似 ， 但是 投资 的 是 安全性 和 流动性 都 很 高 的 货币 市场 工具 。       投资银行 ： 不 吸收 存款 也 不 发放贷款 ， 而是 协助 公司 发行 证券 ， 靠收 佣金                   金融体系 监管           证券法 ， 1933       金融 恐慌       准入 限制       信息 披露       对于 资产 和 业务 活动 的 限制 ： 比如 限制 参与 高风险 投资 活动       存款 保险       中国 从 2015 年 开始 实施 存款 保险 ， 消息来源   http : / / finance . people . com . cn / money / GB / 217428 / 391026 /   ，     也 有 知乎 网友 表示   这种 制度 短期内 是 负面 的   ，     因为 中国 本来 存款 是 由 政府 兜底 的 ， 这个 政策 一出 表明 政府 不想 兜底 了 ，     也 就是 暗示 银行 可能 破产 ， 会 导致 挤兑 ， 对 小型 银行 不利 。 历史 上 第一家 破产 的 银行 是 海南 发展 银行 ，     后来 债务 并入 工行 才 得以 解决 。               对 竞争 的 限制 ： 限制 竞争 来 降低 盲目 竞争 带来 的 风险       利率 管制 ： 放置 利率 恶性竞争 带来 的 风险           网络 练习题           联邦储备委员会 发布 的 美国 资金 流动 报告     https : / / www . federalreserve . gov / releases / z1 /         证券 交易所     https : / / www . nyse . com             什么 是 货币               银行券 ： 早期 由 私立 银行 发行 的 纸币               货币 的 含义           任何 一种 被 普遍 接受 的 、 可 用于 购买 商品 和 服务 支付 行为 的 或者 偿付 债务 的 物品 。       通货 ( currency ) ： 纸币 和 硬币       如果 很 容易 将 它们 迅速 地 转换成 通货 或者 支票 账户 存款 ， 那么 它们 就 具备 了 货币 的 功能 ， 比如 储蓄存款       财富 ： 指 储藏 的 各种 财产 综合 ， 不仅 包括 货币 ， 还有 各种 长期 资产 ， 如 不动产 、 债券 、 股票 、 艺术品 、 汽车 等                   货币 的 功能 ： 1 .   交易 媒介 ，   2 .   记账 单位 ，   3 .   价值 储藏               作为 交易 媒介 ， 提高 经济效益 ， 因为 可以 提高 交易 的 成功率 ， 降低 交易成本 。 比如 物物 交换 需要 交易 双方 都         需要 对方 的 物品 时 ， 才能 进行 交易 。 提高 社会分工 ， 使得 程序员 不用 去 种菜 也 能 买 到 便宜 的 菜 吃 。               记账 单位 ， 如果 没有 货币 ， 需要 知道 N 个 商品 的 价格 ( 相对 价格 ) ， 需要 知道 N ^ 2 个 数据 ， 而 有 了 货币 计量 的话 ，         就 只要 N 个 数据 。               价值 储藏 ： 货币 作为 价值 储藏 的 效果 取决于 物价水平 ， 物价上涨 的话 就 相当于 贬值 。 但是 由于 货币 具有 最大 的         流动性 ！ 它 是 流动性 最高 的 资产 。               支付 体系 ：           商品 货币 ： 如 贵金属 ， 但是 过于 沉重 。       不 兑现 纸币 ： 轻 ， 但是 需要 提高 伪造 成本 才能 实施 ， 易 被盗 。       支票 ：       电子 支付 ： 借记卡 、 储值卡 、 预付卡 、 智能卡 、 电子 现金 等                   货币 的 计量 ： M1 和 M2                   犯罪分子 是 拥有 现金 最多 的 人           网络 练习  ", "tags": "economics", "url": "/wiki/economics/money-banking-and-financial-markets.html"},
      
      
      
      
      
        
      
      {"title": "Hive full join 的优化", "text": "    Table   of   Contents           问题 背景           优化 方案                 问题 背景       有 多个 表 A , B , ... , Z ， 主 key 是 用户 ID ， 需要 full   join   到 一个 表中 。               select       coalesce     (     A     .     ID     ,       B     .     ID     ,       C     .     ID     )       as       ID     ,       A     .     col1     ,       B     .     col2     ,       C     .     col3       from       A       full       outer       join       B       on       A     .     ID     =     B     .     ID       full       outer       join       C       on       coalesce     (     A     .     ID     ,       B     .     ID     )     =     C     .     ID                 如果 有 很多 个表 ， 由于 上述 多个 连接 操作 的 key 中 并 没有 一个 固定 的 key ， 所以 HIVE 无法 优化 到 一个 MR ， 只能 顺序 的 join ， 导致 速度 较慢 。       优化 方案       可以 通过   UNION   ALL   优化               select       ID     ,       sum     (     col1     )       as       col1     ,       sum     (     col2     )       as       col2     ,       sum     (     col3     )       as       col3       from       (               select       ID     ,       col1     ,       NULL       as       col2     ,       NULL       as       col3       from       A                 UNION       ALL                 select       ID     ,       NULL       as       col1     ,       col2     ,       NULL       as       col3       from       B                 UNION       ALL                 select       ID     ,       NULL       as       col1     ,       NULL       as       col2     ,       col3       from       C       )     T       group       by       ID                 对于 数值 类型 ， 用   sum   聚合 ，   如果 是 字符串 类型 ， 可以 用   concat _ ws ( ' ' ,   collect _ list ( col ) )   聚合 。       在 一个 任务 上 实测 优化 前 需要 20min ， 优化 后 减少 到 10min ！  ", "tags": "hive", "url": "/wiki/hive/hive-full-join-optimization.html"},
      
      
      {"title": "Hive join 数据倾斜的一个case优化", "text": "    Table   of   Contents           问题           解决方案                 问题       设有 一个 关于 用户 信息 表 A ， 用户 ID 字段 为 uid ， 存在 少量 ( & lt ; 1000 ) uid 有 大量 记录 ( & gt ; 100000 ) ， 使得 表 A 和 其他 表以 uid 为 key 做 JOIN 的 时候 ， 存在 数据 倾斜 。             uid   |     f1     |   f2   1       |     xx     |   yy   1       |     xx     |   yy   2       |     xx     |   yy               解决方案       思路 一 ： 在 WHERE 条件 中 过滤 掉 那些 导致 数据 倾斜 的 uid 。       可以 解决 ， 但是 如果 这种 uid 太多 ， 或者 每天 会 随 时间 变化 ， 那么 这种 硬 编码 的 方法 并 不 通用 。       思路 二 ： 利用 skewjoin 的 优化 选项               set       hive     .     optimize     .     skewjoin     =     true     ;       set       hive     .     skewjoin     .     key     =     1000     ;                       将 一个 join 操作 变为 两个 ， 第一个 会 将 同一个 key 分散 到 不同 的 reduce ， 从而 解决 JOIN 时 数据 倾斜 问题 。   hive . skewjoin . key   要 设置 为 需要 分散 的 key 最少 条数 ， 默认 10W 条才 会 做 这个 操作 。       思路 三 ： 利用   JOIN   过滤 掉 那些 导致 倾斜 的 uid       根据上述 思路 ， 写下 如下 代码               select       A     .     *       from       A       JOIN       (               select       uid       as       guid       from       A               group       by       uid               having       sum     (     1     )       & lt ;       1000       )       B       ON       A     .     uid     =     B     .     guid                 B 表 用于 选择 满足条件 的 uid ， 然后 通过 JOIN 过滤 ！ 问题 来 了 ， 这个 过滤 也 是 JOIN ， 所以 仍然 会 有 数据 倾斜 的 问题 ！ ！       由于 这样 的 uid 总体 上 来说 比较 少 ， 而 问题 的 关键 出 在 不能 有 JOIN 的 REDUCE 操作 ， 所以 可以 利用 MAPJOIN 来 解决               select       A     .     *       from       A       LEFT       OUTER       JOIN       (               select       uid       as       guid       from       A               group       by       uid               having       sum     (     1     )       & gt ;       1000       )       B       ON       A     .     uid     =     B     .     guid       WHERE       B     .     guid       is       null                 注意 这里 的 B 表是 一个 小表 ， 可以 通过 MAPJOIN 优化 使得 这个 LEFT   OUTER   JOIN 没有 reduce 操作 ， 实现 在 map 端 过滤 ， 完美 解决 ！  ", "tags": "hive", "url": "/wiki/hive/hive-skew-join-optimize-case.html"},
      
      
      {"title": "Hive 提取最大值所对应行的操作优化", "text": "    Table   of   Contents           问题 背景           解决方案                 问题 背景       设有 表 A 包含 3 列 , 分别 是 c1 ,   c2 ,   c3 ; 其中 c1 到 c2 和 c3 是 1 对 多 的 关系 , 需要 在 每 一个 相同 c1 中 , 提取 c3 最大 ( 或 最小 ) 的 那 一行 的 c2 值 。     即 实现 如下 操作               select       c1     ,       c2       from     (               select       c1     ,       c2     ,       row _ number     ( )       over       (     partition       by       c1       order       by       c3       desc     )       as       r               from       A       )     T       where       r     =     1                 如果 不 存在 某些 c1 对应 大量 的 c2 , 即 不 存在 数据 倾斜 , 上述 代码 非常容易 实现 上述 逻辑 。 但是 当 存在 数据 倾斜 , 那么 这种 写法 将 导致 程序 卡死 在 reduce 操作 上面 。     窗 函数 在 实现 的 时候 会 将 同一个 partition 放在 一个 reduce 操作 , 貌似 没有 什么 优化 参数 可以 设置 这种 情况 的 数据 倾斜 。       解决方案       方案 1 : 实现 一个 通用 的 UDAF ,     select _ max ( c3 ,   c1 )   , 选出 c3 最大 的 c1 的 值 。       方案 2 : 如果 c3 是 日期 或者 其他 满足 字典 序 关系 的 字 段 ,   比如 日期   yyyy - MM - dd 。 那么 可以 用 一个 简单 的 trick 搞定 。               select       c1     ,       split     (     max     (     concat     (     c3     ,     &# 39 ; __&# 39 ;     ,       c2     ) ) ) [     1     ]       as       c2       from       A       group       by       c1                 即先 将 c3 作为 前缀 与 c2 通过 一个 特殊 分隔符 拼接 成 一个 字符串 , 然后 里面 字符串 求 最大值 的 方法 求 出 !     然后 设置   set   hive . groupby . skewindata = true   可以 有效 的 解决 数据 倾斜 的 问题 。       通过 这种 优化 之后 , 能够 有效 的 解决 数据 倾斜 问题 , 速度 有 了 很大 的 提升 !  ", "tags": "hive", "url": "/wiki/hive/hive-first-row-optimization.html"},
      
      
      {"title": "Hive 用户自定义函数开发", "text": "    Table   of   Contents           几个 重要 概念           ObjectInspector   常用 方法           UDF           UDAF           UDTF                 几个 重要 概念             ObjectInspector     用于 对象 描述 , 实际上 应该 理解 为 对 SQL 一个 字 段 属性 的 抽象 ?         ConstantObjectInspector     用于 描述 常 数字 段 的 描述 , 可以 通过 方法   getWritableConstantValue   获取 到 常数 的 值         WritableConstantIntObjectInspector     整数                   ObjectInspector   常用 方法           创建 基础 类型 的 OI                   PrimitiveObjectInspectorFactory     .     writableDoubleObjectInspector                     创建 复杂 的 OI                   ArrayList     & lt ;     ObjectInspector     & gt ;       foi       =       new       ArrayList     & lt ;     ObjectInspector     & gt ; ( ) ;         foi     .     add     (     PrimitiveObjectInspectorFactory     .     writableLongObjectInspector     ) ;       foi     .     add     (     PrimitiveObjectInspectorFactory     .     writableDoubleObjectInspector     ) ;       foi     .     add     (     PrimitiveObjectInspectorFactory     .     writableDoubleObjectInspector     ) ;         ArrayList     & lt ;     String     & gt ;       fname       =       new       ArrayList     & lt ;     String     & gt ; ( ) ;       fname     .     add     (     & quot ; count & quot ;     ) ;       fname     .     add     (     & quot ; sum & quot ;     ) ;       fname     .     add     (     & quot ; variance & quot ;     ) ;         ObjectInspectorFactory     .     getStandardStructObjectInspector     (     fname     ,       foi     ) ;                     利用   ObjectInspector   将     Object     转换成 基础 类型                   PrimitiveObjectInspectorUtils     .     getXXX     (     obj     ,       oi     )                     区分     javaXXXObjectInspector     与     writableXXXXObjectInspector   ;           很 简单     javaXXXObjectInspector     对应 的 是   Java 原生 的 数据类型 ,   而     writableXXXXObjectInspector     对应   Hive   可 序列化 的 数据类型 。     以     javaStringObjectInspector     和     writableStringObjectInspector     为例 , 前者 对应     String     而 后者 对应     Text   。     因此 , 如果 你 的 UDF 返回 的 是 java 原生 类型 , 那么 你 的 输出   ObjectInspector   必须 是 前者 ,   否则 将会 报错 !       UDF       UDAF       UDTF  ", "tags": "hive", "url": "/wiki/hive/hive-udf.html"},
      
      
      {"title": "Hive 行列互转", "text": "    Table   of   Contents           行 转列           列 转行                 行 转列       比如 表 A 有 三个 字 段 如下       ID ,   key ,   value     1 ,     A ,     1     1 ,     B ,     0.4     2 ,     A ,     0.3       需要 将 同一个 ID 的 多条 记录 转为 一列 ， 显然 需要 通过 聚合 操作 实现 。     对于 value 是 数值 类型 的 ， 聚合 函数 可以 选 为   sum   ， 对于 字符串 类型 ， 需要 使用 聚合 函数   collect _ list   。               select       ID     ,       sum     (     if     (     key     =     &# 39 ; A &# 39 ;     ,       value     ,       NULL     ) )       as       A _ value     ,       sum     (     if     (     key     =     &# 39 ; B &# 39 ;     ,       value     ,       NULL     ) )       as       B _ value       group       by       ID                 字符串 类型               select       ID     ,       concat _ ws     (     &# 39 ; &# 39 ;     ,       collect _ list     (     if     (     key     =     &# 39 ; A &# 39 ;     ,       value     ,       NULL     ) ) )       as       A _ value     ,       concat _ ws     (     collect _ list     (     if     (     key     =     &# 39 ; B &# 39 ;     ,       value     ,       NULL     ) ) )       as       B _ value       group       by       ID                 列 转行       这个 需要 利用   UDTF   实现 ！  ", "tags": "hive", "url": "/wiki/hive/hive-col-row-convert.html"},
      
      
      {"title": "Hive中窗函数数据倾斜的问题", "text": "    Table   of   Contents           问题 背景           现象           解释           解决办法                 问题 背景       现有 源表 A ， 包含 两列 ， id 和 score                   id       score                       1       0.3               2       0.5               8       0.9                   A 表 大约 有 3 亿条 记录 ， 现在 需要 根据 score 的 排序 将 记录 分为 10 个 等级 。     将 score 按照 从小到大 排序 ， score 在 前 10 % 的 记录 ， 等级 为 0 ； 10 % - 20 % 之间 的 ， 等级 为 1 ； 以此类推 。                   id       score       level                       1       0.3       1               2       0.5       5               8       0.9       8                   如果 用 全局 排序 实现 ， 需要 对 3 亿条 记录 排序 ， 非常 慢 。 可以 先 利用 分 位点 近似算法 ， 近似 寻找 10 % , 20 % , ... , 90 % 分 位点 来 优化 。     于是 ， 很 容易 用 聚合 函数     percentile _ approx ( )     通过 转换 为 窗 函数 实现               select       id     ,       array _ search     (     score     ,       splits     )       from       (               select       id     ,       score     ,                       percentile _ approx     (     score     ,       array     (     0     .     1     ,     0     .     2     ,     0     .     3     ,     0     .     4     ,     0     .     5     ,     0     .     6     ,     0     .     7     ,     0     .     8     ,     0     .     9     ) )       over       ( )       as       splits               from       A       )     t                   array _ search     是 自己 写 的 一个 UDF ， 寻找 score 所在 的 分段 ！       现象       运行 的 时候 发现 ， 多个 reduce 最后 一个 运行 特别 慢 ， 而且 当 运行 到 100 % 后 会 卡死 ， 直到 大约 20 分钟 结束 ，     在 输出 的 数据 分片 中 ， 发现 所有 的 数据 都 集中 到 一个 分片 了 。       解释       问题 在     over   ( )     将 聚合 函数 转换 为 窗 函数 上 ， 窗 函数 会 将 同一个 partition 的 数据 分 到 一个 reduce 上 ，     如果 是     over   ( partition   by   xx )   ， 那么 xx 字段 相同 的 值会 在 同一个 reduce 上 ， 然而 这里 是 将 所有 的 记录 当做 同一个 分片 ，     因而 出现 了 所有 数据 集中 到 一个 reduce 上面 的 现象 ， 进而 导致 写入 数据 的 时候 ， 只有 一个 reduce 在 写入 3 亿条 记录 ，     使得 看起来 就 像 卡死 在 100 % 这个 状态 了 ！       解决办法       不要 用窗 函数 ， 而用   ( map )   join   ， HIVE 会 自动 将 这个 join 操作 转为   map   join 。               select       id     ,       array _ search     (     score     ,       splits     )       from       A       join       (               select       percentile _ approx     (     score     ,       array     (     0     .     1     ,     0     .     2     ,     0     .     3     ,     0     .     4     ,     0     .     5     ,     0     .     6     ,     0     .     7     ,     0     .     8     ,     0     .     9     ) )       as       splits               from       A       )     t                 经过 改造 后 ， 运行 时间 从 之前 接近 1 小时 减少 到 5 分钟 ！  ", "tags": "hive", "url": "/wiki/hive/hive-window-function-question.html"},
      
      
      {"title": "Hive中窗函数的妙用", "text": "    Table   of   Contents           问题 背景                 问题 背景       先有表 如下                       A       B       C                       1       5       abc               7       4       abc               5       6       abc               - - - - - - - - - - - - - - - - - - - -                               现在 需要 对 同一个 C ， 按照 A 列 排序 （ 倒序 ） ， 计算 B 的 累积 分布 。     例如 ， 在 这个 例子 中 ， C = abc ， A = 7 ， 累积 值   CB = 4 ;   A = 5 ,   累计 值   CB = 4 + 6 = 10 ；   A = 1 ,   累积 值 CB = 4 + 6 + 5 = 15 ；    ", "tags": "hive", "url": "/wiki/hive/hive-window-function-usage.html"},
      
      
      {"title": "分区取TOP N问题", "text": "    Table   of   Contents           问题 背景           基本 方法           改进 方法                 问题 背景       设想 你 对 用户 在 不同 品类 上 的 行为 打分 聚合 后 得到 这样 一个 表   user _ cate _ score                   uid       cate       score                       1       1       0.3               2       2       0.5               8       3       0.9                   现在 , 你 想 将 每个 品类 的 TOP   100W   用户 取出 来 , 这种 场景 在 推荐 、 营销 中 很 常见 。       基本 方法       这个 很 容易 通过 窗 函数 写出 如下 解法               select       uid     ,       cate     ,       score       from     (               select       uid     ,       cate     ,       score     ,       row _ number     ( )       over       (     partition       by       cate       order       by       score       desc     )       as       r               from       user _ cate _ score       )     T         where       r       & lt ; =       1000000                 通过 窗 函数   row _ number ( )   在 每个 cate 分区 上 按照 score 排序 , 得到 row _ number , 然后 取 TOP   100W 即可 。       问题 :   HIVE 在 执行 的 时候 由于 每个 partition 会分 到 一个 reducer 上 , 这 导致 单个 reducer 处理 的 数据量 非常 大 , 严重 拖慢 执行 时间 , 没法 充分利用 到 分布式 的 效率 。       改进 方法       近似 取 TOP   100W               select       uid     ,       cate     ,       score       from     (               select       uid     ,       cate     ,       score     ,       row _ number     ( )       over       (     partition       by       cate     ,     rnd         order       by       score       desc     )       as       r               from     (                       select       *     ,       cast     (     rand     ( )       *       100       as       int     )       as       rnd                       from       user _ cate _ score               )     Ta       )     Tb       where       r       & lt ; =       10000                 思路 :   加上 一个 0 - 100 的 随机数 进行 partition , 可以 将 一个 cate 分散 到 100 个 reducer 上 执行 , 然后 对 每个 cate 的 每个 reducer 取 TOP   1W , 就 近似 实现 了 对 每个 cate 取 TOP   100W 。 好处 , 速度 快 了 很多 。  ", "tags": "hive", "url": "/wiki/hive/partition-topn.html"},
      
      
      
      
      
        
      
        
        
      
      {"title": "Overlapping Experiment Infrastructure: More, Better, Faster Experimentation", "text": "    Table   of   Contents           简介                 简介           解决 的 问题       如何 让 可以 运行 的 试验 更 多       如何 让 试验 更好 的 指导 决策       如何 让 试验 跑 得 更 快               文字 颜色 和 背景 颜色 不能 独立 变化 , 所以 没法 独立试验 , 解决 方法 , 将 参数 分为 多个 子集 , 保证 每个 子集 里面 是 可用 的 , 然后 每个 子集 占用 一层 试验 ? ?            ", "tags": "machine-learning/ab-test", "url": "/wiki/machine-learning/ab-test/google2010.html"},
      
      
      
      {"title": "Active Learning 主动学习", "text": "    Table   of   Contents           关于           Active   Deep   Learning   to   Tune   Down   the   Noise   in   Labels                 关于           主动 学习 算法 草图       给定 一个 模型 M 和 未 标注 数据 U       使用 当前 模型 M 预测 第 c 类 的 概率   P ( c | U )   =   M ( U )       根据 概率 P ( c | U ) 从 U 中 采样 出 一个 集合 L 进行 标注       将 标注 的 集合 L 加到 训练 集中 , 重新 训练 并 更新 模型 M               重复 上述 步骤 知道 提升 / 标注 成本 & lt ; 某个 阈值       不确定性 采样 方法       query   synthesis       selective   sampleing       pool - based   active   learning               不确定性 采样 最 简单 的 方法       熵 :   每个 类别 上 的 预测 概率 对应 的 熵 越 大 , 表明 越 不 确定 。 对于 二 分类 , 认为 在 0.5 附近 的 具有 很大 不确定性       least   confident :   1   -   max _ c   P ( c | x )   代表 样本 x 的 不确定性       margin   sampling :   P ( c _ 1 | x )   -   P ( c _ 2 | x )   即 最大 的 概率 减去 第二 大 的 概率 作为 样本 x 的 不确定性               Query   by   Commitee :       版本 空间 不确定性 最小化       训练 很多 个 模型 ,   如果 这些 模型 在 某个 未 标注 数据 上 预测 的 结果 差异 很大 , 表明 这个 样本 的 不确定性 很大               综述 论文 :   Active   Learning   Literature   Survey           Active   Deep   Learning   to   Tune   Down   the   Noise   in   Labels           KDD2018       通过 主动 学习 , 消除 标注 样本 中 的 噪声       通过 众包 平台 标注 的 样本 会 包含 噪声       主动 学习       Query   by   Commitee :   如果 先验 分布 已知 , 就 能 找到 决策 面 附近 的 样本 进行 标注 ;   但是 如果 位置 的话 , 就 会 有 偏       通过   ee 搜索 , 可以 避免 这种 偏差                    ", "tags": "machine-learning", "url": "/wiki/machine-learning/active-learning.html"},
      
      
        
        
      
      {"title": "计算广告的调研", "text": "    Table   of   Contents           关于           广告 计费 方式           广告 检索系统           Beyond   Keywords   and   Relevance :   A   Personalized   Ad   Retrieval   Framework   in   E - Commerce   Sponsored   Search                   参考                 关于           计算 广告 相关 的 调研           广告 计费 方式           CPC ,   按 点击 付费 ,   广告主 的 转化 效果 无法 保证       CPA ,   按 转化 付费 ,   广告商 的 收益 很难 保证       OCPC ,   按 转化 作为 优化 目标 ,   但是 按照 点击 付费           广告 检索系统       Beyond   Keywords   and   Relevance :   A   Personalized   Ad   Retrieval   Framework   in   E - Commerce   Sponsored   Search           传统 广告 检索 基于 广告主 购买 关键词 构建 倒排 索引 ,   在 query 到来 的 时候 ,   通过 query 改写 得到 关键词 ,   然后 通过 倒排 索引 检索 广告       这种 方式 在 广告主 没有 购买 关键词 时 无法 将 相关 广告 发给 相关 用户 ,   在 电商 广告 中 还有 其他 GMV 等 目标 要求       没有 用到 个性化 信息 帮组 检索 相关 广告       广告 搜索 过程       广告商 为 自己 的 广告 购买 一些 关键词 ,   并 为 关键词 出价       用户 提交 查询 请求 ,   并 携带 一些 反应 意图 的 信号 ,   如 query       广告 检索系统 基于 相关性 和 收入 估计 , 为 用户 检索 出 TOP   N 条 广告                               处于 性能 和 效率 的 考虑 ,   广告 搜索 系统 包含 两个 关键 模块 :   检索系统 、 和 排序 系统       广告 检索系统 又 包含 两个 关键 模块 :         改写 :   将 query 信息 改写 、 扩展 到 更 多 的 关键词       广告 选择 :   利用 改写 的 关键词 从 倒排 索引 中 高效 地 检索 出 相关 广告               三层 :       信号 节点 :   query ,   画像 ,   短期 点击 item ( 在 这个 query 下点 过 的 商品 ) ,   长期 点击 item       key 节点 :   query ,   item ,   shop ,   brand 。 key 节点 用来 作为 检索 的 key , 类似 于 query 的 关键词 , 所以 必须 保证 覆盖 的 广告 数量 不会 太 多 ,   所以 要 选取 广告 细粒度 的 属性       广告 节点 :   就是 每 一个 广告 了               两种 边       改写 边 ,   信号 节点 - - key 节点 ,   类似 于 传统 query 改写 的 作用 ,   相当于 query 改写 的 推广 , 用到 了 更 多 的 信号源 , 改写 后 也 不 限于 关键词 , 还有 item , shop 等 。 实现 的 时候 通过 倒排 索引 实现       广告 选择 边 ,   key - - 广告 ,   和 传统 的 广告 选择 一样 ,   key 就是 关键词 的 作用 ,   通过 倒排 索引 实现 检索               边 的 初始化       信号 节点 - - key 节点 的 边 不是 那么 容易 获取 ,   key - - 广告 的 边 就 比较 容易 ,   但 也 可以 通过 这个 方法 增强       点击 计数 ,   统计 每 一对   ( 信号 节点 ,   key 节点 )   点击 次数 ,   保留 点击 次数 大于 某个 阈值 的 ;   问题 在于 点击 次数 依赖于 展示 次数       信息 价值 ,   计算 每 一对   ( 信号 节点 ,   key 节点 )   的 信息 价值 ,   保留 信息 价值 大 的       基于 session 的 相关性 ,   同一个 session 里面 的 行为 是 相关 的 ,   因此 session 里面 的 query 、 点击 的 item 、 点击 的 广告 之间 是 相关 的 。 用 每个 node 在 session 空间 中 的 表示 向量 , 计算 余弦 距离 得到 相关性               初始化 的 边 很多 , 且 是 基于 相关性 的 , 而 不是 基于 RPM / CTR 导向 的 。 在 收集 一定 的 数据 后 可以 通过 模型 打分 预估 每个 边 的 权重       数据 集 的 生成       & lt ; { signal   nodes } ,   ad ,   label & gt ; ,   激活 的 信号 节点 、 广告 二元 组 ,   label 是 点击 或 未 点击       基于 初始化 的 网络 , 将 信号 节点 到 ad 改写 为边   & lt ; { signal   - & gt ;   key } ,   { key   - & gt ;   ad } ,   label & gt ; ,   注意 这里 边 是 一个 集合 ,   所以 一个 样本 包含 了 不定 数量 条边               特征 抽取       稀疏 特征 ,   节点 和 边 的 id 直接 作为 特征       连续 统计 特征 ,   点击 次数 、 展示 次数 、 点击率                   参考           《 计算 广告 》       Beyond   Keywords   and   Relevance :   A   Personalized   Ad   Retrieval   Framework   in   E - Commerce   Sponsored   Search      ", "tags": "machine-learning/ad", "url": "/wiki/machine-learning/ad/brief.html"},
      
      
      
      {"title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate", "text": "    Table   of   Contents           关于           摘要 与 间接                 关于           参考 :   Adaptive   Gradient   Methods   with   Dynamic   Bound   of   Learning   Rate           摘要 与 间接           关键点 :   解决 Adam 的 不 收敛 问题 , 动态 地 调整 学习 率 的 界 , 实现 平滑 地 从 Adam 和 AMSGRAD 演变 为 SGD           历史进程           Robbins   & amp ;   Monro   1951 年 发明 了 SGD       SGD 在 所有 方向 的 缩放 是 相同 的 , 对于 稀疏 优化 不太 适合       所以 , 人们 发明 了 自 适应 方法 ,   在 不同 的 方向 上以 累积 梯度 的 均 方根 成反比 的 缩放 学习 率       ADAgrad       用 累积 的 二阶 梯度 计算 学习 率               RMSProp       用 指数 平均 的 二阶 梯度 计算 学习 率               ADAM       在 RMSProp 基础 上 , 用 指数 平均 的 一 阶梯 度 计算 动量                                   自 适应 学习 率 方法 在 初始 的 时候 ,   学习 的 很快 ,   但是 很快 在 测试 集上 的 效果 出现 饱和 ,   反而 是 最 原始 的 SGD 收敛 的 更好           AMSGRAD       在 ADAM 基础 上 , 让 学习 率不增 , 实现 方法 也 很 简单 , 取上 一次 二阶 梯度 和 本次 二阶 梯度 的 最大值                                       $ ( \ \ phi _ t :   \ \ mathcal { F } ^ t   \ \ rightarrow   \ \ mathbf { R } ^ d ) $   相当于 梯度 的 一阶矩 , 它 是 历史 上 t 个 梯度 的 函数       $ ( \ \ psi _ t :   \ \ mathcal { F } ^ t   \ \ rightarrow   \ \ mathbf { S }   _   + ^ d ) $   $ ( \ \ mathbf { S }   _+ ^ d ) $ 是 d 阶 正定 方阵 ,   相当于 二阶 矩 ,   不过 是 一个 方阵 ,   常见 的 那种 二阶 矩是 一个 向量 , 可以 看做 对角 方阵 ,   那么 向量 的 元素 除法 就可以看 做 对 对角 矩阵 的 除法 了       $ ( \ \ Pi _ { \ \ mathcal { F } ,   M } ( y )   =   \ \ arg \ \ min _ { x   \ \ in   \ \ mathcal { F } }   | | M ^ { \ \ frac { 1 } { 2 } } ( x   -   y ) | | ) $   即点 y 向 空间 $ ( \ \ mathcal { F } ) $ 中 的 投影 , M 可以 理解 为 空间 的 度规 。       这个 框架 可以 将 主流 的 基于 一 阶梯 度 的 优化 算法 统一 起来 ,   如下 表 所示              ", "tags": "machine-learning", "url": "/wiki/machine-learning/adabound.html"},
      
      
        
        
      
      {"title": "关于应用数据科学", "text": "    Table   of   Contents           关于                 关于           本 栏目 主要 收集 以下 资源       KDD   applied   data   science   系列 论文 笔记       工业界 的 一些 问题 解决方案 调研              ", "tags": "machine-learning/applied-data-science", "url": "/wiki/machine-learning/applied-data-science/readme.html"},
      
      
      
      {"title": "BERT与Transformer", "text": "    Table   of   Contents           关于           Transformer 要点           Transform 用作 语言 模型           OpenAI           BERT 论文 要点           相关 关键 论文                 关于       被 BERT 和 Transformer 刷屏 了 , 研究 一下 。       关键 论文 :           Devlin   J ,   Chang   M   W ,   Lee   K ,   et   al .   Bert :   Pre - training   of   deep   bidirectional   transformers   for   language   understanding [ J ] .   arXiv   preprint   arXiv : 1810.04805 ,   2018 .       Ashish   Vaswani ,   Noam   Shazeer ,   Niki   Parmar ,   Jakob   Uszkoreit ,   Llion   Jones ,   Aidan   N   Gomez ,   Lukasz   Kaiser ,   and   Illia   Polosukhin .   2017 .   Attention   is   all   you   need .   In   Advances   in   Neural   Information   Pro -   cessing   Systems ,   pages   6000 – 6010 .           Transformer 要点           Attention 可以 抽象 为   ( Q ,   K ,   V ) 三元组 ,   Q 代表 query , 在 具体 场景 中 可以 是 搜索 的 query ,   推荐 排序 中 的 候选   item ,   广告 点击率 预估 中 的 候选 广告 etc   对应 的   embedding   向量 ;   K   和   V 一般 都 是 用户 历史 行为 序列 , 比如 用户 的 查询 query 序列 , 过去 一段时间 点击 过 的 item 序列 ,   过去 一段时间 点击 过 的 广告 序列   对应 的   embedding   向量 。 ( 如下 图 所示 )       KV 存储 作为 一个 特殊 的 Attention 机制 :   hard   attention       KV 存储 查询 的 时候 , 可以 看做 一个 Attention 机制 , 一个 query 过来 , 先 计算 Attention   score , Attention   score 可以 定义 为 如果 k = q , 那么 Attention   score   ai = 1 ,   否则 ai = 0 。 最终 的 输出 可以 看做 所有 的 V 按照 Attention   score 的 加权 和 。 即 只有 等于 q 的 那个 k 对应 的 v 才 被 输出 来       KV 这种 可以 看做 hard   Attention , 即 只 输出 其中 一个       在 NLP 中 也 有 这种 hard   Attention , 计算 q 和 k 的 相似 度 , 然后 只 输出 相似 度 最大 的 k 对应 的 v 。               一般 的 Attention 都 是 这种 soft   Attention , 不是 取 最 匹配 的 那个 , 而是 计算 一个 匹配 程度 权重 ( 即 Attention   score ) , 然后 按照 这个 权重 对 所有 的 v 做 加权 和 , 如下 图 所示 , 在 推荐 系统 中 , 可以 用 候选 item 和 用户 历史 点击 过 的 item 计算 相似 度 , 得到 Attention   score , 然后 计算 得到 的 加权 和 作为 最终 的 输出 。 soft   Attention 一般 会 对 Attention   score 用 softmax   归一化 ,   而 不是 直接 除以 他们 的 和 来 归一化 。                       对应 的 矩阵 计算 过程 就是 ,   除了 一个   $ ( \ \ sqrt { d _ k } ) $   也 就是 向量 的 维度 , 是 为了 防止 softmax 和 内积 导致 差异 太 大 。 比如 , 如果 向量 维度 非常 大 , 那么 相似 度 得分 很 容易 非常 大 , 比例 跟 维度 $ ( d _ k ) $ 正 相关 , 除以 维度 后 , 可以 在 一定 程度 上 解决 这个 问题 。 参考 下图 , 蓝色 箭头 表示 对 k 和 q 做 内积 , 黑色 + 表示 用 橙色 的 作为 权重 对 v 做 加权 和 。 注意   attention 的 输出 序列 长度 跟 query 序列 长度 一致 ,   key   和   value   序列 长度 一致 , 实际上 大多数 情况 K = V ,   在 self - attention 的 情况 下 ,   Q   =   K   =   V             $ $     Attention ( Q ,   K ,   V )   =   softmax ( \ \ frac { Q   K ^ T } { \ \ sqrt { d _ k } } )   V     $ $                   多头 Attention ,   一个 Attention 只 在 原始 的 向量 空间 中 操作 , 可以 将 Q , K , V 投影 到 不同 的 子 空间 甚至 更高 维度 空间 中 ( 数学 上 就是 分别 乘以 一个 矩阵 ) , 再 计算 Attention 向量 , 每 一种 投影 就 会 有 这样 一个 Attention 向量 , 搞 多个 投影 就 得到 多个 Attention 向量 了 。 将 多个 向量 拼接 再 投影 , 或者 直接 叫做 线性组合 , 因为 concat   +   投影 ( 乘以 一个 矩阵 )   等价 于 分别 投影 到 一个 共同 的 空间 中 然后 求和 ( 也 就是 线性组合 ) 。 也就是说 , 多头 Attention 的 输出 和 Attention 输出 是 一样 的 ,   所以 能用 Attention 的 地方 就 能 用 多头 Attention 。           $ $     MultiHead ( Q ,   K ,   V )   =   Concat ( head _ 1 ,   ... ,   head _ h ) W ^ O   \ \ \ \     head _ i   =   Attention ( QW _ i ^ Q   ,   KW _ i ^ K   ,   VW _ i ^ V   )     $ $                       self - attention ,   Q = K = V 时 , 就是 self - attention 。 self - attention 时 , 不论是 单头 还是 多头 , 输入 一个 向量 序列 , 输出 是 一个 相同 长度 的 向量 序列 , 每 一个 向量 的 维度 可以 不同 , 但是 向量 的 数目 是 相同 的 。 这 恰好 是 一层 RNN 和 一层 CNN 做 的 事情 , 这 说明 , self - attention 可以 实现 RNN 和 CNN 相同 的 事情 , 并且 可以 不断 堆叠 。               Position - wise   Feed - Forward   Networks :   输入 已经 attention 模块 的 输出 不 都 是 向量 序列 吗 ? 可以 在 这些 地方 插入 所谓 的 PFFN , 也 就是 用 同一个 神经网络 对 每 一个 向量 做 一个 变换 。 相当于 对 这个 向量 序列 使用 了 kernel   size = 1 的 一维 卷积 ( 下面 这个 式子 实际上 代表 了 两层 这种 卷积 , 第一层 带 了 非线性 激活 函数 , 第二层 是 一个 线性 层 )               $ $     FFN ( x )   =   \ \ max ( 0 ,   xW _ 1   +   b _ 1 )   W _ 2   +   b _ 2     $ $               transformer 也 是 Encoder - Decoder 架构           Encoder 包含 多个 multi - head   attention   +   FFN 的 子 模块 , Encoder 的 attention 是 self - attention 。       Decoder 模块 是 由 以下 这 三个 模块 重复 堆叠 N 次       输入 是 将 output 往 右移 一位 之后 的 向量 序列 , 为了 让 当前 的 输出 只 依赖 之前 , 所以 会 用 一个 mask 将 未来 的 序列 给 干掉 。       output 的 输入 encode 完 了 之后 , 与 Encoder 的 输出 一起 放到 一个 multi - head   attention 中 , 这个 attention 的 K 和 V 由 Encoder 的 输出 向量 序列 提供 , Q 由 Decoder 的 输入 提供 。       attention 输出 在 经过 一个 FFN 变换 。               输出 是 softmax , 相当于 一个多 分类 任务 , 每 一个 词 就是 一个 类别 , 所以 类别 数量 特别 多                   position   Encoder ,   将 位置 编码 加入 输入 向量 中 。 一种 是 直接 embedding , 文中 用 的 是 固定 向量 , v ( t ,   i )   =   f ( w _ i   t ) ,   t 是 偶数 是 就是 cos , t 是 奇数 时 是 sin ,   w _ i   都 很小 , 相当于 在 多个 正弦波 / 余弦 波中 同一 时刻 ( 位置 ) 采样 一个 值 作为 位置 编码 的 向量 。                       Transform 用作 语言 模型           P .   J .   Liu ,   M .   Saleh ,   E .   Pot ,   B .   Goodrich ,   R .   Sepassi ,   L .   Kaiser ,   and   N .   Shazeer .   Generating   wikipedia   by   summarizing   long   sequences .   ICLR ,   2018 .           OpenAI           Alec   Radford ,   Karthik   Narasimhan ,   Tim   Salimans ,   and   Ilya   Sutskever .   2018 .   Improving   language   understanding   with   unsupervised   learning .   Technical   report ,   OpenAI .       Decoder 中 的 Transform   block 可以 看做 对 输入 的 向量 序列 做 一个 变换 , 变成 另外 一个 向量 序列 , 并且 可以 不断 堆叠 , 堆叠 后 的 最后 一层 再 。 如果 把 用 上下文 的 那个 attention 模块 改成 self - attention 就 不用 Encoder 的 输入 了 。 那么 Decoder 就是 一个 语言 模型 的 预测器 ! !       用 语言 模型 预 训练 Transform 的 参数 , 并 调优 , 就是 这 篇文章 的 核心 要点 。       Decoder 最后 的 输出 全 连接 层 可以 替换成 具体任务 的 全 连接 层 , 重新 训练 。       训练 的 时候 可以 将 语言 模型 和 监督 任务 联合 训练 , 用 语言 模型 做 辅助 损失 , 相当于 正则 项 提高 泛化 能力           BERT 论文 要点           之前 的 工作 都 是 用 相同 的 目标 函数 ,   使用 单向 语言 模型 学习 通用 的 语义 表示       作者 认为 这种 单向 的 目标 函数 是 制约   pre - trained   方法 的 表达能力 的 关键 ,   所以 , 作者 搞了个 双向 语言 模型 ,   即     B   idirectional     E   ncoder     R   epresentations   from     T   ransformers 。 还是 基于   Transformers       新 的 目标 函数 :   masked   language   model ( MLM   马赛克 语言 模型 ? )   随机 mask 输入 句子 中 的 一些 词 ,   然后 让 模型 根据 上下文 推断 mask 掉 的 这些 词               Input :   the   man   went   to   the   [ MASK1 ]   .   he   bought   a   [ MASK2 ]   of   milk .     Labels :   [ MASK1 ]   =   store ;   [ MASK2 ]   =   gallon               \" next   sentence   prediction \"           相关 关键 论文           Matthew   Peters ,   Mark   Neumann ,   Mohit   Iyyer ,   Matt   Gardner ,   Christopher   Clark ,   Kenton   Lee ,   and   Luke   Zettlemoyer .   2018 .   Deep   contextualized   word   representations .   In   NAACL .       Alec   Radford ,   Karthik   Narasimhan ,   Tim   Salimans ,   and   Ilya   Sutskever .   2018 .   Improving   language   understanding   with   unsupervised   learning .   Technical   report ,   OpenAI .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/bert-transformer.html"},
      
      
      {"title": "Beyes optimization", "text": "    Table   of   Contents           关于           导论           Bayesian   Optimization   with   Gaussian   Process   Priors           高斯 过程           Acquisition   Functions                   In   Action           Covariance   Functions   and   Treatment   of   Covariance   Hyperparameters                         关于       贝叶斯 优化 用来 调参 ！     论文 ： Practical   Bayesian   Optimization   of   Machine   Learning   Algorithms ,   Jasper   Snoek ,   Hugo   Larochelle ,   Ryan   P .   Adams .       导论           关键词 ： 高斯 过程 （ Gaussian   Process ）       关键 要素 ： type   of   kernel ， treatment   of   its   hyperparameters       贝叶斯 优化 原始 文献 ：       Jonas   Mockus ,   Vytautas   Tiesis ,   and   Antanas   Zilinskas .   The   application   of   Bayesian   methods   for   seeking   the   extremum .   Towards   Global   Optimization ,   2 : 117 – 129 ,   1978 .       D . R .   Jones .   A   taxonomy   of   global   optimization   methods   based   on   response   surfaces .   Journal   of   Global   Optimization ,   21 ( 4 ) : 345 – 383 ,   2001 .       Niranjan   Srinivas ,   Andreas   Krause ,   Sham   Kakade ,   and   Matthias   Seeger .   Gaussian   process   optimization   in   the   bandit   setting :   No   regret   and   experimental   design .   In   Proceedings   of   the   27th   International   Conference   on   Machine   Learning ,   2010 .       Adam   D .   Bull .   Convergence   rates   of   efficient   global   optimization   algorithms .   Journal   of   Machine   Learning   Research ,   ( 3 - 4 ) : 2879 – 2904 ,   2011 .               假定 连续函数 采样 自 高斯 过程 ， 并 保存 一个 后验 概率 ， 根据 观测 值来 调整 。           expected   improvement   ( EI ) ；   Gaussian   process   upper   confidence   bound   ( UCB )               random   search   is   beter   than   grid   search :   James   Bergstra   and   Yoshua     Bengio   .   Randomsearch   for   hyper - parameter   optimization .   Journal   of   Machine   Learning   Research ,   13 : 281 – 305 ,   2012 .               Bayesian   Optimization   with   Gaussian   Process   Priors           优化 函数   $ ( f ( x ) ,   x   \ \ in   \ \ mathcal { X } ) $ ， $ ( \ \ mathcal { X } ) $   是   $ ( \ \ mathbb { R } ^ n ) $   上 的 闭集 。       贝叶斯 优化 基本 思想 ： 为 函数   $ ( f ( x ) ) $   建立 概率模型 ， 然后 根据 贝叶斯 法则 决定 下 一个 搜索 点 ！       贝叶斯 优化     综述   ： Eric   Brochu ,   Vlad   M .   Cora ,   and   Nando   de   Freitas .   A   tutorial   on   Bayesian   optimization   of   ex -   pensive   cost   functions ,   with   application   to   active   user   modeling   and   hierarchical   reinforcement   learning .   pre - print ,   2010 .   arXiv : 1012.2599 .           高斯 过程           在 有限 集   $ ( \ \ {   x _ n   \ \ in   \ \ mathcal { X }     \ \ } _ { n = 1 } ^ N ) $ ， 第 n 个点 的 值 为   $ ( f ( x _ n ) ) $ 。       均值 函数   $ ( m :   \ \ mathcal { X }   \ \ rightarrow   \ \ mathbb { R } ) $       正定 协方差 函数 （ covariance   function ）   $ ( K :   \ \ mathcal { X }   \ \ times   \ \ mathcal { X }   \ \ rightarrow   \ \ mathbb { R } ) $       高斯 过程 综述 ： Carl   E .   Rasmussenand   Christopher   Williams .   Gaussian   Processes   for   Machine   Learning .   MIT   Press ,   2006 .           Acquisition   Functions           假定 函数   $ ( f ( x ) ) $   来自 高斯 先验 ， 观测 量   $ ( \ \ { x _ n ,   y _ n   \ \ } _ { n = 1 } ^ N ,   y _ n   \ \ sim   \ \ mathcal { N } ( f ( x _ n ) ,   v ) ) $ ， v 是 观测 噪声 。       Acquisition   function :   $ ( a :   \ \ mathcal { X }   \ \ right   \ \ mathbb { R } ^ + ) $ ， 下 一个 最优 搜索 点 通过 该 函数 得到 ： $ ( x _ { next }   =   \ \ arg \ \ max _ x   a ( x ) ) $ 。 跟 之前 的 观测 值 有关 ， 也 写作   $ ( a ( x ;   \ \ { x _ n ,   y _ n   \ \ } ,   \ \ theta ) ) $ ，   $ ( \ \ theta ) $ 是 超 参数 。           In   Action       Covariance   Functions   and   Treatment   of   Covariance   Hyperparameters  ", "tags": "machine-learning", "url": "/wiki/machine-learning/beyes-opt.html"},
      
      
      {"title": "catboost", "text": "    Table   of   Contents           简介           论文 简述           类别 特征           Target   Statistic           LightGBM           参考                         简介           论文 :   CatBoost :   unbiased   boosting   with   categorical   features ,   Liudmila   Prokhorenkova1 , 2 ,   Gleb   Gusev1 , 2 ,   Aleksandr   Vorobev1 , Anna   Veronika   Dorogush1 ,   Andrey   Gulin1       用于 解决 决策树 模型 对 类别 特征 , 尤其 是 高维 稀疏 特征 拟合 的 不足       现有 基于 onehot 将 cat 特征 转化 为 数值 特征 的 问题       如果 cat 取值 过多 ( 高维 稀疏 ) , 那么 决策树 需要 拟合 深度 很深 , 泛化 能力 也 不足 。       如果 使用 hash   trik 降维到 几百 维 , 那么 会 导致 效果 下降 明显       不能 泛化 到 未 见 过 的 取值               基于   label - encode   的 方法 , 将 类别 特征 转化 为 基于 label   的 统计 量 的 数值 特征       例如 , 对于 item 的 某个 类别 特征 x , 不是 直接 用 x 作为 特征 , 而是 使用 概率   P ( y = 1 | x )   的 数值 作为 特征 , 例如 将 广告 id 转化 为 广告 id 对于 的 点击率 特征 , 注意 , 这里 和 历史 点击率 的 区别 , 这里 是 直接 用 样本 标签 计算 的 统计 量 , 而 不是 用 历史数据 统计 出来 的       但是 这种 方法 容易 过 拟合       使用 两个 技巧 ,   贝叶斯 先验 估计 ,   在线 更新   P ( y = 1 | x )               解决 特征 交叉 的 问题           论文 简述           prediction   shift       target   leakage           类别 特征           high   cardinality   features       聚类 后 在 做 onehot       target   statistic   方法               LightGBM       每 一次 都 要 计算 每 一个 取值 的 统计 信息       为了 节省 空间 和 时间 , 将 长尾 取值 放到 一个 类别 中                   Target   Statistic           对于 类别 特征   $ ( x ^ i ) $ ,   其中 某个 取值   $ ( x ^ i _ k ) $   可以 用 target 的 统计 量 来 替代           $ $     \ \ hat { x } ^ i _ k   =   \ \ frac { \ \ sum   \ \ mathbf { 1 }   _   { x ^ i _ j = x ^ i _ k }   y _ j   + a   p } { \ \ sum   \ \ mathbf { 1 }   _   { x ^ i _ j = x ^ i _ k }   +   a }     $ $           即用 target 平滑 后 的 平均值 来 替代 这个 类别 特征       但是 这种 方法 存在 一个 问题 , 在 训练 集 和 测试 集中 ,   相同 的 类别 特征 的 target   statistic   不 相等 , 存在 所谓 的 condition   shift 和 target 泄露 问题       解决 方法 :   不 在 全部 数据 集中 计算 TS , 而是 对 不同 的 样本 用 不同 的 集合 计算 TS ,   例如 对 样本   $ ( x _ k ) $ ,   用 排除 该 样本 的 其他 集合 计算 的 TS 作为 $ ( x _ k ) $ 的 类别 特征 的 替代 值       Holdout   TS :   将 数据 集 划分 为 两 部分 ,   一部分 用来 计算 TS , 另 一部分 用来 训练       Leave - one - out   TS :   对 训练样本   $ ( x _ k ) $ 用 排除 了 $ ( x _ k ) $ 的 剩下 样本 计算 TS , 对 测试 样本 则 用 全 量计算 TS 。 但是 并 没有 解决 target 泄露 的 问题       Ordered   TS :   借鉴 自 在线 学习 ,   在 离线 训练 时 , 先 打散 , 得到 一个 认为 的 时间 顺序 , 在 对 每个 样本 计算 TS 时 , 只用 时间 在 之前 的 那些 样本 。 对 测试 集 , 就 用 全量 训练 集来 计算 TS       如果 只用 一次 随机 排列 , 那么 前面 的 树 的 方差 将 比 后面 的 树大 ,   Catboost 每 一轮 都 重新 使用 一次 随机 排列                   LightGBM           通过 采样 减少 计算 增益 时 的 计算 量       基于 梯度 的 单边 采样 GOSS ,   出发点 :   梯度 大 的 样本 对 增益 影响 大 , 所以 只 对 梯度 小 的 样本 下 采样       Exclusive   Feature   Bundling :   对于 稀疏 特征 , 求解 最优 分组 问题 ( 很 复杂 ) , 将 它 转化 为 图 着色 问题                   GBDT 复杂度           找到 特征 分割 点       预 排序 :   先 将 特征值 排序 , 然后 枚举 所有 可能 的 分割 点       直方图 算法 :   将 连续 特征 离散 分桶 , 构造 直方图 , 只 在 直方图 的 交界 点 尝试 分割 , 并用 直方图 计算 Gain 。 对 稀疏 特征 兼容                           Exclusive   Feature   Bundling :   将 稀疏 特征 分组 , 从而 将 复杂度 从   O ( # data   x   # feature )   减少 到   O ( # data   x   # bundle ) , 在 稀疏 特征 存在 的 时候 , 特征 数量 会远 多于 分组 的 数量               稀疏 特征 的 分割           将 稀疏 特征 的 取值 按照 累积 的   ( sum _ gradient   /   sum _ hessian )   排序 , 然后 在 排序 后 的 直方图 中 找到 最优 分割 点                   参考           D .   Micci - Barreca .   A   preprocessing   scheme   for   high - cardinality   categorical   attributes   in   classifi -   cation   and   prediction   problems .   ACM   SIGKDD   Explorations   Newsletter ,   3 ( 1 ) : 27 – 32 ,   2001 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/catboost.html"},
      
      
      {"title": "Collaborative Filtering with Recurrent Neural Networks", "text": "    Table   of   Contents           论文 导读           协同 过滤 作为 一个 序列 预测 问题                         论文 导读       Collaborative   Filtering   with   Recurrent   Neural   Networks ，   IRIDIA ， 2016       协同 过滤 作为 一个 序列 预测 问题       传统 方法 ， 用 t 时刻 之前 ， 用户 消费 的 item 集合 预测 t 时刻 之后 用户 消费 的 集合 。     没有 考虑 顺序 ！       新 方法 ： 用 t 时刻 之前 的 消费 序列 $ ( x _ 1 ,   x _ 2 ,   x _ 3 ) $ ， 预测 t 之后 消费 $ ( x _ 4 , x _ 5 ) $ 。     考虑 顺序 ！ 预测 结果 也 考虑 顺序 ！           短期 预测 ： 预测 下 一个 消费 的 item       长期 预测 ： 预测 最终 消费 的 item           序列 预测 方法 适合 短期 预测 ！ 短期 预测 可以 提高 推荐 的 多样性 ！     而 不管 顺序 的 静态方法 适合 长期 预测 ！       作者 任务 隐藏 在 序列 里面 的 信息 很 重要 ！ 静态方法 没有 充分考虑 。     可以 找到 用户 兴趣 变化 ， 帮助 辨别 哪些 item 与 当前 用户 兴趣 是 无关 的 ， 或者 哪些 商品 导致用户 兴趣 的 消失 。     也 能 了解 哪些 item 对 用户 变化 的 兴趣 更 有 影响力 ？ ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cf-rnn.html"},
      
      
        
        
      
      {"title": "埋点技术方案", "text": "    Table   of   Contents           关于           埋点 技术           埋点 方式           如何 做好 数据 埋点                 关于       人工智能 、 机器 学习 首先 得 收集 数据 , 埋点 是 一个 重要 的 方式 。       埋点 技术       埋点 方式       如何 做好 数据 埋点  ", "tags": "machine-learning/collect-data", "url": "/wiki/machine-learning/collect-data/maidian.html"},
      
      
      
        
        
      
      {"title": "2018-CVPR论文集", "text": "    Table   of   Contents           关于           SENet           shuffle - net           Learning   Transferable   Architectures   for   Scalable   Image   Recognition           MobileNetV2           Bottom - Up   and   Top - Down   Attention   for   Image   Captioning   and   Visual   Question   Answering                 关于       CVPR2018 论文 简读 笔记       SENet           论文 :   Squeeze - and - Excitation   Networks ,   中科院 软件 所       ILSVRC   2017   冠军 模型       代码 :   https : / / github . com / hujie - frank / SENet       相关 解读 :     https : / / zhuanlan . zhihu . com / p / 32733549         主要 创新 点 是 在 普通 的 卷积 层 后面 增加 了 一个 模块 :   Squeeze - and - Excitation   block                       模块 解读       对 输入   H ' xW ' xC   的 特征 图 ,   首先 进行 正常 的 卷积 层 变换 成   HxWxC       然后 对 每 一个 通道 ( 即 C 这个 维度 ) 加权 ,   用来 提升 重要 的 特征 通道 ,   抑制 不 重要 的 特征 通道 ,   作者 称之为   feature   recalibration ,   其实 就是 通道 维度 的 attention 机制 嘛       通道 权重 计算       squeeze 操作 :   将   HxWxC   特征 图 通过 全局 平均 池化 , 变成   1x1xC   的 向量       excitation :   通过 一个 单隐层 的 神经网络 变成 一个 recalibration 后 的 C 维 向量                           shuffle - net           face ++       略 ,   有 精度 笔记 ,   关键点 是 通道 shuffle ,   将 通道 间 的 全 连接 变成 部分 连接 ,   又 通过 通道 shuffle ,   避免 通道 间 的 完全 隔离           Learning   Transferable   Architectures   for   Scalable   Image   Recognition           Googlebrain       创新 点 :   用 算法 学习 模型 结构 ,   先用 个 小 数据 集 搜索 一个 比较 好 的 结构 , 然后 在 大 数据 集上 重新 训练       NASNet 搜索 空间   B .   Zoph   and   Q .   V .   Le .   Neural   architecture   search   with   rein -   forcement   learning .   In   International   Conference   on   Learning   Representations ,   2017 .       用 一个 RNN 来 输出 神经网络 每 一层 的 参数 序列 ( 见图 ) , 即 动作 A , RNN 输出 的 是 动作 序列       RNN 的 参数 当做 策略 函数 的 参数       用 当前 层 之前 层 的 超 参数 作为 状态 S ?       模型 的 层数 超过 一个 阈值 就 停止       用 RNN 输出 的 参数 作为 模型 超 参数 的 NN 在 验证 集上 的 准确率 作为 回报 R       采用 策略 梯度 算法 REINFORCE 对 RNN 的 参数 进行 优化                           MobileNetV2           论文 :   MobileNetV2 :   Inverted   Residuals   and   Linear   Bottlenecks       Google       创新 点 :   新 的 模块 层 ,   先 用 一个 1x1 卷积 层升维 ,   然而 利用 深度 分离 卷积 在 每 一个 通道 上 单独 做 卷积 ,   然后 又 用 1x1 卷积 降维 ,   还 增加 了 残差 模块       TensorFlow - slim   官方 实现           Bottom - Up   and   Top - Down   Attention   for   Image   Captioning   and   Visual   Question   Answering           JD 、 微软 、 澳大利亚 大学 etc      ", "tags": "machine-learning/conference", "url": "/wiki/machine-learning/conference/cvpr2018.html"},
      
      
      {"title": "2018-KDD论文集", "text": "    Table   of   Contents           Real - time   Personalization   using   Embeddings   for   Search   Ranking   at   Airbnb           Adversarial   attacks   on   classification   models   for   Graphs           XiaoIce   Band :   A   Melody   and   Arrangement   Generation   Framework   for   Pop   Music           待 阅读           有 价值 的 文章           参考                 Real - time   Personalization   using   Embeddings   for   Search   Ranking   at   Airbnb       参考 精读 笔记       Adversarial   attacks   on   classification   models   for   Graphs           图 分类 模型 的 对抗 攻击       图是 离散 的 , 无法 利用 图像 的 基于 梯度 的 方法           XiaoIce   Band :   A   Melody   and   Arrangement   Generation   Framework   for   Pop   Music           流行音乐 编曲 框架       Google 的 先驱 工作 ,   GANSynth :   Adversarial   Neural   Audio   Synthesis           待 阅读           XiaoIce   Band :   A   Melody   and   Arrangement   Generation   Framework   for   Pop   Music       ActiveRemediation :   The   Search   for   Lead   Pipes   in   Flint ,   Michigan ? ? ? 什么 鬼       STAMP :   Short - Term   Attention / Memory   Priority   Model   for   Session - based   Recommendation       R2SDH :   Robust   Rotated   Supervised   Discrete   Hashing       StockAssIstant :   A   Stock   AI   Assistant   for   Reliability   Modeling   of   Stock   Comments           有 价值 的 文章           I   Know   You ’ ll   Be   Back :   Interpretable   New   User   Clustering   and   Churn   Prediction   on   a   Mobile   Social   Application     Snapchat   用户 流失 建模 和 解释       Large - Scale   Order   Dispatch   in   On - Demand   Ride - Sharing   Platforms :   A   Learning   and   Planning   Approach   滴滴 派单 模型 加强版       WattHome :   Identifying   Energy - Inefficient   Homes   at   City - scale   发现 能耗 浪费       Perceive   Your   Users   in   Depth :   Learning   Universal   User   Representations   from   Multiple   E - commerce   Tasks   淘宝 用户 模型       TATC :   Predicting   Alzheimer ’ s   Disease   with   Actigraphy   Data   港 中文 和 腾讯 的 阿兹海默征 研究       Automatic   Discovery   of   Tactics   in   Spatio - Temporal   Soccer   Match   Data   足球 的 什么 东西 , 没 搞懂       Corpus   Conversion   Service :   A   machine   learning   platform   to   ingest   documents   at   scale .   IBM 的 一个 在线 文档 处理 服务       Using   Machine   Learning   to   Assess   the   Risk   of   and   Prevent   Water   Main   Breaks   水管 爆裂 预测       OpenTag :   Open   Attribute   Value   Extraction   from   Product   Profiles   亚马逊 ,   提取 商品 的 属性 值       Detecting   Spacecraft   Anomalies   Using   LSTMs   and   Nonparametric   Dynamic   Thresholding   NASA 用 机器 学习 检测 太空船 异常       Online   Parameter   Selection   for   Web - based   Ranking   Problems   LinkedIn 在线 模型 参数 选择       Rare   Query   Expansion   Through   Generative   Adversarial   Networks   in   Search   Advertising   微软 用 GAN 做 query 扩展       Learning   and   Transferring   IDs   Representation   in   E - commerce ,   ID 表达 ,   阿里巴巴       Collaborative   Deep   Metric   Learning   for   Video   Understanding ,   视频 理解 , Google       False   Discovery   Rate   Controlled   Heterogeneous   Treatment   Effect   Detection   for   Online   Controlled   Exper ,   在线 AB 测试 的 问题 ,   Snapchat       Rosetta :   Large   scale   system   for   text   detection   and   recognition   in   images ,   ORC   Facebook       Explaining   Aviation   Safety   Incidents   Using   Deep   Temporal   Multiple   Instance   Learning ,   解释 航空 安全事件   NASA       Towards   station - level   demand   prediction   for   effective   rebalancing   in   bike - sharing   systems ,   共享 单车 系统 中 的 需求预测       Du - Parking :   Spatio - Temporal   Big   Data   Tells   You   Realtime   Parking   Availability ,   停车位   Baidu       Near   real - time   optimization   of   activity - based   notifications ,   通知 的 近 实时 优化 , LinkedIn       Accelerating   Prototype - Based   Drug   Discovery   using   Conditional   Diversity   Networks ,   医药 发现       Releasing   eHealth   Analytics   into   the   Wild :   Lessons   Learnt   from   the   SPHERE   Project ,   SPHERE 项目 的 经验 ,   智能家居       Detecting   Illegal   Vehicle   Parking   Events   using   Sharing   Bikes ’   Trajectories ,   用 共享 单车 轨迹 检测 非法 停车       Graph   Convolutional   Neural   Networks   for   Web - Scale   Recommender   Systems ,   Pinterest 的 图 卷积 网络       Audience   Size   Forecasting :   Fast   and   Smart   Budget   Planning   for   Media   Buyers ,   广告 的 智能 预算 , Dstillery       Q & amp ; R :   A   Two - Stage   Approach   Toward   Interactive   Recommendation ,   Google 问答式 推荐       ActiveRemediation :   The   Search   for   Lead   Pipes   in   Flint ,   Michigan ,   管道 搜索 ?       Winner ’ s   Curse :   Bias   Estimation   for   Total   Effects   of   Features   in   Online   Controlled   Experiments ,   AB 测试 ,   Airbnb 的 一个 框架 Experiment   Reporting   Framework       Detection   of   Apathy   in   Alzheimer   Patients   by   Analysing   Visual   Scanning   Behaviour   with   RNNs ,   阿兹海默症 研究       A   Dynamic   Pipeline   for   Spatio - Temporal   Fire   Risk   Prediction ,   火灾 预测 ,   CMU       Optimization   of   a   SSP ’ s   Header   Bidding   Strategy   using   Thompson   Sampling ,   广告 定价 ?       Applying   the   Delta   method   in   metric   analytics :   A   practical   guide   with   novel   ideas ,   Delta 方法 ? 大规模 AB 测试 ,   微软           Where   Will   Dockless   Shared   Bikes   be   Stacked ? — -   Parking   Hotspots   Detection   in   a   New   City   共享 单车 ,   上海交通大学               Visual   Search   at   Alibaba ,   视觉 搜索 ,   阿里巴巴           Deep   Distributed   Fusion   Network   for   Air   Quality   Prediction ,   空气质量 预测 ,   京东       Deep   Interest   Network   for   Click - Through   Rate   Prediction ,   DIN ,   阿里巴巴       Learning   Tree - based   Deep   Model   for   Recommender   Systems ,   TDM ,   阿里巴巴       Dynamic   Pricing   under   Competition   on   Online   Marketplaces :   A   Data - Driven   Approach ,   动态 价格 ,         Web - Scale   Responsive   Visual   Search   at   Bing ,   可视化 搜索 ,   微软       Distributed   Collaborative   Hashing   and   Its   Applications   in   Ant   Financial ,   协同 hash ,   蚂蚁 金服       Reinforcement   Learning   to   Rank   in   E - Commerce   Search   Engine :   Formalization ,   Analysis ,   and   Application ,   RL 搜索引擎 ,   阿里巴巴       COTA :   Improving   the   Speed   and   Accuracy   of   Customer   Support   through   Ranking   and   Deep   Networks ,   客服 ? ?   Uber       RapidScorer :   Fast   Tree   Ensemble   Evaluation   by   Maximizing   Compactness   in   Data   Level   Parallelization ,   树 模型 打分 加速 ,   微软       A   real - time   framework   for   detecting   efficiency   regressions   in   a   globally   distributed   codebase ,   代码 仓库 检测 效率 ,   Facebook       Inferring   Metapopulation   Propagation   Network   for   Intra - city   Epidemic   Control   and   Prevention ,   传染病 控制 ,   北航       BigIN4 :   Instant ,   Interactive   Insight   Identification   for   Multi - Dimensional   Big   Data ,   大 数据 商业智能 ,   微软       Managing   Computer - Assisted   Detection   System   Based   on   Transfer   Learning   with   Negative   Transfer   Inhibi ,   计算机 负责 检测 系统       Billion - scale   Commodity   Embedding   for   E - commerce   Recommendation   in   Alibaba ,   十亿 级别 的 embedding 实现 ,   阿里巴巴       A   Data - Driven   Three - Layer   Algorithm   for   Split   Delivery   Vehicle   Routing   Problem   with   3D   Container   Loa ,   运输 规划 ,   华为       Predicting   Estimated   Time   of   Arrival   for   Commercial   Flights ,   预估 商飞 到达 时间       SQR :   Balancing   Speed ,   Quality   and   Risk   in   Online   Experiments ,   AB 测试   LinkedIn       Buy   It   Again :   Modeling   Repeat   Purchase   Recommendations ,   复购 的 推荐 ,   Amazon       Learning   to   Estimate   the   Travel   Time ,   到达 时间 预测 ,   滴滴       Anatomy   of   a   Privacy - Safe   Large - Scale   Information   Extraction   System   Over   Email ,   隐私 安全 的 信息 抽取 ,   Google       EANN :   Event   Adversarial   Neural   Networks   for   Multi - Modal   Fake   News   Detection ,   假新闻 检测       Interpretable   Representation   Learning   for   Healthcare   via   Capturing   Disease   Progression   through   Time ,   病毒传播       Notification   Volume   Control   and   Optimization   System   at   Pinterest ,   通知 容量 控制 和 优化 ,   Pinterest       Deep   Learning   for   Practical   Image   Recognition :   Case   Study   on   Kaggle   Competitions ,   图像识别 实战 ,   Kaggle       Shield :   Fast ,   Practical   Defense   and   Vaccination   for   Deep   Learning   using   JPEG   Compression ,   用 JPEG 压缩 实现 对 DNN 的 快速 防御 ?       Deploying   Machine   Learning   Models   for   Public   Policy :   A   Framework ,   机器 学习 模型 的 部署 框架       Active   Deep   Learning   To   Tune   Down   the   Noise   in   Labels ,   主动 学习 降低 label 噪声       Dynamic   Recommendations   for   Sequential   Hiring   Decisions   in   Online   Labor   Markets ,   劳动力 市场 的 动态 推荐       Tax   fraud   detection   for   under - reporting   declarations   using   an   unsupervised   machine   learning   approach ,   检测 个 税 诈骗       Optimal   Allocation   of   Real - Time - Bidding   And   Direct   Campaigns ,   最优 预算 广告       Recommendations   with   Negative   Feedback   via   Pairwise   Deep   Reinforcement   Learning ,   DRL + 负反馈 推荐 , 京东       Assessing   Candidate   Preference   through   Web   Browsing   History ,   选举       A   Scalable   Solution   for   Rule - Based   Part - of - Speech   Tagging   on   Novel   Hardware   Accelerators ,   POS   tagging   FPGA 加速       Buy   It   Again :   Modeling   Repeat   Purchase   Recommendations ,   复购 ,   Amazon       Adaptive   Paywall   Mechanism   for   Digital   News   Media ,   自 适应 地 让 用户 订购 , 动态 定价 , 动态 补贴       Deep   Sequence   Learning   with   Auxiliary   Information   for   Traffic   Prediction ,   路由 流量 预测       Scalable   Query   N - Gram   Embedding   for   Improving   Matching   and   Relevance   in   Sponsored   Search ,   query   embedding 的 广告 检索       Scalable   Optimization   for   Embedding   Highly - Dynamic   and   Recency - Sensitive   Data ,   清华 , 不 知道 干 啥       Pangloss :   Fast   Entity   Linking   in   Noisy   Text   Environments ,   实体 关系 链       Learning   to   Estimate   the   Travel   Time ,   滴滴   ETA       Exploring   Student   Check - In   Behavior   for   Improved   Point - of - Interest   Prediction ,   基于 社交 打开 数据 预测 POI       Notification   Volume   Control   and   Optimization   System   at   Pinterest ,   push 频率 控制 ,   Pinterest       Identify   Susceptible   Locations   in   Medical   Records   via   Adversarial   Attacks   on   Deep   Predictive   Models ,   medical 记录 的 对抗 攻击 ?       Active   Deep   Learning   To   Tune   Down   the   Noise   in   Labels ,   主动 学习 降低 label 的 噪声       Mobile   access   record   resolution   on   large - scale   identifier - linkage   graphs ,   自然人 识别 , 基于 图 挖掘 ,   浙大 、 阿里巴巴       Deep   Reinforcement   Learning   for   Sponsored   Search   Real - time   Bidding ,   DRL 实时 竞价 ,   浙大 、 阿里巴巴       MIX :   Multi - Channel   Information   Crossing   for   Text   Matching ,   多通道 信息 交叉 ,   腾讯       Discovering   latent   patterns   of   urban   cultural   interactions   in   WeChat   for   modern   city   planning ,   微信 城市 计算 ,   腾讯       StepDeep :   A   Novel   Spatial - temporal   Mobility   Event   Prediction   Framework   based   on   Deep   Neural   Network ,   空时 预测 框架 ,   清华 、 科大 、 CMU   etc           参考           Paper   list     https : / / www . kdd . org / kdd2018 / accepted - papers         Tutorial     https : / / www . kdd . org / kdd2018 / hands - on - tutorials        ", "tags": "machine-learning/conference", "url": "/wiki/machine-learning/conference/kdd2018.html"},
      
      
      {"title": "2019-AAAI论文集", "text": "    Table   of   Contents           论文集           论文 列表                 论文集           接受 论文 列表   & lt ; & gt ;             AAAI   2019   有 什么 有意思 的 NLP / ML 文章 ？                 Outstanding   Paper :   How   to   Combine   Tree - Search   Methods   in   Reinforcement   Learning           Honorable   Mention :   Solving   Imperfect - Information   Games   via   Discounted   Regret   Minimization       Outstanding   Student   Paper :   Zero   Shot   Learning   for   Code   Education :   Rubric   Sampling   with   Deep   Learning   Inference       Outstanding   Student   Paper   Honorary   Mention :   Learning   to   Teach   in   Cooperative   Multiagent   Reinforcement   Learning       Classic   Paper :   Content - Boosted   Collaborative   Filtering   for   Improved   Recommendations       Blue   Sky   Idea   2019       Explainable ,   Normative ,   and   Justified   Agency （ Pat   Langley ）       Building   Ethically   Bounded   AI （ Francesca   Rossi 、 Nicholas   Mattei ）       Recommender   Systems :   A   Healthy   Obsession （ Barry   Smyth ）                   论文 列表           DFANet :   Deep   Feature   Aggregation   for   Real - Time   Semantic   Segmentation ,   高清 版单 摄 实现 虚化 ,   旷视 科技       DRr - Net :   Dynamic   Re - read   Network   for   Sentence   Semantic   Matching ,   动态 重读 attention 机制 , USTC            ", "tags": "machine-learning/conference", "url": "/wiki/machine-learning/conference/aaai2019.html"},
      
      
      {"title": "2019-CVPR论文集", "text": "    Table   of   Contents           关于           论文 列表           检测           分类           人脸                         关于       CVPR2019 年 会议 ,   召开 地点 :   Long   Beach ,   CA , 时间 :   June   16th   -   June   20th       论文 列表       检测           Stereo   R - CNN   based   3D   Object   Detection   for   Autonomous   Driving ,   3D 目标 检测 ,   港科 、 大疆       Generalized   Intersection   over   Union :   A   Metric   and   A   Loss   for   Bounding   Box   Regression ,   Bounding   Box   Regression 的 优化 ,       ROI - 10D :   Monocular   Lifting   of   2D   Detection   to   6D   Pose   and   Metric   Shape ,   3D 目标 检测 、 形状 检索       Bi - Directional   Cascade   Network   for   Perceptual   Edge   Detection ,   边缘 检测 , 得到 图片 的 轮廓图 , 北大       RepMet :   Representative - based   metric   learning   for   classification   and   one - shot   object   detection ,   距离 测度 学习 , 实现 one - shot 目标 检测 , 学习 检测 新 的 目标       Region   Proposal   by   Guided   Anchoring ,         Less   is   More :   Learning   Highlight   Detection   from   Video   Duration ,   关键帧 检测 ,   Facebook       AIRD :   Adversarial   Learning   Framework   for   Image   Repurposing   Detection ,         Feature   Selective   Anchor - Free   Module   for   Single - Shot   Object   Detection           分类           Bag   of   Tricks   for   Image   Classification   with   Convolutional   Neural   Networks ,   一堆 tricks           人脸           Deep   Tree   Learning   for   Zero - shot   Face   Anti - Spoofing ,        ", "tags": "machine-learning/conference", "url": "/wiki/machine-learning/conference/cvpr2019.html"},
      
      
      {"title": "2019-ICLR论文集", "text": "    Table   of   Contents          ", "tags": "machine-learning/conference", "url": "/wiki/machine-learning/conference/iclr2019.html"},
      
      
      
      {"title": "Convolutional Neural Networks", "text": "    Table   of   Contents           关于           AlexNet           结构 上 的 创新           降低 过 拟合 技巧           量化 评估                   ZFNet           卷积 网络 可视化           特征 泛化 能力           问题                   VGG   net           GoogleLeNet           ResNet           定位 与 检测           OverFeat           目标 检测   HOG           RCNN           Region   proposals           Feature   extraction           Test - time   detection           Train                   Fast   R - CNN           Faster   R - CNN           CNN 可视化                 关于       卷积 网络 经典 文章 导读 ， 文章 列表 是 参考   CS231N   课程 。       AlexNet       论文 ： Imagenet   classification   with   deep   convolutional   neural   networks     Alex   Krizhevsky ,   Ilya   Sutskever ,     Geoffrey   E   Hinton   ,   2012       Hinton   带 学生 打 比赛 的 故事 。           求解 问题 ：   ImageNet   LSVRC - 2010   比赛 ， 1.2 M 高精度 图片 ， 1000 分类 ！ ILSVRC - 2012   TOP5   error ： 15.3 % ， 第二名 是   26.2 % ！       效果 ：   TOP1   error ： 37.5 % ，   TOP2   error ： 17.0 % 。       网络 参数 ： 60M   参数 ， 650 , 000 个 神经元       重要 创新 ：   ReLU 激活 函数 ，   GPU 计算 卷积 ， dropout       5 层 卷积 层 + 3 层全 连接 层 ， 卷积 层 的 深度 是 很 关键 的 ， 移除 任何 一层 都 将 导致 性能 的 降低 ！       GTX   580   3GB   GPUs   训练   5 - 6 天             Amazon ’ s   Mechanical   Turk   crowd - sourcing   tool             对 图像 做下 采样 到 固定 大小   256x256 ， 满足 固定 大小 输入 ； 对 每个 像素 减去 在 整个 训练 集上 的 均值           结构 上 的 创新           ReLU   非线性 ： 加速 训练 ， CIFAR - 10 上 达到 25 % 错误率 ， 比 tanh 快 6 倍 ！           相关 论文 ： V .   Nair   and   G .   E .   Hinton .   Rectified   linear   units   improve   restricted   boltzmann   machines .   In   Proc .   27th   International   Conference   on   Machine   Learning ,   2010           多   GPU   训练 ： 2 个 GPU       Local   Response   Normalization ： 将 错误 减少 1 - 2 个点 。       Overlapping   Pooling ： Pooling 尺寸 = 3 ， 步长 却是 2       结构 ： 前面 五层 是 卷积 层 ， 每个 卷积 层 分为 两个 部分 ， 每个 部分 放在 一个 GPU 中 ， 在 卷积 过程 中 ， 第 2 、 4 、 5 层 的 两个 GPU 互不 干扰 ， 第 3 层 和 全 连接 层 又 相互 交错 连接 的 部分 。 maxpooling 层 在 第 1 ， 2 ， 5 层 卷积 层 ， Local   Response   Normalization   layer   在 第 1 、 2 层 。                       每 一层 的 详细 参数 ： 输入   224x224x3       96 个 11x11x3 的 滤波器 ， 分为 上下 两 部分 ， 每 部分 48 个       256 个 5x5x48 的 滤波器 ， 两个 GPU 互不 干扰       384 个 3x3x256 的 滤波器 ， 两个 GPU 有 交互       384 个 3x3x192 的 滤波器 ， 两个 GPU 互不 干扰       256 个 3x3x192 的 滤波器       全 连接 层为 4096 个 神经元               卷积 层 参数 ： 1.45 M ， 卷积 层 输出 为 6x6x256 ； 三个 全 连接 层 分别 是 ： 37.75 M ， 21.92 M ， 4.10 M ！ ！ 可以 看到 参数 主要 集中 在 卷积 层 最近 的 两个 全 连接 层 ！ ！           降低 过 拟合 技巧                   Data   Augmentation :   数据 增强 ：       平移 和 水平 翻转 ， 从 256x256 的 图片 ， 截取 224x224 的 图片 块 ， 加上 水平 翻转 ， 一张 图片 就 变成 了 32x32x2 = 2048 个 样本 ！ 预测 的 时候 ； 预测 的 时候 ， 截取 四个 角 + 中央 以及 他们 的 水平 翻转 10 张 图片 ， 结果 取 平均 ！       加噪 ， 有点像   denoise   的 概念 ， 对 每 一个 像素   $ ( I _ { xy }   =   [ I _ { xy } ^ R ,   I _ { xy } ^ G ,   I _ { xy } ^ B ] ^ T ) $ ， 不是 简单 的 在 每个 分量 上 简单 地 叠加 ， 而是 在 三个 通道 的 协方差 矩阵 的 三个 主 方向 上 ， 叠加 对应 比例 的 噪声 。 下式 中 ， p 与 lambda 分别 是 协方差 矩阵 的 三个 特征向量 和 特征值 ， $ ( \ \ alpha _ i ) $   是 叠加 的 噪声 比例 ， 服从 0 均值 方差 为 0.1 的 高斯分布 。                   $ $     [ \ \ mathbf { p } _ 1 ,   \ \ mathbf { p } _ 2 ,   \ \ mathbf { p } _ 3 ]   [ \ \ alpha _ 1   \ \ lambda _ 1 ,   \ \ alpha _ 2   \ \ lambda _ 2 ,   \ \ alpha _ 3   \ \ lambda _ 3 ] ^ T     $ $               Dropout ： 可以 看做 一种 大量 的 神经网络 的 模型 组合 。 可以 解决 过 拟合 问题 ， 学习 到鲁邦 的 特征 ， 预测 的 时候 ， 则 将 神经元 的 值 乘以 概率 即可 。   dropout   技术 大致 使得 收敛 的 迭代 次数 增加一倍 。               配置 ： NVIDIA   GTX   580   3GB   GPUs ， 两块               量化 评估       用 最后 的 4096 维 特征 作为 图像 向量 ， 评估 图像 的 相似 度 ， 效果 很 不错 ， 用   auto - encoder   于 这些 特征 上 比 在 raw   data 上 效果 应该 会 更好 。               ZFNet       论文 ： Zeiler   M   D ,   Fergus   R .   Visualizing   and   Understanding   Convolutional   Networks [ C ] .   european   conference   on   computer   vision ,   2013 :   818 - 833 .           ZFNet   在   AlexNet   上 改进 的 不 多 ， 主要 贡献 在   CNN   的 可视化 。       解释   AlexNet   为什么 效果 好 （ 主要 是 通过 可视化 分析 ） ， 以及 怎么 进一步 改进 。       数据 集 ： Caltech - 101 ， Caltech - 256 .       可视化 技术 ：   解 卷积   ， 通过 显示 激活 任意 一层 的 单一 的   feature   map   的 输入 图像 的 方法 ， 可视化 某个 神经元 学到 的 东西 。     Zeiler ,   M . ,   Taylor ,   G . ,   Fergus ,   R . :   Adaptive   deconvolutional   networks   for   mid   and   high   level   feature   learning .   In :   ICCV   ( 2011 )         敏感性 分析 ： 通过 遮蔽 输入 图片 的 一部分 ， 展示 图片 的 哪 一部分 对 分类 结果 比较 重要 。       对   AlexNet   改进 ， 并 迁移 到 其他 任务 ， 只 将 最后 一层   softmax   重新 训练 ， 有 监督 的   pre - training 。       之前 的 可视化 工作 一直 停留 在 第一层 。       通过 梯度 下降 最大化 某个 神经元 的 输出 ， 从而 找出 最优 激励 图像 （ BP   to   Image ）   Dumitru   Erhan ,   Yoshua   Bengio ,   Aaron   Courville ,   and   Pascal   Vincent ， Visualizing   higher - layer   features   of   a   deep   network ， 2009   ， 没有 解释 神经元 的 不变性 ！ ？       计算 在 最 优点 处 的   Hessian   矩阵 ， 理解 这种 不变性 ？       解 卷积 是 无 监督 学习 ， 相当于 一个 探针 ， 探测 一个 已经 学好 的 网络                       解 卷积 过程 ： 将 同 一层 的 其他 神经元 置 0 ， 将 该层 作为 解 卷积 的 输入 ， 依次 经历 了 ( i )   unpool ,   ( ii )   rectify   and   ( iii )   filter       Unpooling :   Max - pooling   不 可逆 ， 为了 解决 这个 问题 ， 在 做   Max - pooling   的 时候 ， 用 一个   switch   变量 记录 最大值 的 位置 。   问题 ， 可视化 的 时候 ， 没有 正向 卷积 过程 ， 这个   switch   变量 从 哪来 ？         Rectification ： 直接 将 重构 信号 通过   ReLU ？       Filtering ： 将 卷积 核做 水平 、 垂直 翻转 后 ， 再 进行 卷积 。 这 就 可以 解 卷积 了 ？ 不 应该 要 做 个 逆 滤波 ？               解 卷积 解释 ： 设 原始 信号 为   $ ( f ) $ ， 卷积 核为 $ ( k ) $ ， 解 卷积 核为 $ ( k ' ) $ ， 那么 经过 卷积 和解 卷积 ， 信号 变为     $ ( f   *   k   *   k ' ) $ ， 利用 卷积 运算 的 结合律 ， 也 可以 表达 为   $ (   f   *   ( k   *   k ' )   ) $ ， 如果 要 使得 解 卷积 后 的 信号     和 原始 信号 一致 ， 那么 需要   $ (   k   *   k '   =   \ \ delta   ) $ ， 即 两个 卷积 核 的 卷积 为 单位 冲击 函数 ， 也 就是     $ (   \ \ sum _ { x ' , y ' }   k ( x   -   x ' ,   y   -   y ' )   k ' ( x ' ,   y ' )   =   \ \ delta ( x ,   y ) ) $ ， 即 只有 在 $ ( x = 0 , y = 0 ) $ 时为 1 ，     其他 情况 为 0 。 这里 将 卷积 核 水平 和 垂直 翻转 后 ， 相当于   $ (   \ \ sum _ { x ' , y ' }   k ( x   -   x ' ,   y   -   y ' )   k ( - x ' ,   - y ' ) ) $     可以 看到 ， 当 x 和 y 都 为 0 时 取得 最大值 （ 达到 匹配 ） ， 其他 情况 虽然 不为 0 ， 但 小于 匹配 的 时候 的 值 ， 所以 可以 看做 逆 滤波 的 一种 近似 实现 .   不过 简单 试验 结果表明 ， 这种 近似 太 粗糙 了 。               CNN   训练 的 输入 是 [ - 128 , 128 ] ， 居然 没有 归一化 ？ ！ 初始化 是 随机 取 的 ， 幅度 为 $ ( 10 ^ { - 2 } ) $           卷积 网络 可视化                   特征 可视化 ： 选取 TOP9       结构 选择 ： 11x11 滤波器 改为 7x7 ， stride 减少 到 2 ， 从而 使得 第 1 ， 2 层 滤波器 提取 到 更 多 有用 的 信息 。 ？ ？       遮挡 敏感性 ： 测试 分类器 是否 真的 检测 到 了 图片 中 的 目标 ， 还是 只是 用 周围 的 信息 。       选取 第 5 层 最强 的   feature   map   的 响应值 之 和 ， 随着 遮挡 的 位置 的 变化 。 可视化 的 结果 如图 ( b ) 。                   特征 泛化 能力           利用   ImageNet   学 出来 的 模型 ， 应用 到 其他 任务 ， 例如 ： Caltech       只 改变 最后 一层 ， 前面 的 层 都 固定 不变 。           问题           解 卷积 可视化 具体 是 怎么样 做 的 ？           VGG   net           论文 ： VERY   DEEP   CONVOLUTIONAL   NETWORKS   FOR   LARGE - SCALE   IMAGE   RECOGNITION ,   Simonyan   and   Zisserman ,   2014   @ ICLR   2015       重要 贡献 ： 通过 非常 小 的 卷积 核   3x3 ， 提升 模型 的 深度 ！       1x1   卷积 核 的 使用 ！       卷积 的   stride   保持 为 1 ， 保证 卷积 层后 空间 分辨率 是 不变 的 。       pooling 层 保持 为   2x2   大小 的 窗 ， stride = 2 .       没有 使用   AlexNet   的   Local   Response   Normalisation   层 ！ 没有 明显 收益 ！                       用 两层 3x3 的 卷积 层 代替 一层 5x5 卷积 层 ； 3 层 3x3 的 卷积 侧 代替 一层 7x7 卷积 层 ； 这种 方法 可以 在 不 减少 卷积 核 的 覆盖范围 情况 下 ， 增加 非线性 变换 次数 并 减少 参数 ！       1x1 卷积 层 在 不 影响 空间 变换 情况 下 ， 增加 非线性 变幻 的 次数 ！ Network   in   network .       训练 参数 细节 ：       mini - batch   sgd ,   momentum   =   0.9 ,   batch   size = 256       L2   正则 参数 5e - 4       dropout   0.5 ， 最 前面 两侧 全 连接 层       学习 率 初始值 1e - 2 ， 当 验证 集 不 降低 时 除以 10       74   epoch               训练 图像 尺寸 ： CNN 输入 块 大小 是 224x224 ， 图像 被 rescale 尺寸 为 S 。 S 可以 是 固定 大小 ， 也 可以 是 多个 分辨率 。       固定 尺寸 ： 256 ， 384 （ 用 256 的 权重 初始化 网络 ）       可变 尺寸 ， 在 [ 256 , 512 ] 之间 随机 变动 S ， scale   jittering .               testing : 将 全 连接 层 变成 全 卷积 层 ： 将 最后 一层 的 通道 作为 class 通道 ， 然后 在 空间 上 平均 得到 不同 位置 的 分类 概率 的 平均值 ！ 这样 就 不用 切割 原始 图像 为 多个 块 了 ！ 只要 简答 的 rescale 为 固定 的 Q 值 即可 。       对于 固定 resclae ， 取 Q = S ， 对于 scale   jittering ， 取 Q 为 S 的 平均值 ！                                   单个 scale 的 评估 ： Q 的 取值 策略 如 上 ； 试验 表明 ， scale   jittering 帮助 提升 效果 ！       多个 scale 的 评估 ： Q 取值 策略 ， 对 固定 S 值 ， 取 S - 32 , S , S + 32 ; 对 变动 S 值 ， 取 S _ min , S _ avg , S _ max 三个 值 。 可以 看到 ， 比 单个 scale 效果 要 好 ！       多个 crop 的 评估 ： 略       卷积 网络 融合 ： 将 每 一个 网络 输出 的 多类 概率 平均 。       D 模型 参数 数目 分析 ： 138M 参数 ！ 大约 是 Alex 的 两倍 ！       参数 主要 集中 在 FC 层 ， 而 内存 消耗 主要 集中 在 前面 几层 ！           GoogleLeNet           论文 ： Going   Deeper   with   Convolutions ， CVPR2015       最大 创新 ： 增加 网络 的 深度 和 宽度 ， 但是 保持 计算 代价 不变 ！ 22 层 网络 ！       参数 数目 只有 Alexnet 的 1 / 12 .       Inception 结构 ， 借鉴 自   network   in   network .       Hebbian   principle :   neurons   that   fire   together ,   wire   together               if   the   probability   distribution   of   the   dataset   is   representable   by   a   large ,   very   sparse   deep   neural   network ,   then   the   optimal   network   topology   can   be   constructed   layer   after   layer   by   analyzing   the   correlation   statistics   of   the   pre -   ceding   layer   activations   and   clustering   neurons   with   highly   correlated   outputs               将 稀疏 矩阵 乘法 通过 聚集 后 变成   dense   matrix   乘法 ， 可以 充分利用 计算资源 。       non - uniform   deep - learning   architectures                       通过 多个 不同 的 滤波器 ， 实现 多 尺度 的 抽取 ； 通过 1x1 滤波器 实现 降维 ， 减少 大 尺寸 滤波器 计算 复杂度 ， 也 减少 了 参数 ！       22 层 ， 为了 减少 梯度 消失 效应 ， 增加 了 中间 的 输出 ， 以 期望 中间 的 特征 也 有 一定 的 区分度 ！ 提供 一种 正则 。     训练 的 时候 ， 将 这些 低层 分类 的 损失 函数 加 到 最终 损失 函数 中 ， 作为 正则 项 ！ 结果显示 ， 这种 效果 不 明显 。                               GoogLeNet   的 网络结构 参数 如表 所示 ， 其中 # 1x1 , # 3x3 , # 5x5 分别 代表 对应 的 滤波器 数目 ， 而   # 3 × 3   reduce   和   # 5x5   reduce   分别 代表   inception   中 3x3 滤波器 和 5x5 滤波器 前面 用作 降维 的 1x1 滤波器 数目 ， pool   proj 代表 inception 中 Pool 层 后面 的 1x1 滤波器 数目 ！       完全 移 除了 全 连接 层 ， 取而代之 的 是   avg   pool   层 （ top - 1 准确率 提高 了 0.6 % ） ！ 但是 保留 了 dropout ！       为了 减少 梯度 消失 的 问题 ， 在 中间 加 了 两个 输出 抽头 ！ 抽头 的 结构 如下 ：       5x5   avg   pooling ,   stride = 3 ， 分别 将 图片 降维至   4x4x512 ,   4x4x528 ！       1x1 滤波器 降维至 128 维       全 连接 层 1024 的 神经元       70 %   dropout                   训练方法 ： 用 的 是   DistBelief   分布式 训练 ！ 模型 并行 和 数据 并行 训练 。 CPU 集群           异步   SGD ，   momenton = 0.9       固定 学习 率 策略 ， 每 8 个 poch 减少 4 %       Polyak   averaging ： B .   T .   Polyak   and   A .   B .   Juditsky .   Acceleration   of   stochastic   approximation   by   averaging .   SIAM   J .   Con -   trol   Optim . ,   30 ( 4 ) : 838 – 855 ,   July   1992 .       采样 不同 尺寸 不同 位置 的 patch       photometric   distortions ： A .   G .   Howard .   Some   improvements   on   deep   con -   volutional   neural   network   based   image   classification .   CoRR ,   abs / 1312.5402 ,   2013 .                   参赛 配置 ：           没有 额外 的 训练 数据       训练 了 7 个 不同 的 版本 ， 然后 做 融合 ： 相同 的 初始 权重 ， 学习 率 ， 只 在 采用 方法 和 图片 的 随机 顺序 不同       testing 阶段 每个 图片 采样 了 不同 尺寸 不同 位置 不同 镜像 的 多个 块 进行 预测 。                   ResNet       参考 WIKI   残差 网络         定位 与 检测           简单 回归 问题 ：       将 定位 作为 一个 回归 问题 ， 输出 定位 的 坐标 和 尺寸 4 个 数字 ， 用 L2 损失 函数 ， 简单 ！       直接 从 分类 模型 最后 一层 的 feature   map 引出 一个 回归 抽头 ！               滑动 窗 ：       在 高分辨率 图片 中 的 不同 尺寸 和 不同 位置 运行   分类 + 回归   网络       融合 所有 尺寸 的 分类 + 回归 结果 作为 最终 的 输出                   OverFeat       论文 ： OverFeat : Integrated   Recognition ,   Localization   and   Detection     using   Convolutional   Networks ， Pierre   Sermanet ,   David   Eigen ,     Xiang   Zhang ,   Michael   Mathieu ,   Rob   Fergus ,     Yann   LeCun   ， 2014 .           classification ,   localization   and   detection   的 CNN 集成 框架       multiscale   和   sliding   window   技术       ILSVRC2013   目标 识别 冠军               combining   many   localization   predictions ,   detection   can   be   performed   without   training   on   background   samples   and   that   it   is   possible   to   avoid   the   time - consuming   and   complicated   bootstrapping   training   passes               在 不同 位置 和 不同 scale 使用 CNN ： 大量 的 窗 只 包含 目标 的 一部分 ， 分类 效果 好 ， 但是 定位 和 检测 效果 不好 。       每 一个 window 不但 输出 不同 类别 的 预测 概率分布 ， 还 输出 目标 相对 window 的 位置 和 大小 ！           累积 每 一个 类别 的 每 一个 window 的 预测 结果 ！               文本 检测 ： M . DelakisandC . Garcia . Textdetectionwithconvolutionalneuralnetworks . InInternationalConference   on   Computer   Vision   Theory   and   Applications   ( VISAPP   2008 ) ,   2008 .           人脸识别 ：   C .   Garcia   and   M .   Delakis .   Convolutional   face   finder :   A   neural   architecture   for   fast   and   robust   face   detection .   IEEE   Transactions   on   Pattern   Analysis   and   Machine   Intelligence ,   2004 .       人脸 检测 ： M .   Osadchy ,   Y .   LeCun ,   and   M .   Miller .   Synergistic   face   detection   and   pose   estimation   with   energy - based   models .   Journal   of   Machine   Learning   Research ,   8 : 1197 – 1215 ,   May   2007 .           行人 检测 ： P .   Sermanet ,   K .   Kavukcuoglu ,   S .   Chintala ,   and   Y .   LeCun .   Pedestrian   detection   with   unsupervised   multi -   stage   feature   learning .   In   Proc .   International   Conference   on   Computer   Vision   and   Pattern   Recognition   ( CVPR ’ 13 ) .   IEEE ,   June   2013 .               预测 的 box 和 groundtruth 的 box 至少 相交 50 % （ IOU ） 才 认为 是 对 的 。                   IOU 的 定义 ： label 框为 A ， groundtruth 框为 B ， $ ( IOU   =   \ \ frac {   area ( A   \ \ bigcap   B ) } {   area ( A   \ \ bigcup   B ) }   ) $               通过 滑动 窗 ， 产生 多个 块 ， 得到 多个 块 预测 结果 ， 然后 平均 。 滑动 窗 可以 自 底向上 计算 ， 不用 每个 滑动 窗 计算 一个 结果 ， 减少 计算 量 ！       不同 尺寸 和 位置 检测 得到 的 box 融合 成 一个 高 可信 的 box ， 实现 定位 ！           目标 检测   HOG       论文 ： Histograms   of   Oriented   Gradients   for   Human   Detection ， Navneet   Dalal   and   Bill   Triggs ， 2005 .       RCNN       论文 ： Rich   feature   hierarchies   for   accurate   object   detection   and   semantic   segmentation               Region   proposals           objectness :   B .   Alexe ,   T .   Deselaers ,   and   V .   Ferrari .   Measuring   the   objectness   of   image   windows .   TPAMI ,   2012 .         selective   search   :   J . Uijlings , K . vandeSande , T . Gevers , andA . Smeulders . Selective   search   for   object   recognition .   IJCV ,   2013 .       category - independent   object   proposals :   I .   Endres   and   D .   Hoiem .   Category   independent   object   proposals .   In   ECCV ,   2010 .       constrained   parametric   min - cuts   ( CPMC ) :   J .   Carreira   and   C .   Sminchisescu .   CPMC :   Automatic   object   segmentation   using   constrained   parametric   min - cuts .   TPAMI ,   2012 .       multi - scale   combinatorial   grouping :   P . Arbelaez , J . Pont - Tuset , J . Barron , F . Marques , andJ . Malik . Multiscale   combinatorial   grouping .   In   CVPR ,   2014 .       CNN :   D . Cires   ̧ an , A . Giusti , L . Gambardella , andJ . Schmidhuber . Mitosis   detection   in   breast   cancer   histology   images   with   deep   neural   networks .   In   MICCAI ,   2013 .           Feature   extraction           利用 一个 训练 好 的 CNN 网络 （ 如   AlexNet   网络 ） ， 对 每个 区域 提取 特征 。       将 每个 区域 补全 和 变形 到 标准 的 输入 尺寸 ，   alex   net   要求 输入 时   227x227           Test - time   detection           利用 选择性 搜索 选出 近 2000 个 候选 区域       用   CNN   提取 每 一个 区域 的 特征向量 ， 对 每 一个 类别 ， 使用 对应 的   SVM   分类器 对 特征 打分           采用 贪心 的 非 最大值 抑制 方法 （ greedy   non - maximum   suppression ，   每 一个 类 是 独立 的 ） ： 如果 一个 区域 和 另 一个 得分 更 高 的 区域   IoU   重叠 度 高于 某个 阈值 ， 那么 就 拒绝 这个 得分 低 的 区域 。 阈值 是 学习 到 的 阈值 ？               性能 对比 （ 10K 个类 被 ） ： DPM + Hashing ， 5min / image ;   RCNN ,   1min / image .   T .   Dean ,   M .   A .   Ruzon ,   M .   Segal ,   J .   Shlens ,   S .   Vijayanarasimhan ,   and   J .   Yagnik .   Fast ,   accurate   detection   of   100 , 000   object   classes   on   a   single   machine .   In   CVPR ,   2013 .               Train           将 在 ImageNet 上 训练 好 的 CNN 最后 一层 替换成 多个 SVM （ 每 一个 类别 一个 ， 背景 一个 ， SVM 参数 随机 初始化 ） ， CNN 参数 也 通过 SGD 调优       将 于 ground - truth 重叠 度 IoU 超过 50 % 的 区域 作为 该类 的 正 样本 ， 其他 的 作为 负 样本       CNN 调优 的 学习 率 降低 10 倍       每 一个 SGD 的 minbatch 中 ， 均匀 采样 32 个 正例 和 96 个 负例       hard   negative   mining   method ： 将 分值 较 高 的 负例 放到 样本 中 重新 训练       pool5 的 特征 就 很 好 的 ， 全 连接 层 可以 不要 ！           bbox   regression ： 为 每个 区域 训练 一个 回归 模型 ， 用 相同 的 特征 ， 只 改变 最后 一层 ， 预测 目标 的 相对 偏移 。               最大 的 问题 ： 慢 ， 需要 对 每 一个 区域 用 CNN 提 特征 ！               Fast   R - CNN           先用 CNN 对 整个 图片 进行 特征 抽取 （ pool5 特征 ， 有 空间 维度 的 特征 ） ， 在 选取 的 RoI 区域 ， 用 一个 RoI   Pooling 层 将 特征 尺寸 变成 固定 的 空间 尺寸 HxW ， （ 空间 尺寸 固定 了 ， 整个 特征 的 尺寸 也 固定 了 ） ， 然后 为 每 一个 区域 中 建立 分类 和 回归 模型 。       RoI 还是 通过 预先 的   region   proposal   方法 得到 ， 这个 部分 是   Fast   R - CNN   计算 的 瓶颈 。       由于 ROI 将近 2000 个 ， 计算 最后 的 全 连接 层 是 计算 瓶颈 ， 可以 通过   Truncated   SVD   优化 ， 其 效果 相当于 用 两层 线性网络 替换 。       优点 ： 只 需要 计算 一次 CNN 即可 ！       优点 ： 将 回归 和 分类 损失 函数 加 到 一起 ， 优化 一个 目标 ， multi - task   loss ， 端到 端 学习 ！       相比   R - CNN ， 训练 时间 加速 8.8 倍 ， 预测 时间 加速 146 倍 ！ 每张 图片 的 预测 时间 降低 到 0.32 s ， 之前 在 分钟 量级 ！ 效果 也 稍 好 ； 但是 加上 区域 搜索 时间 （ 大约 2s ） 后 ， 只有 25 倍 速度 提升 ， 搜索 时间 是 瓶颈 ！       关键 层 ：   RoI   pooling   layer                   Faster   R - CNN       用 CNN 做   region   proposal ， 关键技术 ：   Region   Proposal   Networks       CNN 可视化           R - CNN   计算 所有 的 区域 对 某个 神经元 激活 值 ， 按照 激活 值 从 大到 小 排序 ， 选取 TOP 区域 可视化 。                           直接 可视化 权重 ： 只能 可视化 第一层               可视化 特征 表达 ， 例如 用   t - SNE   可视化   AlexNet   最后 一层 的 4096 维 特征                 http : / / cs . stanford . edu / people / karpathy / cnnembed /                 遮挡 试验 ： 分类 概率 与 遮挡 位置 的 函数 关系 ！ ZFNet               解 卷积 方法               BP   to   Image   方法 ：       可视化 某个 神经元 的 响应 对 输入 图片 的 梯度 （ BP   to   Image ） ， 将 该层 所有 神经元 的 梯度 置 0 ， 将要 可视化 的 那个 神经元 梯度 置 1 ！     然后 运用 BP 算法 ， 求 出 梯度 。       $ $     \ \ frac { \ \ Delta   active } { \ \ Delta   I }     $ $       由于 高级 特征 具有 不变性 ， 不是 针对 某 一个 图片 的 ， 直接 解 卷积 可视化 得到 的 效果 不好 。     可以 对于 特定 的 图片 ， 用 这个 图片 做 引导 ， 通过   guided   bp   得到 条件 梯度 。                     ZF   解 卷积 方法           Deep   Inside   Convolutional   Networks :   Visualising   Image   Classification   Models   and   Saliency   Maps           寻找 图像 I 使得 在 类 c 上 的 score   $ ( S _ c ( I ) ) $   最大 ！       $ $     \ \ arg   \ \ max _ I   S _ c ( I )   -   \ \ lambda   | | I | |   _   2 ^ 2     $ $       利用   BP   算法 优化 ， 固定 权重 ， 优化 输入 ！ 输入 初始化 为 0 值 图片 ！   Sc   是 未 归一化 的 score ， 优化 归一化 的 score （ 即 概率 ） 效果 反而 不 明显 。       给定 图像 $ ( I _ 0 ) $ ， 根据 输入 像素 对 某个 类 的 score 影响 效果 排序 ， 影响 效果 通过 梯度 刻画       $ $     w   =   \ \ frac { \ \ partial   S _ c } { \ \ partial   I }   |   _   { I _ 0 }     $ $           给定 一个 图片 的 code ， 寻找 最 接近 这个 code 的 图片           $ $     x *     =   \ \ arg   \ \ min   _   x   l ( \ \ Phi ( x )   -   \ \ Phi ( x _ 0 ) )   +   \ \ lambda   R ( x )     $ $           DeepDream ： 从 一个 初始 图片 开始 ， 每次 梯度 沿着 正反馈 方向 下降   dx   =   x ! ! 这里 x 是 神经网络 某 一层 的 响应值 。 即 目标 函数 是 ， 使得 某个 已经 训练 好 的 模型 的 某 一层 ， 激活 函数 的 幅度 最大化 !     其 结果 是 ， 如果 最大化 的 是 前面 的 层 ， 那么 图片 中 会 显示 出 一些 低级 纹理 ， 如果 是 后面 的 层 ， 那么 图片 中 会 显示 出 一些 学到 的 高级 目标 ， 如狗 、 猫 的 一些 局部 ！      ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn.html"},
      
      
      {"title": "Convolutional Neural Networks for NLP", "text": "    Table   of   Contents           关于           字母 级别 的 CNN           模型           数据           设计 细节           结论                   Very   Deep   Convolutional   Networks   for   Natural   Language   Processing           结论                   CNN   句子 建模           CNN 句子 分类                 关于       长期以来 ， RNN 、 LSTM 及其 变种 模型 被 应用 到 自然语言 处理 方面 。     近年来 ， 将 CNN 应用 到 自然 处理 方面 也 有 一些 工作 。       字母 级别 的 CNN       主要 论文 ： Character - level   Convolutional   Networks   for   Text   Classification ,     Xiang   Zhang   ,   Junbo   Zhao ,     Yann   LeCun   ,   2016       目前 文本 分类 研究 已 从 设计 好 的 特征 转为 选择 好 的 分类 模型 。     目前 ， 所有 的 文本 分类 技术 都 是 以 词为 基本 单位 的 ， 简单 统计 词和词 的 n - gram 就 可以 做到 最好 的 效果 。       T .   Joachims .   Text   categorization   with   suport   vector   machines :   Learning   with   many   relevant   features .   In     Proceedings   of   the   10th   European   Conference   on   Machine   Learning ,   pages   137 – 142 .   Springer - Verlag ,     1998 .       卷积 网络 很 适合 从 raw   signals 中 提取 有用 的 特征 ， 已 在 机器 视觉 和 语音 识别 等 任务 中 得到 应用 。     而 实际上 ， time - delay   networks   早 在 深度 学习 出来 以前 就 将 卷积 网络应用 到 序列 数据 之上 。           L .   Bottou ,   F .   Fogelman   Soulie ,   P .   Blanchet ,   and   J .   Lienard .   Experiments   with   time   delay   networks   and   ´     dynamic   time   warping   for   speaker   independent   isolated   digit   recognition .   In   Proceedings   of   EuroSpeech     89 ,   volume   2 ,   pages   537 – 540 ,   Paris ,   France ,   1989 .       R .   Johnson   and     T .   Zhang   .   Effective   use   of   word   order   for   text   categorization   with   convolutional   neural     networks .   CoRR ,   abs / 1412.1058 ,   2014 .           在 这 篇文章 中 ， 将 文本 当做 字符 为 单位 的 序列 数据 ， 然后 应用 时间 卷积 网络 ( temporal   ( one - dimensional )   ConvNets ) 。       卷积 网络应用 到 文本 和 自然语言 处理 已有 一些 研究 了 ， 它 既 可以 应用 到 连续 值 的 embedding 数据 ， 也 可以 应用 到 离散 值 的 embedding 数据 ，     并不需要 任何 语法 和 语义 信息 ！ 其 结果 也 和 经典 的 方法 具有 可比性 ！           C .   dos   Santos   and   M .   Gatti .   Deep   convolutional   neural   networks   for   sentiment   analysis   of   short   texts .   In     Proceedings   of   COLING   2014 ,   the   25th   International   Conference   on   Computational   Linguistics :   Technical     Papers ,   pages   69 – 78 ,   Dublin ,   Ireland ,   August   2014 .   Dublin   City   University   and   Association   for     Computational   Linguistics .       Y .   Kim .   Convolutional   neural   networks   for   sentence   classification .   In   Proceedings   of   the   2014   Conference     on   Empirical   Methods   in   Natural   Language   Processing   ( EMNLP ) ,   pages   1746 – 1751 ,   Doha ,   Qatar ,     October   2014 .   Association   for   Computational   Linguistics .       R .   Johnson   and   T .   Zhang .   Effective   use   of   word   order   for   text   categorization   with   convolutional   neural     networks .   CoRR ,   abs / 1412.1058 ,   2014           使用 字母 级别 特征 来 做 NLP 也 有 一些 早起 工作 ， 能够 在 POS   tagging 和 IR 方面 的 提升 。 This   article   is   the   first   to   apply   ConvNets   only   on   characters .     可以 简化 特征 工程 ， 能够 学到 拼写错误 和 emoji 符号 。           character - level   n - grams   with   linear   classifiers :   I .   Kanaris ,   K .   Kanaris ,   I .   Houvardas ,   and   E .   Stamatatos .   Words   versus   character   n - grams   for   anti - spam   filtering .   International   Journal   on   Artificial   Intelligence   Tools ,   16 ( 06 ) : 1047 – 1067 ,   2007       incorporating   character - level   features   to   ConvNets :   C .   D .   Santos   and   B .   Zadrozny .   Learning   character - level   representations   for   part - of - speech   tagging .   In   Proceedings   of   the   31st   International   Conference   on   Machine   Learning   ( ICML - 14 ) ,   pages   1818 – 1826 ,   2014       Y .   Shen ,   X .   He ,   J .   Gao ,   L .   Deng ,   and   G .   Mesnil .   A   latent   semantic   model   with   convolutional - pooling   structure   for   information   retrieval .   In   Proceedings   of   the   23rd   ACM   International   Conference   on   Conference   on   Information   and   Knowledge   Management ,   pages   101 – 110 .   ACM ,   2014 .           模型       采用 一位 的 卷积 和 max - pooling ！           对   Pooling   的 分析 文章 ： Y . - L .   Boureau ,   J .   Ponce ,   and     Y .   LeCun   .   A   theoretical   analysis   of   feature   pooling   in   visual   recognition .     In   Proceedings   of   the   27th   International   Conference   on   Machine   Learning   ( ICML - 10 ) ,   pages   111 – 118 ,   2010       ReLU   最早 文章 ： V .   Nair   and     G .   E .   Hinton   .   Rectified   linear   units   improve   restricted   boltzmann   machines .   In   Proceedings     of   the   27th   International   Conference   on   Machine   Learning   ( ICML - 10 ) ,   pages   807 – 814 ,   2010           数据       字母 通过 one - hot 编码 为 70 维 向量 ， 包括 26 个 字母 ， 10 个 数字 ， 33 个 其他 字符 和 换行符 。 字符 包括 ：             abcdefghijklmnopqrstuvwxyz0123456789   - , ; . ! ? : ’ ’ ’ / \ \ | _ @ # $ % ˆ & amp ; * ˜ ‘ + - = & lt ; & gt ; ( ) [ ] { }               对 中文 的 处理 ， 将 中文 转换 为 拼音   pypingyin 。       设计 细节       模型 设计       6 层 卷积 层 + 3 层全 连接 层 ， kernel 维度 为 7 和 3 ， pool 维度 为 3 。       数据 增强       用 近义词 进行 替换 ， 增加 样本 ！ 平移 不 适应 于 这里 ！       和 传统 方法 比较 ：           Bag   of   word   with   TFIDF       Bag   of   n - gram   with   TFIDF       Bag   of   means   with   word   embedding           深度 学习 方法 比较 ：           word - based   CNN :       LSTM :           结论       数据 集 达到 百万 量级 才能 观察 到 这种 方法 的 优势 ， 数据 集小 的 时候 还是   n - gram   with   TFIDF   好             There   is   no   free   lunch   .   Our   experiments   once   again   verifies   that   there   is   not   a   single   machine     learning   model   that   can   work   for   all   kinds   of   datasets .   The   factors   discussed   in   this   section   could   all     play   a   role   in   deciding   which   method   is   the   best   for   some   specific   application .           Very   Deep   Convolutional   Networks   for   Natural   Language   Processing       论文 ：   Very   Deep   Convolutional   Networks   for   Natural   Language   Processing ,     Le   Cun   ,   2016       目前 RNN , LSTM , CNN 应用 到 NLP 中 的 深度 和 在 CV 中 相比 ， 还 比较 浅 ， 这 篇文章 提出 一种 方案 可以 从 字母 级别 开始 学习 ， 模型 深度 达到 29 层 ！       CNN ： 将 特征提取 和 分类 进行 联合 训练 ！ 除了 自动 特征提取 之外 ， 还 可以 根据 具体任务 调整 特征 ！       目前 主流 的 方法 ， 是 利用   word   embedding   +   RNN ( LSTM ) 。           Martin   Sundermeyer ,   Ralf   Schl ü ter ,   and   Hermann   Ney .   LSTM   neural   networks   for   language     modeling .   In   Interspeech ,   2012 .       Ilya   Sutskever ,   Oriol   Vinyals ,   and   Quoc   V .   Le .   Sequence   to   sequence   learning   with   neural     networks .   In   NIPS ,   pages   3104 – 3112 ,   2014 .           作者 argue ：           作者 认为   LSTM   是 一种 一般 的 序列 学习 方法 ， 缺乏 领域 特性   \" lacking   task   specific   structure \"       单词 按照 顺序 进入 ， 第一个 单词 变换 了 很 多次 ， 而 最后 一个 词 只 变换 一次 ！   = & gt ;   bidirectional   LSTM       深度 不够 ， 超过 4 层 就 没 啥 提升 了 ， 尽管 加入 了   dropout   正则 化 ！           观点 ：           We   believe   that     the   challenge   in   NLP     is   to   develop   deep   architectures   which   are   able   to   learn   hierarchical     representations   of   whole   sentences ,   jointly   with   the   task .           recursive   neural   network   :   在 RNN 上 增加 了 序列 融合 的 顺序 结构 （ 树结构 ） ， RNN 可以 看做 一个 特殊 的   recursive   NN .           Richard   Socher ,   Jeffrey   Pennington ,   Eric   H .   Huang ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .     Semi - supervised   recursive   autoencoders   for   predicting   sentiment   distributions .   In     EMNLP ,   2011 .           模型 结构 ：               s   是 时间 窗 长度 ， 首先 将 字符 embedding 到 16 维 的 向量 ！     第一层 64 个 特征 ， 后续 是 ConvNet   Block ， 采用 下述 策略 （ from   VGG   and   ResNet ） ：           如果 时间 分辨率 不变 ， 输入 和 输出 特征 维度 相同       如果 时间 分辨率 减半 ， 输出 特征 维度 加倍           更 多 的 卷积 层 ， 意味着 能够 学习 更长 的 依赖 关系 ！ 并且 ， 对 所有 的 时间 几乎 是 平等 的 ！ 而 不 像 RNN ， LSTM 那样 ！     其中 一个 CNN   Block   结构 如图 Fig . 2 。 包含 两个 维度 为 3 的 核 的 卷积 层 ， 每个 卷积 层后 跟 一个 BN 层 和 一个 非线性 层 ！     多个 小 尺寸 的 卷积 层 可以 用 较 少 的 参数 实现 一个 大 尺寸 的 卷积 层 相同 的 功能 （ 视野 和 非线性 度 ） ！               输入 字符 增加 了 一个 表示 未知 符号 的 特殊字符 ， 一共 72 个 token 。 输入 文本 padding 到 长度 为 1014 ！ 字符 embedding 到 16 维 的 向量 。     其他 参数 ：           mini - batch   of   size   128       initial   learning   rate   of   0.01       momentum   of   0.9       每次 验证 集 错误 增加 就 将 学习 率 减半       初始化 采用   何凯明   的 方案       采用   BN   而 没有 dropout           结论           在 大 数据 集上 有 明显 提升 ， 即使 深度 较 小       深度 可以 提升 效果 ！       Max - pooling   最优       degradation ： 增加 深度 ， 性能 下降 ， 通过 shortcut 减少 这种 效果 。               Exploring   the   impact   of   the   depth   of   temporal     convolutional   models   on   categorization   tasks   with   hundreds   or   thousands   of   classes   would   be   an     interesting   challenge   and   is   left   for   future   research .           CNN   句子 建模       A   Convolutional   Neural   Network   for   Modelling   Sentences ， 2014 .       Dynamic   Convolutional   Neural   Network   ( DCNN ) :   采用 Dynamic   k - Max   Pooling ， 即 pooling 的 时候 ， 选取 最大 的 k 个值 ， 而 不是 一个 最大值 。     采用 多个 滤波器 ， 提取 多个 特征 。           Neural   Bag - of - Words   ( NBoW )   models ：       投影 层 ： 将   word ,   sub - word ,   n - gram   映射 到 高维   embedding   向量 。       组合 ： 将 这些 向量 组合 （ 求和 ， 均值 ， 加权 和 等 ）       将 组合 后 的 向量 作为 句子 的 特征 表达 ， 传入 全 连接 神经网络 进行 监督 学习 。               Recursive   Neural   Network   ( RecNN ） ： 利用 一个 额外 的   parse   tree 。       递归 地 组合 叶子 节点       将 根 节点 的 向量 作为 句子 的 特征 表达 ， 传入 全 连接 神经网络 进行 监督 学习 。               RNN ： 最为 RecNN   的 一个 特例 ， 即 线性 地 组合 相邻 的 向量 ， 把 最后 节点 对应 的 向量 作为 句子 的 特征向量 。       TDNN ： Time   delay ， 利用 卷积 。           一维 卷积 ， 即 只 对 时间 维度 进行 卷积 。   TDNN 对 时间 维度 是 卷积 ， 对 另 一个 维度 （ 每个 词 都 对应 一个 向量 ） 则 是 全 连接 ， 并且 采用 窄 版 的 卷积 。     Max - TDNN 解决 可变 长度 问题 ： 每 一个 滤波器 ， 最终 只 得到 一个 特征 ， 即 对 卷积 后 的 序列 只取 一个 最大值 。 最终 有 几个 卷积 核 ， 特征 的 维度 就是 几 。     最后 得到 的 特征 传入 全 连接 神经网络 进行 监督 学习 。 整个 过程 是 联合 优化 的 。           特点 ： 对 词序 不 敏感 ； 不 需要 额外 的 语言 特征 （ dependency   tree ， parse   tree ) 。       缺点 ： 首先 于 卷积 的 长度 ， 相当于 只 考虑   m - gram   的 词 特征 ， 长距离 的 词 关联 无法 学到 。 一个   max - pooling   导致 的 问题 ： 抹 去 了 顺序 的 信息 。           解决 的 办法 ： Dynamic   k - max   pooling       最后 一层 的 k 是 固定 的 ， 保证 最后 一层 的 输出 特征 数目 是 定长 的 ， 但是 中间 的 k 是 动态 的 。       $ $     k _ l   =   \ \ max   ( k _ { top } ,   \ \ ceil { \ \ frac { L   -   l } { L }   s } )     $ $       L   是 所有 的 卷积 层 数目 ， l 是 当前 层 的 序号 ， s 是 句子 长度 ， 实际上 是 为了 使得 pooling 是 平滑 的 线性 降低 维度 。       CNN 句子 分类       Convolutional   Neural   Networks   for   Sentence   Classification ， Kim   2014 .           要点 ：       双通道 ， 一个 词 向量 通道 可变 ， 用于 学习 与 目标 有关 的 词 向量 （ 情感 相关 ） ， 另 一个 通道 不可 变 ， 防止 过 拟合 。     这个 很 关键 ， 可以 参看 论文 ， 解决   word2vec   反义词 的 问题 。       时间 卷积 ， 在 另 一个 维度 求和       max - over - time   pooling       dropout              ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn-for-nlp.html"},
      
      
      {"title": "CS224d: Deep Learning for Natural Language Processing", "text": "    Table   of   Contents           关于           一些 疑惑           word2vec           负 采样 近似           skip - gram 模型           CBOW 模型                   问题           Multitask   learning           神经网络 TIPS           Language   Models           神经网络 语言 模型   Bengio   2003           递归 神经网络           实现 细节           bidirectionl   RNNS   双向 RNN                   Deep - learning   package   zoom           RNN   机器翻译           传统 统计 机器翻译 ：           RNN   模型           GRU           LSTM           更 多 的 门   GRUs                   Project           NLP   benchmark   tasks           tasks           models                   Reference           NLP   综述           POS   tagging ： 数据 集 ： Wall   Street   Journal   ( WSJ )   data           Chunking           Named   Entity   Recognition           Semantic   Role   Labeling           神经网络 方法           Transforming   Words   into   Feature   Vectors           Extracting   Higher   Level   Features   from   Word   Feature   Vectors                   训练           Word - Level   Log - Likelihood           Sentence - Level   Log - Likelihood           Stochastic   Gradient                   结果           更 多 的 未 标注 数据           Training   Language   Models                   Sequence   to   Sequence   Learning   with   Neural   Networks           模型           实验                   RNN   for   QA           Matching   Text   to   Entities :   Quiz   Bowl                   Image   retrieval   by   Sentences           Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions           RNN 语言 模型           优化 技巧                   A   Neural   Probabilistic   Language   Model           EXTENSIONS   OF   RECURRENT   NEURAL   NETWORK   LANGUAGE   MODEL           Opinion   Mining   with   Deep   Recurrent   Neural   Networks           Gated   Feedback   Recurrent   Neural   Networks           Recursive   Deep   Models   for   Semantic   Compositionality   Over   a   Sentiment   Treebank                         关于       cs224d 这门 课 是 将 深度 学习 应用 到 自然语言 处理 上面 的 课程 ， 十分 推荐 。       一些 疑惑         https : / / www . quora . com / How - is - GloVe - different - from - word2vec       对于 word2vec 与 GloVe 的 比较 的 见解 。       word2vec           单词 的 表达 ：   Word - Net ，   ONE - HOT       文档 - 单词   共生 矩阵 ，   SVD 提取 ，   LSA       潜在 问题 ： SVD 计算 复杂度 高当 词典 或者 文档 数目 很大 时 ， 对 新词 和 新 的 文档 难以 处理 ， 与 其他 DL 不同 的 学习 体制 。               直接 学习 低维词 向量 ： word2vect       Learning     representa4ons     by     back - propaga4ng   errors .     Rumelhart     et     al . ,         1986       A   neural     probabilis4c         language         model       ( Bengio   et     al . ,         2003 )       NLP       ( almost )         from         Scratch   ( Collobert     & amp ;       Weston ,   2008 )       A   recent ,   even         simpler   and   faster     model :     word2vec         ( Mikolov         et     al .   2013 )       à       intro       now               不是 直接 统计 共同 发生 的 次数 ， 而是 预测 每 一个 单词 周围 的 单词 ； 速度 快 ， 易于 应用 到 新词 和 新 文档       目标 函数     $ $     J ( \ \ theta )   =   \ \ frac { 1 } { T }   \ \ sum _ { t = 1 } ^ T     \ \ sum _ { - m   \ \ le   j   \ \ le   m ,   j   \ \ neq   0 }   \ \ log   p ( w _ { t + j }   |   w _ t )     $ $     其中 条件 概率 采用 如下 指数 形式     $ $     p ( o | c )   =   \ \ frac { \ \ exp ( u _ o ^ T   v _ c ) } { \ \ sum _ { w = 1 } ^ W   \ \ exp ( u _ w ^ T   v _ c ) }     $ $       每 一个 单词 有 两个 向量 $ ( u ,   v ) $ .   最终 的 词 向量 是   $ ( u + v ) $ ?       词 向量 的 线性关系       $ (   X _ { apple }   -   X _ { apples }   \ \ approx   X _ { car }   -   X _ { cars }   \ \ approx   X _ { family }   -   X _ { families } ) $                   负 采样 近似       单个 输入 词 向量 与 单个 输出 词 向量 的 损失 函数     $ $     J ( u _ o ,   v _ c ,   U )   =   -   \ \ log ( \ \ sigma ( u _ o ^ T   v _ c ) )   -   \ \ sum _ { k   \ \ sim   P }   \ \ log ( \ \ sigma ( -   u _ k ^ T   v _ c ) ) .     $ $     其中 求和 是 对 总体 的 一个 采样 。 实际上 ， 负 采样 方法 相当于 将 极大 似然 估计 的 问题 ， 转化 为 多个 二 分类 问题 。     正例 是 两个 词 出现 在 同一个 上下文 ， 负例 是 两个 词 没有 出现 在 同一个 上下文 ！     且 正例 的 概率 为 ：       $ $     \ \ sigma ( u _ o ^ T   v _ c )     $ $       skip - gram 模型       设由 $ ( w _ c ) $ 预测 $ ( w _ o ) $ 的 单个 损失 函数 为 $ ( F ( w _ o ,   w _ c ) ) $ ， 那么 skip - gram 模型 可以 表示 为     由 中心 单词 预测 周围 的 单词 ， 损失 函数 为     $ $     J   =   \ \ sum _ { - m   \ \ le   j   \ \ le ,   j   \ \ neq   0 }   F ( w _ { c + j } ,   v _ c ) .     $ $       CBOW 模型       CBOW 模型 使用 周围 单词 的 词 向量 之 和 来 预测 中心 单词 $ w _ c $ 。     $ $     \ \ hat { v }   =   \ \ sum _ { - m   \ \ le   j   \ \ le ,   j   \ \ neq   0 }   v _ { c + j }     $ $     他 的 损失 函数 为     $ $     J   =   F ( w _ c ,   \ \ hat { v } )     $ $           WHY ？       一般而言 ， 这种 方式 上 的 区别 使得 CBOW 模型 更 适合 应用 在 小规模 的 数据 集上 ， 能够 对 很多 的 分布式 信息 进行 平滑 处理 ； 而 Skip - Gram 模型 则 比较 适合 用于 大规模 的 数据 集上 。               问题           为什么 每 一次 SGD 后 需要 对 参数 向量 进行 标准化 ？       一般 的 交叉 熵 能够 理解 为 最大 似然 估计 么 ？           Multitask   learning       共享 网络 前 几层 的 权值 ， 只 针对 不同 任务 改变 最后 一层 的 权值 。     总 的 代价 函数 是 各 代价 函数 （ 如 交叉 熵 ） 之 和 。       神经网络 TIPS           对词 向量 的 监督 训练 的 重新 调整 ， 对 任务 也 有 提升 。   C & amp ; W   2011           非线性 函数           sigmoid       tanh   ：   对 很多 任务 ， 比 sigmoid 好 ， 初始值 接近 0 ， 更快 的 收敛 ， 与 sigmoid 一样 容易 求导       hard   tanh   :   - 1 ,   if   & lt ;   - 1 ;   x ,   if   - 1   & lt ; =   x   & lt ; =   1 ;   1 ,   if   x   & gt ;   1 .       softsign ( z )   =   z / ( 1   +   | z | )       rect ( z )   =   max ( 0 ,   z )     ref :   Glorot   and   Bengio ,   AISTATS   2011                   MaxOut   network   ( Goodfellow   et   al .   2013 )           梯度 下降 优化 建议 ， 大 数据 集 采用 SGD 和 mini - batch   SGD ， 小 数据 集 采用 L - BFGS 或者 CG 。         大 数据 集 L - BFGS   Le   et     al .   ICML         2011 。       SGD 的 提升 ， 动量     $ $     v   =   \ \ mu   v   -   \ \ alpha   \ \ nabla _ { \ \ theta }   J _ t ( \ \ theta )       \ \ \ \     \ \ theta ^ { new }   =   \ \ theta ^ { old }   +   v     $ $       学习 率 ： adagrad ，   adam       防止 过 拟合 ：       减少 模型 大小 ， 隐藏 节点 数目 等       L1   or   L2 正则 化       提前 停止 ， 选择 在 验证 集合 上 最好 的 结果       隐藏 节点 的 稀疏 约束 ， 参考 UFLDL 教程     $ $     KL ( 1 / N   \ \ sum _ { n = 1 } ^ N   a _ i ^ { ( n ) | 0.001 } )     $ $       dropout ， 输入 以 一定 概率 随机 置 0       denoise               超 参数 的 搜索 ： 随即 搜索 。     Y .     Bengio     ( 2012 ) ,   “ Practical     Recommendations   for   GradientBased     Training         of     Deep         Architectures ”                   Xavier   initialization   初始化 策略           Language   Models       所谓 语言 模型 就是 建立 单词 的 联合 概率模型 $ ( P ( w _ 1 , ... , w _ T ) ) $ .       神经网络 语言 模型   Bengio   2003       一个 直接 连接 部分 和 一个 非线性 变换 部分 。 输入 为 前 n 个 词 的 词 向量     $ $     y   =   b   +   Wx   +   U   tanh ( d   +   Hx )   .     \ \ \ \     P ( w _ t | w _ { t - 1 } , ... , w _ { t - n + 1 } )   =   \ \ frac { e ^ { y _ { w _ t } } } { \ \ sum _ i   e ^ { y _ i } } .     $ $     缺点 是 窗口 是 固定 的 。 记忆 能力 有限 ？       递归 神经网络       基于 之前 见到 的 所有 单词 （ 理论 上 有 无限 长 的 时间 窗 ）           Condition   the   neural     network   on     all   previous     words   and   tie   the   weights   at     each         time         step           设词 向量 列表 为   $ ( x _ 1 ,   x _ 2 ,   ... ,   x _ t ,   ... ,   x _ T ) $ 。 L 矩阵 中 的 列 向量 。     $ $     h _ t   =   \ \ sigma ( W ^ { ( hh )   h _ { t - 1 } }   +   W ^ { hx }   x _ { t } ) .   \ \ \ \     \ \ hat { y } _ t   =   softmax ( W ^ { ( S ) }   h _ t ) .   \ \ \ \     P ( x _ { t + 1 } = v _ j | x _ t ,   ... ,   x _ 1 )   =   \ \ hat { y } _ { t ,   j } .     $ $     所有 时刻 的 权值 都 是 相同 的 。 损失 函数 为 所有 时刻 交叉 熵 的 平均值     $ $     J ^ { ( t ) } ( \ \ theta )   =   - \ \ sum _ { j = 1 } ^ { | V | }   y _ { t , j }   \ \ log   \ \ hat { y } _ { t , j } .   \ \ \ \     J   =   -   \ \ frac { 1 } { T }   \ \ sum _ t   J ^ { ( t ) }     $ $     Perplexity   ? ? ?           训练 困难 ， 梯度 容易 衰减 或者 很大 。 Bengio   et   al     1994           初始化 策略           $ ( W ^ { ( hh ) } ) $   初始化 为 单位 阵       非线性 函数 用 rect 函数 替换       Parsing       with         Compositional     Vector         Grammars ,       Socher     et     al .   2013       A   Simple     Way   to     Initialize     Recurrent       Networks         of     Rectified       Linear     Units ,         Le     et     al .   2015       On         the   difficulty       of   training         Recurrent       Neural     Networks ,       Pascanu   et     al .   2013                               梯度 消减   Mikolov ， 如果 梯度 的 范数 超过 阈值 ， 就 将 梯度 归一化 到 范数 等于 该 阈值 的 向量 或 矩阵 。               补充 对 RNN 求 梯度 的 理论 推导               利用 RNN 学到 的 语言 模型 ， 生成 词               实现 细节           dropout 正则 化 ， 在 TensorFlow 里面 ， 可以 使用   tf . nn . dropout   来 实现 。           bidirectionl   RNNS   双向 RNN       每 一个 隐层 存在 两个 变量 ， $ ( h ^ L ,   h ^ R ) $ 。       $ $     h _ t ^ R   =   f ( W ^ R   x _ t   +   V ^ R   h _ { t - 1 } ^ R   +   b ^ R )   \ \ \ \     h _ t ^ L   =   f ( W ^ L   x _ t   +   V ^ L   h _ { t + 1 } ^ L   +   b ^ L )   \ \ \ \     y _ t   =   g ( U   [ h _ t ^ R ;   h _ t ^ L ]   +   c )     $ $       问题 ： $ ( h _ { t + 1 } ^ L ) $ 的 值 怎么 来 ？       数据 集   MPQA         1.2   corpus       Deep - learning   package   zoom           Torch       Caffe       Theano ( Keras ,   Lasagne )       CuDNN       Tensorflow       Mxnet           RNN   机器翻译       传统 统计 机器翻译 ：       参考 CS224n           翻译 模型   p ( f | e )   和   语言 模型   f ( e ) ， 然后 得到 目标 语   Decoder   :   $ ( argmax _ e   p ( f | e )   p ( e ) ) $       alignment           RNN   模型               最 简单 的 encoder   +   decoder   模型 ：           Encoder ： 利用 RNN 将 句子 变成 一个 向量     $ ( h _ t   =   f ( W ^ { hh }   h _ { t - 1 }   +   W ^ { hx }   x _ t ) ) $       Decoder ： 将 句子 向量 变成 一句 话     $ ( h _ t   =   W ^ { hh }   h _ { t - 1 } ,   y _ t   =   softmax ( W ^ S   h _ t ) ) $       损失 函数 ： 最小化 所有 输出 结果 的 交叉 熵 ，   $ ( \ \ max _ { \ \ theta }   \ \ frac { 1 } { N }   \ \ sum _ { n = 1 } ^ N   \ \ log   p _ { \ \ theta }   ( y ^ { ( n ) } |   x ^ { n } ) ) $     用 句子 向量 作为 中间 桥梁 。                   对 上述 模型 的 改进 措施           对 Encoder 和 Decoder 训练 不同 的 权值       对 Decoder ， 采用 三个 变量 计算 隐层 ， 上 一个 时间 的 隐层 $ ( h _ { t - 1 } ) $ ， Encoder 最后 的 状态 c ， 上 一个 输出 结果 $ ( y _ { t - 1 } ) $ ： $ ( h _ t   =   \ \ phi ( h _ { t - 1 } ,   c ,   y _ { t - 1 } ) ) $       多层 网络 训练       双向 RNN   Encoder       反过来 训练 ？       门限 递归 单元 GRU                   论文 ： 2014 年 ， Kyunghyun   Cho ,   Yoshua   Bengio ,   Learning   Phrase   Representations   using   RNN   Encoder - Decoder   for   Statistical   Machine   Translation       GRU       门限 RNN 单元           更新 门 $ ( z _ t ) $ ， 基于 当前 输入 和 上 一 时刻 隐层 状态       重置 门 $ ( r _ t ) $ ， 如果 重置 门 接近 于 0 ， 那么 当前 隐层 状态 将 忘记 之前 的 隐层 状态 ， 只 依赖 当前 输入       新 的 隐层 $ ( \ \ widetilde { h } _ t ) $ ， 另外 一个 的 隐层 状态 ， 最终 的 隐层 状态 是 基于 更新 门 组合 这个 新隐层 和 上 一 时刻 的 隐层           $ $     z _ t   =   \ \ sigma ( W ^ { ( z ) }   x _ t   +   U ^ { ( z ) }   h _ { t - 1 } )     \ \ \ \     r _ t   =   \ \ sigma ( W ^ { ( r ) }   x _ t   +   U ^ { ( r ) }   h _ { t - 1 } )     \ \ \ \     \ \ widetilde { h } _ t   =   tanh ( W   x _ t   +   r _ t   \ \ circ   U   h _ { t - 1 } )         \ \ \ \     h _ t   =   z _ t   \ \ circ   h _ { t - 1 }   +   ( 1   -   z _ t )   \ \ circ   \ \ widetilde { h } _ t     $ $       当 重置 门 接近 0 ， 允许 模型 忘记 历史 ， 实现 短期 依赖 。     当 更新 门 接近 1 ， 简单 复制 上 一 时刻 的 隐层 ， 导致 更少 的 vanishing   gradients ， 实现 长期 依赖 。       LSTM       跟 多 的 门 ， 每 一个 门 都 是 当前 输入 和 上 一 时刻 的 隐层 的 函数 ， 只是 权值 不同 。           输入 门   $ ( i _ t ) $       忘记 门   $ ( f _ t ) $           输出 门   $ ( o _ t ) $               新 的 存储单元 ： $ ( \ \ widetilde { c } _ t   =   tanh ( w ^ c   x _ t   +   U ^ c   h _ { t - 1 } ) ) $           最终 的 存储单元 ： $ ( c _ t   =   f _ t   \ \ circ   c _ { t - 1 }   +   i _ t   \ \ circ   \ \ widetilde { c } _ t ) $       新 的 隐层 ： $ ( h _ t   =   tanh ( c _ t ) ) $           存储单元 可以 保存 输入 信息 ， 除非 输入 让 它 忘记 或者 重写 它 ； 它 可以 决定 是否 输出 信息 或者 只是 简单 地 保存信息 。       论文 ： 2014 ,   Sutskever ,   Sequence   to   Sequence   Learning   with   Neural   Networks ,   Google   inc       比赛 ： WMT     2016   competition       更 多 的 门   GRUs       Gated   Feedback   Recurrent   Neural   Networks ,   Chung   et     al ,   Bengio .   2015       更 多 的 门 来 控制 多个 隐层 之间 互相 连接 。       Project           利用 deeplearning 去 解决 kaggle 上 的 NLP 问题 。           NLP   benchmark   tasks       tasks           Part - Of - Speech   tagging       chunking       Named   Entity   Recognition   ( NER )       Semantic   Role   Labeling   ( SRL )           models               CRF   conditional   random   field               词语 分布式 假说 ： 词 的 上下文 相似 ， 那么 这 两个 词 也 相似               Reference             http : / / cs224d . stanford . edu / syllabus . html         An       Improved         Model       of     Seman4c   Similarity     Based       on     Lexical   Co - Occurrence   Rohde   et     al .   2005           NLP   综述       论文 ：   Natural   Language   Processing   ( almost )   from   Scratch       文章 提出 一种 统一 的 神经网络 结构 ， 可以 用 在 很多 自然语言 处理 任务 当中 ： POS   tagging ， chunking ， NER ， semantic   role   labeling 。     这种 方案 可以 不用 针对 特定 任务 进行 特征 工程 和 先验 知识 。       POS   tagging ： 数据 集 ： Wall   Street   Journal   ( WSJ )   data           Toutanova   et   al .   ( 2003 ）   最大 熵   +   bidirectional   dependency   network   = & gt ;   97.24 %       Gim   ́ enez   and   Marquez   ( 2004 )   SVM   +   双向 维特 比 译码   = & gt ;   97.16 %       Shen   et   al .   ( 2007 )   双向 序列 分类   = & gt ;   97.33 %           Chunking       句法 成分 标记           Kudoh   and   Matsumoto   ( 2000 ) ：   93.48 %       ( Sha   and   Pereira ,   2003 ;   McDonald   et   al . ,   2005 ;   Sun   et   al . ,   2008 ) ： random   fields           Named   Entity   Recognition           Florian   et   al .   ( 2003 ) ：   F1   = & gt ;   88.76 %       Ando   and   Zhang   ( 2005 ) :   89.31 %           Semantic   Role   Labeling       [ John ] ARG0   [ ate ] REL   [ the   apple ] ARG1       预测 关系 词 ？       神经网络 方法           将 词 映射 到 特征向量       在 一个 窗口 中 ， 将 词 对应 的 特征向量 拼接 成 一个 大 的 向量 ， 作为 下 一层 的 输入       一个 正常 的 神经网络 ， 线性 层 + 非线性 层   的 多次 堆叠 ！           Transforming   Words   into   Feature   Vectors       将 词   embedding   到 一个 低维 的 词 向量 ， 也 可以 理解 为 一个 查找 表   W ， 输入 词 的 索引   idx ， 输出 W ( : ,   idx ) ，     而 参数   W   通过 BP 算法 学习 ！       如果 加入 其它 离散 特征 ， 把 每 一个 特征 做 同样 的   embedding   操作 ， 每 一个 特征 都 有 一个 查找 表   $ ( W _ k ) $ ， 这些 参数 都 要 通过 后续 学习 。     最终 输出 的 特征向量 是 这些 离散 特征   embedding   后 的 特征 拼接 而成 ！       经过 这 一层 后 ， 每 一个 词 输出 为 一个   $ ( d _ { wrd } ) $   维 的 向量 。       Extracting   Higher   Level   Features   from   Word   Feature   Vectors           Window   Approach ：   加窗 ， 只 使用 词 的 邻居 词 ， 将 窗内 的 词 对应 的 向量 拼接 起来 ， 成为 一个 大 的 向量 ， 设 窗口 为   $ ( k _ { sz } ) $ ，     那么 ， 加窗 后 的 向量 长度 为   $ ( d _ { wrd }   \ \ times   k _ { sz } ) $ 。 将 这个 固定 长度 的 向量 输入 到 一个 正常 的 多层 全 连接 神经网络 。           边界 效应 ： 对于 句子 起始 和 结束 的 词 ， 在 前后 补充 半个 窗口 的 特殊 词 “ PADDING ” ， 这个 词 对应 的 词 向量 也 是 通过 学习 得到 的 。 等价 于 学习 序列 的 开始 和 结束 ！           Sentence   Approach ： 利用 时间 一维 卷积 层   Waibel   et   al .   ( 1989 )   ， Time   Delay   Neural   Networks   ( TDNNs ) 。     卷积 层 可以 学习 局部 特征           卷积 层 ， Max   pooling 层 。 最后 得到 的 固定 长度 的 特征向量 进入 一个 标准 的 全 连接 神经网络 。     边界 通过 相同 的   PADDING   方法 解决 不同 长度 的 问题 ！       训练       Word - Level   Log - Likelihood       每 一个 词是 独立 的 ， 最大化 极大 似然 函数 等价 于 最小化 交叉 熵 损失 函数 。       Sentence - Level   Log - Likelihood       词 的 标注 之间 是 不 独立 的 ！ 从 一个 TAG 到 另 一个 TAG 可能 是 不 允许 的 。     设 TAG 之间 转移 用 score 矩阵   A   表示 ， 最终 对 一个 句子 $ ( [ x ] _ 1 ^ T ) $   标注 序列 $ ( [ i ] _ 1 ^ T ) $   的 score 是     标注 序列 转移 score 和 神经网络 输出 的 score 之 和 ！       $ $     s ( [ x ] _ 1 ^ T ,   [ i ] _ 1 ^ T ,   \ \ hat { \ \ theta } )   =   \ \ sum _ { t = 1 } ^ T   ( [ A ] _ { [ i ] _ { t - 1 } ,   [ i ] _ t }   +   [ f _ { \ \ theta } ] _ { [ i ] _ t , t } )   \ \ \ \     \ \ hat { \ \ theta }   =   \ \ theta   \ \ union   { A _ { ij } ,   \ \ forall   i , j   }     $ $       最后 输出 的 是 score 对 所有 路径 的 softmax 归一化 后 的 值 ， 解释 为 路径 的 条件 概率 。     因为 路径 数目 随 句子 长度 字数 增长 ， 所以 分母 上 的 求和 项 也 有 指数 个 。     幸运 的 是 ， 可以 在 线性 时间 复杂度 内 求得 。       优化 算法 ： 动态 规划 ， Viterbi   algorithm   ！           其他 方法 ：       Graph   Transformer   Networks       Conditional   Random   Fields                   Stochastic   Gradient       stochastic   gradient   ( Bottou ,   1991 )       目标 函数 的 可 微性 ， 因为   max   层 的 引入 ， 导致 在 某些 点 不可 微 ， 但是 随机 梯度 下降 仍然 可以 找到 极小值 点 ！       结果       很 不幸 ， 神经网络 的 结果 都 不如   baseline ！ 无 监督 学习 加入 可能 有用 ？ ！       更 多 的 未 标注 数据       利用 更 多 未 标注 数据 ， 学习 词 向量 ， 然后 初始化   embedding   权重 ！               数据 集 ：           English   Wikipedia       Reuters   RCV1   ( Lewis   et   al . ,   2004 )   dataset                   排序 准则 和 熵 准则               Training   Language   Models       训练 一系列 词典 增大 的 神经网络 ， 每 一个 网络 用 之前 的 网络 初始化   embedding   层 ！ ( Bengio   et   al . ,   2009 )       Sequence   to   Sequence   Learning   with   Neural   Networks       序列 到 序列 学习 ！ 经典 文献 。 来自   Google 。       DNN   只能 对 固定 长度 的 输入 ， 进行 建模 ， 但是 很多 时候 需要 实现 序列 到 序列 的 学习 ： 语音 识别 ， 机器翻译 ！       序列 学习 的 方法 ： 用 一个 RNN （ 通常 是 LSTM ） 将 一个 序列 编码 成 一个 大 的 固定 长度 的 向量 （ Encoder ） ，     然后 再用 一个 RNN 将 该 向量 解码 成 一个 新 的 序列 （ Decoder ） 。     译码 方案 ： Beam - search   decoder 。       评估 指标 ： BLEU ？       一个   trick ：   将 序列 （ 句子 ） 反序 后 ， 加入 训练 集 。 ？       模型       标准 的 RNN 需要 将 输入 和 输出 对齐 才能 用 。     利用 标准 的   RNN   做 编码 和 译码 的 方案 ：   Cho   et   al .   [ 5 ]     这种 方案 的 问题 在于 标准 的 RNN 难以 学到 长期 依赖 。       LSTM 可以 学到 长期 依赖 ， 编码器 将 输入 序列 编码 到 一个 固定 长度 的 向量 ， 解码器 是 一个 标准 的   LSTM - LM   形式 的 解码 结构 ，     用 上述 向量 初始化 该 结构 的 初始 隐层 状态 ！ 解码 序列 直到 输出     结束 标记     才 停止 。           这 篇文章 的 创新 点 ：       编码器 和 解码器 采用 不同 的   LSTM ， 参数 不同 ， 同时 学习 两种 序列 的 结构       深层   LSTM   比 浅层 好 ， 用 了 4 层       将 输入 反序 而 输出 不 反序 ， 再 进行 训练 ，   LSTM 学得 更好 ！                   实验       WMT ’ 14   English   to   French   MT   task       解码 通过 一个 从左到右 的 简单   beam   search   方案 ， 每次 保存   B   个 最 可能 的 前缀 ！     每次 将 这 B 个 前缀 扩展 ， 然后 再 保存 最 可能 的   B   个 新 的 前缀 ， 直到 都 碰到 结束符 ！     B = 1   就 很 好 了 ！ 增加 B 的 值 ， 收益 不是 很大 ！ ？       将 输入 反序 进行 训练 带来 的 收益 很大 ！           正序 ： 1 , 2 , 3   = & gt ;   a , b , c       反序 ： 3 , 2 , 1   = & gt ;   a , b , c               the   LSTM ’ s   test   perplexity   dropped   from   5.8   to   4.7 ,   and   the   test   BLEU   scores   of   its   decoded   translations   increased   from   25.9   to   30.6           作者 给 了 一个 简单 的 解释 ： 不 反序 ， 第一个 词 .....   没看 懂       比   baseline   好 ， 但 比 最好 的 结果 还是 稍微 差点 。     baseline ： phrase - based   SMT   system ？       H .   Schwenk .   University   le   mans .   http : / / www - lium . univ - lemans . fr / ~ schwenk / cslm _ joint _ paper / ,     2014 .   [ Online ;   accessed   03 - September - 2014 ]       RNN   for   QA           一般 的   RNN   应用 :   词 序列 （ 句子 ）   = & gt ;   连续 值 或者 有限 的 离散 值       QA ：   词 序列 （ 句子 ）   = & gt ;     富 文本           Matching   Text   to   Entities :   Quiz   Bowl       对 同一个 结果 ， 需要 有 冗余 的 样本 进行 学习 。       模型 ： dependency - tree   rnn   ( dt - rnn ) ， 对 语法 变化 具有 鲁棒性 ， 同时 训练 问题 和 答案 ， 映射 到 同一个 向量 空间 。       问题 ： 词 之间 的 这个 树结构 怎么 得到 的 ？ ！ De   Marneffe   et   al . ,   2006           每 一个 关系 的 终止 节点 （ 词 ） 通过 矩阵   $ ( W _ v   \ \ in   \ \ mathbb { R } ^ { d   \ \ times   d } ) $   映射 到 隐层 。       中间 节点 也 关联 一个 词 ， 通过 下式 将 该词 和子 节点 映射 到 隐层 。           $ $     h _ n   =   ( W _ v   w _ n   + b   +   \ \ sum _ { k   \ \ in   K ( n ) }   W _ { R ( n ,   k ) }   h _ k )     $ $       权重   $ ( W _ { R ( n ,   k ) } ) $   描述 当前 词 n 与子 节点 隐层   $ ( h _ k ) $   之间 的 组合 关系 。       设 S 是 所有 的 节点 ， 给定 一个 句子 ， 设词 c 是 正确 结果 ， Z 是 所有 不 正确 的 结果 集合 。 那么 对 这 一个 样本 ， 损失 函数 为       $ $     C ( S ,   \ \ theta )   =   \ \ sum _ { s   \ \ in   S }   \ \ sum _ { z   \ \ in   Z }   L ( rank ( c ,   s ,   Z ) )   \ \ max ( 0 ,   1 -   x _ c   h _ s   +   x _ z   h _ s )   \ \ \ \     L ( r )   =   \ \ sum _ { i = 1 } ^ r   1 / i     $ $       Image   retrieval   by   Sentences       论文 ： Grounded   Compositional   Semantics   for   Finding   and   Describing   Images   with   Sentences ,   2013 ,   Richard   Socher ,     Andrej   Karpathy   ,   Quoc   V .   Le * ,   Christopher   D .   Manning ,     Andrew   Y .   Ng         DT — RNN ： CT - RNN ,   Recurrent   NN       将 图像 和 句子 映射 到 同一个 空间 ， 这样 就 可以 用 一个 来查 另外 一个 了 。       zero   shot   learning       最 简单 的 将 词 向量 变成 句子 或 短语 的 方式 是 ， 简单 地 线性 平均 这些 词 向量 ， （ 词 向量 中 的   bag   of   word ） 。     RNN   的 方法 就 没有 这些 问题 。       句子   parsed   by   the   dependency   parser   of   de   Marneffe   et   al .   ( 2006 )       每 一个 句子 被 表达 为 一个 词 ， 词 向量 序列 ，   $ ( s   =   (   ( w _ 1 ,   x _ { w _ 1 } ) ,   ( w _ 2 ,   x _ { w _ 2 } ) ,   ... ,   ( w _ n ,   x _ { w _ n } )   ) ) $ 。     parse 后 得到 树状 结构 ， 可以 用 ( 孩子 ， 父亲 ) 对来 表示   $ ( d ( s )   =   {   ( i ,   j )   } ) $ 。     最后 输入   DT - RNN   的 样本 是 两者 组成 的   ( s ,   d )       图像 特征提取 是 用   DNN ( Le   et   al . ,   2012 ) ， 利用 未 标注 的 web 图片 和 标注 的   ImageNet   训练 学 出来 的 ， dim = 4096 。     输入 ： 200x200 ， 使用 了 三个 层 ： 滤波 （ CNN ） ， pooling ( L2 ) ， local   contrast   normalization .       local   contrast   normalization :   将 输入 的 子 图块 （ 文章 中 5x5 ） 减去 均值 ， 除以 方差 进行 归一化 。   有点像   layer   nomalize .           Multimodal   Mappings             -   固定 图片 特征 4096 维       联合 训练 图片 特征向量 映射 到 联合 空间 矩阵 和 DT - RNN 参数 。                   损失 函数 ： 大 间隔 损失 函数 ， 略 。       Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions       论文 ： Deep   Visual - Semantic   Alignments   for   Generating   Image   Descriptions ,     Andrej   Karpathy   ,     Li   Fei - Fei             图像 特征 ：   RCNN       文本 特征 ：   双向 RNN           将 文本 特征 和 图像 特征 映射 到 同一个 空间 ， 并 学习 图像 块 和 文本 的 对齐 向量 。       RNN 语言 模型       Recurrent   neural   network   based   language   model       动态 模型 ： 在 训练 的 时候 ， 每个 样本 在 多个 epoch 出现 ， 测试 的 时候 ， 也 更新 模型 ， 不过 一个 样本 只 出现 在 一个 epoch 中       cache   techiques 。       优化 技巧           将 出现 频率 低于 某个 阈值 的 词 映射 为 同一个 词 ， 称作 rare   token 。 条件 概率 变为 ：           $ $     p ( w _ i ( t + 1 ) |   w ( t ) ,   s ( t - 1 ) )   =   \ \ begin { cases }                                                             y _ { rare } ( t ) / C _ { rare } ,   w _ i ( t + 1 )   is   rare .   \ \ \ \                                                             y _ i ( t ) ,   otherwise                                                             \ \ end { cases }     $ $       s   是 隐层 ， 即 上下文 向量 。 因为   rare   是 多个 词 概率 之 和 ， 所以 对 某个 词 来说 ， 它 的 概率 就要 把   rare   的 概率 除以 rare 词 的 数目 。     对 这些 词 来说 ， 概率 都 是 一样 的 。       RNN   LM ：   6 小时 ；   Bengio ： 几天 ， 24 小时   sampling       A   Neural   Probabilistic   Language   Model       Yoshua   Bengio ， 2003 .       传统 的   n - gram   模型 的 问题 ， 维数 灾难 。 随着 n 增大 ， 测试 集中 的   n - gram   是 训练 集中 没有 的 概率 越来越 大 。     解决之道 ： 神经网络 模型 ， 词 的 分布式 表达 。           学习 到 了 词 的 分布 是 表达       基于 这种 词 的 表达 的 条件 概率模型 ， 语言 模型           维数 灾难 ： 建模 离散 随机变量 的 联合 分布 时 ， 10 个 变量 就 有   | V | ^ 10   个 可能 的 状态 （ 参数 ） 。     而 建模 连续变量 就 容易 一些 ， 可以 用 神经网络 ， 等等 。 （ 连续函数 一般 具有 局部 光滑 ， 即 局部 可微 ） 。       传统 n - gram 的 问题 ： 考虑 的 近邻 词 数目 太 少 ， 最好 的 结果 也 就是 trigram ；     没有 考虑 到 词 之间 的 相似性 。       维数 灾难 解决办法 ： 词 的 分布式 表达 。       n - gram   语言 模型 ， 也 会 通过 所有 的 gram 进行 平滑 。       perplexity ，   条件 概率 的 倒数 的 集合 平均值 ！       impotance   sampling :   Quick   training   of   probabilistic   neural   nets   by   importance   sampling ，   Bengio ,   2003 .       EXTENSIONS   OF   RECURRENT   NEURAL   NETWORK   LANGUAGE   MODEL       Toma   ́ s ˇ   Mikolov 。 问题 ， 计算 复杂度 太高 。 要 计算   softmax       BPTT ，   4 - 5 步 就 可以 达到 不错 的 精度 了 。       将 输出 层 分解 ， 减少 计算 量 ？ ？ ？       Opinion   Mining   with   Deep   Recurrent   Neural   Networks       将 观点 挖掘 作为 序列 标注 的 问题 。           BIO   tagging   scheme ：       B ， 表达 观点 的 开始 位置       I ， 表达 观点 的 词中       O ， 词外                   3 - 4 层后 ， 性能 也 上 不 去 了 ！       Gated   Feedback   Recurrent   Neural   Networks           多层   RNN （ 简单 RNN ，   LSTM ，   GRU 都 可以 ）       不同 层 RNN 互相 连接 ， 通过 全局   rest   门 控制 不同 层 之间 的 交互           其中 ， 第 i 层到 第 j 层 的 全局   reset   门 ， 由 当前 时刻 第   j - 1   层 的 输入 （ 对于 第一层 ， 是 x ） ，     以及 上 一 时刻 所有 的 隐层 共同 决定 ， 如下 式 ：       $ $     g ^ { i   \ \ rightarrow   j }   =   \ \ sigma ( w _ g ^ { i   \ \ rightarrow   j }   h _ t ^ { j - 1 }   +   u _ g ^ { i   \ \ rightarrow   j }   h _ { t - 1 } ^ { * } )     $ $       隐层 的 更新 ， 将 单层 隐态 更新 方程 中上 一 时刻 隐层 项 ， 变成 对 所有 隐层 通过 全局   reset   门 的 组合 。     所有 类型 的   RNN 如 LSTM ， GRU 都 适用 ， 详细 见 论文 。           试验 任务 ：       character - level   语言 模型 ， 评估 指标   bits - per - character       python   程序 结果 预测 ： 输入 一段 python 程序 ， 要求 预测 输出 结果 。 every   input   script   ends   with   a   print   statement 。   属于   sequence   to   sequence   的 问题 。 通过 调节 程序 的 难度 ， 可以 在 不同 难度 上 评估 不同 模型 的 优劣 。                   Recursive   Deep   Models   for   Semantic   Compositionality   Over   a   Sentiment   Treebank       作者 ： Richard   Socher ,   Alex   Perelygin ,   Jean   Y .   Wu ,   Jason   Chuang ,   Christopher   D .   Manning ,     Andrew   Y .   Ng     and   Christopher   Potts       Semantic   vector   spaces ： ？       在 合并 子 节点 的 时候 ， 除了 传统 RNN 的 二阶 张量 线性 项 ， 还 增加 了 三阶 张量 线性 项 ， 对 短 句子 建模 更 有效 ？ ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dlnlp-cs224d.html"},
      
      
      {"title": "CS231N - Convolutional Neural Networks for Visual Recognition", "text": "    Table   of   Contents           关于           BP   算法 与 计算 图           高效 的 BP 算法           自动 微分           参考                   神经网络 历史           感知器           三层 神经网络           RBM 深度 网络           第一个 强 结果           激活 函数           CNN                   目标 检测           HOG ( Histogram   of   Oriented   Gradient )           Deformable   Parts   Model   :   DPM                   表达 可视化           RNN           CNN   practice           Data   Augmentation   数据 增强 ：           Transfer   Learning   迁移 学习           CNN   细节           卷积 的 实现           浮点 精度                   软件包   Caffe   /   Torch   /   Theano   /   TensorFlow           Caffe           Torch           Theano           TensorFlow                   Video           无 监督 学习           Autoencoders           Variational   autoencoder           Generative   adversarial   nets                         关于       李菲菲 在 Stanford 开 的 课程 ， 见   http : / / cs231n . stanford . edu /         BP   算法 与 计算 图       一个 图 节点 实现   forward   计算 激活 函数 和   backward   计算 梯度 ， 图 的 变 对应 于 变量 。       高效 的 BP 算法       LeeCun   1998   的 论文 中 给出 了 BP 算法 的 一些 trick ：           采用 随机 梯度 下降 ， 更 快 ： 考虑 对 样本 的 10 次 复制 ， 随机 梯度 相当于 训练 了 10 次 ， 而 批量 梯度 下降 只有 1 次 ！     结果 通常 更好 ： 可以 跳出 鞍点 ， 也 可以 更大 概率 跳出 局部 最优 。 可以 随 时间 进一步 训练 ！ online   learning 。     对于 SGD ， 理论 上 最优 学习 率 随 时间 线性 下降 ！       每 一次 epoch 重新 打散 样本 ！       归一化 输入 ， 均值 接近 0 通常 收敛 更 快 ！ 同样 ， 输出 也 尽可能 是 0 均值 。 去 相关 ， 归一化 方差 ， 会 加快 收敛 。       基于 上 一个 原则 ， tanh 比 sigmoid 好 。       目标 ， 匹配 输出 激活 函数       初始化 权值 ， 权值 要 使得 激活 函数 工作 在 线性 区 ， （ 这样 才 好学 ， 否则 梯度 为 0 ） 目标 是 让 输入输出 的 方差 相同 （ 都 为 1 ） 。     当 输入 方差 为 1 的 时候 ， 输出 方差 为           $ $     \ \ sigma _ { y _ i }   =   ( \ \ sum _ j   w _ { ij } ^ 2 ) ^ { 1 / 2 }     $ $       为了 保证 输出 方差 也 为 1 ， 那么       $ $     \ \ sigma _ w   =   m ^ { - 1 / 2 }     $ $       对于 部分 连接 的 网络 （ 如 CNN ， DTNN ） ， m 应该 是 连接 的 节点 个数 。           学习 率 ， 自动 调整 衰减 。 动量 机制 ， 减少 震荡 。           $ $     \ \ Delta   w ( t + 1 )   =   \ \ eta   \ \ frac { \ \ partial   E _ { t + 1 } } { \ \ partial   w }   +   \ \ mu   \ \ Delta   w ( t )     $ $           自 适应 学习 率           自动 微分       四种 计算 梯度 的 方法 ：           手动 推导 梯度 的 公式 ， 然后 编码 实现 ： 易错 ， 费时       数值 微分 （ 有限 差分 ） ： 简单 实现 ， 但是 效率 低 ， 而且 不 精确       符号 微分 （ Mathematica ,   Maple ) ： 表达式 通常 会 比较复杂 ， 存在   expression   swell   的 问题 ， 而且 对 表达式 形式 有 要求 （ close   form ）       自动 微分 ： 可以 达到 机器 精度 ， 和 理想 的 渐进性 能 只 差 一个 常数 因子 （ 性能 牛 逼 ！ ）           参考           Automatic   differentiation   in   machine   learning :   a   survey ,   2015       Efficient   BP ,   LeeCun   1998         https : / / cs231n . github . io / optimization - 2 /           Stochastic   Gradient   Tricks             神经网络 历史       感知器       Frank   Rosenblatt   1957       $ $     y   =   f ( w   x   +   b )   \ \ \ \     f ( z )   =   1   when   z & gt ; 0   else   0     $ $       更新 权值       $ $     w _ i ( t + 1 )   =   w _ i ( t )   +   \ \ alpha   ( d _ j   -   y _ j ( t ) ) x _ { j ,   i }     $ $       相当于 下述 损失 函数   +   SGD 优化 （ 注意 这里 label 是 + 1 ， - 1 和 上面 有 区别 ， 这里 只是 便于 表达 ）       $ $     \ \ max ( 0 ,   -   d _ j   *   y _ j )     $ $       三层 神经网络       Rumelhart   et   al .   1986 ， BP 算法       RBM 深度 网络       Hinton   2006       第一个 强 结果       Context - Dependent   Pre - trained   Deep   Neural   Networks   for   Large   Vocabulary   Speech   Recognition   George   Dahl ,     Dong   Yu ,   Li   Deng ,   Alex   Acero ,   2010   MSR       Imagenet   classification   with   deep   convolutional   neural   networks     Alex   Krizhevsky ,   Ilya   Sutskever ,     Geoffrey   E   Hinton   ,   2012       激活 函数               sigmoid :   将 结果 映射 到 [ 0 , 1 ] 之间 ， 有 概率 解释 。 问题 在于 饱和 将 梯度 变为 0 了 ， 非 0 均值 。 而 非 0 均值 ， 导致 梯度 被 限制 在 各 分量 全为 正 或者 全为 负 的 区域 。     导致 收敛 变慢 。   exp   函数 计算 复杂度 较大 。               tanh :   解决 了 0 均值 的 问题           ReLU :   在 正值 区 不饱和 ， 计算 效率 较 高 ， 但是 还 是不是 0 均值 ， 负 向 梯度 为 0       Leaky   ReLU   =   max ( 0.1 x ,   x ) , 解决 负 向 梯度 饱和 问题 ，       Maxout   =   max ( w1   x   +   b1 ,   w2   x   +   b2 ) ,   基本 解决 上述 问题 ， 但是 参数 变多 了   double       ELU           $ $     x   & gt ;   0 :   x   \ \ \ \     x   & lt ;   0 :   \ \ alpha   ( \ \ exp ( x )   -   1 )     $ $       中心化 ： 减去 图像 均值 （ AlexNet ） ， 减去 每个 通道 的 均值 （ VGGNet ）       初始化 ：     “ Xavier   initialization ”   [ Glorot   et   al . ,   2010 ] ：   $ ( n _ { in }   Var ( w )   =   1 ) $       对于   ReLU ， 因为 一半 恒为 0 ， 因此 有 一个 0.5 因子 。 $ ( \ \ frac { 1 } { 2 }   n _ { in }   Var ( w )   =   1 ) $   He   et   al . ,   2015       论文 ：           Understanding   the   difficulty   of   training   deep   feedforward   neural   networks ,   Glorot   and   Bengio ,   2010       Delving   deep   into   rectifiers :   Surpassing   human - level   performance   on   ImageNet   classification   by   He   et   al . ,   2015           Batch   Normalize [ Ioffe   and   Szegedy ,   2015 ] ： 归一化 到 标准 正态分布 ， 然后 让 它 自己 学 一个 纺射 变换 。 通常 插入 在 全 连接 层 和 激活 函数 之间 。       CNN           1998 ， LeNet - 5 ,   LeCun       LeNet - 5 :   Gradient - based   learning   applied   to   document   recognition ， 1998 ， LeCun ,   Bottou ,   Bengio ,   Haffner       AlexNet :   ImageNet   Classification   with   Deep   Convolutional   Neural   Networks ， Hinton   2012 ,       ZFNet :       VGGNet :   Very   Deep   Convolutional   Networks   for   Large - Scale   Image   Recognition       GoogLeNet :   Going   Deeper   with   Convolutions       ResNet           目标 检测       任务 ： 分类   +   定位       Location   as   Regression :   输入 图片 ， 输出 4 个 坐标 ！ （ 非常简单 ） L2 损失 函数 ， 作为 回归 问题 处理 。     分类   +   定位   作为 多任务 ， 共用 同一个 CNN 做 特征提取 层 。       输入 图像   = & gt ;   CNN   = & gt ;   FC   = & gt ;   softmax / Regression       回归 层 连接 的 位置   ： 在 CNN 层 后面 （ Overfeat ，   VGG ） ；   在 全 连接 层 ( FC ) 后面 ： DeepPose ， R - CNN       检测 多个 目标 ： 共用 CNN 层 做 特征提取 层 。       姿势 估计 ： Toshev   and   Szegedy ,   “ DeepPose :   Human   Pose   Estimation   via   Deep   Neural   Networks ” ,   CVPR   2014       sliding   window :   Overfeat : Integrated   Recognition ,   Localization   and   Detection   using   Convolutional   Networks ,     ICLR   2014           在 高精度 图片 的 不同 位置 训练 模型 进行 分类 和 回归 。       将 FC 也 变成 卷积 层 ， 减少 运算量 ： Overfeat       结合 所有 的 位置 的 结果 ， 得到 最终 的 结果 （ MAX   Pool ）       实际 使用 中 ： 采用 多个 不同 位置 不同 尺寸 的 窗           通过 一个 滑动 窗 ， 目标 检测 可以 作为 一个 分类 问题 ！ 需要 大量 的 计算 匹配 ！       HOG ( Histogram   of   Oriented   Gradient )       Dalal   and   Triggs ,   “ Histograms   of   Oriented   Gradients   for   Human   Detection ” ,   CVPR   2005           在 不同 分辨率 计算 方向 梯度 直方图       略 ， 不 懂           Deformable   Parts   Model   :   DPM       Felzenszwalb   et   al ,   “ Object   Detection   with   Discriminatively     Trained   Part   Based   Models ” ,   PAMI   2010       DPM   is   CNN ?       Girschick   et   al ,   “ Deformable   Part   Models   are   Convolutional   Neural   Networks ” ,   CVPR   2015       Detection   as   Classification :       问题 ： 需要 测试 很多 位置 和 尺寸 ， 计算 量 大 ！       方案 ： 仅仅 测试 一个 很小 的 子集 ！       How   to ：       Region   Proposals :   Selective   Search       自 底向上 ， 分割 图像 ， 然后 在 不同 层级 合并 相似 区域 ， 得到 不同 层级 的 分割 结果 。       Uijlings   et   al ,   “ Selective   Search   for   Object   Recognition ” ,   IJCV   2013       其他 方法 ： EdgeBox ？       检测   Review ：     Hosang   et   al ,   “ What   makes   for   effective   detection   proposals ? ” ,   PAMI   2015       R - CNN ！   Girschick   et   al ,   “ Rich   feature   hierarchies   for     accurate   object   detection   and   semantic     segmentation ” ,   CVPR   2014           Step   1 :   Train   ( or   download )   a   classification   model   for   ImageNet   ( AlexNet )       Step   2 :   Fine - tune   model   for   detection       把 1000 个 分类 变成 20 个 目标 + 背景       扔掉 最后 一层 FC 层 ， 重新 初始化       用 正负 样本 区域 训练               Step   3 :   抽取 特征       对 所有 图片 ， 找到 感兴趣 区域       对 每 一个 区域 ， 剪切 或者 压缩 到 CNN 输入 尺寸 ， run   forward   through   CNN ， 保存 pool5 特征 到 硬盘               Step   4 :   对 每 一个 类 ， 用 上述 抽取 的 特征 ， 训练 一个 2 分类 SVM       Step   5 :   ( bbox   regression )   对   每 一个 类 ， 训练 一个 线性 回归 模型 ， 从 上述 特征 得到 box 的 偏移量 ！           目标 检测 数据 集 ： PASCAL   VOC   ( 2010 ) ，   ImageNet   Detection   ( ILSVRC   2014 ) ，   MS - COCO   ( 2014 )       评估 指标 ： “ mean   average   precision ”   ( mAP )       RCNN 问题 ：           Slow   at   test - time :   对 每 一个 区域 都 要 计算 CNN 抽取 的 特征       SVM 和 回归 都 不会 对 CNN 的 特征 进行 更新 ， 不 存在 调优       复杂 的 多 阶段 流程           Fast - RCNN ： Girschick ,   “ Fast   R - CNN ” ,   ICCV   2015           计算 慢 的 问题 ： 对 整个 图像 计算 CNN 后 的 特征 ， 共享 计算 量       end - to - end   地 训练 一次 ！           ROI ( region   of   interest )   抽取 ：           对 整个 图像 卷积 + Pooling ， 得到 高精度 的 特征       将 投影 区域 划分 为   h * w   个 格子           训练 加速 8.8 倍 ， 测试 加速 146 倍 ！       问题 ： 测试 加速 不 包过   ROI   提取 ！ ？       Faster   RCNN ： 在 最后 一层 卷积 层 加入 一层 Region   Proposal     Network   ( RPN )       Ren   et   al ,   “ Faster   R - CNN :   Towards   Real - Time   Object     Detection   with   Region   Proposal   Networks ” ,   NIPS   2015       进一步 将 test 时间 加速 10 倍 ！       表达 可视化       t - SNE   visualization ： two   images   are   placed   nearby   if   their   CNN   codes   are   close .     Laurens   van   der   Maaten   ,     Geoffrey   Hinton   ,   2008 .       Deconv 方法 ： 选择 某个 CNN 层 ， 将 该 层 的 梯度 全部 置 0 ， 除了 其中 一个 ！ 然后 BP 到 输入 ， 得到 Deconv 图像 ！   BP   to   image .       Visualizing   and   Understanding   Convolutional   Networks ,   Zeiler   and   Fergus   2013       Optimization   to   Image   方法 ： 寻找 最大化 某些 类别 的 score ！       $ $     \ \ arg   \ \ max _ { I }   S _ c ( I )   -   \ \ lambda   | | I | | _ 2 ^ 2     $ $           将 输入 层置 0 ， 即 输入 全零 图像 。       将 输出 层 的 梯度 为 单位向量 ， 某个 类别 为 1 其他 为 0 ， 然后   BP   to   image !           Deep   Inside   Convolutional   Networks :   Visualising   Image   Classification   Models   and   Saliency   Maps ,   Karen   Simonyan ,   Andrea   Vedaldi ,   Andrew   Zisserman ,   2014 .       Understanding   Neural   Networks   Through   Deep   Visualization ,   Yosinski   et   al .   ,   2015       问题 ： 给定 一个 CNN 编码 ， 能否 重构 出 原始 图像 ？       Understanding   Deep   Image   Representations   by   Inverting   Them ，   Mahendran   and   Vedaldi ,   2014       DeepDream ，     https : / / github . com / google / deepdream         Understanding   Neural   Networks   Through   Deep   Visualization ,   Jason   Yosinski ,   2015       RNN       字母 维度 的 语言 模型 ：       Image   Captioning ： 将 CNN 抽取 的 特征 ， 作为 RNN 隐层 额外 的 输入 ！ RNN 的 初始 输入 用 一个 固定 的 值 ， 后续 时刻 用前 一 时刻 的 输出 作为 输入 ！       Image   Sentence   Datasets ： Microsoft   COCO   [ Tsung - Yi   Lin   et   al .   2014 ]       RNN   在 产生 单词 的 时候 ， 关注 图像 的 部分 ： Show   Attend   and   Tell ,   Xu   et   al . ,   2015       CNN   practice       Data   Augmentation   数据 增强 ：       对 图像 进行 变换 ：           水平 翻转       随机 裁剪 和 缩放           Training :   sample   random   crops   /   scales     ResNet :           Pick   random   L   in   range   [ 256 ,   480 ]       Resize   training   image ,   short   side   =   L       Sample   random   224   x   224   patch           Testing :   average   a   fixed   set   of   crops     ResNet :     1 .   Resize   image   at   5   scales :   { 224 ,   256 ,   384 ,   480 ,   640 }     2 .   For   each   size ,   use   10   224   x   224   crops :   4   corners   +   center ,   +   flips           color   jitter ： 色彩 抖动 ？       更 多 ： Random   mix / combinations   of   :       translation       rotation       stretching       shearing ,       lens   distortions ,   …   ( go   crazy )                   加 噪声 ！     训练 ： 添加 随机噪声 ；   测试 ： 排除 噪声 ！       Transfer   Learning   迁移 学习           在 ImageNet 上 训练 CNN       在 小 数据 集上 ， 固定 前面 所有 层 ， 只 改变 最后 一层 参数 ！ 相当于 用 CNN 做 特征提取 ， 没有 调优 ！       在 中等 数据 集上 ， 固定 前面 大多数 层 ， 只 改变 后面 少许 层 参数 ， 进行 调优 ！           CNN   Features   off - the - shelf :   an   Astounding   Baseline   for   Recognition ， [ Razavian   et   al ,   2014 ]       DeCAF :   A   Deep   Convolutional   Activation   Feature   for   Generic   Visual   Recognition ,   2014 .       CNN   细节       多个 小 滤波器 堆叠 比 一个 大 滤波器 好 ！ 因为 可以 用 较 少 的 参数 ， 得到 相同 的 非线性 ！ （ 用 最后 一层 的 神经元 所 能 看到 的 输入 像素 个数 来 度量 ？ ）     并且 计算 量 更 小 ！       1x1   大小 的 滤波器 ， 用来 降维 ！ ？ GoogleNet ！       用 两个 1xN 和 Nx1 的 滤波器 ， 代替 一个 NxN 的 滤波器 ？ ！ ！ 减少 参数       Szegedy   et   al ,   “ Rethinking   the   Inception   Architecture   for   Computer   Vision ”       卷积 的 实现       将 多个 卷积 计算 变成 一个 矩阵 乘法 运算 ！   im2col ， 需要 大 额外 的 内存           设 图像 特征 map 为   H   M   C   维 ， D 个 卷积 滤波器 维度 为 K   K   C   维 。       将 图像 reshape 成   ( K ^ 2   C ) * N 维 矩阵 ， 而 将 滤波器 变为 ( K ^ 2   C ) * D 维 矩阵 ， 然后 计算 矩阵 乘法 ， 最后 将 D * N 维 结果 再 reshape 为 给定 的 大小 。           FFT 实现 ： 对于 小 的 滤波器 没有 提升 ！     Vasilache   et   al ,   Fast   Convolutional   Nets   With   fbfft :   A   GPU   Performance   Evaluation       Strassen ’ s   矩阵 乘法 算法 ！ 加速 。     Lavin   and   Gray ,   “ Fast   Algorithms   for   Convolutional   Neural   Networks ” ,   2015       GPU ： NVIDIA   is   much   more   common   for   deep   learning       CEO   of   NVIDIA :   Jen - Hsun   Huang     ( Stanford   EE   Masters   1992 )       CPU :     Few ,   fast   cores   ( 1   -   16 ) ,     Good   at   sequential   processing .       GPU :     Many ,   slower   cores   ( thousands ) ,     Originally   for   graphics ,     Good   at   parallel   computation       GUDA   vs   OpenCL .       Udacity :   Intro   to   Parallel   Programming       GPU   非常适合 矩阵 乘法 ！       多 GPU 训练 ：           模型 并行 ： FC 全 连接 层       数据 并行 ： CNN 层           Alex   Krizhevsky ,   “ One   weird   trick   for   parallelizing   convolutional   neural   networks ”       Google ： 分布式   CPU   训练 ！ 数据 并行   and   模型 并行 ！       Large   Scale   Distributed   Deep   Networks ,     Jeff   Dean     et   al . ,   2013       Google ： 异步   and   同步       Abadi   et   al ,   “ TensorFlow :   Large - Scale   Machine   Learning   on   Heterogeneous   Distributed   Systems ”       GPU   -   CPU   通信 瓶颈 ：           CPU ： 数据 预取   +   data   augment       GPU ： forward / backward           CPU   -   disk   瓶颈 ： 磁盘   = & gt ;   SSD       GPU   memory   瓶颈 ：     Titan   X :   12   GB   & lt ; -   currently   the   max 。     GTX   980   Ti :   6   GB       AlexNet :   ~ 3GB   needed   with   batch   size   256       浮点 精度           大多数 编程 环境 ： 64bit   双 精度       CNN ： 32bit   单精度       16bit   半 精度 将 成为 新 的 标准 ！ cuDNN   已经 支持 ！           最低 精度 能 到 多少 ？     16bit   定点   with   随机   round ！       Gupta   et   al ,   “ Deep   Learning   with   Limited   Numerical   Precision ” ,   ICML   2015       10bit   激活 函数 ， 12bit 参数 更新 ！     Courbariaux   et   al ,   “ BinaryNet :   Training   Deep   Neural   Networks   with   Weights   and   Activations   Constrained   to   + 1   or   - 1 ” ,   arXiv   2016       未来 ： binary   network ？       软件包   Caffe   /   Torch   /   Theano   /   TensorFlow       Caffe           U .   C .   Berkeley       C++       Has   Python   and   MATLAB   bindings       Good   for   training   or   finetuning   feedforward   models           主要 类 ：           Blob :   存储 数据       Layers ： 将 底层 Blob 变成 顶层 Blob       Net ： 很多 Layers       Solver ： 使用 梯度 更新 权值           Protocol   Buffers :   \" Typed   Json \"   from   Google       训练 和 调优 ： 不 需要 写 代码 ！       Caffe :   Model   Zoo ， 预 训练 好 的 模型 ！       提供 Python 接口       not   good   for   RNN       Torch           NYU   +   IDIAP       C   and   Lua       Used   a   lot   a   Facebook ,   DeepMind           Tensors ：   ndarray       not   good   for   RNN       Theano             Yoshua   Bengio   ’ s   group   at   University   of   Montreal       High - level   wrappers :     Keras   ,   Lasagne           计算 图 ！       问题 ： 每次 更新 权值 需要 将 权值 和 梯度 移 到   CPU   计算 ！     可以 通过   shared _ variable   得到 解决 ！       TensorFlow           From   Google       Tensorboard   for   可视化           目前 还 比较慢 ！       Video       feature   based   方法 （ 运动 识别 ） ：           Dense   trajectories   and   motion   boundary   descriptors   for   action   recognition     Wang   et   al . ,   2013       检测 不同 尺度 的 图像 的 特征 点       跟踪 特征 点   optical   flow       在 局部 坐标 中 抽取   HOG / HOF / MBH   特征       相关 文献 ：       [ G .   Farneb ä ck ,   “ Two - frame   motion   estimation   based   on   polynomial   expansion , ”   2003 ]       [ T .   Brox   and   J .   Malik ,   “ Large   displacement   optical   flow :   Descriptor   matching   in   variational   motion   estimation , ”   2011 ]       [ J .   Shi   and   C .   Tomasi ,   “ Good   features   to   track , ”   CVPR   1994 ]       [ Ivan   Laptev   2005 ]                       Action   Recognition   with   Improved   Trajectories     Wang   and   Schmid ,   2013       Spatio - Temporal   Conv ：       [ 3D   Convolutional   Neural   Networks   for   Human   Action   Recognition ,   Ji   et   al . ,   2010 ]       Sequential   Deep   Learning   for   Human   Action   Recognition ,   Baccouche   et   al . ,   2011       [ Large - scale   Video   Classification   with   Convolutional   Neural   Networks ,   Karpathy   et   al . ,   2014 ]       3D   VGGNet   :   [ Learning   Spatiotemporal   Features   with   3D   Convolutional   Networks ,   Tran   et   al .   2015 ]                   无 监督 学习       Autoencoders           Encoder   and   Decoder ：       线性   +   非线性 激活 函数 ( sigmoid )       Deep   全 连接       ReLU   CNN               loss   function :   L2       使用 Encoder 初始化 神经网络       逐层 训练 ： Greedy   training ： RBM   2006     Hinton   。 现在 不再 流行 了 ， 因为   ReLU ,   合理 的 初始化 ， batchnorm ,   Adam   etc   easily   train     from   scratch       生成 样本 ！           Variational   autoencoder       Kingma   and   Welling ,   “ Auto - Encoding     Variational   Bayes ” ,   ICLR   2014           intuition ： $ ( z ) $ 以 概率   $ ( p _ { \ \ theta * } ( x | z ) ) $ 产生 图片 样本 $ ( x ) $ ， z   可以 使 类别 ， 属性 等 ！       problem ： 在 不 知道 z 的 情况 下 ， 估计 参数 $ ( \ \ theta ) $       prior ： $ ( p ( z ) ) $ 是 标准 高斯分布       condition ： $ ( p ( x | z ) ) $ 是 对角 高斯分布 ！ 用 神经网络 预测 均值 和 方差           Generative   adversarial   nets       Goodfellow   et   al ,   “ Generative     Adversarial   Nets ” ,   NIPS   2014  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cs231n.html"},
      
      
        
        
      
      {"title": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems", "text": "    Table   of   Contents           关于           主要 结论                 关于       论文 :   xDeepFM :   Combining   Explicit   and   Implicit   Feature   Interactions   for   Recommender   Systems     作者 单位 :   科大 、 微软       主要 结论           学习 特征 的 显式 、 向量 级别 的 高阶 交叉       DCN 学到 的 是 标量 级 的 高阶 交叉 ,   因为 每 一层 的 输出 向量 都 是 最 原始 输入 向量 的 标量 积 ,   只是 这个 标量 系数 是 输入 向量 的 高阶 非线性 函数       推荐 系统 的 数据 主要 是   multi - field   的 离散 特征 , 假设 经过 embedding 层 之后 ,   得到 的 向量 是 多个 embedding 向量 的 concat 的 结果   $ (   e   =   [ e _ 1 ,   e _ 2 ,   ... ,   e _ m ]   ) $       隐式 高阶 交叉 :   FNN ,   DNN   等 模型 , 直接 将 上述 得到 的 向量   e   放到 MLP 中 , 学习 特征 间 的 高阶 交叉       embedding   +   concat   相当于 对 直接 embedding 的 权重 矩阵 施加 了 自由度 限制 , 权值 矩阵 的 某些 部分 限制 为 0           显式 高阶 交叉 DCN     $ $     x _ { k + 1 }   =   x _ 0   x _ k ^ T   w _ { k + 1 }   +   b _ k   +   x _ 0     $ $               $ ( x _ k ) $ 是 列 向量 ,   第一项 的   $ ( x _ k ^ T   w _ { k + 1 } ) $ 是 一个 标量 , 相当于 将 上 一层 的 向量 做 了 个 线性组合 得到 一个 系数 , 重新 乘以 $ ( x _ 0 ) $ ,   这 一项 加上 残差 项 相当于 对 原始 输入 $ ( x _ 0 ) $ 乘以 了 一个 系数   $ ( x _ k ^ T   w _ { k + 1 }   +   1 ) $   所以 说 是 标量 级 的 交叉 ,   而 常数 项 ( 非 齐次 项 ) 在 迭代 中 的 作用 , 相当于 引入 k , k - 1 , k - 2 , ... , 1 阶 交叉 , 否则 就 只有   k + 1   阶项 了 。 相比 于 直接 的 二项式 ,   DCN 的 参数 数目 跟阶数 k 是 线性关系 , 而 一般 多项式 是 k 次方 关系           作者 认为 这种 DCN 的 交叉 只 实现 某种 特定 的 交叉 :       隐层 是 输入 的 标量 乘法       交互 限制 在 向量 的 位 级别 ,   即 输入 向量   x0 的 不同 位 之间 通过 权值 向量 w 实现 交叉 ? ?               作者 提出 的   Compressed   Interaction   Network ( CIN )   实现 向量 级别 的 交叉 ,   模型 复杂度 跟 交叉 次数 指数 增长 ( 这个 怎么 验证 )       CIN 每 一层 都 是 Hk 个 D 维 向量 , D 保持 不变 , Hk 可以 变 。 生成 方式 是 , 将 他们 看做 D 个 Hk 维 向量 , 和 D 个 m 维 向量 , 然后 依次 将 上 一层 的 每 一个 向量 和 第 0 层 ( 最 原始 的 向量 X0 ) 的 每 一个 向量 做 张量积 , 升维到   $ ( K   =   H _ { k - 1 }   \ \ times   m ) $ 维 向量 , 然后 用 一个   $ ( H _ k   \ \ times   K ) $ 的 矩阵 投影 到   $ ( H _ k ) $ 维 。 就 得到 了   $ ( H _ k   \ \ times   D ) $ 维 矩阵 。 这个 操作 实质 上 是 在 D 方向 上 的 每 一维 上 做 了 一个双 线性变换 , 让 他们 互相 交叉 。       如果 双 线性变换 是 张量积 + sum   Pooling , 那么 就是 FM 的 操作 了       最后 在 D 方向 上 做 了 一个 sum   Pooling 操作 得到 一个 向量 , 每 一层 都 有 这样 一个 向量 , 这 这些 向量 都 拼接 起来 , 形成 一个 特征向量 , 再 送入 逻辑 回归              ", "tags": "machine-learning/ctr", "url": "/wiki/machine-learning/ctr/xdeepfm.html"},
      
      
      {"title": "召回方法的调研", "text": "    Table   of   Contents           关于           协同 过滤           基于 内容 的 召回           基于 用户群           倒排 链           向量 召回           参考资料                 关于       本文 收集 了 在 搜索 、 广告 、 推荐 系统 中 的 一些 召回 策略 和 算法       协同 过滤           主要 是 基于 行为 的 协同 过滤       userCF       itemCF       矩阵 分解   SVD / SVD ++ / LFM       基于 行为 的 方法 的 问题 在于 冷启动 ,   因为 它 只用 到 行为 ,   而 新 的 item 是 没有 行为 的           基于 内容 的 召回           计算 用户 的 画像 与 内容 的 画像 之间 的 距离 ( 相似 度 )           基于 用户群           对 用户 分 群       获得 user   embedding 向量 ,   方法 可以 是 主题 模型 中 的 主题 向量 ,   用户 画像 等 数据 用 SVD , autoencoder 降维后 的 向量 等       利用 user 向量 做 聚类 :   k - means 、 层级 聚类 、 GMM               对 用户群 中 的 用户 推荐 在 这个 用户群 上 的 点击率 高 的 item           倒排 链           召回 实现 的 时候 是从 标签 反查 item ,   所以 需要 建立 倒排 索引       在 拿到 用户 标签 后 ,   通过 标签 反查 item ,   倒排 链 放到 分布式 索引 数据库 中 加快 检索 速度                   {                 &# 39 ; tag _ 1 &# 39 ;     :                         [       {       itemID     :       &# 39 ; 13 &# 39 ;     ,       weight     :       0.7       } ,       {       itemID     :       &# 39 ; 2 &# 39 ;     ,       weight     :       0.53       }       ] ,               &# 39 ; tag _ 2 &# 39 ;     :                         [       {       itemID     :       &# 39 ; 1 &# 39 ;     ,       weight     :       0.37       }       ] ,               ...       }                 向量 召回           hash       KNN         faiss             参考资料             https : / / cloud . tencent . com / developer / article / 1174893         J .   Weston ,   A .   Makadia ,   and   H .   Yee .   Label   partitioning   for   sublinear   ranking .   In   S .   Dasgupta   and   D .   Mcallester ,   editors ,   Proceedings   of   the   30th   International   Conference   on   Machine   Learning   ( ICML - 13 ) ,   volume   28 ,   pages   181 – 189 .   JMLR       T .   Liu ,   A .   W .   Moore ,   A .   Gray ,   and   K .   Yang .   An   investigation   of   practical   approximate   nearest   neighbor   algorithms .   pages   825 – 832 .   MIT   Press ,   2004      ", "tags": "machine-learning/ctr", "url": "/wiki/machine-learning/ctr/recall.html"},
      
      
      
      {"title": "CTR预估中的深度模型", "text": "    Table   of   Contents           数据 集           数据 预处理                 数据 集       Electronics   Dataset ,   亚马逊 1996 - 2014 年间 的 评论 数据 ,   Electronics   只是 其中 电子产品 这个 类别 的 数据 , 它 包括 两个 文件 , 一个 是 评论 数据 , 另 一个 是 电子产品 的 元 数据 。           评论 数据 :       reviewerID   评论 用户 ID       asin   产品 ID       reviewerName   评论 用户 名字       helpful   有用 的 评分       reviewText   评论 文本       overall   对 产品 的 打分       summary   评论 摘要       unixReviewTime   评论 时间 戳       reviewTime   评论 时间                           {     & quot ; reviewerID & quot ;     :       & quot ; A3BY5KCNQZXV5U & quot ;     ,       & quot ; asin & quot ;     :       & quot ; 0594451647 & quot ;     ,       & quot ; reviewerName & quot ;     :       & quot ; Matenai & quot ;     ,       & quot ; helpful & quot ;     :       [     3     ,       3     ] ,       & quot ; reviewText & quot ;     :       & quot ; This   product   really   works   great   but   I   found   the   following   items   you   need   to   keep   in   mind : -   You   must   have   your   power   adapter   connected   for   it   to   work ... it   plugs   in   the   the   bottom .   It   appears   it   needs   power   from   the   nook   power   adapter   to   operate . -   The   plug   fits   in   loosely   and   you   cannot   move   the   Nook   around   much   without   holding   the   adapter   in   place . -   On   initial   plugin   it   seems   you   need   to   rock   it   around   to   get   the   connection   but   then   it   seems   solid . -   It   works   with   a   25ft   high   quality   HDMI   cable   so   you   can   put   the   NOOK   across   the   room   with   you .   Not   tested   with   cheap   cables . Warning ... I   found   that   my   LG   SmartTV   3D   from   a   few   years   back   does   not   work   with   this   adapter   but   it   does   not   seem   to   work   with   many   things ... bad   software .   This   adapter   works   fine   with   other   HDMI   devices   I   have   used   like   monitors   and   I   am   sure   other   TVs . Gave   it   five   stars   because   it   really   is   nice   to   extend   the   screen   and   use   your   Nook   as   a   streaming   server   to   your   TV .   Nice   they   made   such   a   device .& quot ;     ,       & quot ; overall & quot ;     :       5.0     ,       & quot ; summary & quot ;     :       & quot ; This   works   great   but   read   the   details ...& quot ;     ,       & quot ; unixReviewTime & quot ;     :       1390176000     ,       & quot ; reviewTime & quot ;     :       & quot ; 01   20 ,   2014 & quot ;     }                     产品 元 数据 :       asin   产品 ID       title   产品 名字       price   价格       imUrl   图片 URL       related   相关 产品 ( 推荐 结果 )       salesRank         brand   品牌       categories   类别                           {     &# 39 ; asin &# 39 ;     :       &# 39 ; 0594287995 &# 39 ;     ,       &# 39 ; imUrl &# 39 ;     :       &# 39 ; http : / / g - ecx . images - amazon . com / images / G / 01 / x - site / icons / no - img - sm ._ CB192198896 _. gif &# 39 ;     ,       &# 39 ; categories &# 39 ;     :       [ [     &# 39 ; Electronics &# 39 ;     ,       &# 39 ; eBook   Readers   & amp ;   Accessories &# 39 ;     ,       &# 39 ; Covers &# 39 ;     ] ] ,       &# 39 ; title &# 39 ;     :       &# 39 ; Kate   Spade   Rain   or   Shine   Magazine   Cover   for   Nook   Simple   Touch &# 39 ;     }                       http : / / jmcauley . ucsd . edu / data / amazon /         评论 数据     http : / / snap . stanford . edu / data / amazon / productGraph / categoryFiles / reviews _ Electronics _ 5 . json . gz         元 数据     http : / / snap . stanford . edu / data / amazon / productGraph / categoryFiles / meta _ Electronics . json . gz             数据 预处理       在 这次 实验 中 ,   只用 到   reviewerID ,   asin ,   unixReviewTime ,   categories   这 4 个 字 段 ,   并且 简单 起 见 ,   categories 只用 到 最后 一个 类别 。 对 每 一个 用户 , 照 时间 对 他 评论 的 产品 排序 ,   评论 的 产品 作为 正 样本 ,   之前 评论 的 产品 列表 作为 历史 ,   再 随机 采样 相等 数量 的 其他 产品 ( 不 在 历史 评论 中 也 不 在 未来 的 评论 列表 中 ) 作为 负 样本 ,   最后 按 最后 一个 评论 的 样本 作为 测试 集 ( 正 样本 和 负 样本 一致 ,   实际 评论 的 作为 正 样本 ,   采样 的 其他 产品 作为 负 样本 ) 。 这样 , 保证 正 样本 和 负 样本 都 覆盖 了 所有 用户 。       预处理 流程 参考 代码 , 我 复用 了 来自 DIN 论文 的 代码     https : / / github . com / zhougr1993 / DeepInterestNetwork         基本 信息 :   user _ count :   192403         item _ count :   63001       cate _ count :   801   example _ count :   1689188             #   训练 数据 集合   #   reviewerID ,   hist ,         asin ,     label   104760 ,   [ 3737 ,   19450 ] ,   18486 ,   1     #   测试数据 集合   #   reviewerID ,     hist ,               ( review   asin ,     no   review   asin )   91788 ,   [ 16942 ,   42346 ,   38112 ,   36550 ,   45547 ,   31289 ,   48828 ] ,   ( 57905 ,   11716 )     #   asin 的 类别 映射 表   cate _ list ,   key   是 asin   ID ,   value   是   cate _ id   [ 738   157   571   ...     63   674   351 ]      ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-ctr.html"},
      
      
        
        
      
      {"title": "关于计算机视觉、图像、视频", "text": "    Table   of   Contents           关于                 关于           本 栏目 主要 收集 以下 资源       CVPR 论文 笔记       工业界 的 一些 机器 视觉 领域 的 解决方案 调研              ", "tags": "machine-learning/cv-image-video", "url": "/wiki/machine-learning/cv-image-video/readme.html"},
      
      
      
      {"title": "Decoupled Neural Interfaces using Synthetic Gradients", "text": "    Table   of   Contents           论文 导读           导言                 论文 导读       这 篇文章 是   Google   Deepmind   团队 的 一片 文章 ， 改进 了 BP 算法 在 Leayer   层面 上 的 串行 计算 结构 。       导言       神经网络   BP   算法 的 几个   Lock ：           Forward   Locking ： 每一 模块 （ 层 ） 必须 等到 前面 的 层 全部 计算 结束 才能 得到 输入 数据 。       Update   Locking ： 每 一个 模块 都 要 等到 forward 模式 下 依赖 的 模块 计算 完成 才能 更新 。       Backwards   Locking ： 所有 模块 要 等到 forward 模式 和 backward 模式 依赖 的 所有 模块 计算 完成 后 才能 更新 。           现有 神经网络 更新 权值 的 问题 ： Update   Locking           多个 异步 模块 构成 的 系统       分布式 模型           本 论文 主要 工作 是 移 除了   Update   Locking   依赖 。 权值 更新 方法 ：       $ $     \ \ frac { \ \ partial   L } { \ \ partial   \ \ theta _ i }   =   f _ { Bprop } ( ( h _ i , x _ i , y _ i , \ \ theta _ i ) , ( h _ { i + 1 } , x _ { i + 1 } , y _ { i + 1 } , \ \ theta _ { i + 1 } ) , ... )   \ \ frac { \ \ partial   h _ i } { \ \ partial   \ \ theta _ i }   \ \ \ \     \ \ approx   \ \ hat { f } _ { Bprop } ( h _ i )   \ \ frac { \ \ partial   h _ i } { \ \ partial   \ \ theta _ i }     $ $       这里 下标 表示层 数 （ 或者 模型 序号 ） ， $ ( x ,   y ) $ 表示 输入 和 监督 ？ h 表示层 的 输出 ， 该 方法 用 一个 神经网络 对 第一 部分 建模 ， 而且 只 采用 该层 输出 的 局部 信息 ！       Decoupled   Neural   Interface   ( DNI ) .   预测 误差 仅 依赖 与 该 层 的 输出 ， 因而 一旦 该层 计算 完毕 ， 就 可以 马上 得到 BP 误差 ！     该 梯度 仅 跟 输出 h 有关 ， 成为   synthetic   gradients   ( 合成 梯度 ) ， 可以 利用 这个 合成 梯度 立即 更新 该层 网络 权值 ！     从而 移 除了   Update   Locking   和   Backwards   Locking ！       利用 同样 思想 ， 移除 Forward   Locking ? 预测 输入 ？           J .   Baxter   and   P .   L .   Bartlett .   Direct   gradient - based   reinforcement   learning .   In   Circuits   and   Systems ,       Proceedings .   ISCAS   2000   Geneva .   The   2000   IEEE   International   Symposium   on ,   volume   3 ,   pages     271 – 274 .   IEEE ,   2000 .         Y .   Bengio   .   How   auto - encoders   could   provide   credit   assignment   in   deep   networks   via   target   propagation .     arXiv   preprint   arXiv : 1407.7906 ,   2014 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/synthetic-gradients.html"},
      
      
      {"title": "Deep & Wide model : Google", "text": "    Table   of   Contents           关于                 关于       Google   宽广 度 模型       用 深度 模型 学习 特征 组合 ， 将 离散 特征 全部   embedding     到 低维 向量 ， 加上 连续 特征 一起 ，     用 DNN 建模 。 深度 模型 和 浅层 模型 联合 优化 。       线下 AUC 和 线 上 效果 不 一致 ？ 单独 深度 模型 线下 AUC 略低 ， 但是 线上 效果 确 提高 了 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-wide.html"},
      
      
      {"title": "Deep Forest: Towards An Alternative to Deep Neural Networks", "text": "    Table   of   Contents           关于           gcforest           Cascade   Forest           Multi - Grained   Scanning           试验 结果                   Ref                 关于       论文 ： Deep   Forest :   Towards   An   Alternative   to   Deep   Neural   Networks ， 南 大 周志华       gcforest               Cascade   Forest       gcforest 由 多层 构成 ， 每 一层 是 多个 决策树 森林 的 集成 。     我们 用 两个 完全 随机 树 森林 和 两个 随机 森林 ， 利用 不同 类型 的 树来 增加 集成 的 多样性 ，     因为 集成 学习 中 的 多样性 是 至关重要 的 。           完全 随机 树 森林 ： 1000 个 完全 随机 树 构成 。       完全 随机 树 ： 树 的 每 一个 节点 随机 选择 一个 特征 进行 分裂 ， 生长 树 直到 每个 叶子 结点 只有 一类 或者 样本量 不 多于 10 个 。       随机 森林 ： 1000 个 决策树 ， 每棵 树 都 是 随机 选择   $ ( \ \ sqrt { d } ) $   个 特征 ， 选择   gini   增益 最大 的 进行 分裂 。           对 每 一个 样本 ， 每个 森林 将 输出 其 在 每个 类 的 概率分布 向量 （ 通过 平均 每棵 树 的 概率分布 向量 得到 ， 每棵 树是 通过 统计 该 叶子 结点 上 的 分布 得到 ） 。       将 所有 森林 输出 的 分布 向量 链接 成 一个 大 的 向量 （ augmented   features   ） ， 并 和 原始 向量 拼接 到 一起 后 输入 下 一级 继续 学习 ！       为了 避免 过 拟合 ， 类 向量 通过 k - fold 交叉 验证 产生 的 ， 将 每个 样本 作为 训练 集 的   k - 1   次 结果 平均 !       每 一层 训练 结束 后 ， 将 在 验证 集上 计算 score ， 如果 score 没有 明显增加 ， 那么 训练 流程 将会 停止 ！     也就是说 层数 是 训练 过程 中 确定 的 。       Multi - Grained   Scanning       对于 序列 数据 、 图像 数据 等 ， 借鉴 了 卷积 的 思想 ， 利用 滑动 窗将 一个 样本 变成 多个 样本 ， 提取 特征 ！     将 一个 样本 变成 多个 小 尺寸 的 样本 ， 这个 跟 多 示例 学习 中 的   bag   generate 一致   [ Wei   and   Zhou ,   2016 ] 。     然后 把 每个 小 尺寸 样本 预测 出来 的 类 向量 拼接 到 一起 变成 一个 大 的 向量 ， 作为 原始 样本 的 组合 输出 特征 ， 叠加 到 下 一层 。     可以 采用 多个 滑动 窗 ， 从而 获取 跟 多 不同 尺寸 的 特征 ， 类似 于小波 分析 。 这些 扫描 后 的 特征 作为   Cascade   Forest   的 输入 特征 ， 进行 学习 。           X . - S .   Wei   and   Z . - H .   Zhou .   An   em -   pirical   study   on   image   bag   generators   for   multi - instance   learning .   Machine   Learning ,   105 ( 2 ) : 155 – 198 ,   2016 .                           试验 结果       试验 配置 ， gcforest 都 是 用 基本相同 的 配置 。 具体 结果 参看 原始 论文 ， 这里 列举 一些 结论 。           gcforest   在 所有 的 试验 都 显示 出 强大 的 建模 能力 ， 尤其 是 在 小 数据 集上 的 效果 ， 在 IMDB 情感 分类 这样 级别 的 数据 集 也 和 目前 最好 的 CNN 效果 略优 。       Multi - Grained   Scanning   对 图像 数据 和 序列 数据 非常 必要 ， 对 最终 效果 影响 特别 明显 ， 在 GTZAN 和 sEMG 数据 及 上 效果 相差 10 个点 ！       运行 速度 快 ， 都 采用   CPU   版本 时 ， gcforest 大约 只 需要   MLP   的   1 / 4   时间 ， 接近   GPU   版本 的   MLP       更 大规模 的 数据 集 的 效果 和   CNN   的 对比 尚有 待 研究 ， 例如 和   res150   对比 ？ ！           Ref       F .   T .   Liu ,   K .   M .   Ting ,   Y .   Yu ,   and   Z . - H .   Zhou .   Spectrum   of   variable - random   trees .   Journal   of   Ar -   tificial   Intelligence   Research ,   32 : 355 – 384 ,   2008 .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/gcforest.html"},
      
      
      {"title": "Deep Learning Book - Bengio", "text": "  关于  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-learning-bengio.html"},
      
      
      {"title": "Deep learning 环境搭建大坑", "text": "  关于       暂时 记录 了 windows 和 mac 系统 下 遇到 的 问题 。       MAC   SIP 权限 问题       经常 因为 SIP 无法 取得 最高 权限 导致 安装 出错 ， 关闭 SIP 权限 的 方法 ：     1 .   重启 MAC ， 在 重启 过程 中 按住 command ＋ R 键 ， 进入 安全 模式 ， 因为 只有 在 安全 模式 下 才能 关闭 SIP 安全 特性     2 .   在 顶部 菜单 打开 终端 ， 在 终端 输入 命令     csrutil   disable   ， 可以 查看 这个 命令 。 开启 可以 使用   enable   参数 。     3 .   重启 系统 。       theano       tensorflow       linux   安装 pip 可能 找 不到 对应 的 版本 ， 需要 从 tensor 的 github 提供 的 带   non   的 whl 包 安装 。     可能 安装 后 ， import 会报 glibc 版本 找 不到 ， 需要 下载   gnu / glibc   对应 的 版本 ， 编译 并 安装 ！       glibc   安装 ， 在 一个 方便 的 镜像 下载 glibc 源码 ， 安装             mkdir   build     cd     build   .. / configure   - - prefix   =   path - to - install   make     & amp ;     make   install      ", "tags": "machine-learning", "url": "/wiki/machine-learning/env.html"},
      
      
      {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "text": "    Table   of   Contents           关于           语音 识别 现状                 关于       深度 学习 在 语音 识别 中 的 应用 。 参考文献             Hinton   G   E   ,   Deng   L ,     Yu   D   ,   et   al .   Deep   Neural   Networks   for   Acoustic   Modeling   in   Speech   Recognition [ J ] .   IEEE   Signal   Processing   Magazine ,   2012 ,   29 ( 6 ) .           语音 识别 现状           GMM - HMM       GMM   表达   HMM   态 之间 的 关系 ；       输入 的 表达 ： MFCC （ Mel - frequency   cepstral   coefficients ） ， PLPs （ perceptual   linear   predictive   coefficients ） ；     以及 一阶 和 二阶 时域 差分 ；                       每 一个 音节 （ ？ ） 用 一个 态   s   表示 ， 输入 特征   x   到态 s 的 建模   $ ( P ( x   |   s ) ) $   采用 高斯 混合 模型   GMM ， 这是 一个 生成 模型 。 （ 利用 EM 算法 ， 很 容易 拟合 数据 ）       用隐 马尔科夫 模型 建 模态 转移       DNN   替换   GMM ： GMM 不能 很 好 的 建模 低维 非线性 流形 。 DNN 直接 建模 条件 概率   $ ( P   ( s   |   x ) ) $ ， 然后 通过 贝叶斯 法则 得到   $ ( P ( x | s )   =   P ( s | x )   *   P ( x )   /   P ( s ) ) 。 态 的 标注 通过 基本 的   HMM - GMM   得到 ？           TIMIT   database ；   LVCSR               逐层 的 训练   RBM ， 第一层 隐层 保持 二进制 （ 硬 判决 的 noise 可以 作为 正则 防止 过 拟合 ） ， 其他 层 隐层 都 用 实值 的 概率 值 。           实值 数据 （ MFCC ） 建模 ： 高斯 贝 努力   RBM （ Gaussian – Bernoulli   RBM   ( GRBM ) ） ， 能量 函数 为 ：           $ $     E ( v ,   h )   =   \ \ sum _ { i   \ \ in   vis }   \ \ frac { ( v _ i   -   a _ i ) ^ 2 } { 2   \ \ sigma _ i ^ 2 }   -   \ \ sum _ { j   \ \ in   hid }   b _ j   h _ j   -   \ \ sum _ { i , j }   \ \ frac { v _ i } { \ \ sigma _ i }   h _ j   w _ { ij }     $ $       两个 条件 分布 为 ：       $ $     p ( h _ j | v )   =   \ \ text { logistic } ( b _ j   +   \ \ sum _ i   \ \ frac { v _ i } { \ \ sigma _ i }   w _ { ij } )     \ \ \ \     p ( v _ i | h )   =   \ \ mathcal { N } ( a _ i   +   \ \ sigma _ i   \ \ sum _ j   h _ j   w _ { ij } ;   \ \ sigma _ i ^ 2 )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dl-asr.html"},
      
      
      {"title": "DeepGBM: A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks", "text": "    Table   of   Contents           摘要           相关 工作           DeepGBM           试验 结论           疑问                 摘要           在线 预估 的 两个 重要 的 任务       tabular   input   space       online   data   generation               GBDT 和 NN 都 有 自己 的 弱点       GBDT 弱点 在于 sparse 特征       NN 弱点 在于 dense 特征               DeepGBM 两个 模块       CatNN   处理 sparse 特征 的 NN       GBDT2NN   处理 dense 特征 ,   将 GBDT 中学 到 的 知识 、 特征 重要性 、 特征 分割 点 等 知识 蒸馏 到 NN 中               GBDT 的 两个 问题       不能 做 online   learning ,   也 意味着 难以 处理 极 大规模 的 训练 数据       对 ( 高度 ) 稀疏 特征 难以 拟合 得 很 好 。 因为 分裂 是 根据 特征 的 统计 信息 , 而 稀疏 特征 onehot 之后 , 分裂 对 单个 特征 的 统计 信息 的 变化 很小 , 所以 难以 拟合 的 很 好 。 一些 解决 的 方法 , 将 sparse 特征 通过 一些 编码方法 变成 连续 特征 。 但 每 一种 编码方法 都 有 其 局限性 , 无法 充分 表示 原有 特征 中 的 信息量 。       sklearn 的 一个 编码 库     https : / / github . com / scikit - learn - contrib / categorical - encoding         onehot 之后 的 统计 信息 还有 偏 ?   LightGBM                       NN 的 问题       dense 特征 拟合 效果 通常 不如 GBDT , 原因 是 局部 最优 。 我 认为 还有 模型 结构 的 限制 也 是 一个 原因 , 因为 dense 特征 要 拟合 好 需要 高度 非线性 , 那么 需要 很深 的 NN , 而 这 导致 优化 更加 困难                   相关 工作           GBDT 在线 学习 方法       XGBoost 、 LightGBM   固定 树结构 , 改变 叶子 节点 的 权重               GBDT 处理 sparse 特征       CatBoost ,   通过 target   statistic 信息 将 sparse 特征 编码 成 连续 值 特征               深度 GBDT       周志华 的 两篇 文章 , DeepForest 和 mGBDT , 将 树 模型 多层 堆叠 起来               DNN 处理 连续 特征       归一化 、 正则       离散 化 :     Supervised   and   unsupervised   discretization   of   continuous   features               DNN 与 GBDT 结合 的 工作       Facebook 将 GBDT 的 叶子 节点 作为 sparse 特征       Microsoft 用 GBDT 学习 NN 的 残差                   DeepGBM           CatNN   sparse 特征 ,   目前 主流 的 DNN 都 可以 使用 , 没有 限制       GDBT2NN   dense   特征       树 的 特征选择 信息 。 用 $ ( I ^ t ) $ 表示 第 t 棵 树 的 特征选择 向量 , 即 第 k 个 元素 为 1 表示 第 k 个 特征 被 第 t 棵 树 选择 用于 构建 决策树 。 然后 只是 用 这些 选出 来 的 特征 $ ( x ( I ^ t ) ) $ 来 构建 NN       树结构 蒸馏 。 训练 一个 神经网络 , 用 $ ( x ( I ^ t ) ) $ 作为 输入 特征 , 用 决策树 的 输出 叶子 节点 ID 作为 target 。 $ ( \ \ theta ) $ 是 NN 的 参数 , $ ( L ^ { t , i } ) $ 表示 原始 训练 数据 中 第 i 个 样本 被 第 t 棵 树分 到 的 叶子 节点 ID 的 onehot 编码 ,   L 是 交叉 熵 损失 函数     $ $     \ \ min _ { \ \ theta }   \ \ frac { 1 } { n } \ \ sum _ { i = 1 } ^ n   L ( NN ( x ^ i [ I ^ t ] ;   \ \ theta ) ,   L ^ { t , i } )       $ $       叶子 节点 输出 。   叶子 节点 输出 向量 $ ( q ^ t ) $ 不用 学习 , 直接 映射 即可   。 最终 决策树 输出 为     $ $     y ^ t ( x )   =   NN ( x [ I ^ t ] ;   \ \ theta )   \ \ times   q ^ t     $ $       多棵 树 的 蒸馏 ,   直接 用 多个 NN 来 拟合 多个 树太 低效 了       leaf   embedding   蒸馏 ,   $ ( p ^ { t , i } ) $   是 预测值 , 直接 用 一个 NN 将 leaf   id 映射 到 预测值 , 从而 得到 leaf   embedding 向量 $ ( H ^ { t , i } = H ( L ^ { t , i } ;   w ^ t ) ) $ 。 这样一来 也 可以 将 树结构 蒸馏 的 target 改为 $ ( H ^ { t , i } ) $     $ $     \ \ min _ { w , w _ 0 , w ^ t }   \ \ frac { 1 } { n } \ \ sum _ { i = 1 } ^ n   L ( w ^ TH ( L ^ { t , i } ;   w ^ t )   +   w _ 0 ,   p ^ { t , i } )   \ \ \ \     \ \ min _ { \ \ theta }   \ \ frac { 1 } { n } \ \ sum _ { i = 1 } ^ n   L ( NN ( x ^ i [ I ^ t ] ;   \ \ theta ) ,   H ^ { t , i } )     $ $       tree   grouping :   从 一组 tree 中 做 蒸馏       如何 分组 :   随机 均匀 分组 、 根据 重要性 均匀 分组 etc , 文中 使用 随机 , 目测 差异 不会 很大       如何 蒸馏 :   输入 由 单棵树 的 叶子 id   onehot 值 $ ( L ^ { t } ) $ 变成 多个 树 的 concat ,   target 变成 多个 叶子 节点 的 和 !   concat 之后 embedding 向量 ( 相当于 多个 embedding 向量 之 和 ) 作为 NN 蒸馏 学习 的 target 。 NN 蒸馏 的 输入 则 是 该组 决策树 用到 的 特征 , 也 可以 根据 特征 重要性 取 TOP 。     $ $     \ \ min _ { w , w _ 0 , w ^ t }   \ \ frac { 1 } { n }   \ \ sum _ { i = 1 } ^ n   L ( w ^ TH ( concat _ { t \ \ in   T } L ^ { t , i } ;   w ^ t )   +   w _ 0 ,   \ \ sum _ { t \ \ in   T }   p ^ { t , i } )   \ \ \ \     G ^ i   =   H ( concat _ { t \ \ in   T } L ^ { t , i } ;   w ^ t )     $ $               最终 GBDT2NN 的 输出 是 多个 模型 最终 输出 之 和 。                       DeepGBM 训练 :       端到 端 离线 训练 , 批量 更新 。 首先 训练 一个 GBDT , 然后 利用 叶子 节点 id 和 权重 训练 出 leaf   embedding ;   然后 端到 端 训练 , 一方面 利用 leaf   embedding 训练 一个 新 的 NN 对模型 结构 做 蒸馏 构 , 造出 对 GBDT 结构 蒸馏 损失 , 另一方面 利用 GBDT2NN 的 输出 与 CatNN 的 输出 加权 和 拟合 的 误差 损失 。 相当于 蒸馏 结构 损失 作为 一个 辅助 任务 或 正则 项 。 k 是 决策树 分组 数 ,   $ ( L ^ { T _ j } ) $ 代表 该 分组 的 对 leaf   embedding 的 拟合 损失 。     $ $     \ \ hat { y } ( x )   =   \ \ sigma ( w _ 1 \ \ times   y _ { GBDT2NN } ( x )   +   w _ 2 \ \ times   y _ { cat } ( x ) )   \ \ \ \     L _ { offline }   =   \ \ alpha   L ( \ \ hat { y } ( x ) ,   y )   +   \ \ beta \ \ sum _ { j = 1 } ^ { k }   L ^ { T _ j }     $ $       在线 学习 :   只 更新 拟合 误差 损失 部分 ( 及 上式 的 第一 部分 作为 在线 更新 损失 ) 。 问题 :   那么 这种 方法 跟 直接 用 GBDT 叶子 id 作为 额外 的 NN   sparse 特征 的 方法 有 什么 优势 ? 因为 二者 都 没有 更新 GBDT 模型                   试验 结论           试验 对比 :   D1   是 直接 用 GBDT + NN ( 应该 微软 17 年 的 论文 方法 , 直接 boost 融合 ) ;   D2   是 只用 GBDT2NN , 没有 CatNN       将 GBDT 蒸馏 为 NN 这 一步 离线 性能 上 有 轻微 的 额外 收益       将 GBDT 和 NN 联合 训练 比 单独 的 GBDT 和 单独 的 NN 有 额外 收益       将 GBDT 蒸馏 为 NN 这 一步 对 在线 更新 上 有 显著 收益       开 源代码     https : / / github . com / motefly / DeepGBM                           疑问           为啥 FM 和 wide   & amp ;   deep 在 这种 CTR 预估 数据 集上 的 性能 被 采用 了 label   encoded 处理 过 sparse 特征 后 的 LightGBM 完虐 ?   似乎 不太 符合 常理 ?       个人 认为 这种 方案 还是 不太 方便 ,   有没有 更好 的 方案 ,   可以 让 NN 也 能 具有 GBDT 处理 连续 特征 的 能力 ?      ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-gbm.html"},
      
      
      {"title": "Deeplearning4j", "text": "    Table   of   Contents           分布式 训练           在   Spark   上 运行 训练任务           注意事项                         分布式 训练       DL4J   支持 分布式 训练 ， 加快 训练 速度 ， 目前 仅 支持 数据 并行 ， 参数 平均 方式 的 训练 ！               TrainingMaster       tm       =       new       ParameterAveragingTrainingMaster     .     Builder     (     int       dataSetObjectSize     )                               ...       (     your       configuration       here     )                               .     build     ( ) ;                 在   Spark   上 运行 训练任务       注意事项           需要 指定 内存 ， 否则 会 报 各种 错误 ， 方法 和 参数设置 见 官方 文档     https : / / deeplearning4j . org / spark   ，     从官 网上 抄下来 的 一个 例子 ：                 - - class   my . class . name . here   - - num - executors   4   - - executor - cores   8   - - executor - memory   4G   - - driver - memory   4G   - - conf   & quot ; spark . executor . extraJavaOptions = - Dorg . bytedeco . javacpp . maxbytes = 5368709120 & quot ;   - - conf   & quot ; spark . driver . extraJavaOptions = - Dorg . bytedeco . javacpp . maxbytes = 5368709120 & quot ;   - - conf   spark . yarn . executor . memoryOverhead = 6144      ", "tags": "machine-learning", "url": "/wiki/machine-learning/deeplearning4j.html"},
      
      
      {"title": "DNC: Hybrid computing using a neural network with dynamic external memory", "text": "    Table   of   Contents           关于                 关于       论文 ： Hybrid   computing   using   a   neural   network   with   dynamic   external   memory       Google   Deepmind 在 nature 上 的 一篇 文章 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/dnc.html"},
      
      
      {"title": "FAISS", "text": "    Table   of   Contents           关于           FAISS   简单 使用           相关 文献 概述           Video   Google :   A   Text   Retrieval   Approach   to   Object   Matching   in   Videos           Product   quantization   for   nearest   neighbor   search           Searching   in   one   billion   vectors :   re - rank   with   source   coding           The   Inverted   Multi - Index           Optimized   Product   Quantization                   其他 参考                 关于       向量 最近 邻 索引 在 文本 、 图像 、 推荐 、 搜索 等 领域 具有 重要 意义 ,   Facebook 开源 的 FAISS 就是 这样 一个 工具 ,   提供 大规模 的 向量 索引 。       FAISS   简单 使用           安装     conda   install   faiss - cpu       其他 安装 方法     https : / / github . com / facebookresearch / faiss / blob / master / INSTALL . md             相关 文献 概述       Video   Google :   A   Text   Retrieval   Approach   to   Object   Matching   in   Videos           以 文本检索 的 速度 , 实现 视频 中 目标 检索 匹配       基本 方法       从 视频 的 关键帧 图像 中 提取 多个 关键点 的   SIFT   描述 子       将 这些 描述 子 做 矢量 量化 ,   每 一个 量化 编码 的 码字 可以 看做 一个   visual   word       基于 上述   visual   word   建立 倒排 索引 ,   同时 可以 考虑 stop   word ,   tf - idf 权重 等 和 文本检索 类似 的 trick               矢量 量化     http : / / blog . pluskid . org / ? p = 57   ,     https : / / blog . csdn . net / zouxy09 / article / details / 9153255         相当于 标量 量化 的 高阶 推广       将 这些 矢量 进行 聚类 ,   用 聚类 中心 作为 该类 的 代表                   Product   quantization   for   nearest   neighbor   search           包含 了 很多 ANN 的 参考文献 ,   后面 可以 精读 一下       矢量 量化       码 向量 c :   指 的 是 代替 原始 向量 的 量化 后 对应 的 那个 中心 向量       cell :   归属于 同一个 码 的 所有 点 组成 的 区域       码号 i :   码 向量 的 索引       Lloyd 最优 条件       向量 x 的 码字 应该 是 离 x 最近 的 码 向量 c       推论 :   cell 的 分 界面 是 超平面               码 向量 应该 是 cell 中 的 向量 的 概率 平均 向量               求解 算法 :   k - means               直接 使用 矢量 量化 的 问题       当 码字 数目 k 很多 时 计算 k 个 中心 复杂度 高 ,   而且 k 个 码字 的 存储空间               乘积 量化 的 思想 是 , 将 D 维 向量 分解成 m 个 D / m 维 向量 , 每个 子 空间 的 维度 是 D / m , 在 这 m 个子 空间 分别 进行 矢量 量化 , 那么 每个 子 空间 的 码字 数目 只要   2 ^ ( logk / m )   个 , 总 的 码字 数目 为   m   2 ^ ( logk / m )       典型 的 分治 法 啊 , 将 复杂度 降低 到   log   k       将 量化 误差 继续 量化 , 但是 是 用 相同 的 量化 函数       ADC           Searching   in   one   billion   vectors :   re - rank   with   source   coding           量化 误差 的 量化 函数 重新 拟合 一个       索引 阶段 :       将 向量 y 量化 函数 $ ( q _ c ) $ 和 误差 量化 函数 $ ( q _ r ) $ 从 数据 中 学习 得到         将 待 索引 的 向量 用 $ ( q _ c ) $ 量化 后 得到 码字       将 误差 用 $ ( q _ r ) $ 量化 后 , 关联 上 索引 向量               检索 阶段 :       通过 压缩 域 计算 距离 的 方式 找到 query 的 最近 k 个 列表 L       对 L 中 每 一个 向量 , 利用 误差 量化 结果 修正 距离 计算       从 L 中 根据 修正 后 的 距离 排序 获取 最近 邻 的 若干个 向量                   The   Inverted   Multi - Index           提出 倒排 多 索引 数据结构       每个 子 空间 不 单独 做 索引 , 而是 将 整个 空间 做 索引 , 这 将 导致 很多 key 是 空 的       rank 的 时候 不是 直接 对 多个 key 的 结果 排序 , 而是 利用 multi - sequence 算法 ， 高效 输出 top - k 检索 结果 。           Optimized   Product   Quantization                     其他 参考           很 好 的 博客 文章     https : / / yongyuan . name / blog / opq - and - hnsw . html        ", "tags": "machine-learning", "url": "/wiki/machine-learning/faiss.html"},
      
      
      {"title": "fast text Facebook开源词向量工具背后的理论", "text": "    Table   of   Contents           关于           论文 导读           Enriching   Word   Vectors   with   Subword   Information           摘要           导言           subword   模型           结论                   Bag   of   Tricks   for   Efficient   Text   Classification           模型 结构                                 关于       fast   text   开源 工具 见   https : / / github . com / facebookresearch / fastText   。     主要 涉及 的 两篇 文章 是 ：           P .   Bojanowski ,   E .   Grave * ,   A .   Joulin ,     T .   Mikolov   ,   Enriching   Word   Vectors   with   Subword   Information       A .   Joulin ,   E .   Grave ,   P .   Bojanowski ,     T .   Mikolov   ,   Bag   of   Tricks   for   Efficient   Text   Classification           论文 导读       Enriching   Word   Vectors   with   Subword   Information       摘要       论文 的 基本 思想 ： 由于 之前 的 词 向量 方法 都 是 以 词为 单位 进行 学习 ， 而 通常 一个 词 的 有 很多 不同 的 形态 ！ （ 不能 用 stem 将 形态 标准化 么 ？ ）     为此 ， 作者 提出 一种 基于 skip - gram 结构 的 新 方法 ， 词 被 表达 为 多个   n - gram   字母 的 bag 。     对 每 一个 n - gram 学习 一个 向量 ， 而词 向量 用 这些 n - gram 的 向量 求和 得到 。     这种 方法 非常 快 ！ 作者 在 5 种 不同 的 语言 中 训练 词 向量 ， 评估 了 在 word   similarity   and   analogy     tasks 上 的 效果 ！       导言       subword   模型       在 skip - gram 模型 中 ， 两个 词 之间 相似 度用 两个 词词 向量 的 内基 表示 $ ( score   =   s ( w , c )   =   w ^ T   c ) $ 。     这个 模型 将 一个 词 的 n - gram   g 单独 赋予 一个 向量 $ ( z _ g ) $ ， 而词 向量 用 该词 所有 的 n - gram 向量 之 和 来 表示 。     不同 词 之间 ， 共享 n - gram 向量 ！ 那么 $ ( s ( w , c )   =   \ \ sum   z _ g ^ T   v _ c   ) $ 。     词 本身 也 加到 了 n - gram 集合 中 ， 并且   词 向量 和 n - gram 向量 不同 ， 比如 as 作为 单词 和 作为 paste 的 n - gram 是 不 共享 向量 的 ！       n - gram   词典 的 设计 ， 论文 只 保留 了 长度 为 3 - 6 的 n - gram 。 为 开始 位置 和 结束 位置 添加 特殊字符 ， 用以 区分 前缀 和 后缀 。       限制 内存 ： 将 n - gram   hash   到   1   to   K ， 论文 中 K 取 200W ！       效率 提升 ： 最最 常 出现 的 P 个 单词 ， 不 使用 n - gram ！       结论       对   rare   words ,   morphologically   rich   languages   and   small   training   datasets   提升 明显 。       实现 性能比 skip - gram   baseline   慢 1.5 倍 。 105k   words / second / thread   VS   145k   words / second / thread   for   the   baseline 。       Bag   of   Tricks   for   Efficient   Text   Classification       论文 ： Bag   of   Tricks   for   Efficient   Text   Classification ,   Armand   Joulin ,   Edouard   Grave ,   Piotr   Bojanowski ,     Tomas   Mikolov         fastText   在 多 核 CPU 上 ， 训练 超过 10 亿 的 单词 ， 不到 10 分钟 ！     分类 50W 句子 ， 312K 个 类别 ， 只 需要 不到 1 分钟 ！       现有 的 神经网络 建模 句子 的 表达 ， 速度慢 ：           [ Bengio   et   al . 2003 ]       Ronan   Collobert   and   Jason     Weston .   2008 .   A   unified   architecture   for   natural   language     processing :   Deep   neural   networks   with   multitask     learning .   In   ICML .           线性 模型 建模 ， 速度 快 ：   ( Mikolov   et   al . ,   2013 ;   Levy   et   al . ,   2015 ) .     通过 加入 n - gram   信息 ， 可以 将 线性 模型 的 性能 提高 到 和 深度 模型 接近 ， 但是 速度 快 几个 量级 ！       模型 结构       baseline ： 将 句子 表达 为 bag   of   word ， 然后 训练 一个 线性 模型   LR   or   linearSVM 。     线性 模型 不能 共享 权值 ， 导致 泛化 能力 不 强 。               Hierarchical   softmax   优化 ， 略       N - gram   features ： n - gram   特征 加入 会 极大 影响 速度 ， 采用   hash   trick 解决 ！       trick 方案 论文 ： Strategies   for   training   large   scale   neural   network   language     models .          ", "tags": "machine-learning", "url": "/wiki/machine-learning/fast-text.html"},
      
      
      {"title": "FFM", "text": "    Table   of   Contents           关于           原理                 关于       相比 于 逻辑 回归 ， FM 是 一个 二阶 模型 。       原理       线性 模型 ： 逻辑 回归       $ $     \ \ phi ( w ,   x )   =   w ^ T   x   \ \ \ \     y   =   sigmoid ( \ \ phi ( w ,   x ) )     $ $       考虑 所有 二阶 组合       $ $     \ \ phi ( w ,   x )   =   x ^ T   W   x     $ $       上 式 计算 量 为 O ( n ^ 2 ) ， 通过 将 交叉 项 变换 为 平方差 ， 然后 合并 同类项 ， 可以 将 计算 量减少 到 O ( n ) 线性 时间 复杂度 。       FM ： 将   W   分解 为 两个 低 秩 矩阵 的 乘积 ！       $ $     \ \ phi ( w ,   x )   =   x ^ T   W ^ T   W   x ,   W   \ \ in   \ \ mathbb { R } ^ { k   \ \ times   n }   \ \ \ \     W   =   ( w _ 1 ,   w _ 2 ,   ... ,   w _ n ) ,   w _ i   \ \ in   \ \ mathbb { R } ^ { k }     $ $       FFM ： 对 每 一个 特征 ， 将 它 划分 到 某个 feild ， 一个 特征 和 不同 field 的 特征 交叉 时 ， 使用 不同 的   $ ( w ) $ 。       $ $     W _ { i , j }   =   w _ { i ,   f _ j } ^ T   w _ { j ,   f _ i }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ffm.html"},
      
      
      {"title": "FTRL", "text": "    Table   of   Contents               参考 ： Follow - the - Regularized - Leader   and   Mirror   Descent :   Equivalence   Theorems   and   L1   Regularization ， H .   Brendan   McMahan     Google ,   Inc .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ftrl.html"},
      
      
      {"title": "GAN: 生成对抗网络", "text": "    Table   of   Contents           GAN           理论 结论                   DCGAN           生成 自然 图像                   VAE - GAN           训练 技巧 2016           CGAN2014           Image - to - image   translation   with   conditional   adversarial   nets ( pix2pix )           GAN 单 图像 超 分辨   SRGAN           Hing   loss           参考                 GAN       论文 ： Generative   Adversarial   Nets ， an   J .   Goodfellow ,   Jean   Pouget - Abadie ∗ ,   Mehdi   Mirza ,   Bing   Xu ,   David   Warde - Farley ,   Sherjil   Ozair † ,   Aaron   Courville ,   Yoshua   Bengio ‡ ， 2014           生成 模型 G ， 对于 随机 noise   z ， 分布 $ ( p _ z ( z ) ) $ ， 生成 一个 样本   $ ( G ( z ;   \ \ theta _ g ) ) $       判别 模型 D ， 是 一个 常规 的 二 分类 模型 ， 输出 是 样本 来自 真实 数据 的 概率 ， $ ( D ( x ;   \ \ theta _ d ) ) $       极大 似然 估计 ， 来自 样本 的 认为 是 正例 ， 来自 生成 模型 的 认为 是 负例       minmax   value   function   $ ( V ( D ,   G ) ) $           $ $     \ \ min _ G   \ \ max _ D   V ( D ,   G )   =   \ \ mathbf { E }   _   { x   \ \ in   p _ { data } ( x ) } [ \ \ log   D ( x ) ]   +   \ \ mathbf { E }   _   { x   \ \ in   p _ { z } ( z ) } [ \ \ log ( 1 - D ( G ( z ) ) ) ]     $ $       训练 算法 ：               理论 结论           全局 最优 ： $ ( p _ g   =   p _ { data } ) $       对 固定 的 G ，           $ $     V ( G , D )   =   \ \ int _ x   dx   p _ { data } ( x )   \ \ log ( D ( x ) )   +   p _ g ( x )   \ \ log ( 1   -   D ( x ) )     $ $       容易 验证 ， 最优 的 D 满足       $ $     D ^ *   _   G ( x )   =   \ \ frac { p _ { data } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) }     $ $       最优   value   function 为       $ $     C ( G )   =   \ \ mathbf { E }   _   { x   \ \ in   p _ { data } ( x ) } [ \ \ log   \ \ frac { p _ { data } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) } ]   +   \ \ mathbf { E }   _   { x   \ \ in   p _ { z } ( z ) } [ \ \ log ( \ \ frac { p _ { g } ( x ) } { p _ { data } ( x )   +   p _ g ( x ) } ) ]   \ \ \ \     =   -   \ \ log   4   +   2   JSD ( p _ { data }   | |   p _ g )     $ $       JSD   是   Jensen – Shannon   divergence .   上 式 最优 的 结果 是   $ ( - \ \ log4 ) $ ， 当   $ ( p _ g   =   p _ { data } ) $   取得 。           算法   Algorithm   1   的 收敛性 , 结论 是 : 如果 G 和 D 有 足够 的 容量 ( 可以 拟合 任意 函数 ) , 那么 算法 1 可以 保证 $ ( p _ g ) $ 收敛 到 $ ( p _ { data } ) $           DCGAN           论文 :   Unsupervised   Representation   Learning   with   Deep   Convolutional   Generative   Adversarial   Networks       通过 GAN 进行 无 监督 学习 ,   学习 到 图像 的 层级 的 特征 表示       将 GAN   的 一部分 作为 特征 抽取 器 用于 监督 学习 的 任务       贡献 :       对模型 结构 做 了 一些 约束 , 使得 模型 在 大多数 情况 下 都 能 稳定 的 快速 收敛       生成器 的 算术 性质                   生成 自然 图像           之前 生成 的 自然 图像 都 比较 模糊       LAPGAN       几个 技巧 :       将 全 连接 层 去掉 不要       用   strided   convolution   替换 Pooling 操作                               图像 去 重 :   3072 - 128 - 3072   de - noising   dropout   regularized   RELU   autoencoder   on   32x32   downsampled   center - crops   of   training   examples .   得到 编码 后 , 二值化 作为 hash 值 , 用于 去 重 。       图像 算术 ,   对 输入 空间 Z 进行 算术 运算 ,   运算 的 结果 再 通过 生成 模型 生成 图像 。                   VAE - GAN       UNsupervised   Image - to - image   Translation   Networks       训练 技巧 2016           论文 :   Improved   Techniques   for   Training   GANs       代码 :     https : / / github . com / openai / improved - gan         GAN 与 纳什 均衡 的 关系 ?       梯度 交替 优化 的 问题 :   比如 优化 目标 是   xy ,   第一个 player 可以 修改 x , 目标 是 使 xy 尽可能 大 ,   第二个 player 可以 修改 y , 目标 是 使 xy 尽可能 小 。 均衡 解是   x = y = 0 ,   但是 基于 梯度 优化 的 会 不 收敛 。       特征 匹配 :   让 判别 器 的 中间层 的 特征 的 期望值 , 在 真实 数据 和 生成器 生成 的 数据 中 , 尽可能 相同       新 目标 函数   $ ( | | E _ { x   \ \ in   p _ { data } f ( x )   -   E _ { z   \ \ in   p _ { z } }   f ( G ( z ) ) } | | ) $   其中 f 代表 每个 网络 中间 的 某个 神经元 的 输出               GAN 模型 折叠 ,   总是 输出 一个 固定 的 结果 ,   我 还 真 碰到 过       标签 平滑 ,   用 0.1 , 0.9 代替 0 和 1 作为 标签 ,   可以 减少 对抗 样本 的 漏洞 ,   但是 会 使得 p _ data 为 0 的 区域 不为 0 ,   所以 作者 认为 用 单边 平滑 , 只 讲 1 用 一个 小于 1 的 数 替代           virtual   batch   normalization ,   不是 基于 当前 batch 的 统计 信息 , 而是 用 一个 固定 的 batch ? ?               半 监督 学习           以 K 分类 的 MNIST 数据 集为例 , 将 生成 模型 生成 的 样本 的 标签 作为 K + 1 ,   这样 交叉 熵 损失 函数 就 可以 分解 为 K 类 的 交叉 熵 损失 函数 和 GAN 的 判别 模型 损失 函数 之 和 。                   CGAN2014           基于 label 生成 , 而 不是 随意 生成       D ( x )   替换成   D ( x | y ) ,   G ( z )   替换成   G ( z | y )                   def       discriminator     (     x     ,       y     ) :               inputs       =       tf     .     concat     (     axis     =     1     ,       values     =     [     x     ,       y     ] )               D _ h1       =       tf     .     nn     .     relu     (     tf     .     matmul     (     inputs     ,       D _ W1     )       +       D _ b1     )               D _ logit       =       tf     .     matmul     (     D _ h1     ,       D _ W2     )       +       D _ b2               D _ prob       =       tf     .     nn     .     sigmoid     (     D _ logit     )                 return       D _ prob     ,       D _ logit         def       generator     (     z     ,       y     ) :               inputs       =       tf     .     concat     (     axis     =     1     ,       values     =     [     z     ,       y     ] )               G _ h1       =       tf     .     nn     .     relu     (     tf     .     matmul     (     inputs     ,       G _ W1     )       +       G _ b1     )               G _ log _ prob       =       tf     .     matmul     (     G _ h1     ,       G _ W2     )       +       G _ b2               G _ prob       =       tf     .     nn     .     sigmoid     (     G _ log _ prob     )                 return       G _ prob                 Image - to - image   translation   with   conditional   adversarial   nets ( pix2pix )           伯克利 AI 实验室       损失 函数 ,   x 是 草图 ,   z 是 随机噪声 ,   y 是 真实 的 图像     $ $     L _ { cGAN } ( G ,   D )   =   E _ { x , y } [ log   D ( x ,   y ) ]   +   E _ { x ,   z } [ 1   -   log   D ( x ,   G ( z ) ) ]     $ $                       在 GAN 损失 函数 的 基础 上 , 加上 L1 损失 函数 可以 帮助 减少 模糊 。 需要 将 x 和 y 关联 起来 , x 是 草图 , y 是 groundtruth , z 是 采样 值 , 为啥 还要 z ? 不 直接 训练 { x ,   y } , 用 L1 损失 函数 就行 。       解释 :   GAN 损失 函数 可以 让 模型 更具 泛化 能力 , 相当于 一种 正则           GAN 单 图像 超 分辨   SRGAN           Photo - Realistic   Single   Image   Super - Resolution   Using   a   Generative   Adversarial   Network       重新 设计 了 损失 函数 , 低 分辨 图像 通过 生成 网络 G 后 的 输出 与 groundtruth 之间 的 损失 函数       损失 函数 包括 内容 损失 和 对抗 损失       内容 损失 不但 包括 了 在 图像 空间 中 的 MSE , 还 包括 了 在 VGG 不同 特征 层上 了 MSE       对抗 损失 主要 是 G 生成 的 高清 图像 的 对数 损失   -   log [ D ( G ( I _ LR ) ) ]                   Hing   loss       参考           Generative   Adversarial   Networks   ( GANs ) ,   Ian   Goodfellow ,   OpenAI   Research   Scientist   NIPS   2016   tutorial       Introduction   to   GANs ,   Ian   Goodfellow ,   Staff   Research   Scientist ,   Google   Brain ,   CVPR   Tutorial   on   GANs      ", "tags": "machine-learning", "url": "/wiki/machine-learning/gan.html"},
      
      
      {"title": "Keras 深度学习库", "text": "    Table   of   Contents           关于           快速 入门   Step   by   step           配置           Sequential   model           Sequential   model   的 属性 和 方法           The   Merge   layer           编译 模型           训练           模型 评估           模型 预测           序列 模型 的 例子                   functional   API           多 输入 多 输出 模型           Shared   layers   共享 层           layer   node           一些 例子                   Layers           对 Layer 的 抽象           内置 的 核心 Layers                   TensorFlow   API           模型 可视化           sklearn   API                 关于       据说 pylearn2 停止 开发 了 ， 当时 觉得 pylearn2 虽然 编码 少 ， 但是 配置 和 文档 使用 不便 ， 而且 和 其他 库 的 融合     也 不 方便 。 后来 看到 有人 推荐   keras   ， 了解 了 一下 ， 发现 很 不错 。 他 的 底层 编译 采用   theano   ， 现在 也 加入     了   tensorflow   的 支持 。 并且 还 可以 与   scikit - learn   融合 ， 将   keras   的 模型 包装 成   scikit - learn       里面 的 模型 。 基于 这两点 ， 决定 学习 这个 库 ， 初试 了 一下 ， 感觉 很 不错 。       快速 入门   Step   by   step       快速 入门教程 参考 官方 文档   http : / / keras . io         Step1 .   创建   Sequential   模型 ， 通过   Sequential . add   方法 添加 层 。               from       keras . models       import       Sequential       model       =       Sequential     ( )         from       keras . layers       import       Dense     ,       Activation       model     .     add     (     Dense     (     output _ dim     =     64     ,       input _ dim     =     100     ) )       model     .     add     (     Activation     (     & quot ; relu & quot ;     ) )       model     .     add     (     Dense     (     output _ dim     =     10     ) )       model     .     add     (     Activation     (     & quot ; softmax & quot ;     ) )                 Step2 .   编译 模型 ， 可以 指定 优化 方法 和 损失 函数 等               model     .     compile     (     loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,       optimizer     =     &# 39 ; sgd &# 39 ;     ,                                 metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )                 Step3 .   调用   fit   方法 训练 模型 ， 这里 采用 随机 生成 的 数据               x _ train       =       np     .     random     .     randn     (     1000     ,     100     )       y _ labels       =       np     .     random     .     randint     (     0     ,     10     ,     size     =     (     1000     , ) )       y _ train       =       np     .     zeros     ( (     1000     ,     10     ) )       y _ train     [     range     (     1000     ) ,       y _ labels     ]       =       1         model     .     fit     (     x _ train     ,       y _ train     ,       nb _ epoch     =     5     ,       batch _ size     =     20     )                 Step   4 .   模型 预测 ， 这里 采用 另一组 随机 生成 的 数据               x _ test       =       np     .     random     .     randn     (     1000     ,     100     )       y _ labels       =       np     .     random     .     randint     (     0     ,     10     ,     size     =     (     1000     , ) )       y _ test       =       np     .     zeros     ( (     1000     ,     10     ) )       y _ test     [     range     (     1000     ) ,       y _ labels     ]       =       1         classes       =       model     .     predict _ classes     (     x _ test     ,       batch _ size     =     20     )       proba       =       model     .     predict _ proba     (     x _ test     ,       batch _ size     =     20     )                 配置       配置文件     ~ / . keras / keras . json             从 theano 切换 到 TensorFlow ， 将   backend   的 值 修改 为   tensorflow   即可 ， 默认 是   theano             Sequential   model       所谓   Sequential   模型 ， 就是 多个 layer 的 线性 堆叠 。 可以 通过 构造函数 创建 一个 多层 的 序列 模型 ，     也 可以 通过   . add ( )   方法 添加 层 。               ##   通过 构造函数 创建 模型 ， 参数 是   List [ Model ]       model       =       Sequential     ( [               Dense     (     32     ,       input _ dim     =     784     ) ,               Activation     (     &# 39 ; relu &# 39 ;     ) ,               Dense     (     10     ) ,               Activation     (     &# 39 ; softmax &# 39 ;     ) ,       ] )       ##   通过   . add ( )   添加 层       model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )       model     .     add     (     Activation     (     &# 39 ; relu &# 39 ;     ) )                     指定 输入 的 shape ， 通常 只有 第一层 必须 指定 ， 后面 的 层 都 可以 自动 获取       通过     input _ shape     指定 ， 不 需要 样本 大小 ， 见 例子       通过     batch _ input _ shape     指定 ， 需要 指定 样本 大小       2D   Layer   通过   input _ dim   指定 各维 大小 ， 3D   Layer 通过   input _ dim     和     input _ length     两个 参数 指定                           model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ shape     =     (     784     , ) ) )         model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       batch _ input _ shape     =     (     None     ,       784     ) ) )       #   note   that   batch   dimension   is   & quot ; None & quot ;   here ,       #   so   the   model   will   be   able   to   process   batches   of   any   size .         model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )           ##   3D       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       input _ shape     =     (     10     ,       64     ) ) )       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       batch _ input _ shape     =     (     None     ,       10     ,       64     ) ) )       model       =       Sequential     ( )       model     .     add     (     LSTM     (     32     ,       input _ length     =     10     ,       input _ dim     =     64     ) )                 Sequential   model   的 属性 和 方法           compile   编译       fit   拟合       evaluate   评估       predict   预测       predict _ classes   预测 类别       predict _ proba   预测 概率       train _ on _ batch   在 一个 batch 上 更新 模型 ， online   learning   ?       test _ on _ batch       predict _ on _ batch       fit _ generator   从 一个 generator 而 不是 矩阵 拟合 模型 ， 可以 用来 拟合 数据 保存 在 磁盘 上 的 数据       evaluate _ generator   从 generator 评估 模型           The   Merge   layer       可以 通过   merge   Layer   将 多个 输出 融合 到 一起 。 融合 的 模式 可以 选择 ：           sum   ( default ) :   element - wise   sum       concat :   tensor   concatenation .   You   can   specify   the   concatenation   axis   via   the   argument   concat _ axis .       mul :   element - wise   multiplication       ave :   tensor   average       dot :   dot   product .   You   can   specify   which   axes   to   reduce   along   via   the   argument   dot _ axes .       cos :   cosine   proximity   between   vectors   in   2D   tensors .                   from       keras . layers       import       Merge         left _ branch       =       Sequential     ( )       left _ branch     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )         right _ branch       =       Sequential     ( )       right _ branch     .     add     (     Dense     (     32     ,       input _ dim     =     784     ) )         ##   mode = &# 39 ; concat &# 39 ;   表示 将 两个 tensor 链接 成 一个 长 的 tensor       merged       =       Merge     ( [     left _ branch     ,       right _ branch     ] ,       mode     =     &# 39 ; concat &# 39 ;     )         final _ model       =       Sequential     ( )       final _ model     .     add     (     merged     )       final _ model     .     add     (     Dense     (     10     ,       activation     =     &# 39 ; softmax &# 39 ;     ) )         final _ model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,       loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     )       final _ model     .     fit     ( [     input _ data _ 1     ,       input _ data _ 2     ] ,       targets     )         #   we   pass   one   data   array   per   model   input                 也 可以 采用 自定义 的 函数 进行 融合 。               merged       =       Merge     ( [     left _ branch     ,       right _ branch     ] ,       mode     =     lambda       x     ,       y     :       x       -       y     )                 编译 模型       在 训练 一个 模型 之前 ， 需要 先 编译 ， 通过 模型 的   compile   方法 进行 。 这个 函数 接受 3 个 参数 ：           optimizer ， 预定 义 优化 器 字符串 或者   Optimizer   实例 。 预定 义 的 优化 器有 ：         sgd           rmsprop           adagrad           adadelta           adam           adamax           nadam                     loss ， 损失 函数 ， 字符串 或者   Theano / TensorFlow   symbolic   function ， 传入 两个 参数 ： y _ true , y _ pred ，     传出 一个 标量 。 下面 列出 一部分 ， 更 多 参考 官方 文档     http : / / keras . io / objectives /             mse   均方 误差       mae       mape       msle       squared _ hinge       hinge   SVM 用 的 损失 函数       binary _ crossentropy   对数 损失 函数       categorical _ crossentropy   多 类别 对数 损失 函数                   metrics 列表 ， 注意 是 列表 。 也 接收 字符串 和 用户 定义 函数 。           accuracy                           #   for   a   multi - class   classification   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )         #   for   a   binary   classification   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )         #   for   a   mean   squared   error   regression   problem       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; mse &# 39 ;     )                 训练       输入   np . ndarray   ， 调用   fit   训练 模型 。               #   train   the   model ,   iterating   on   the   data   in   batches       #   of   32   samples       model     .     fit     (     data     ,       labels     ,       nb _ epoch     =     10     ,       batch _ size     =     32     )                 模型 评估       调用     evaluate     方法 评估 。       模型 预测               classes       =       model     .     predict _ classes     (     X _ test     ,       batch _ size     =     32     )       proba       =       model     .     predict _ proba     (     X _ test     ,       batch _ size     =     32     )                 序列 模型 的 例子       序列 模型 的 更 多 例子 参考 官方 文档     http : / / keras . io / getting - started / sequential - model - guide /   。     这里 确实 有 很多 例子 ， 都 比较 短 。       functional   API       用来 解决   Sequential   模型   和   merge   无法 构建 的 复杂 模型 。           The   Keras   functional   API   is   the   way   to   go   for   defining   complex   models ,   such   as   multi - output   models ,   directed   acyclic   graphs ,   or   models   with   shared   layers .               一个   Layer   是 一个 callable 实例 ， 传入 一个 tensor ， 输出 一个 tensor       输入 tensor   和 输出 tensor   可以 用来 定义 一个 模型 ， 与   theano   的 函数 一样       上述 定义 的 模型 可以 和 Sequential   模型 一样 训练                   from       keras . layers       import       Input     ,       Dense       from       keras . models       import       Model         #   this   returns   a   tensor       inputs       =       Input     (     shape     =     (     784     , ) )         #   a   layer   instance   is   callable   on   a   tensor ,   and   returns   a   tensor       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     inputs     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       predictions       =       Dense     (     10     ,       activation     =     &# 39 ; softmax &# 39 ;     ) (     x     )         #   this   creates   a   model   that   includes       #   the   Input   layer   and   three   Dense   layers       model       =       Model     (     input     =     inputs     ,       output     =     predictions     )       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; categorical _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )       model     .     fit     (     data     ,       labels     )         #   starts   training                     所有 模型 都 是 callable ， 所以 可以 重用 一个 模型 。 利用   TimeDistributed   ， 可以 将 图像 的 模型 应用 到 video 处理 。                   x       =       Input     (     shape     =     (     784     , ) )       #   this   works ,   and   returns   the   10 - way   softmax   we   defined   above .       y       =       model     (     x     )         from       keras . layers       import       TimeDistributed         #   input   tensor   for   sequences   of   20   timesteps ,       #   each   containing   a   784 - dimensional   vector       input _ sequences       =       Input     (     shape     =     (     20     ,       784     ) )         #   this   applies   our   previous   model   to   every   timestep   in   the   input   sequences .       #   the   output   of   the   previous   model   was   a   10 - way   softmax ,       #   so   the   output   of   the   layer   below   will   be   a   sequence   of   20   vectors   of   size   10 .       processed _ sequences       =       TimeDistributed     (     model     ) (     input _ sequences     )                 多 输入 多 输出 模型       例如 下图                       from       keras . layers       import       Input     ,       Embedding     ,       LSTM     ,       Dense     ,       merge       from       keras . models       import       Model         #   headline   input :   meant   to   receive   sequences   of   100   integers ,   between   1   and   10000 .       #   note   that   we   can   name   any   layer   by   passing   it   a   & quot ; name & quot ;   argument .       main _ input       =       Input     (     shape     =     (     100     , ) ,       dtype     =     &# 39 ; int32 &# 39 ;     ,       name     =     &# 39 ; main _ input &# 39 ;     )         #   this   embedding   layer   will   encode   the   input   sequence       #   into   a   sequence   of   dense   512 - dimensional   vectors .       x       =       Embedding     (     output _ dim     =     512     ,       input _ dim     =     10000     ,       input _ length     =     100     ) (     main _ input     )         #   a   LSTM   will   transform   the   vector   sequence   into   a   single   vector ,       #   containing   information   about   the   entire   sequence       lstm _ out       =       LSTM     (     32     ) (     x     )         #   Here   we   insert   the   auxiliary   loss ,   allowing   the   LSTM   and   Embedding   layer       #   to   be   trained   smoothly   even   though   the   main   loss   will   be   much   higher   in   the   model .       auxiliary _ loss       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ,       name     =     &# 39 ; aux _ output &# 39 ;     ) (     lstm _ out     )         #   concat   lstm _ out   和   auxiliary _ input   作为 后续 模型 的 输入       auxiliary _ input       =       Input     (     shape     =     (     5     , ) ,       name     =     &# 39 ; aux _ input &# 39 ;     )       x       =       merge     ( [     lstm _ out     ,       auxiliary _ input     ] ,       mode     =     &# 39 ; concat &# 39 ;     )         #   we   stack   a   deep   fully - connected   network   on   top       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )       x       =       Dense     (     64     ,       activation     =     &# 39 ; relu &# 39 ;     ) (     x     )         #   and   finally   we   add   the   main   logistic   regression   layer       main _ loss       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ,       name     =     &# 39 ; main _ output &# 39 ;     ) (     x     )         #   创建 这个 多 输入 多 输出 模型       model       =       Model     (     input     =     [     main _ input     ,       auxiliary _ input     ] ,       output     =     [     main _ loss     ,       auxiliary _ loss     ] )         #   编译 模型       #   We   compile   the   model   and   assign   a   weight   of   0.2   to   the   auxiliary   loss .       #   To   specify   different   loss _ weights   or   loss   for   each   different   output ,       #   you   can   use   a   list   or   a   dictionary .   Here   we   pass   a   single   loss   as   the   loss   argument ,       #   so   the   same   loss   will   be   used   on   all   outputs .       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,       loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   loss _ weights     =     [     1 .     ,       0.2     ] )       #   训练 模型       model     .     fit     ( [     headline _ data     ,       additional _ data     ] ,       [     labels     ,       labels     ] ,                           nb _ epoch     =     50     ,       batch _ size     =     32     )         #   因为 我们 为 每 一个 输出 层 设置 了 name ， 所以 也 可以 通过 字典 而 不是 list 指定 参数       model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     {     &# 39 ; main _ output &# 39 ;     :       &# 39 ; binary _ crossentropy &# 39 ;     ,       &# 39 ; aux _ output &# 39 ;     :       &# 39 ; binary _ crossentropy &# 39 ;     } ,                                   loss _ weights     =     {     &# 39 ; main _ output &# 39 ;     :       1 .     ,       &# 39 ; aux _ output &# 39 ;     :       0.2     } )         #   and   trained   it   via :       model     .     fit     ( {     &# 39 ; main _ input &# 39 ;     :       headline _ data     ,       &# 39 ; aux _ input &# 39 ;     :       additional _ data     } ,                           {     &# 39 ; main _ output &# 39 ;     :       labels     ,       &# 39 ; aux _ output &# 39 ;     :       labels     } ,                           nb _ epoch     =     50     ,       batch _ size     =     32     )                 Shared   layers   共享 层       比如 训练 一个 模型 ， 预测 两个 tweets 是否 来自 同一个 人 ， 首先 可以 用 LSTM 将 两个 tweet 转换 为 两个 向量 ，     而 这个 LSTM 对 两个 tweet 都 能 用 ， 所以 可以 将 这个 LSTM 层 共享 。               from       keras . layers       import       Input     ,       LSTM     ,       Dense     ,       merge       from       keras . models       import       Model         tweet _ a       =       Input     (     shape     =     (     140     ,       256     ) )       tweet _ b       =       Input     (     shape     =     (     140     ,       256     ) )         #   this   layer   can   take   as   input   a   matrix       #   and   will   return   a   vector   of   size   64       shared _ lstm       =       LSTM     (     64     )         #   when   we   reuse   the   same   layer   instance       #   multiple   times ,   the   weights   of   the   layer       #   are   also   being   reused       #   ( it   is   effectively   * the   same *   layer )       encoded _ a       =       shared _ lstm     (     tweet _ a     )       encoded _ b       =       shared _ lstm     (     tweet _ b     )         #   we   can   then   concatenate   the   two   vectors :       merged _ vector       =       merge     ( [     encoded _ a     ,       encoded _ b     ] ,       mode     =     &# 39 ; concat &# 39 ;     ,       concat _ axis     = -     1     )         #   and   add   a   logistic   regression   on   top       predictions       =       Dense     (     1     ,       activation     =     &# 39 ; sigmoid &# 39 ;     ) (     merged _ vector     )         #   we   define   a   trainable   model   linking   the       #   tweet   inputs   to   the   predictions       model       =       Model     (     input     =     [     tweet _ a     ,       tweet _ b     ] ,       output     =     predictions     )         model     .     compile     (     optimizer     =     &# 39 ; rmsprop &# 39 ;     ,                                   loss     =     &# 39 ; binary _ crossentropy &# 39 ;     ,                                   metrics     =     [     &# 39 ; accuracy &# 39 ;     ] )       model     .     fit     ( [     data _ a     ,       data _ b     ] ,       labels     ,       nb _ epoch     =     10     )                 layer   node       当 输入 或者 输出 只有 一个 时 ， 可以 通过     . input _ shape     和     . get _ output ,   . output ,   . output _ shape     获取 输入输出 的 信息 。     当有 多个 的 时候 ， 需要 用     . get _ output _ at ,   . get _ input _ shape _ at     替代 。       一些 例子       一些 前沿 的 例子 ， 见   http : / / keras . io / getting - started / functional - api - guide /             inception   modeule       residual   connection                   from       keras . layers       import       merge     ,       Convolution2D     ,       Input         #   input   tensor   for   a   3 - channel   256x256   image       x       =       Input     (     shape     =     (     3     ,       256     ,       256     ) )       #   3x3   conv   with   3   output   channels   ( same   as   input   channels )       y       =       Convolution2D     (     3     ,       3     ,       3     ,       border _ mode     =     &# 39 ; same &# 39 ;     )       #   this   returns   x   +   y .       z       =       merge     ( [     x     ,       y     ] ,       mode     =     &# 39 ; sum &# 39 ;     )                     Shared   vision   model       Visual   question   answering   model       Video   question   answering   model           Layers       对 Layer 的 抽象       一个 layers 需要 有 以下 方法 ：           -     . get _ weights ( )       -     . set _ weights ( )       -     . get _ config ( )         一个 Layers 可以 通过 构造函数 创建 ， 也 可以 通过 config 创建 ， 采用   layer _ utils   包中 的   layer _ from _ config ( )   函数 。       对于 单 节点 的 layer ， 可以 通过 这些 属性 获取 输入输出             -     . input       -     . output       -     . input _ shape       -     . output _ shape         对于 多 节点 的 layer ， 则 需要 使用 这些 方法 ：             -     . get _ input _ at ( idx )       -     . get _ output _ at ( idx )       -     . get _ input _ shape _ at ( idx )       -     . get _ output _ shape _ at ( idx )         内置 的 核心 Layers             Dense     简单 的 全 连接 网络层 ， 至少 需要 一个     output _ dim     参数 ， 对于 非 输入 层 ， 会 自动 获得 输入 的 维数 ；     如果 是 输入 层 ， 还 需要 指定   input _ dim   参数 。 重要 的 参数 ：         activation     激活 函数 ， 默认 是 线性 函数 ，   a ( x ) = x   ， 即 没有 非线性 变换 ， 可以 指定 激活 函数 为 预定 义 的 非线性 函数 或者 自定义 的   element - wise   的 符号 函数 。 预定 义 函数 可以 通过 字符串 指定 ， 常用 的 有 ：   sigmoid ,   relu ,   tanh ,   softmax ,   hard _ sigmoid ,   softsign ,   softplus                   Activation ( name )     name   是 激活 函数 的 名字 。 既然 Dense 可以 指定 activation 参数 ， 为什么 还要 一个 激活 层 ？ ！         Dropout ( p )     Dropout   层 ， 参数 是 dropout 的 概率 。     Dropout :   A   Simple   Way   to   Prevent   Neural   Networks   from   Overfitting           Flatten ( )     将 多维 特征 展开 为 一维 特征 ， 不会 影响 样本 维度 。 常用 在 卷积 网络 。         Reshape ( hape )     shape : Tuple ， 将 特征 尺寸 reshape ， 不 影响 样本 维度 。         Permute ( dims )     dims : Tuple [ int , int , ... ]   将 维度 重新 变换 ， 如果 dims 是 两个 元素 ， 相当于 转置 。                   model       =       Sequential     ( )       model     .     add     (     Permute     ( (     2     ,       1     ) ,       input _ shape     =     (     10     ,       64     ) ) )       #   now :   model . output _ shape   = =   ( None ,   64 ,   10 )       #   note :   ` None `   is   the   batch   dimension                       RepeatVector ( n : Int )     将 输入 重复 n 次 ，                   model       =       Sequential     ( )       model     .     add     (     Dense     (     32     ,       input _ dim     =     32     ) )       #   now :   model . output _ shape   = =   ( None ,   32 )       #   note :   ` None `   is   the   batch   dimension         model     .     add     (     RepeatVector     (     3     ) )       #   now :   model . output _ shape   = =   ( None ,   3 ,   32 )                       Merge ( layers : List [ Layer ] ,   mode : String | Function ,   ... )     融合 层 。         Lambda ( func : Function ,   output _ shape : Tuple ,   args : Dict )     将 任意 符号 函数 应用 到 之前 的 层                   #   add   a   x   - & gt ;   x ^ 2   layer       model     .     add     (     Lambda     (     lambda       x     :       x       * *       2     ) )                       ActivityRegularization ( l1 = 0.0 ,   l2 = 0.0 )     添加 正则 项 ？ 怎么 添加 的 ？         Masking     不 懂 ， 貌似 跟 LSTM 层有 关系         Highway     也 不 懂 ， 貌似 跟 LSTM 层 有关         MaxoutDense     maxout   层 ， 是 线性 层 ， 不像   Dense   ， 不能 添加 激活 函数 ， 需要 在 后面 添加 激活 函数 层 。         TimeDistributedDense     不 懂 ， 貌似 在 RNN 中 有用           TensorFlow   API       模型 可视化       利用 模块     keras . utils . visualize _ util     里面 的 工具 函数 。             plot ( model ,   to _ file = filename ,   show _ shapes = False ,   show _ layer _ names = True )     保存 到 文件         model _ to _ dot ( model ,   show _ shapes = False ,   show _ layer _ names = True ) . create ( format = ' dot ' )     输出 为 dot 绘图 格式 ， 也 可以 指定   format   为 svg 等 格式 。 然后 利用   IPython . display     模块 输出 为 SVG 图像 。                   from       keras . utils . visualize _ util       import       plot       plot     (     model     ,     show _ shapes     =     True     )         from       IPython . display       import       SVG       from       keras . utils . visualize _ util       import       model _ to _ dot         SVG     (     model _ to _ dot     (     model     ,       show _ shapes     =     True     )     .     create     (     prog     =     &# 39 ; dot &# 39 ;     ,       format     =     &# 39 ; svg &# 39 ;     ) )                 sklearn   API       将   Keras   模型 封装 成 sklearn   的 API 。 两个 封装 API ， 分别 是 分类器 和 回归 器             keras . wrappers . scikit _ learn . KerasClassifier ( build _ fn = None ,   * * sk _ params )   ,   which   implements   the   sklearn   classifier   interface ,         keras . wrappers . scikit _ learn . KerasRegressor ( build _ fn = None ,   * * sk _ params )   ,   which   implements   the   sklearn   regressor   interface .             build _ fn     需要 返回 一个 模型 ， sk _ params   是 模型 参数 和   fit / predict   参数 ， 另外 需要 模型 所有 参数 都 存在 默认 参数 。     也 接受   fit ,   predict ,   predict _ proba ,   and   score   函数 的 参数 。       用 sklearn   API 封装 后 ， 就 可以 利用 sklearn 的 Gridsearch 等 工具 进行 调参 了 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/keras.html"},
      
      
      {"title": "Lambda Rank", "text": "    Table   of   Contents           背景           Rank   Net           参考                 背景       排序 问题 和 分类 回归 问题 本质 差异 在于 优化 的 目标 不同 ， 除此之外 ， 分类 回归 问题 的 模型 应该 都 能 用 。     但是 排序 问题 存在 一些 非 连续 优化 目标 ， 如 NDCG ， 直接 优化 该 目标 并 不 容易 。 Lambda   Rank 是 一种 近似 优化 方法 。       排序 问题 归根结底 也 是 对 每个 样本 打分 ， 目标 是 使得 应该 排 在 前面 的 样本 得分 更高 ！     通常 排序 会 有 一个 GROUP ， GROUP 之间 的 序 关系 没有 意义 ， 有 意义 的 是 GROUP 内部 的 序 关系 。     例如 搜索 排序 中 ， query 就是 一个 GROUP ， 对 给定 的 一个 GROUP ， 将 另外 一种 实体 e 进行 排序 。       一般 将 二元 组 ( q ,   e ) 看做 一个 样本 ， 如果 考虑 个性化 ， 还要 加上 用户 这个 实体 。 总而言之 ， 将 这些 实体 多元 组 看做 一个 样本 ，     提取 一些 列 特征 $ ( x _ i ) $ ， 然后 构建 一个 模型 $ ( f ) $ 对 它 进行 打分 $ ( f ( x ) ) $ 。       Rank   Net       将 样本 $ ( U _ i ) $ 排 在 $ ( U _ j ) $ 前面 的 概率 $ ( P ( U _ i   \ \ succ   U _ j ) ) $ 建模 成 得分 差值 的 sigmoid 函数 ！       $ $     P _ { ij }   =   P ( U _ i   \ \ succ   U _ j )   =   \ \ frac { 1 } { 1   +   e ^ { - \ \ sigma ( s _ i   -   s _ j ) } }   \ \ \ \     s _ i   =   f ( x _ i )     $ $       通过 对 所有 可 比较 的 样本 对 计算 此 概率 值 ， 然后 进行 极大 似然 估计 得到 损失 函数 。     由于 一个 样本 $ ( x _ i ) $ 会 同时 出现 在 多个 样本 对 中 ， 损失 函数 对 $ ( s _ i ) $ 的 梯度 $ ( \ \ lambda _ i ) $ 会 跟 多个 样本 对 有关 ，       $ $     \ \ lambda _ i   =   \ \ frac { \ \ partial   l } { \ \ partial   s _ i }   =   \ \ sum _ k   \ \ frac { \ \ partial   l _ k } { \ \ partial   s _ i }     $ $       参考           From   RankNet   to   LambdaRank   to   LambdaMART :   An   Overview      ", "tags": "machine-learning", "url": "/wiki/machine-learning/lambda-rank.html"},
      
      
      {"title": "Layer Normalization", "text": "    Table   of   Contents           关于           摘要           导言           不变性 分析           Geometry   of   parameter   space   during   learning           GLM   的 几何 分析                   实验 结果                 关于       见 论文     http : / / cn . arxiv . org / abs / 1607.06450         摘要       Batch   Normalization   难以 应用 在 RNN ， 因为 是 基于 输入 的 batch 进行 归一化 ，     而 RNN 输入 每次 都 只有 一个 ！       导言       分布式 神经网络 ：   Jeffrey   Dean   ,   Greg   Corrado ,   Rajat   Monga ,   Kai   Chen ,   Matthieu   Devin ,   Mark   Mao ,   Andrew   Senior ,   Paul   Tucker ,   Ke   Yang ,   Quoc   V   Le ,   et   al .   Large   scale   distributed   deep   networks .   In   NIPS ,   2012 .           In   addition   to   training   time   improvement ,   the   stochasticity   from   the   batch   statistics   serves   as   a   regularizer   during   training .           实际上 就是 讲   BN   对 batch 的 归一化 变成 了 对 神经网络 某 一层 进行 归一化 。           BN ： 一个 batch 使用 相同 的 均值 和 方差 ， 但是 不同 神经元 不同 ！ 对 batch 大小 有 要求 ， 不能 太小 ！ 对于 RNN ， 需要 在 每 一个 时隙 单独 保存 均值 和 方差 参数 ！       LN ： 同 一层 神经元 使用 相同 的 均值 和 方差 ， 但是 不同 样本 不同 ！ 对 batch 大小 没有 要求 ， 适用 于 RNN ， 不 需要 单独 保存 参数 ， 隐层 数目 不能 太小 吧           $ $     \ \ mu ^ l   =   \ \ frac { 1 } { H }   \ \ sum _ { i = 1 } ^ H   a _ i ^ l   \ \ \ \     \ \ sigma ^ l   =   \ \ sqrt { \ \ frac { 1 } { H }   \ \ sum _ { i = 1 } ^ H   ( a _ i ^ l - \ \ mu ^ l ) ^ 2   } .     $ $     H 代表 该层 的 神经元 个数 ！       对于 标准 的 RNN ， $ ( a ^ t   =   W _ { hh } h ^ { t - 1 }   +   W _ { xh }   x ^ t ) $ 。       $ $     h ^ t   =   f \ \ left [ \ \ frac { g } { \ \ sigma }   \ \ odot   ( a ^ t - \ \ mu ^ t ) +   b   \ \ right ]     $ $       $ ( b ,   g ) $   是 两个 参数 ， 需要 学习 ， 和 BN 中 类似 。       不变性 分析       BN ， LN ， WN ( weight   Normalization )   都 可以 用 下述 形式 表示 ：       $ $     h _ i   =   f ( \ \ frac { g _ i } { \ \ sigma _ i } ( a _ i - \ \ mu _ i )   +   b _ i )     $ $       对于 WN ， $ ( \ \ mu = 0 ,   \ \ sigma   =   | |   w | | _ 2 ) $                   Weight   matrix   re - scaling   and   re - centering :   对 权值 矩阵 进行 变换 ： $ ( W   \ \ rightarrow   \ \ delta   W   +   \ \ gamma ) $       Weight   vector   re - scaling   and   re - centering :   指对 权值 矩阵 的 某 一个 向量 做 上述 仿射变换 ， BN 和 WN 是 不 变得 ， 因为 他们 每个 神经元 都 有 自己 的 归一化 参数 ， 而 LN 是 变得 ， 因为 它 所有 神经元 共用 同 一组 规划 参数 。       Data   re - scaling   and   re - centering ： 指对 数据 集 的 归一化 操作 ， LN 对 单个 数据 的 rescaling 也 是 不变 的 ！ （ 因为 它 归一化 的 时候 仅 考虑 一个 样本 ！ ）           Geometry   of   parameter   space   during   learning       Under   the   KL   divergence   metric ,   the   parameter   space   is   a   Riemannian   manifold ？ ！ ！       KL 距离 下 的 黎曼 度规 可以 用 Fisher 信息 矩阵 的 二阶 近似       $ $     ds ^ 2   =   D _ { KL } ( P ( y | x ; \ \ theta )   | |   P ( y | x ;   \ \ theta   +   \ \ delta ) )   \ \ approx   \ \ frac { 1 } { 2 } \ \ delta ^ T   F ( \ \ theta )   \ \ delta   \ \ \ \     F ( \ \ theta )   =   \ \ mathbb { E } _ { x \ \ sim   P ( x ) ,   y \ \ sim   P ( y | x ) }   \ \ left [     \ \ frac { \ \ partial   \ \ log   P ( y | x ; \ \ theta ) } { \ \ partial   \ \ theta }   \ \ frac { \ \ partial   \ \ log   P ( y | x ; \ \ theta ) } { \ \ partial   \ \ theta } ^ T   \ \ right ]     $ $       GLM   的 几何 分析       广义 线性 模型 对数 似然 函数 表示 为 LN 的 输入 a 的 形式       $ $     \ \ log   P ( y | x ; w , b )   =   \ \ frac { ( a + b )   y   -   \ \ eta ( a   +   b ) } { \ \ phi }   +   c ( y ,   \ \ phi )     \ \ \ \     \ \ mathbb { E } [ y |   x ]   =   f ( a + b )     =     f ( w ^ T   x   +   b ) ,   \ \ \ \     Var [ y |   x ]   =   \ \ phi   f ' ( a + b )     $ $       这里   $ ( f ) $   是 函数 $ ( \ \ eta ' ) $ ， 为 GLM 的 均值 函数 。     假设 有 多个 独立 的 response 变量 ， $ ( y   =   [ y _ 1 ,   ... ,   y _ H ] ) $ ， 对应 多分 模型 参数 $ ( \ \ theta   =   [ w _ 1 ^ T ,   b _ 1 ,   ... ,   w _ H ^ t ,   b _ H ^ T ] ^ T ) $ ，     将 对数 似然 函数 代入 Fisher 信息 矩阵 ， 并 对 $ ( y ) $ 求 期望 ， 可 得       $ $     F ( \ \ theta )   =   \ \ mathbb { E } _ { x \ \ sim   P ( x ) }   \ \ left [     \ \ frac { Cov [ y | x ] } { \ \ phi ^ 2 }   \ \ otimes   \ \ left [   \ \ begin { matrix }                             xx ^ T   & amp ;   x   \ \ \ \                             x ^ T   & amp ;   1                     \ \ end { matrix }   \ \ right ]   \ \ right ]     $ $       采用 归一化 方法 后 ， 将 额外 的 参数 g 加到 参数 列表 中 ， $ ( \ \ theta   =   vec ( [ W ,   \ \ mathbf { b } ,   \ \ mathbf { g } ] ^ T ) ) $                   for   the   same   parameter   update   in   the   normalized   model ,   the   norm   of   the   weight   vector   effectively   controls   the   learning   rate   for   the   weight   vector .       have   an   implicit   “ early   stopping ”   effect   on   the   weight   vectors   and   help   to   stabilize   learning   towards   convergence .       Riemannian   metric   along   the   magnitude   of   the   incoming   weights   for   the   standard   GLM   is   scaled   by   the   norm   of   its   input ,   whereas   learning   the   gain   parameters   for   the   batch   normalized   and   layer   normalized   models   depends   only   on   the   magnitude   of   the   prediction   error .   Learning   the   magnitude   of   incoming   weights   in   the   normalized   model   is   therefore ,   more   robust   to   the   scaling   of   the   input   and   its   parameters   than   in   the   standard   model .           归一化 改变 曲率 ？ 还 学习 到 了 输入 幅度 ？ ！       实验 结果       在 多个 任务 上 进行 试验 ： 对 RNN 和 全 连接 网络 有效 ， 可以 加速 收敛 ， 但是 CNN 上 的 结果 不如 BN ， 作者 认为 CNN 神经元 之间 的 统计 特性 相差太大 导致 的 ，     因为 每个 神经元 都 只 链接 上 一层 一小块 区域 ， 从而 导致 同 一层 的 神经元 统计 特性 相差 很大 ！ 但是 作者 没有 报道 具体 结果 ， LN 比 BN 差 多少 也 不得而知 。           Order   embeddings   of   images   and   language ：   Ivan   Vendrov ,   Ryan   Kiros ,   Sanja   Fidler ,   and   Raquel   Urtasun .   Order - embeddings   of   images   and   language .     ICLR ,   2016 .       Teaching   machines   to   read   and   comprehend ： Karl   Moritz   Hermann ,   Tomas   Kocisky ,   Edward   Grefenstette ,   Lasse   Espeholt ,   Will   Kay ,   Mustafa   Suleyman ,     and   Phil   Blunsom .   Teaching   machines   to   read   and   comprehend .   In   NIPS ,   2015 .       Skip - thought   vectors ： Ryan   Kiros ,   Yukun   Zhu ,   Ruslan   R   Salakhutdinov ,   Richard   Zemel ,   Raquel   Urtasun ,   Antonio   Torralba ,   and   Sanja     Fidler .   Skip - thought   vectors .   In   NIPS ,   2015 .       Modeling   binarized   MNIST   using   DRAW ： K .   Gregor ,   I .   Danihelka ,   A .   Graves ,   and   D .   Wierstra .   DRAW :   a   recurrent   neural   network   for   image   generation .   arXiv : 1502.04623 ,   2015 .       Handwriting   sequence   generation ： Marcus   Liwicki   and   Horst   Bunke .   Iam - ondb - an   on - line   english   sentence   database   acquired   from   handwritten   text   on   a   whiteboard .   In   ICDAR ,   2005 .       Permutation   invariant   MNIST      ", "tags": "machine-learning", "url": "/wiki/machine-learning/layer-normalization.html"},
      
      
      {"title": "Learn to Rank", "text": "    Table   of   Contents           关于           PointWise   方法                 关于       Learn   to   Rank           Learning   to   Rank   for   Information   Retrieval ,   Tie - Yan   Liu           PointWise   方法       把 排序 问题 当做 单 样本 回归 或 分类 问题 ，  ", "tags": "machine-learning", "url": "/wiki/machine-learning/l2r.html"},
      
      
      {"title": "machine learning resource", "text": "    Table   of   Contents           Conference           Group                 Conference           NIPS     https : / / papers . nips . cc /         ICML     http : / / icml . cc /         CVPR , ICCV     http : / / www . cv - foundation . org /         ACL       AAAI     http : / / www . aaai . org / Awards / paper . php         ECCV           Group           Deepmind :     https : / / deepmind . com / research / publications /         LAMDA :   Nanjing   Univsity   & amp ;         https : / / cs . nju . edu . cn / zhouzh / zhouzh . files / publication / publication . htm           http : / / lamda . nju . edu . cn / CH . Pub . ashx                 LeCun :     https : / / arxiv . org / find / cs / 1 / au : + LeCun _ Y / 0 / 1 / 0 / all / 0 / 1        ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-link.html"},
      
      
      {"title": "Machine Learning: A probabilistic perspective", "text": "    Table   of   Contents           关于           Logistic   Regression           模型 拟合           Bayesian   logistic   regression           Drivation   of   BIC           高斯 近似           后验 预测           Online   learning                   广义 线性 模型 和 指数 族   GLM           指数 族 分布           重要性 ：           定义           极大 似然 估计           贝叶斯 统计           最大 熵 原理                   广义 线性 模型   GLM           Probit   regression           Multi - task   learning           Learn   to   rank                   Latent   linear   models           因子分析           混合 因子分析   ( Hinton   et   al .   1997 )           因子分析 的 EM 算法                   PCA           Classical   PCA           SVD           PPCA   ( Tipping   and   Bishop   1999 )                         关于       这 本书 从 概率 的 角度 阐述 机器 学习 相关 理论 ， 角度 比较 有意思 ！ 贝叶斯 学派 ？ ！       Logistic   Regression           生成 模型 ， 建模 $ ( p ( x ,   y ) ) $ ， 然后 利用 贝叶斯 公式 得到 后验 概率 $ ( p ( y | x ) ) $ .       判别 模型 ， 直接 建模 $ ( p ( y | x ) ) $ .           逻辑 回归 模型 ： $ (     p ( y | x ,   w )   =   Bernoulli ( y |   sigm ( w ^ T   x )       ) $       模型 拟合       极大 似然 MLE ：       负 对数 似然 函数 ( 也 叫 交叉 熵 )       $ $     NLL ( w )   =   -   \ \ sum _ { i = 1 } ^ N   \ \ left [         y _ i     \ \ log   \ \ mu _ i     ( 1 - y _ i )   \ \ log   ( 1 - \ \ mu _ i )           \ \ right ]     $ $       另外 一种 写法 ： $ ( y   \ \ in   \ \ { - 1 ,   + 1 \ \ } ) $ ， 那么 $ ( p ( y )   =   \ \ frac { 1 } { 1 + \ \ exp ( y   w ^ T   x ) } ) $ 。     所以       $ $     NLL ( w )   =   -   \ \ sum _ { i = 1 } ^ N   \ \ log ( 1 + \ \ exp ( y _ i   w ^ T   x ) )     $ $       closed   form :   由 常数 ， 变量 ， 通过 四则运算 ， n 次方 ， 指数 ， 对数 ， 三角函数 ， 反 三角函数 ， 经过 有限 次 运算 和 符合 得到 的 表达式 （ 通常 没有 极限 运算 ）       一个 问题 被 称为 可解 的 （ P 问题 ） ， 表示 可以 用闭 形式 解决 ！ ？       很多 累积 分布 没有 闭 形式 ， 但是 可以 通过 误差 函数 、 gamma 函数 表达 ！               NLL   无法 表达 为闭 形式 ？ 所以 ， 采用 数值 优化 ， 需要 计算 梯度 和 海森 矩阵 ！       $ $     g   =   X ^ T   ( \ \ mu   -   y )   \ \ \ \     H   =   \ \ sum _ i   \ \ mu _ i   ( 1 - \ \ mu _ i )   x _ i   x _ i ^ T   =   X   S   X ^ T     $ $       H   是 正定 的 ， 所以 NLL 是 强 凸函数 ， 且 存在 唯一 解 。       最速 下降 ， 学习 率 通过 线性 搜索 寻找 。 精确 线性 搜索 的 问题 ： zig - zag ， 当 初始 梯度 和 末端 梯度 正交 ， 现象 明显 。     $ ( \ \ eta _ k   =   \ \ arg   \ \ min _ { \ \ eta & gt ; 0 }   f ( \ \ theta   +   \ \ eta   d ) ) $   d 是 下降 的 负 梯度方向 。     精确 的 线性 搜索 会 使得 $ ( d ^ T   g   =   0 ) $ ， g 是 搜索 到 的 最佳 位置 ， 函数 f 的 梯度 ， 要么 g = 0 ， 要么 互相 垂直 ！       减少   zig - zag   的 方法 ， 动量 方法   momentum   ( $ ( \ \ theta _ k   -   \ \ theta _ { k - 1 } ) $ ) 。 heavy   ball   method       $ $     \ \ theta _ { k + 1 }   =   \ \ theta _ k   -   \ \ eta _ k   g _ k   +   \ \ mu _ k   ( \ \ theta _ k   -   \ \ theta _ { k - 1 } )   \ \ \ \     0   \ \ le   \ \ mu _ k   \ \ le   1     $ $       另外 一种 方法 是 共轭 梯度 法 ， 二次 目标 函数 $ ( f   =   \ \ theta   A ^ T   \ \ theta ) $ 常用 在 线性系统 ， 非线性 系统 不 常用 ？ ！       牛顿 法 ， 二阶 方法 ， 步长 $ ( d _ k   =   - H _ k ^ { - 1 }   g _ k ) $ 。 要求 目标 函数 强凸 ， 海森 矩阵 才 会 可逆 ！       解决方案 ： 直接 用 共轭 梯度 法 求解 最优 步长 $ ( H   d   =   - g ) $ 。 线性方程 等价 于 无约束 优化 二次 规划 $ ( 1 / 2   | |   Hd   +   g | | ^ 2 ) $ 。     利用 共轭 梯度 （ CG ） ， 迭代 到 负 曲率 的 地方 停止 迭代 ！       iteratively   reweighted   least   squares   or   IRLS   优化   for   逻辑 回归 。     在 牛顿 法中 ，       $ $     w _ { k + 1 }   =   w _ k   -   H ^ { - 1 } g _ k   \ \ \ \             =   ( X ^ T   S _ k   X ) ^ { - 1 }   X ^ T   S _ k   z _ k   \ \ \ \     z _ k   =   Xw _ k   +   S _ k ^ { - 1 } ( y - \ \ mu _ k )     $ $       恰好 是 带权 最小 二乘 问题   $ ( ( z   -   X ^ T   w ) ^ T   S   ( z   -   X ^ T   w ) ) $   的 解 。       拟 牛顿 法 ： BFGS ， L - BFGS       MAP   估计 导致 正则 项 ， 高斯 先验 ： l2 正则 。       多 分类 逻辑 回归 ， 最大 熵 模型       Bayesian   logistic   regression       需要 计算   $ ( p ( w | \ \ mathcal { D } ) ) $ ， 从而 得到 $ ( p ( y | x ,   \ \ mathcal { D } )   =   p ( y | x ,   w )   p ( w | \ \ mathcal { D } ) ) $ 。       问题 ： 逻辑 回归 没有 一个 方便 的 共轭 先验 （ 和 似然 函数 形式 相同 的 分布 ） 。     一些 解决方案 ： MCMC ， variational   inference ， expectation   propagation ( Kuss   and   Rasmussen   2005 )       拉普拉斯 近似 ， 高斯 近似 ： 假定 （ 对 数据 本身 建立 生成 模型 $ (   \ \ theta   = & gt ;   \ \ mathcal { D }     ) $ ） 后验 概率 为       $ $     p ( \ \ theta |   \ \ mathcal { D } )   =   \ \ frac { 1 } { Z }   e ^ { - E ( \ \ theta ) }     $ $       $ ( E ( \ \ theta ) ) $   叫做 能量 函数 ， 等于 $ (   -   p ( \ \ theta ,   \ \ mathcal { D } )   ) $ ， 而 $ ( Z   =   p ( \ \ mathcal { D } ) ) $ 。     利用 泰勒 级数 ， 将 能量 函数 在 最低 能量 值 $ ( \ \ theta * ) $ 附近 展开 到 二阶 项 。       $ $     E ( \ \ theta )   \ \ approx   E ( \ \ theta * )   +   ( \ \ theta   -   \ \ theta * ) ^ T   g   +   \ \ frac { 1 } { 2 }   ( \ \ theta   -   \ \ theta * ) ^ T   H   ( \ \ theta   -   \ \ theta * )     $ $       g 是 能量 函数 在 最低 能量 位置 的 梯度 ， 等于 0 ， 所以       $ $     \ \ hat { p } ( \ \ theta |   \ \ mathcal { D } )   \ \ approx   \ \ frac { 1 } { Z }   e ^ { - E ( \ \ theta * ) }   \ \ exp   \ \ left [     - \ \ frac { 1 } { 2 }   ( \ \ theta   -   \ \ theta * ) ^ T   H       ( \ \ theta   -   \ \ theta * )   \ \ right ]   \ \ \ \     =   \ \ mathcal { N } ( \ \ theta   |   \ \ theta *   ,   H ^ { - 1 } )   \ \ \ \     Z   =   p ( \ \ mathcal { D } )   \ \ approx   e ^ { - E ( \ \ theta * ) }   ( 2 \ \ pi ) ^ { D / 2 }   | H | ^ { - 1 / 2 }     $ $       最后 一个 式子 是 对 边际 分布 的     拉普拉斯 近似     ， 因此 ， 把 第一个 式子 称作 对 后验 概率 的 拉普拉斯 近似 （ 其实 更 应该 称作     高斯 近似   ） 。     高斯 近似 通常 是 一个 较 好 的 近似 ， 随着 样本数 目的 增加 ， 中心 极限 定理 可以 保证 ！ （   saddle   point   approximation     in   physics )       Drivation   of   BIC       利用 高斯 近似 ， 边际 分布 的 对数 似然 函数 为 ( 去掉 不 相关 常数 ) ：       $ $     p ( \ \ mathcal { D } )   \ \ approx   \ \ log   p ( \ \ mathcal { D } | \ \ theta ^ *   )   +   \ \ log   p ( \ \ theta ^ * )   -   \ \ frac { 1 } { 2 }   \ \ log   | H |     $ $       $ (   p ( \ \ mathcal { D } | \ \ theta ^ *   )   ) $   常 称作     Occam   factor   ， 用作 模型 复杂度 的 度量 ， 如果 假定 均匀分布 先验 ， 即 $ (   p ( \ \ theta ^ )   \ \ varpropto   1       ) $ ， 那么 可以 简化 为 极大 似然 估计 ， 可以 用 MLE 的 值 $ (   \ \ hat { \ \ theta }   ) $ 替换 $ (   \ \ theta ^ *     ) $ 。       对于 第三项 ， 有   $ (   H   =   \ \ sum _ i   H _ i ,   H _ i   =   \ \ nabla   \ \ nabla   \ \ log   p ( \ \ mathcal { D } _ i   |   \ \ theta )   ) $ ，     假定 每个 $ (   H _ i   =   \ \ hat { H }   ) $ ， 是 一个 常数 矩阵 （ 此时 样本分布 是 什么 情况 ？ ） ， 那么       $ $     \ \ log   | H |   =   \ \ log   | N   \ \ hat { H } |   =   \ \ log   ( N ^ D   \ \ hat { H } )   \ \ \ \             =   D   \ \ log   N   +   \ \ log   | \ \ hat { H } |     $ $       其中 $ ( D ) $ 是 参数 空间 的 维度 ， H 是 满 秩 的 。 最后 一项 与 N 无关 ， 可以 作为 常数 丢弃 ！ 那么 可得 BIC   score       $ $     \ \ log   p ( \ \ mathcal { D } )   =   \ \ log   p ( \ \ mathcal { D } | \ \ hat { \ \ theta } )   -   \ \ frac { D } { 2 }   \ \ log   N     $ $       高斯 近似       近似 先验   $ (     p ( w )   =   \ \ mathcal { N } ( 0 ,   V _ 0 )       ) $ ， 后验 概率 为       $ $     p ( w   |   \ \ mathcal { D } )   \ \ approx   \ \ mathcal { N } ( w | \ \ hat { w } ,   H ^ { - 1 } )     $ $       其中 $ ( \ \ hat { w } ) $ 是 极大 似然 估计值 ， $ (   \ \ hat { w }   =   \ \ arg   \ \ min _ w   E ( w )   ) $ ， $ (   E ( w )   =   -   \ \ log   p ( \ \ mathcal { D }   |   w )   -   \ \ log   p ( w )   ) $ 。     而   $ ( H   =   \ \ nabla   \ \ nabla   E ( w )   | _ { w ^ * } ) $ 。       也就是说 我们 之前 用 极大 似然 估计 出来 的 参数 ， 是 参数 后验 分布 的 期望值 ！       当 数据 是 线性 可分 的 情况 下 ， 极大 似然 估计 的 模型 参数 $ ( w ) $ 将 可以 是 任意 大 的 向量 ！ sigmoid 函数 就 变成 了 阶跃 函数 ！       后验 预测       没有 正则 项 的 预测 $ ( p ( y | x ,   \ \ hat { w } ) ) $ 是 对 参数 的 极大 似然 估计 。     带 正则 项 的 预测 $ ( p ( y | x ,   \ \ hat { w } ) ) $   是 对 参数 的 最大 后验 轨迹 。     这 两者 预测 出来 的 数值 都 只是 在 一个 参数 点 的 条件 概率 ！ ！     这个 点 可以 是 极大 似然 估计 出来 的 ， 也 可以 是 最大 后验 估计 出来 的 ！     如果 要 得到 在 数据 集上 的 后验 概率 ， 需要 计算       $ $     p ( y |   x ,   \ \ mathcal { D } )   =   \ \ int   p ( y | x , w )   p ( w |   \ \ mathcal { D } )   dw     $ $       但是 ， 这个 积分 没 难以 求解 ， 一个 简单 的 近似 是 用 w 的     后验 均值   ！       $ $     p ( y |   x ,   \ \ mathcal { D } )   \ \ approx   p ( y |   x ,   \ \ mathbb { E } [ w ] )     $ $       $ ( \ \ mathbb { E } [ w ] ) $ 称作 贝叶斯 点 ！                 蒙特卡洛 近似   ， 即 随机 采样 一些 w ， 近似 积分 ！       $ $     p ( y = 1 |   x ,   \ \ mathcal { D } )   \ \ approx   \ \ frac { 1 } { S }   \ \ sum _ { s = 1 } ^ S   sigm ( ( w ^ s ) ^ T   x )     $ $       $ ( w ^ s   \ \ sim   p ( w | \ \ mathcal { D } ) ) $   采样 自后验 分布 ！ 高斯 近似 下 ， 就 相当于 采样 高斯分布 ！     采样 多个 样本 时 ， 不但 可以 得到 较 好 的 概率 估计值 ， 可以 得到 输出 概率 的 置信区间 ！ ！ ！         probit   近似     当 w 的 后验 分布 用 高斯 近似 $ ( \ \ mathcal { N } ( w | m _ N ,   V _ N ) ) $ 时 ， 可以       $ $     p ( y = 1 |   x ,   \ \ mathcal { D } )   \ \ approx   \ \ int   sigm ( w ^ T   x )   p ( w | \ \ mathcal { D } )   dw   \ \ \ \                     =   \ \ int   sigm ( a )   \ \ mathcal { N } ( a | \ \ mu _ a ,   \ \ sigma _ a ^ 2 )       \ \ \ \     a   =   w ^ T   x   \ \ \ \     \ \ mu _ a   =   \ \ mathbb { E } [ a ]   =   m _ N ^ T   x   \ \ \ \     \ \ sigma _ a ^ 2   =   x ^ T   V _ N   x .     $ $       将 sigmoid 函数 用 probit 函数 近似 ， probit 函数 是 标准 正态分布 的 累积 分布 函数 ！     和 sigmoid 函数 非常 接近 ， 见上图 ！     采用 这种 近似 后 ， 前述 积分 可以 得到 解析 表达式 ！       $ $     \ \ int   sigm ( a )   \ \ mathcal { N } ( a | \ \ mu _ a ,   \ \ sigma _ a ^ 2 )   \ \ approx   sigm ( k ( \ \ sigma ^ 2 ) \ \ mu )   \ \ \ \     k ( \ \ sigma ^ 2 )   =   ( 1 + \ \ pi   \ \ sigma ^ 2 / 8 ) ^ { - \ \ frac { 1 } { 2 } } .     $ $       此时 ， 由于 k 小于 1 ， 因此 相当于 在 极大 似然 估计 的 概率 上 ， 在 横轴 进行 缩放 ！ 通过 $ ( \ \ sigma ) $ 控制 过 拟合 ？ ！     但是 判决 面 并 没有 变 ！         Residul   analysis ( outlier   detection )       在 回归 问题 中 ， 计算 残差 $ ( r _ i   =   y _ i   -   \ \ hat { y _ i } ) $ ， 其中 模型 估计值 $ ( \ \ hat { y _ i }   =   \ \ hat { w } ^ T   x _ i ) $ .     该 残差 应该 服从 正态分布 $ ( \ \ mathcal { N } ( 0 ,   \ \ sigma ^ 2 ) ) $ ， 从而 可以 通过   qq - plot   得到 离异 值 ？ ！       分类 问题 可以 采用 另外 的 方法 ！       Online   learning       广义 线性 模型 和 指数 族   GLM       指数 族 分布       指数 族 分布 ： 高斯 ， Bernoulli ,     gamma 分布 等 ！     非 指数 族 例子 ： Student   t   分布 ,   均匀分布       重要性 ：           finite - sized   sucient   statistics .               under   certain   regularity   conditions ,   the   exponential   family   is   the   only   family   of   distributions   with   finite - sized   sucient   statistics ,   meaning   that   we   can   compress   the   data   into   a   fixed - sized   summary   without   loss   of   information .   This   is   particularly   useful   for   online   learning ,   as   we   will   see   later .                   唯一 存在 共轭 先验 的 分布 族               least   set   of   assumptions                   The   exponential   family   can   be   shown   to   be   the   family   of   distributions   that   makes   the   least   set   of   assumptions   subject   to   some   user - chosen   constraint           定义       概率密度函数       $ $     p ( x )   =   h ( x )   \ \ exp ( \ \ theta ^ T   \ \ phi ( x )   -   A ( \ \ theta ) )   \ \ \ \             =   \ \ frac { 1 } { Z ( \ \ theta ) } h ( x )   e ^ { \ \ theta ^ T   \ \ phi ( x ) }     $ $       $ ( Z ) $ 是 归一化 因子 ， $ ( A ) $ 是 对数 归一化 因子 ， $ ( \ \ theta ) $ 被称作 自然 参数 。     $ ( \ \ phi ( x )   \ \ in   \ \ mathcal { R } ^ d ) $ 被 称为 充分 统计 向量 ( vector   of   sucient   statistics ) 。     如果 $ ( \ \ phi ( x ) = x ) $ ， 称为 自然 指数 族 。 $ ( \ \ theta   \ \ rightarrow   \ \ eta ( \ \ theta ) ) $ 。       Log   partition   function   性质 ：           $ ( A ( \ \ theta ) ) $ 的 一阶导 是 $ ( \ \ phi ( x ) ) $ 的 期望 ， 而 二阶 导是 其 协方差 矩阵 ！ 即 对数 矩母 函数 ！           极大 似然 估计       N 个 iid 的 样本 的 对数 似然 函数 为 ：       $ $     l ( \ \ theta )   =   \ \ theta ^ T   \ \ sum _ i   \ \ phi ( x _ i )   -   N   A ( \ \ theta )   +   constant     $ $       第一项 是 $ ( \ \ theta ) $ 的 线性 函数 ， 第二项 由于 其 二阶 导是 协方差 矩阵 ， 非负 ， 所以 也 为 凸函数 ， 因此 指数 族 N 个 独立 同 分布 样本 的 对数 似然 函数 为 凸函数 ，     因而 通过 极大 似然 估计 参数 ， 可以 很 容易 得到 最优 解 ！ 并且 由       $ $     \ \ nabla _ { \ \ theta }   l   =   0   \ \ \ \     \ \ nabla _ { \ \ theta }   A   =   E   \ \ phi ( X )     $ $       可 得 期望 匹配 条件       $ $     E   \ \ phi ( X )   =   \ \ frac { 1 } { N }   \ \ sum _ i   \ \ phi ( x _ i )     $ $       例如 对   Bernoulli   分布 样本 ， $ ( \ \ phi ( X ) =   \ \ mathbb { I } ( X = 1 ) ) $ ， 因此 要求 模型 预测 的 期望值 要 和 样本均值 相同 ！         Pitman - Koopman - Darmois   theorem     理论 ： 在 一些 约束 下 ， 指数 族 是 为 一个 分布 族 ， 其 充分 统计 量 是 有限 的 （ 不随 样本 个数 增加 而 增加 ） ！     例如 这里 充分 统计 量 数目 不随 N 增长 ， 一直 为   N   和   $ ( \ \ sum _ i   \ \ phi ( x _ i ) ) $ ！       贝叶斯 统计       似然 函数 具有 如下 形式 ：       $ $     p ( \ \ mathcal { D } | \ \ theta )   \ \ varpropto   g ( \ \ theta ) ^ N   \ \ exp ( \ \ eta ( \ \ theta ) ^ T   s _ N )     $ $       这里 $ ( s _ N ) $ 是 N 个 样本 之 和 ， 而 在 canonical   parameters 形式 下 为 ：       $ $     p ( \ \ mathcal { D } | \ \ eta )   \ \ varpropto   g ( \ \ eta ) ^ N   \ \ exp ( N   \ \ eta ^ T   \ \ bar { s }   -   N   A ( \ \ eta ) )     $ $       选择 共轭 先验 与 似然 函数 具有 相同 的 形式 ， 将会 使得 贝叶斯 分析 简化 ， 指数 族 是 唯一 具有 这种 性质 的 分布 族 ！ （ 不是 因为 封闭 形式 的 原因 么 ？ ）       $ $     p ( \ \ eta | v _ 0 ,   \ \ bar { \ \ tau _ 0 } )   \ \ varpropto   g ( \ \ eta ) ^ N   \ \ exp ( v _ 0   \ \ eta ^ T   \ \ bar { \ \ tau _ 0 }   -   v _ 0   A ( \ \ eta ) )     $ $       那么 后验 分布 将 为       $ $     p ( \ \ theta | \ \ mathcal { D } )   =   p ( \ \ theta |   v _ 0   + N   ,   \ \ tau _ 0   +   s _ N )     \ \ \ \     p ( \ \ eta | \ \ mathcal { D } )   =   p ( \ \ eta |   v _ 0   + N   ,   \ \ frac { v _ 0   \ \ bar { \ \ tau _ 0 }   +   N   \ \ bar { s } } { v _ 0   +   N } )     $ $       最大 熵 原理       最大 熵 原理 是 说 ， 要 选择 分布 使得 熵 最大 ， 在 约束条件 ： 对 一些 特殊 函数 （ 特征 ） 期望值 和 样本均值 相同 ！       $ $     \ \ max   - \ \ sum _ x   p ( x )   \ \ log ( p ( x ) )   \ \ \ \     s . t .   \ \ sum _ x   f _ k ( x ) p ( x )   =   F _ k ,   k = 1 , ... , m   \ \ \ \     \ \ sum _ x   p ( x ) = 1     $ $       利用 拉格朗 日 对偶 ， 及 KKT 条件 ， 易知 分布 要 满足       $ $     p ( x )   =   \ \ frac { 1 } { Z }   \ \ exp ( \ \ lambda _ k   f _ k ( x ) )     $ $       最大 熵 分布 是 指数 族 分布 ！ ！ 这个 分布 也 叫   Gibbs   分布 ！       广义 线性 模型   GLM       GLM ： 均值 函数 是 输入 的 线性组合 ， 然后 加上 一个 非线性 变换 ！ 输出 是 指数 族 分布 的 模型 ！       设 标量 响应 变量 满足 分布 ：       $ $     p ( y _ i |   \ \ theta ,   \ \ sigma ^ 2 )   =   \ \ exp   \ \ left [ \ \ frac { y _ i   \ \ theta   -   A ( \ \ theta ) } { \ \ sigma ^ 2 }   +   c ( y _ i ,   \ \ sigma ^ 2 )   \ \ right ]     $ $       $ ( \ \ theta ) $ 是 自然 参数 ， $ ( \ \ sigma ) $ 是 dispersion   parameter （ 通常 是 1 ） ， A 是   partition   function   和 指数 族 里面 的   A   一样 ！     例子 ， 逻辑 回归自然 参数 是 对数 发生 比   $ ( \ \ theta   =   \ \ log   \ \ frac { \ \ mu } { 1 - \ \ mu } ) $ ， 其中 均值 $ ( \ \ mu   =   \ \ mathbb { E } y = p ( y = 1 ) ) $ 。     均值 到 自然 参数 的 转换 函数 $ ( \ \ phi ) $ 被 指数 族 分布 函数 形式 唯一 确定 。               这个 转换 函数 的 逆函数 $ ( \ \ mu   =   \ \ phi ^ { - 1 } ( \ \ theta )   =   A ' ( \ \ theta ) ) $ 。     该值 是 输入 的 线性 变化 加上 一个 非线性 变换 $ ( g ^ { - 1 } ) $ ！       $ $     \ \ mu _ i   =   g ^ { - 1 } ( \ \ eta _ i )   =   g ^ { - 1 } ( w ^ T   x _ i )     $ $       该 非线性 变化 也 叫 均值 函数 ， 其 反函数 $ ( g ) $ 称作   link   function !   例如 在 逻辑 回归 中 均值 函数 就是 sigmoid 函数 ！     均值 函数 的 一种 选取 方式 是 $ ( g = \ \ phi ) $ ， 得到 canonical   link   function ， 此时 自然 参数 就是 输入 的 线性组合 $ ( \ \ theta _ i   =   w ^ T   x _ i ) $ ！     从而       $ $     p ( y _ i |   \ \ theta ,   w ,   \ \ sigma ^ 2 )   =   \ \ exp   \ \ left [ \ \ frac { y _ i   w ^ T   x _ i   -   A ( w ^ T   x _ i ) } { \ \ sigma ^ 2 }   +   c ( y _ i ,   \ \ sigma ^ 2 )   \ \ right ]     $ $       例子 ：             线性 回归   ， $ ( y _ i   \ \ in   \ \ mathbb { R } ,   \ \ theta _ i   =   \ \ mu _ i   =   w ^ T   x _ i ,   A ( \ \ theta ) = \ \ theta ^ 2 / 2 ) $         binomial 回归   ， $ ( y _ i \ \ in { 0 , 1 , ... , N _ i } ,   \ \ pi _ i   =   sigm ( w ^ T   x _ i ) ,   \ \ theta   =   \ \ log ( \ \ pi _ i   /   ( 1 - \ \ pi _ i ) )   =   w ^ T   x ) $         泊松 回归   ， $ ( y _ i   \ \ in   \ \ mathcal { N } ^ + ,   \ \ mu _ i   =   exp ( \ \ theta ) ,   \ \ theta = w ^ T   x ) $           $ $     \ \ mathbb { E } [ y | x , w , \ \ sigma ^ 2 ]   =   \ \ mu _ i   =   A ' ( \ \ theta )   \ \ \ \     Var [ y | x , w , \ \ sigma ^ 2 ]   =   \ \ sigma _ i ^ 2   =   \ \ sigma ^ 2   A ' ' ( \ \ theta )   \ \ \ \     $ $       极大 似然 估计 和 最大 后验 估计       对数 似然 函数 为 为 ：       $ $     l ( w )   =   \ \ log   p ( \ \ mathcal { D } | w )   =   \ \ frac { 1 } { \ \ sigma ^ 2 } \ \ sum _ { i = 1 } ^ N   l _ i   \ \ \ \     l _ i   =   \ \ theta _ i   y _ i   -   A ( \ \ theta _ i )     $ $       当 使用   canonical   link   函数 时 ， 对数 似然 函数 的 梯度 为 ：       $ $     \ \ nabla _ w   l   =   \ \ frac { 1 } { \ \ sigma ^ 2 }   \ \ sum _ { i = 1 } ^ N   ( y _ i   -   \ \ mu _ i )   x _ i     $ $       他 是 用 误差 对 样本 加权 ， 然后 求和 得到 ！ 上式 为 0 的 时候 ， 就是 最大 熵 方法 的 约束条件 ： 特征 的 经验 均值 和 期望值 相等 ！       嗨 森 矩阵 为 ：       $ $     H   =   X ^ T   S   X   \ \ \ \     S   =   diag \ \ { ...   \ \ frac { d \ \ mu _ i } { d \ \ theta _ i }   ... \ \ }     $ $       Fisher   scoring   method ?       带 高斯 先验 的 最大 后验 相当于 增加 了 l2 正则 项 ！       Probit   regression       将 逻辑 回归 里面 的 sigmoid 函数 换成 一般 的 函数 ： $ ( f :   \ \ mathbb { R }   \ \ rightarrow   [ 0 , 1 ] ) $     而 均值 函数 $ ( g ^ { - 1 }   =   \ \ Phi ) $ 是 标准 正态 cdf 。 这个 函数 跟 sigmoid 函数 很 像 ， 都 是 S 型函数 ！               隐 变量 解释 ： 设 存在 两个 隐 变量 $ ( u _ { 0 , i } ,   u _ { 1 , i } ) $ ， 随机 选择 模型       $ $     u _ { 0 , i }   =   w _ 0   ^ T   x _ i   +   \ \ delta _ { 0 , i }   \ \ \ \     u _ { 1 , i }   =   w _ 1   ^ T   x _ i   +   \ \ delta _ { 1 , i }     \ \ \ \     y _ i   =   \ \ mathbb { I } ( u _ { 1 , i }   & gt ;   u _ { 0 , i } )     $ $       对 两隐 变量 作差 ， 可 得 差分 随机 选择 模型 ， 令 $ ( \ \ epsilon _ i   =   \ \ delta _ { 1 , i }   -   \ \ delta _ { 0 , i } ) $ ， 且 假定 服从 标准 正态分布       $ $     p ( y _ i = 1 | x _ i ,   w )   =   \ \ Phi ( w ^ T   x _ i )     $ $       推广 到 有序 的   probit   回归 ， 响应 变量 是 多个 且 有序 的 情况 下 ！       Multinomial   probit   models ： 直接 看 模型 数学公式       $ $     z _ { ic }   =   w ^ T   x _ { ic }   +   \ \ epsilon _ { ic }   \ \ \ \     \ \ epsilon   \ \ sim   \ \ mathcal { N } ( 0 ,   R )   \ \ \ \     y _ i   =   \ \ arg   \ \ max _ c   z _ { ic }     $ $       Multi - task   learning       如果 不同 任务 的 输入 相同 ， 目标 相关 的 情况 下 ， 可以 同时 训练 这 几个 模型 ， 同时 优化 参数 ， 可以 得到 更好 的 性能 ！           multi - task   learning   ( Caruana   1998 )       transfer   learning   ( e . g . ,   ( Raina   et   al .   2005 ) )       learning   to   learn   ( Thrun   and   Pratt   1997 )           Hierarchical   Bayes   for   multi - task   learning ：     设 响应 变量 为 $ ( y _ { ij } ) $ ， i 为 样本 指标 ， j 为 任务 指标 ， 关联 的 特征向量 为 $ ( x _ { ij } ) $ 。     用 GLM 建模   $ ( \ \ mathbb { E } [ y _ { ij } | x _ { ij } ]   =   g ( x _ { ij } ^ T   \ \ beta _ j ) ) $ 。     由于 有 多个 任务 需要 多个 $ ( \ \ beta ) $ 。     可以 单独 训练 每 一个 模型 ， 但 在 实际 问题 中 ， 比如 商品 偏好 模型 ， 由于 长尾 分布 的 原因 ， 某些 商品 数据 很多 ， 而 其他 的 很少 ！     对于 数据 少 的 模型 ， 训练 很 困难 。 可以 通过 隐 变量 ， 让 这些 数据共享 ！     具体做法 是 控制 模型 参数 的 先验 分布 ： $ ( \ \ beta _ j   \ \ sim   \ \ mathcal { N } ( \ \ beta _ * ,   \ \ sigma _ j ^ 2   I ) ,   \ \ beta _ *   \ \ sim   \ \ mathcal { N } ( \ \ mu ,   \ \ sigma _ * ^ 2   I ) ) $     每个 模型 的 参数 通过 先验 分布 参数 $ ( \ \ beta _ * ) $ 联系 到 一起 。     可以 通过 交叉 验证 选取 $ ( \ \ mu ,   \ \ sigma _ j ,   \ \ sigma _ * ) $ 。       案例 ：   个性化 垃圾邮件 过滤         对 每 一个 用户 需要 训练 一个 模型 参数 $ ( \ \ beta _ j ) $ ， 但 由于 用户 标记 通常 很少 ， 因此 难以 单独 训练 ， 可以 采用   multi - task   learning 。       $ $     \ \ mathbb { E } [ y _ i   |   x _ i ,   u = j ]   =   ( \ \ beta _ * ^ T   +   w _ j ) ^ T   x _ i     $ $       这里 $ ( w _ j   =   \ \ beta _ j   -   \ \ beta _ * ) $ 用来 估计 个性化 的 部分 ！       Learn   to   rank       查询 q 和 文档 d 的 相关性 ， 标准 方法 ： bag   of   word   概率 语言 模型 （ 这 不是 朴素 贝叶斯 吗 ？ ）       $ $     sim ( q ,   d )   =   p ( q   |   d )   =   \ \ Pi _ { i = 1 } ^ n   p ( q | q _ i )     $ $       The   pointwise   approach :   对 每 一个 q 和 文档 d ， 生成 一个 特征向量 $ ( x ( q ,   d ) ) $ ，     学习 模型 $ ( p ( y = 1 |   x ( q , d ) ) ) $ ， 用 概率 进行 排序 。       pairwise ：   学习   $ ( p ( y _ { jk } = 1 |   x ( q ,   d _ j ) ,   x ( q ,   d _ k ) ) ) $ ， $ ( y _ { jk } = 1 ) $ 表示 文档 j 相关度 大于 文档 k 相关度 ，     一种 建模 方法 是 ：       $ $     p ( y _ { jk } = 1 |   x ( q ,   d _ j ) ,   x ( q ,   d _ k ) )   =   sigm ( x ( q ,   d _ j )   -   x ( q ,   d _ k ) )     $ $       关键 工作 ： RankNet   ( Burges   et   al .   2005 )       好处 ， 人 比较 两个 文档 哪个 更 相关 比 打分 更 容易 ， 因而 标注 数据 准确性 更高 ？       The   listwise   approach ： 学习 一个 排列 $ ( \ \ pi ) $ 。 分布       $ $     p ( \ \ pi | s )   =   \ \ Pi _ { j = 1 } ^ m   \ \ frac { s _ j } { \ \ sum _ { u = j } ^ m   s _ n }     $ $       $ ( s _ j     =   s ( \ \ pi ^ { - 1 } ( j ) ) ) $   是 文档 排列 在 第 j 个 位置 的 时候 的 score ！     例如 $ ( \ \ pi   =   ( A ,   B ,   C ) ) $ ， 那么 排列 的 概率 为 A 排在 第一 的 概率 乘以 B 排在 第二 的 概率 乘以 C 排在 第三 的 概率       $ $     p ( \ \ pi | s )   =   \ \ frac { s _ A } { s _ A + s _ B + s _ C }   \ \ times   \ \ frac { s _ B } { s _ B + s _ C }     \ \ times   \ \ frac { s _ C } { s _ C }     $ $       而 这个 score 可以 通过 模型 学习 $ ( s ( d )   =   f ( x ( q ,   d ) ) ) $ ， 通常 取为 线性 模型 $ ( w ^ T   x ) $ 。       ListNet   Cao   et   al .   2007         Latent   linear   models       因子分析       实值 隐 变量   $ ( z _ i   \ \ in   \ \ mathbb { R } ^ L ) $ ， 其 先验 分布 假设 为 高斯 （ 后面 会 假设 为 其他 分布 ）       $ $     p ( z _ i )   =   \ \ mathcal { N } ( z _ i   |   \ \ mu _ 0 ,   \ \ Sigma _ 0 )     $ $       观测 变量   $ ( x _ i   \ \ in   \ \ mathbb { R } ^ D ) $ ， 假设 其 服从 高斯分布 ， 其 均值 是 隐 变量 的 线性 函数 ！       $ $     p ( x _ i   |   z _ i ,   \ \ theta )   =   \ \ mathbb { N } ( W   z _ i   +   \ \ mu ,   \ \ Phi )     $ $       W   被 称为   factor   loading   matrix ，   而应 变量 称为 因子 ， 被 强制 要求 为 能够 解释 观测 变量 之间 的 相关性 ， 此时 $ ( \ \ Phi ) $ 是 对角 的 ！     一个 特例 是 $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $ ， 为   Probabilistic   Principal   Components   Analysis 。           因子分析 作为 协方差 的 低 秩 矩阵 分解           边际 分布       $ $     p ( x _ i   |   \ \ theta )   =   \ \ mathbb { N } ( x _ i   |   W \ \ mu _ 0   +   \ \ mu , \ \ Phi   +   W \ \ Sigma _ 0   W ^ T )     $ $       不是 一般性 ， 可以 假设 $ ( \ \ mu _ 0   =   0 ,   \ \ Sigma _ 0   =   1 ) $ ， 一般 情况 可以 作 变量 代换 得到 这种 形式 ， 即 要求 隐 变量 z 是 标准 正态分布 ， 且 独立 同 分布 。     因此 可 得 协方差 矩阵 为       $ $     \ \ text { Cov } ( x )   =   W   W ^ T   +   \ \ Phi     $ $       从 上式 可以 看到 ， 因子分析 只 使用 了   O ( LD   +   D ) 个 参数 ， $ ( \ \ Phi ) $   是 对角 的 ！       因子分析 的 目的 是 为了 通过 隐 变量 z 得到 有用 的 信息 ， 其后 验 分布 为       $ $     p ( z _ i |   x _ i ,   \ \ theta )   =   \ \ mathcal { N } ( z _ i   |   m _ i ,   \ \ Sigma _ i )     \ \ \ \     \ \ Sigma _ i   =   ( \ \ Sigma _ 0   +   W ^ T   \ \ Phi ^ { - 1 }   W ) ^ { - 1 }       \ \ \ \     m _ i   =   \ \ Sigma _ i ( W ^ T   \ \ Phi ^ { - 1 }   ( x _ i   -   \ \ mu )   +   \ \ Sigma _ 0 ^ { - 1 }   \ \ mu _ 0 )     $ $       m   被称作 隐 因子   or   latent   score .         W 不是 唯一 的 ！ ！     例如 可以 通过 做 一个 正交变换 R ， RW   仍然 是 有效 的 ！ 解决 方法 ， 增加 约束 。           要求 W 是 正交 的 ， 然后 按照   latent   factor   的 方差 排序 ：   PCA       要求 W 是 下 三角 ， 即 每个 观测 变量 只 与 前面 的 因子 有关       稀疏 约束 ： l1   regularization   ( Zou   et   al .   2006 ) ,   ARD   ( Bishop   1999 ;   Archambeau   and   Bach   2008 ) ,   or   spike - and - slab   priors   ( Rattray   et   al .   2009 ) .       Choosing   an   informative   rotation   matrix .   varimax       非 高斯 先验   for   隐 变量 ： ICA           因子 旋转 ：     http : / / www . cis . pku . edu . cn / faculty / vision / zlin / Courses / DA / DA - Class7 . pdf         混合 因子分析   ( Hinton   et   al .   1997 )       因子 模型 假设 数据 是 嵌入 在 低维 线性 流形 之中 ！ 而 实际上 大多数 时候 是 曲线 流形 ！     曲线 流形 可以 通过 分片 线性 流行 近似 。               有 K 个 FA 模型 ， 对应 维度 为 $ ( L _ k ) $ ， 参数 $ ( W _ k ) $ ，     隐 变量   $ ( q _ i   \ \ in   \ \ { 1 , 2 , ... , K \ \ } ) $ 。       $ $     p ( x _ i |   z _ i ,   q _ i   =   k ,   \ \ theta )   =   \ \ mathcal { N } ( x _ i   |   \ \ mu _ k   +   W _ k   z _ i ,   \ \ Phi )   \ \ \ \     p ( z _ i |   \ \ theta )   =   \ \ mathcal { N } ( z _ i |   0 ,   I )   \ \ \ \     p ( q _ i |   \ \ theta )   =   \ \ text { Cat } ( q _ i |   \ \ pi )     $ $       因子分析 的 EM 算法       在 E 步 ， 根据 当前 的 参数 ， 对 每 一个 数据 计算 它 来自 cat   c 的 概率 ：       $ $     r _ { ic }   =   p ( q _ i = c |   x _ i ,   \ \ theta )   \ \ propto   \ \ pi _ { c }   \ \ mathcal { N } ( x _ i | \ \ mu _ c ,   W _ cW _ c ^ T   +   \ \ Phi )   \ \ \ \     $ $       在 M 步 ， 利用 E 步 估计 的 c ， 可以 分别 计算 出 每个 cat 的 参数 $ ( \ \ mu _ c ,   W _ c ) $ ， 以及 新 的 $ ( \ \ pi _ { c } ) $ 。       PCA       ( Tipping   and   Bishop   1999 ) :   当   $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $   并且   W   矩阵 是 正交 的 ， 随着 $ ( \ \ sigma ^ 2   \ \ rightarrow   0 ) $ ，     模型 就 变为 principal   components   analysis （ PCA ，   KL 变化 ） 了 ！ $ ( sigma ^ 2   & gt ; 0   ) $ 的 版本 成为 概率 PCA （ PPCA ） 。       Classical   PCA       最小化 重构 误差       $ $     \ \ min   J ( W ,   Z )   =   \ \ sum _ { i = 1 } ^ N   | |   x _ i   -   \ \ hat { x _ i } | | ^ 2   =   | |   X   -   WZ ^ T   | | ^ 2 _ F   \ \ \ \     s . t .   W ^ T   W   =   I _ L   .     $ $       F 表示 Frobenius 范数 。       SVD       略 ： truncated   SVD       PPCA   ( Tipping   and   Bishop   1999 )       因子 模型 中 ， 当   $ ( \ \ Phi   =   \ \ sigma ^ 2   I ) $ ， 并且 W 为 正交 阵 ， 那么 观测 数据 的 对数 似然 函数 为       $ $     \ \ log   p ( X |   W ,   \ \ sigma ^ 2 )   & amp ; =   -   \ \ frac { N } { 2 }   \ \ log   | C |   -   \ \ frac { 1 } { 2 }   \ \ sum _ { i = 1 } ^ N   x _ i ^ T   C ^ { - 1 }   x _ i   \ \ \ \       & amp ; =     -   \ \ frac { N } { 2 }   \ \ log   | C |   -   \ \ frac { 1 } { 2 }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/machine-learning-a-probabilistic-perspective.html"},
      
      
      {"title": "MaxOut 网络", "text": "    Table   of   Contents           关于           Maxout   networks   论文 导读           导言           Review   of   Dropout           Maxout   模型           效果           Keras   实现                         关于       2013 年 ，   Ian   Goodfellow ,   Yoshua   Bengio   提出 maxout 网络 ， 号称 只 需要 两个 隐层 ， 就 可以 逼近 任意 分片 连续函数 。       Maxout   networks   论文 导读       论文 地址     maxout   networks         导言       Dropout   是   Hinton   2012   年 提出 的 一种 简单 的 方法 ，   模型 平均 ？ ！           We   argue   that   rather   than   using   dropout   as   a   slight     performance   enhancement   applied   to   arbitrary   models ,     the   best   performance   may   be   obtained   by   directly     designing   a   model   that   enhances   dropout ’ s   abilities   as     a   model   averaging   technique           Review   of   Dropout       对于 给定 的 输入 向量 $ ( v ) $ ， 预测 输出 $ ( y ) $ ， 一系列 的 隐层   $ ( \ \ vec { h }   =   {   h ^ { ( 1 ) } ,   ... ,   h ^ { ( L ) }   } ) $ 。     Dropout   用 变量 $ ( v ,   \ \ vec { h } ) $   的 一部分 变量 ， 训练 一组 模型 （ 这是 随机 置 0 的 高级 理解 ！ ！ ？ ） 。     这些 模型 采用 同一个 模型 参数   $ ( \ \ theta   \ \ in   \ \ mathcal { M } ) $ ， 和 一个二元   mask   变量   $ ( \ \ mu ) $ 。     对于 每 一个 样本 ， 对 不同 的 随机 参数 $ ( \ \ mu ) $ ， 我们 通过 $ ( log   p ( y | v ;   \ \ theta ,   \ \ mu ) ) $   的 梯度 训练 一个     子 模型 。       Dropout   很 像   bagging   的 方法 ， 在 数据 集 的 很多 子集 上 训练 一组 模型 。     不同 的 是 ，   Dropout   每 一个 模型 只 训练 一步 ， 并且 不同 模型 共享 一个 参数 。     因此 ， 在 训练 的 时候 ， 每 一步 需要 比较 大 的 影响 （ 把 步长 调大 ？ ） 。       Maxout   模型       Maxout   模型 也 是 一个 多层 的 前馈 结构 ， 它 的 基本 单元 是   Maxout   unit 。     对 给定 的   $ ( x   \ \ in   \ \ mathbb { R } ^ d ) $ ， 一个   maxout   隐层 为 ：       $ $     h _ i ( x )   =   \ \ max _ { j   \ \ in   [ 1 ,   k ] }   z _ { i , j }   \ \ \ \     z _ { i ,   j }   =   x ^ T   W _ { . ij }   +   b _ { ij } ,   W   \ \ in   \ \ mathbb { R } ^ { d   \ \ times   m   \ \ times   k } ,   b   \ \ in   \ \ mathbb { R } ^ { m \ \ times   k }     $ $       相当于 $ ( k ) $ 个 常规 的 纺射 变换 （ 一个 线性 变化 $ ( W ) $   加上 平移   $ ( b ) $ ） ， 然后 取 每 一个 模型 的 最大值 作为 最终 的 输出 。       单个   Maxout   单元 可以 理解 为 ， 对 凸 的 分片 连续函数 的 近似 。           Stone - Weierstrass   函数 逼近 理论 说 ， 紧集 （ 闭 区间 的 推广 ） 上 的 连续函数 可以 用 连续 的 分片 线性 函数 ( PWL )     一致 逼近   。       Wang   2004   的 理论 表明 ， 任意 连续 分片 线性 函数 可以 表达 为 两个 凸 的 分片 线性 函数 之差 。           个人 总结 ： Maxout   模型 很 简单 ， 理论 也 容易 理解 ， 就是 要 逼近 效果 好 ， 需要 $ ( k ) $ 值 足够 大 ， 这 将 会 导致 参数 个数 随 $ ( k ) $ 增大 ， 而 线性 增长 。       效果           MNIST   手写 数字 数据 集 两层   Conv .   maxout   +   dropout   得到 最佳 效果   0.45 %           Keras   实现         https : / / github . com / fchollet / keras / blob / master / keras / layers / core . py # L807                 def       call     (     self     ,       x     ,       mask     =     None     ) :               #   no   activation ,   this   layer   is   only   linear .               output       =       K     .     dot     (     x     ,       self     .     W     )               if       self     .     bias     :                       output       + =       self     .     b               output       =       K     .     max     (     output     ,       axis     =     1     )               return       output        ", "tags": "machine-learning", "url": "/wiki/machine-learning/maxout-network.html"},
      
      
      {"title": "Memory Network", "text": "    Table   of   Contents           关于           Memory   Network           A   MemoryNN   Implement   for   Text           基本 模型           训练           词 序列 作为 输入           EFFICIENT   MEMORY   VIA   HASHING           MODELING   WRITE   TIME           对于 unseen 词 的 处理           精确 匹配           结果                   End - To - End   Memory   Networks           Neural   Turing   Machine           基础 研究           Recurrent   Neural   Networks           神经 图灵机 结构           寻址 机制                   实验 结果           未 解决 的 问题                 关于       RNN 相当于 给 神经网络 加 了 记忆 单元 ， 做个 类比 ， 普通 的 前馈 神经网络 （ DNN ， CNN ） 就 好像 是 组合 逻辑电路 ；     而 增加 了 记忆 单元 的   RNN   ( LSTM   etc ) 就 好像 是 时序 逻辑电路 。 有 了 记忆 单元 的 神经网络 就 能够 像 图灵机     一样 记忆 、 推断 等 高级 能力 ， 而 不仅仅 是 像 组合 逻辑 那样 只能 学 一个 数学 函数 ！       Memory   Network           [ 1 ]   Memory   Network ,   Jason   Weston ,   Sumit   Chopra   & amp ;   Antoine   Bordes ,   Facebook   AI   Research   2015 .       [ 2 ]   End - To - End   Memory   Networks ,   Sainbayar   Sukhbaatar ,   Arthur   Szlam ,   Jason   Weston ,   Rob   Fergus   2015 .           RNN   的 缺点 在于 记忆 单元 太小 ， 所以 记忆 能力 很 弱 ， 比如 最 简单 的 问题 ： 输入 一串 单词 ， 输出 刚 看到 的 单词 ， RNN 都 难以实现 ( Zaremba   & amp ;   Sutskever ,   2014 )       结构 ： 一个 记忆 单元 集合   $ (       m   =   ( m _ 1 , m _ 2 ,   ... )       ) $ ， 4 个 模块 ：           I   :   input   feature   map ,   将 输入 转换 为 中间 特征 表达 。       G   :   generalization ,   根据 新 的 输入 更新 存储单元 的 内容 。       O   :   output   feature   map ,   根据 当前 的 输入 和 存储单元 内容 ， 生成 输出 结果 的 特征 空间 表达 。       R   :   response ,   将 输出 的 特征 空间 表达 转换 为 目标 形式 ， 例如 一个 文本 ， 或者 一个 动作 。           给定 一个 输入   x , 例如 一个 句子 ， 一个 视频 ，           首先 将 输入 转换 为 中间 表达   $ (   I ( x )   ) $       对 新 的 输入 x ， 更新 内存   $ ( m _ i   =   G ( m _ i ,   I ( x ) ,   m ) ,   \ \ forall   i ) $       对 新 的 输入 x   和 内存 m ， 计算 输出 向量   $ (   o   =   O ( I ( x ) ,   m )   ) $       最后 解码 输出 序列   r   =   R ( o )           A   MemoryNN   Implement   for   Text       如果 上述 四个 组件 是 用 神经网络 来 实现 ， 就 称作   Memory   NN ， 这些 组件 可以 用 任何 机器 学习 的 方法 ！       基本 模型       输入 是 一个 句子 ( I ) ， 文本 被 存在 下 一个 空 的 记忆 单元 ， 不 做 任何 处理 ( G ) ， O   模块 根据 输入   x   找到 k 个 有关 的 记忆 单元 ，       $ $     o _ j   =   O _ j ( x ,   m )   =   \ \ arg \ \ max _ { i = 1 , ... , N }   s _ O ( [ x ,   m _ { o _ 1 } ... , m _ { o _ { i - 1 } } ] ,   m _ i )     $ $       最终 输出 给   R   的 是   $ (   [ x ,   m _ { o _ 1 } , .. , m _ { o _ k } ]     ) $ ， 在 这个 例子 中 ， R 根据 这些 记忆 单元 和 输入 ，     输出 一个 最 匹配 的 单词       $ $     r   =   \ \ arg \ \ max _ { w   \ \ in   W }   s _ R ( [ x ,   m _ { o _ 1 } , .. , m _ { o _ k } ] ,   w )     $ $       这个 任务 可以 用来 实现 单个 单词 的 问答 问题 。     两个 匹配 函数 $ ( s _ O ,   s _ R ) $ 都 可以 用 下列 方式 建模       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U ^ T   U   \ \ phi _ y ( y )     $ $       U   是 要 学习 的 参数 ， $ ( \ \ phi ) $ 是 对 输入 的   embedding   函数 。       训练       学习 最优 的 参数   $ ( U _ O ,   U _ R ) $   最小化 损失 函数 ， 损失 函数 包含 2 部分 ( k = 2 ) ，           选择 最 匹配 的 记忆 单元 带来 的 风险       选择 最佳 response 带来 的 风险 ， 如果 用 RNN 就 直接 用 RNN 的 损失 函数 就行           上述 风险 都 用 大 间隔 损失 函数 来 刻画 ！       词 序列 作为 输入       没有 区分   statements   和   questions ， 因此 需要 学习 一个 分割 函数 ， 将 词 序列 分割 为 两 部分 。       $ $     seg ( c )   =   W _ { swg } ^ T   U _ S   \ \ Phi _ { seg } ( c )     $ $       上述 三个 参数 分别 是 ： 分割 模型 的 线性 权重 参数 ， 将 特征 空间 映射 到 embedding 空间 的 线性变换 矩阵 （ 即 查找 表 ） ， 输入 词 的 特征向量 （ 高维 稀疏 ） ， c 是 词袋 词典 。       EFFICIENT   MEMORY   VIA   HASHING       利用   hash   trick   加速 内存 查找 ： 将 输入   I ( x )   hash 到 一个 或 多个 桶 ， 只 将 输入 和 同一个 桶 内 的   memory   进行 打分 计算 。     两种   hash   策略 ：           直接   hash   词 ， 因此 对于 一个 句子 ， 只 需 比较 至少 共享 一个 词 的   memory       对词 向量 做   k - means   聚类 ， 对 一个 句子 ， 比较 的 是 和 至少 一个 词 在 同一个 类别 的 memory ， 这种 方法 考虑 了 词 的 语义 。           MODELING   WRITE   TIME       将   memory   写 的 时间 也 记录下来 。   对 内存   $ ( m _ j ) $   将 索引   j   （ 假定 没有 内存 更新 ， 索引 和 时间 一一对应 ） 也 编码 进   $ ( \ \ Phi _ x ,   \ \ Phi _ y ) $ 。 具体做法 ， 将   $ ( \ \ Phi ) $   扩充 三维 ， 都 设置 为 0 ， 对 输入   x   ， 当前 两个 内存   y ,   y ' ,   计算 分数       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U _ { O _ t } ^ T   U _ { O _ t }   ( \ \ phi _ y ( y )   -   \ \ phi _ y ( y ' )   +   \ \ phi _ t ( x ,   y ,   y ' )   )     $ $       其中 $ ( \ \ phi _ t ( x ,   y ,   y ' ) ) $ 只有 扩充 的 三维 不为 0 ， 其他 都 为 0 ， 扩充 的 三维 分别 用于 指示           whether   x   older   than   y ,       whether   x   older   than   y ' ,       whether   y   older   than   y ' ,           实际上 将 原来 的   poinwise   变为   pairwise 了 ， 并 加入 了 时间 先后 关系 信息 。 选出 比较 完胜 的 y 作为 输出 。       对于 unseen 词 的 处理       利用 语言 模型 ， 用 周围 的 词 预测 这个 词 可能 是 什么 词 ， 那么 这个 unseen 的 词 就 和 这个 预测 的 结果 的 词 相似 。     将 词 特征 表达 从   3 | W |   增加 到   5 | W | ， 增加 的 两个 部分 分别 表达 这个 词 的 左 上下文 和 后 上下文 。       精确 匹配       embedding   因为 将 词 降维到 低维 连续 向量 ， 所以 无法 做 精确 匹配 。     解决 的 办法 是 为   score   增加 一项 精确 匹配 score 。       $ $     s ( x ,   y )   =   \ \ phi _ x ( x ) ^ T   U ^ T   U   \ \ phi _ y ( y )   +   \ \ lambda   \ \ phi _ x ( x ) ^ T     \ \ phi _ y ( y )     $ $       结果                   可以 看出   只用   embedding   特征 和 之前 最好 结果 差不多 ， 稍低 ， 但是 加上   BOW   特征 ， 效果 马上 就 提升 了 9 个点 。       不同 hash 策略 的 影响 ， word   hash   实现 了 1000 倍 的 加速 但是 精度 减少 很 明显 ， cluster   hash   在 精度 只 减少 1 个点 的 情况 下 ， 实现 了 80 倍 的 加速 。                       可以 看到 时间 特征 对 提升 非常 显著       输出 选取 的 内存 个数 k 对 效果 提升 也 非常 显著       MemNN   完胜   RNN ， LSTM           End - To - End   Memory   Networks       Neural   Turing   Machine       图灵机 三个 基本 机制 ： 元素 操作 （ 算数 计算 ） ， 逻辑 流控制 （ 分支 ） ， 外部 存储 。     现代 机器 学习 模型 侧重 建模 复杂 数据 ， 缺乏 流控制 和 存储 。       RNN   是 图灵 完备 的 ， 即 可以 实现 图灵机 的 所有 操作 ， 即可 用 图灵机 完成 的 事情 ， RNN 有 这种 潜能 可以 解决 ！       NTM 之于 RNN ， 就 如 图灵机 之 于 有限 状态机 ！ 最大 的 区别 在于 前者 有 接近 无限 的 存储空间 ！       NTM   通过   attention   机制 实现 内存 的 读取 和 写入 操作 。       基础 研究       这 一部分 主要 是从 心理学 、 神经科学 、 语言学 等 角度 阐述   working   memory   和   NTM   的 相关 原理 和 解释 ！     总之 一句 话 ： memory 是 非常 重要 的 。       working   memory   在 心理学 中 用于 解释 短期 信息处理 能力           Baddeley ,   A . ,   Eysenck ,   M . ,   and   Anderson ,   M .   ( 2009 ) .   Memory .   Psychology   Press .       Miller ,   G .   A .   ( 1956 ) .   The   magical   number   seven ,   plus   or   minus   two :   some   limits   on   our   capacity   for   processing   information .   Psychological   review ,   63 ( 2 ) : 81 .           Recurrent   Neural   Networks       与 隐 马尔科夫 模型 比较 ， 前者 只有 有限 个 离散 的 状态 ， 而   RNN   的 状态 是   distributed ， 具有 无限 个 状态 ， 具有 更大 的 计算能力 ！           state ： 隐 马尔科夫 模型 具有 有限 个 离散 状态 ； RNN 具有 无限 个 分布式 状态       状态 转移 概率 ： 隐 马尔科夫 模型 通过 转移 概率 矩阵 建模 ， 依赖于 当前 状态 和 当前 的 输入 ； RNN 通过 隐层 建模 ， 可以 是 简单 的 RNN 隐层 单元 ， LSTM 单元 ， GRU ， 甚至 更 复杂 的 多层 结构 ， 依赖于 当前 状态 和 当前 的 输入 。       输出 ： 只 依赖于 当前 的 状态 ， 通常 用 生成 模型 建模 这个 条件 概率 ； RNN 则 用 （ 单层 或 多层 ） 神经网络 建模 这个 条件 概率 。       模型 训练 ： 隐 马尔科夫 模型 根据 输出 的 结果 ， 用 维特 比 算法 解码 出 状态 ， 转移 概率 和 条件 概率 通过 EM 算法 优化 得到 （ 参考 语音 识别 ） ； RNN 则 是 端 到 端 用 梯度 下降 联合 优化 所有 参数 得到 。           LSTM   解决 RNN 梯度 消失 和 爆炸 的 问题 是 通过 嵌入 一个 理想 积分器 ？               RNN   的 一些 应用 场景 ：           语音 识别       Graves ,   A . ,   Mohamed ,   A . ,   and   Hinton ,   G .   ( 2013 ) .   Speech   recognition   with   deep   recurrent   neural   networks .   In   Acoustics ,   Speech   and   Signal   Processing   ( ICASSP ) ,   2013   IEEE   International   Conference   on ,   pages   6645 – 6649 .   IEEE .       Graves ,   A .   and   Jaitly ,   N .   ( 2014 ) .   Towards   end - to - end   speech   recognition   with   recurrent   neural   networks .   In   Proceedings   of   the   31st   International   Conference   on   Machine   Learn -   ing   ( ICML - 14 ) ,   pages   1764 – 1772 .                           神经 图灵机 结构               神经 图灵机 包括 两个 基础 结构 ： 神经网络 控制器   controller ， memory   bank .     控制器 通过 输入 向量 和 输出 向量 和 外界 交互 ， 和 一般 的 神经网络 不同 的 是 ， 他 还 会 选择性 地 和 内存 交互 。     和 内存 交互 的 部件 成为   head ， 读写 头 。 读写 头 通过   attention   机制 ， 对 不同 的 内存 读取 的 值 赋予 不同 的 权重 ！       读写 头要 读 所有 的 内存 岂 不是 很 慢 ， 如何 实现 稀疏 的 读 和 写 ？     几个 参数 ：           $ (   \ \ mathbf { M } _ t   ) $   $ ( N   \ \ times   M ) $   尺寸 的 内存 矩阵 ， M 是 每个 内存 向量 的 尺寸 ， N 是 内存 向量 的 个数       $ (   \ \ mathbf { w } _ t   ) $   是 读头 给出 的 每个 内存 的 权重 向量 ， 他 应该 满足 概率 约束条件 ， 非负 ， 和 为 1 .       读头 读 内存 后 返回 的 结果 为           $ $     \ \ mathbf { r } _ t   =   \ \ sum _ i   w _ t ( i )   \ \ mathbf { M } _ t ( i )     $ $       $ (   \ \ mathbf { M } _ t ( i )   ) $   是 行向量 ， 也 就是 一个 内存 单元 。       写入 过程 ： 借鉴 了   LSTM   的 设计 ， 写 过程 包括 两个 部分 ， forget   和   add .           擦除 向量   $ ( \ \ mathbf { e } _ t ) $   M   个 元素 全为 0 - 1 之间 ， 设 写入 权重 为 $ ( w _ t ( i ) ) $ 内存 更新 方程 为           $ $     \ \ tilde { \ \ mathbf { M } } _ t ( i )   =   \ \ mathbf { M } _ { t - 1 } ( i ) [ \ \ mathbf { 1 }   -   w _ t ( i ) \ \ mathbf { e } _ t ]     $ $           add   向量   $ ( \ \ mathbf { a } _ t ) $ ， 用 add   向量 更新 擦 出后 的 内存           $ $     \ \ mathbf { M } _ t ( i )   =   \ \ tilde { \ \ mathbf { M } } _ t ( i )   +   w _ t ( i )   \ \ mathbf { a } _ t     $ $           擦除 向量 和 add 向量 每 一维 都 是 独立 的 。           寻址 机制       即 确定 读写 权重 $ ( w _ t ) $                       两种 基本 机制 ：           基于 内容 的 寻址 ： 找 和 控制器 发出 的 值 最 相似 的 位置 ， Hopfield   networks   1982 ： 简单 、 可以 获取 内存 的 精确 值 ； 不 适合 算术 问题 ， 例如 计算   $ ( x + y ) $ ， 寻址 跟 内容 无关       基于 位置 的 寻址 ， 可以 看做 基于 内容 的 寻址 的 特例 ， 因为 位置 也 可以 看做 内容 的 一部分                   基于 内容 的 寻址 原理 ： 每 一个   head （ 读 或者 写 ） 先生 成 一个 长度 为   $ ( M ) $   的   key   vector   $ ( \ \ mathbf { k } _ t ) $ ， 通过 这个 向量 和 内存 中 的 所有 向量 进行 比较 ， 计算 相似 度   $ ( K [ · ,   · ] ) $ ， 相似 度 在 所有 的 内存 上 归一化 ， $ ( \ \ beta _ t ) $   是 缩放 因子 。 论文 中 相似 度 度量 采用 向量 的 余弦 相似 度               $ $     w _ t ^ c ( i )   =   \ \ frac { \ \ exp \ \ left ( \ \ beta _ t   K [ \ \ mathbf { k } _ t ,   \ \ mathbf { M } _ t ( i ) ] \ \ right ) } { \ \ sum _ j   \ \ exp \ \ left ( \ \ beta _ t   K [ \ \ mathbf { k } _ t ,   \ \ mathbf { M } _ t ( j ) ] \ \ right ) }     $ $           基于 位置 的 寻址 ： 每 一个   head   生成 一个 标量   interpolation   gate   $ ( g _ t   \ \ in   ( 0 ,   1 )   ) $ ， 利用 这个 门去 控制 当前 权重 向量 $ ( \ \ mathbf { w }   t ^ c ) $   和 历史 权重 $ ( \ \ mathbf { w }   ) $ 混合 生成 门限 权重           $ $     \ \ mathbf { w } _ { t } ^ g   =   g _ t   \ \ mathbf { w } _ t ^ c   +   ( 1   -   g _ t )   \ \ mathbf { w } _ { t - 1 }     $ $       经过 插值 后 ， head 生成 一个 shift   weighting   $ ( \ \ mathbf { s } _ t ) $ ， 它 是 一个 向量 ， 刻画 了 移动 的 所有 可能 的 整数 上 的 一个 分布 。 这个 权重 可以 通过 一个   softmax   层 近似 ， 也 可以 输出 单个 标量 ， 例如 6.7 代表 $ ( s _ t ( 6 )   =   0.3 ,   s _ t ( 7 )   =   0.7 ) $ ， 其他 都 为 0 .   利用 这个 向量 对 插值 后 的 向量 加权 ， 可以 表达 为 一个 循环 卷积       $ $     \ \ tilde { w } _ t ( i )   =   \ \ sum _ { j = 0 } ^ { N - 1 }   w _ t ^ g ( j )   s ( i   -   j )     $ $       为了 让 寻址 更加 sharp ， head   还 生成 一个 变量   $ ( \ \ gamma _ t   \ \ ge   1 ) $ ， 对 卷积 后 的 权重 做 变换 得到 最终 的 权重       $ $     w _ t ( i )   =   \ \ frac { \ \ tilde { w } _ t ( i ) ^ { \ \ gamma _ t } } { \ \ sum _ j   \ \ tilde { w } _ t ( j ) ^ { \ \ gamma _ t } }     $ $       实验 结果       未 解决 的 问题           擦除 向量 $ ( e _ t ) $ 和 add 向量 $ ( a _ t ) $ 怎么 确定 ？       输入 数据 如何 影响 读写 头 的 权重 ？ 输入 数据 和   key   vector   $ ( k _ t ) $   有 什么 关系 ？      ", "tags": "machine-learning", "url": "/wiki/machine-learning/memory-network.html"},
      
      
        
        
      
      {"title": "Conformal Prediction", "text": "    Table   of   Contents           参考           摘要                 参考           A   Tutorial   on   Conformal   Prediction           摘要           解决 的 问题 : 用 机器 学习 模型 预测 的 时候 , 预测 结果 为 $ ( \ \ hat { y } ) $ , 它 离 真实 值 $ ( y ) $ 的 距离 是 多少 ?       预测 一个 区域   $ ( \ \ Gamma ) $   而 不是 一个点   $ ( \ \ hat { y } ) $ ,   这个 区域 以 95 % 的 概率 包含 了 真实 值 $ ( y ) $       $ ( \ \ Gamma ^ { \ \ epsilon } ) $   表示 该 区域 以 概率 $ ( 1 - \ \ epsilon ) $ 包含 了 真实 值       对于 分类 问题 , $ ( \ \ epsilon ) $   越大 ,   那么 预测 集合 $ ( \ \ Gamma ^ { \ \ epsilon } ) $ 包含 的 类别 数目 越 少 , 当 $ ( \ \ epsilon ) $ 增大 到 $ ( \ \ Gamma ^ { \ \ epsilon } ) $ 恰好 只有 一个 类别 的 时候 , 那么 可以 将 $ ( 1 - \ \ epsilon ) $ 作为 这个 类别 的 预测 置信 概率       在线 预测 配置 ,   预测 第 n 个 样本 时 , 必须 知道 前 n - 1 个 的 真实 值 ? ?            ", "tags": "machine-learning/ml-research", "url": "/wiki/machine-learning/ml-research/conformal-prediction.html"},
      
      
      {"title": "关于机器学习研究栏目", "text": "    Table   of   Contents           关于                 关于           本 栏目 主要 收集 以下 资源       NIPS 、 ICML 等 会议 论文 笔记       学术界 关于 机器 学习 、 人工智能 的 一些 报告              ", "tags": "machine-learning/ml-research", "url": "/wiki/machine-learning/ml-research/readme.html"},
      
      
      
      {"title": "MXNET学习", "text": "    Table   of   Contents           Rabit           checkpoint           Allreduce   and   Lazy   Preparation                         Rabit       教程 地址 ：   https : / / github . com / dmlc / rabit / tree / dev / guide         RABIT ： Reliable   Allreduce   and   Broadcast   Interface       两个 关键 的 操作 ： Allreduce   和   Broadcast         Allreduce   :   和 不同 的 reduce 不同 ， 以   max   操作 为例 ， reduce 将 很多 个 节点 的 值 变成 一个 最大 的 值 ，     而   allreduce   将 每 一个 节点 都 变成 最大值 ； 这 在 机器 学习 操作 中 经常 用到 ， 比如 LBFGS 同步 所有 节点     的 梯度 为 全局 梯度 ！         Broadcast   ： 和   Spark   的 broadcast 一样 ， 将 一个 节点 的 值 广播 到 所有 的 节点 ！       参考   KMEANS   的 例子 ， 了解 这 两个 操作 ：   https : / / github . com / dmlc / wormhole / blob / master / learn / kmeans / kmeans . cc         checkpoint       用于 每 一次 迭代 保存 模型 ， 以此 保证 容错 能力 ， API ：   LoadCheckPoint   ,     CheckPoint         Allreduce   and   Lazy   Preparation               Allreduce     & lt ;     operator     & gt ;     (     pointer _ of _ data     ,       size _ of _ data     ) ;        ", "tags": "machine-learning", "url": "/wiki/machine-learning/mxnet.html"},
      
      
      {"title": "ND4J", "text": "    Table   of   Contents           关于           指南                 关于       ND4J   JVM 上 的 Numpy       指南  ", "tags": "machine-learning", "url": "/wiki/machine-learning/nd4j.html"},
      
      
      {"title": "Node2Vec", "text": "    Table   of   Contents           关于           摘要           特征 学习 框架           邻居 节点 的 搜索 策略                         关于       论文 ： node2vec :   Scalable   Feature   Learning   for   Networks       摘要           将 一个 网络 中 的 节点 变成 一个 低维 的 连续 向量 ， 作为 其他 模型 的 输入 特征 。       通过 最大化 网络邻居 的 似然 函数 。       基于 特征值 分解 的 线性 或 非线性 降维 方法 在 实际 的 大规模 数据 应用 中 ， 计算 太慢 ？ 并且 性能 还 不好 ！       目标 函数 ， 保证 邻居 节点 依然 相近 ； 保证 具有 相似 结构 的 节点 的 嵌入 向量 也 相近 ！                       目标 函数 ： maximize   the   likelihood   of   pre -   serving   network   neighborhoods   of   nodes   in   a   d - dimensional   feature   space 。       2 阶   random   walk   方法 产生 节点 的 网络 上 的 邻居 样本 。       node2vec 可 扩展 到 边       用 学到 的 向量 去 做 分类 任务 的 特征 ， 结果 比 其他 方法 好 很多 ， 并且 这种 方法 很鲁棒 ！ 即使 缺少 边 也 没 问题 。       可 扩展 到 大规模   node ！       基于 特征值 分解 的 方法 难以 扩展 到 大规模 ？ suffer   from   both   computational   and   statistical   performance   drawbacks ； 不够 鲁邦 ！ 不能           特征 学习 框架           网络 ： $ ( G   =   ( V ,   E ) ) $       学习 目标 ： $ ( f   :   V   \ \ rightarrow   R ^ d ) $ ， 实际上 是 一个 $ ( | V |   \ \ times   d ) $ 参数       采样 策略 $ ( S ) $ 生成 的 节点 $ ( u ) $ 的 网络邻居   $ ( N _ S ( u )   \ \ in   V ) $       极大 似然 估计 ：           $ $     \ \ max _ f   \ \ sum _ { u   \ \ in   V }   \ \ log   Pr ( N _ S ( u ) |   f ( u )   )     $ $           几个 假设 ：       条件 独立 ： $ (   Pr ( N _ S ( u ) |   u   )   =   \ \ Pi _ { n _ i   \ \ in   N _ S ( u ) }   Pr ( n _ i | f ( u ) )   ) $       对称性 ： 特征 空间 中 ， 两个 互为 邻居 的 边 有 对称 效应 ：                   $ $     Pr ( n _ i | f ( u ) )   =   \ \ frac { \ \ exp ( f ( n _ i )   \ \ dot   f ( u ) ) } { \ \ sum _ { v \ \ in   V }   f ( v )   \ \ dot   f ( u ) }     $ $       和   word2vec   一样 ， 可以 通过 负 采样 来 优化 分母 的 计算 量 ！           最大 的 问题 是 对 领居 节点 的 采样 ， skip - gram 是 通过 一个 固定 宽度 的 滑动 窗 ， 网络 由于 不是 线性 的 ， 比较 麻烦 。 不 一定 是 直接 邻居 可以 当 邻居 ， 这 取决于 采样 策略   $ ( S ) $           邻居 节点 的 搜索 策略           经典 的 搜索 策略 ：       BFS ： 宽度 优先 搜索 ， 找 直接 相连 的 节点       DFS ： 深度 优先 搜索               node2vec   的 搜索 策略 综合 了 这 两种 策略                       对于 源 节点 $ ( u ) $ ， 通过   random   walk   ( 马尔科夫 链 )   采样   l   长度 的 邻居 节点   $ ( c _ i ,   c _ 0 = u ) $       条件 概率 为 ：           $ $     P ( c _ i = x | c _ { i - 1 } = v )   =   \ \ begin { cases }             \ \ frac { \ \ pi _ { vx } } { Z } ,   if   ( v , x )   \ \ in   E .   \ \ \ \             0 ,   otherwise     \ \ end { cases }     $ $       其中 $ ( \ \ pi _ { vx } ) $ 是 未 归一化 的 概率 ， Z 是 归一化 常数 。 对于 最 简单 的 情况 ， 可以 用边 的 权重 作为 未 归一化 概率     $ ( \ \ pi _ { vx }   =   w _ { vx } ) $ 。       对于 2 阶   random   walk ， 未 归一化 概率 和 权重 之间 的 关系 为 ： $ ( \ \ pi _ { vx }   =   \ \ alpha _ { pq } ( t , x ) w _ { vx } ) $     t 是 上 一个 节点 ， v 是 当前 节点 ， x 是 下 一个 可能 的 节点 ， 系数       $ $     \ \ alpha _ { pq } ( t ,   x )   =   \ \ begin { cases }                             \ \ frac { 1 } { p } ,   d _ { tx }   =   0 ,   \ \ \ \                             1 ,   d _ { tx } = 1 , \ \ \ \                             \ \ frac { 1 } { q } ,   d _ { tx } = 2 .     \ \ end { cases }     $ $       $ ( d _ { tx } ) $ 是 两个 节点 的 距离 ， p 是 return 参数 ， q 是 in - out 参数 。           random   walk   的 好处 ：       可以 减少 邻居 的 存储空间 到 $ ( O ( a ^ 2 | V | ) ) $ . 本来 是 $ ( O ( E ) ) $ 。       每 一次 产生 的 链 可以 复用 ， 因为 马尔科夫 性 。              ", "tags": "machine-learning", "url": "/wiki/machine-learning/node2vec.html"},
      
      
      {"title": "Numerical Optimization", "text": "    Table   of   Contents           关于           8 .   拟 牛顿 法                 关于       8 .   拟 牛顿 法  ", "tags": "machine-learning", "url": "/wiki/machine-learning/numerical-optimization.html"},
      
      
        
        
      
      {"title": "FTRL", "text": "    Table   of   Contents               在 先 学习 求解 L1 范数 优化 问题 , 利用 近似 梯度 法 ,       $ $       $ $  ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/ftrl.html"},
      
      
      {"title": "优化相关论文摘要", "text": "    Table   of   Contents           参考                 参考           论文 :   DON ’ T   DECAY   THE   LEARNING   RATE ,   INCREASE   THE   BATCH   SIZE       论文 :   A   Bayesian   Perspective   on   Generalization   and   Stochastic   Gradient   Descent       如果 要 减少 学习 率 ,   那么 可以 替换 为 同等 比例 地 增加 batch   size       梯度 下降 可以 表述 为 一个 随机 微分方程 ,   w 是 模型 参数 , C 是 损失 函数 , t 是 步长 , $ ( \ \ eta ) $ 是 随机噪声       随机噪声 的 的 均值 为 0 ,   方差 幅度   $ ( g   =   \ \ epsilon   ( N / B   -   1 ) ) $ ,   epsilon   是 学习 率 , N 是 总 训练样本 , B 是 batch   size ,   因此 ,   减少 学习 率 也 可以 通过 增加 B 来 实现 。     $ $     \ \ frac { d   w } { d   t }   =   -   \ \ frac { d   C } { d   t }   +   \ \ eta ( t )     $ $      ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/optimization-paper-abstract.html"},
      
      
      {"title": "方程求解方法汇总", "text": "    Table   of   Contents           线性方程           高斯消 元法           迭代 解法                   非线性 方程           不动点 迭代           线性 搜索                         线性方程       高斯消 元法       高斯消 元法 求解 线性方程 是 最为 经典 的 方法 ， 基本 思想 就是 一次 消 去 多个 变量 ， 直到 留下 一个 ， 从而 解 出 结果 。 用 矩阵 的 语言 描述 ， 就是 对 线性方程       $ $     Ax   =   b     $ $       的 矩阵 A 和 b 构成 的 增广 矩阵 [ A   b ] 做 一系列 的 行 变换 $ ( P _ i ,   i = 1 , ... , k ) $ ， 将 矩阵 A 变成 只有 对角线 为 1 或 0 的 单位 阵       $ $     P _ k   P _ { k - 1 }   ...   P _ 1   A   x   =   P _ k   P _ { k - 1 }   ...   P _ 1   b   \ \ \ \     x   =   P   b     $ $       $ ( P   =   P _ k   P _ { k - 1 }   ...   P _ 1 ) $ 是 一系列 的 航 变换 ， 将 A 矩阵 变为 单位 阵 ， 实际上 就是 A 矩阵 的 逆 ！     高斯消 元法 计算 复杂度 是 $ ( O ( n ^ 3 ) ) $ 。       迭代 解法         Jacobi   迭代   ： 将 系数 矩阵 A 分解 为 $ ( A   =   D   +   B ) $ ， 其中 D 是 对角 阵 ， B 是 对角线 上 为 0 的 矩阵 。 那么 有       $ $     x   =   -   D ^ { - 1 }   B   x   +   D ^ { - 1 }   b     $ $       上 式 可以 把 x 看做 迭代 方程 的 不动点 ， 这 就是 Jacobi 迭代法 。 收敛 的 充分条件 是 $ ( | | D ^ { - 1 } B | | & lt ; 1 ) $ ， 如果 取 无穷 范数 ， 则 表示 A 矩阵 是 主 对角 元 最大 的 矩阵 ， 即 $ ( | a _ { ii } |   & gt ;   | a _ { ij } | ,   \ \ forall   i , j ) $ 。 一般 的 方差 ， 总 可以 通过 简单 的 行 初等变换 变成 主 对角 元 最大 的 矩阵 ！         Gauss - Seidel 迭代   ： 将 矩阵 分解 为 $ ( A   =   L   +   U ) $ ， 其中 L 是   有   对角 元素 的 下 三角 阵 ， U 是   没有   对角 元 的 上 三角 阵 。 迭代 方程 为       $ $     L   x _ { k + 1 }   =   -   U   x _ k   +   b     $ $       每 一次 迭代 是 在 解 稀疏 矩阵 是 下 三角 矩阵 的 线性方程 ， 可以 在 $ ( O ( n ^ 2 ) ) $ 时间 复杂度 内 求解 ！ 迭代 速度 高于 Jacobi 迭代 。       非线性 方程       不动点 迭代       与 线性方程 的 迭代 方法 类似 ， 构造 不动点 方程 ， 使得 迭代 是 压缩 映象 ， 那么 不断 迭代 该 方程 可以 得到 不动点 ， 就 是非 线性方程 的 解 ！       $ $     x   =   g ( x )     $ $       线性 搜索       二分法 、 牛顿 法等 方法 。 实际上 牛顿 法 求解 方程 可以 看做 梯度 是 f 的 原函数 的 最优化 问题 。  ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/equation-solve.html"},
      
      
      {"title": "约束凸优化", "text": "    Table   of   Contents           等式 约束 问题           内点法                 等式 约束 问题       对于 线性方程 等式 约束 问题       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   \ \ \ \     s . t .   \ \ quad   & amp ;   A   x   = b     \ \ end { align }     $ $       根据 KKT 最优 条件 可知 最优 解 满足 方程       $ $     Ax ^   *   =   b ,   \ \ nabla   f ( x ^   *   )   +   A   v ^   *   =   0     $ $       后 一个 式子 实际上 是 说 ， 在 最优 解处 ， f 的 梯度 在 A 列 向量 的 子 空间 中 ！ A 是 向量 的 时候 ， 有个 直观 的 几何 解释 ！ 即 f 的 等 高面 与 超平面 相切 ！         等式 约束 二次 规划     有 闭式 解 ， 上述 条件 变成 了 一个 线性方程 问题 ！ 设 $ ( f ( x )   =   1 / 2   x ^ T   P   x   +   q ^ T   +   r ) $ ， 那么 KKT 条件 变为       $ $     Ax ^   *   =   b ,   P   x ^   *     +   A   v ^   *   +   q   =   0     $ $       这是 一个 线性方程 ， 可以 利用 线性方程 求解 方法 如 高斯消 元法 、 迭代 算法 等 求解 。     如果 方程 有解 ， 那么 所有 的 解 都 是 可行 解 ！ 如果 方程 无 解 ， 那么 表示 原 问题 没有 最小值 ！         消除 等式 约束     方法 ： 先 求解 线性方程 得到 通解   $ ( \ \ { Fz   +   \ \ hat { x } |   z   \ \ in   R ^ { n - p } \ \ } ) $ ， 然后 代入 目标 函数 ， 消除 等式 约束 ！         求解 对偶 问题     :   对偶 问题 是 无约束 优化 问题 。       $ $     \ \ min _ v   b ^ T   v   +   f ^   *   ( - A ^ T   v )     $ $         牛顿 方法     即 在 迭代 点 附近 用 一个 二次 函数 近似 f ( x ) ， 这样一来 ， 在 每次 迭代 的 时候 ， 就是 在 求解 等式 约束 二次 规划 ， 有 闭式 解 ！ 从而 可以 很 方便 求 出 下降 步长 ！       内点法       凸 优化 问题       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   \ \ \ \     s . t .   \ \ quad   & amp ;   f _ i ( x )   \ \ le   0   \ \ \ \                           & amp ;   Ax = b     \ \ end { align }     $ $       定义 示性 函数       $ $     I _ - ( x )   =   \ \ begin { cases }                     0   \ \ quad   x   \ \ le   0   \ \ \ \                     \ \ infty     \ \ quad   x   & gt ;   0                     \ \ end { cases }     $ $       通过 这个 示性 函数 ， 可以 将 不等式 约束 去       $ $     \ \ begin { align }     \ \ min   \ \ quad   & amp ;   f ( x )   +   \ \ sum _ i   I _ - ( f _ i ( x ) ) \ \ \ \     s . t .   \ \ quad   & amp ;   Ax = b     \ \ end { align }     $ $       对数 壁垒 函数 ： 由于 示性 函数 不可 微 ， 可以 用 可微 函数 近似 ， 一种 选择 是 采用 如下 对数函数       $ $     \ \ hat { I _ - } ( x )   =   -   \ \ frac { 1 } { t }   \ \ log ( - x )     $ $       随着 参数 t 趋近 于 无穷大 ， 对 示性 函数 的 近似 度 越来越 好 ！ 在 这种 壁垒 函数 选取 下 ， 最优 解 可以 通过 牛顿 法 求解 ， 最优 解 跟 t 有关 ， $ ( x ( t ) ) $ 随 t 变动 而 形成 的 轨迹 叫做     中心 路径   。 并且 有       $ $     f ( x ^   *   ( t )   )   -   p   ^   *   & lt ;   m / t     $ $       m 是 不等式 约束 的 个数 。 p 是 最优 f 值 ， $ ( x ^   *   ( t )   ) $ 是 壁垒 函数 近似 的 最优 解 ！ 因此 ， 随着 t 增大 ， 可以 控制 误差 在 指定 的 范围 内 ！          ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/interior-point-method.html"},
      
      
      {"title": "近似算法", "text": "    Table   of   Contents           投影 算子           Moreau   分解           近似 梯度 法           forward - backward   splitting           投影 梯度 算法           交替 投影 算法                   投影 算子 计算           二次 函数           标量 函数           纺射集 投影           半 空间           集合 的 支持 函数           范数           L2 范数           L1 范数                           软 阈值 算法           L1 范数 正则           L2 范数           L21 范数                   软 阈值 算法 实现           交替 方向 乘子 ADMM           多个 函数           相关 资料                 投影 算子       点到 闭凸 集合 的 投影       $ $     prox _ C ( x )   =   \ \ arg \ \ min _ y   I _ C ( y )   +   \ \ frac { 1 } { 2 } | | x   -   y | | ^ 2     $ $       其中 $ ( I _ C ) $ 是 集合 C 的 示性 函数       $ $     I _ C ( x )   =   \ \ begin { cases }                     0 ,   \ \ quad   x   \ \ in   C   \ \ \ \                     \ \ infty ,   \ \ quad   x   \ \ not \ \ in   C                     \ \ end { cases }     $ $       几何 解释 就是 点 x 到 集合 C 的 投影 就是 C 中到 x 最近 的 点 ！       把 示性 函数 替换成 一般 的 凸函数 f ， 可以 得到 一般 的 投影 算子       $ $     prox _ f ( x )   =   \ \ arg \ \ min _ y   f ( y )   +   \ \ frac { 1 } { 2 } | | x   -   y | | ^ 2     $ $       不动点 方程 ： 如果 一个点 x * 在 f 的 投影 下 是 他 自己 ， 那么 根据 上 式 ， 第二项 为 0 ， 所以       $ $     x ^   *   =   prox _ f ( x ^ *   )   =   \ \ arg \ \ min _ y   f ( x )     $ $       几何 解释 就是 在 f 的 最小值 等 高 面上 的 点 的 投影 是 它 自己 ！       由于 点 x 到 投影 点 $ ( prox _ f ( x ) ) $ 的 方向 向量 $ ( x   -   prox _ f ( x ) ) $ 与 等 高面 垂直 （ 投影 的 几何 解释 ） ， 因此 ， 投影 操作 可以 看做 梯度 下降 的 推广 ！     投影 是 往 函数 f 的 较 小值 等 高 面上 进行 投影 ！       令 $ ( p   =   prox _ { \ \ lambda   f } ( x ) ) $ ，       $ $     p   =   \ \ arg \ \ min _ y   \ \ frac { 1 } { 2 }   | | y   -   x | | ^ 2   +   \ \ lambda   f ( y )   \ \ \ \     0   \ \ in   p - x   +   \ \ lambda   \ \ partial   f ( p )   \ \ \ \     x   \ \ in   ( I   +   \ \ lambda   \ \ partial   f )   ( p )   \ \ \ \     p   =   ( I   +   \ \ lambda   \ \ partial   f ) ^ { - 1 }   ( x )     $ $       注意 ， 次梯度 的 逆 有 唯一 的 像 ！ 所以 投影 算子 与 次梯度 关系 为       $ $     prox _ { \ \ lambda   f }     =   ( I   +   \ \ lambda   \ \ partial   f ) ^ { - 1 }     $ $       如果 $ ( \ \ lambda ) $ 很小 且 f 存在 常规 梯度 ， 那么 可以 近似 为       $ $     prox _ { \ \ lambda   f } ( v )   \ \ approx   v   -     \ \ lambda   \ \ nabla   f ( v )     $ $       也就是说     投影 操作 是 梯度 下降 的 一种 推广   ！ 实际上 ， 可以 看做 一种 前向 梯度 下降 ， 即 下降 的 梯度 不是 在 当前 点 v 计算 得到 的 ， 而是 在 下降 的 目标 点 p 计算 得到 的 ！       如果 投影 操作 计算 方便 （ 有 简单 的 解析 解 ） ， 那么 用 投影 操作 做 优化 可以 取代 梯度 下降 ， 并且 可以 应用 到 梯度 下降 没法用 的 场景 — — 梯度 不 存在 的 函数 优化 ！       近似 点 算法 ： 求函数 f 的 最小值 ， 利用 投影 算子 是 压缩 算子 ， 且 投影 算子 的 不动点 是 f 的 最小值 点 性质 可 得 迭代 近似 点 算法       $ $     x _ { n + 1 }   =   prox _ { \ \ lambda   f } ( x _ n )     $ $       前面 说 到 投影 算子 就 相当于 梯度 下降 的 推广 ， 那么 近似 点 算法 可以 看做 梯度 下降 求 最小值 的 推广 ！       Moreau   分解       点 v 可以 分解 为 投影 和 共轭 投影 之 和       $ $     v   =   prox _ f ( v )   +   prox _ { f ^   *   } ( v )   \ \ \ \     f ^   *   ( y )   =   \ \ sup _ x   \ \ left ( y ^ T   x   -   f ( x ) \ \ right )     $ $       这个 分解 可以 看做 几何 中 的 正交 分解 ！       近似 梯度 法       forward - backward   splitting       如果 目标 函数 存在 不可 微分 部分 ， 可以 将 目标 函数 分解 为 两 部分 ， 一部分 可以 微分 ， 另 一部分 不可 微分       $ $     \ \ min _ x   f ( x )   +   g ( x )     $ $       假设 f 可微 ， g 是 不可 微 部分 。 若 $ ( p ) $ 是 最优 解 ， 那么 根据 最优 条件 可知       $ $     0   \ \ in   \ \ lambda   \ \ nabla   f ( p   )   +   \ \ lambda   \ \ partial   g ( p   )   \ \ \ \     0   \ \ in   \ \ lambda   \ \ nabla   f ( p )   -   p   +   p +   \ \ lambda   \ \ partial   g ( p   )   \ \ \ \     ( I   -   \ \ lambda   \ \ nabla   f )   ( p )   \ \ in   ( I   +   \ \ lambda   \ \ partial   g )   ( p   )   \ \ \ \     p   =   ( I   +   \ \ lambda   \ \ partial   g ) ^ { - 1 }   ( I   -   \ \ lambda   \ \ nabla   f )   ( p )   \ \ \ \     p   =   prox _ { \ \ lambda   g } \ \ left (   p   -   \ \ lambda   \ \ nabla   f ( p )     \ \ right )     $ $       上述 不动点 方程 给出 了 优化 迭代 步骤 ，   先 按着 可微 函数 梯度 下降 ， 然后 对 不可 微 函数 做 投影 下降   ！       如果 两步 都 采用 投影 来 做       $ $     p   =   prox _ {   g } \ \ left (   prox _ f   ( p )   \ \ right )     $ $       就是   backward - backward   splitting .       投影 梯度 算法       求解 约束 优化 问题       $ $     \ \ min _ { x   \ \ in   C }   f _ 2 ( x )     $ $       迭代 算法       $ $     x _ { k + 1 }   =   P _ C \ \ left ( x _ k   -   \ \ gamma   \ \ nabla   f _ 2 ( x _ k ) \ \ right )     $ $       $ ( P _ C ) $ 是 到 凸集 的 投影 ， 该 算法 的 集合 意义 是 先 沿着 f2 的 梯度方向 下降 ， 让 后 将 结果 点 投影 到 集合 C 中 ， 以 保证 解 不会 离开 约束 区域 ！       交替 投影 算法       求解 约束 优化 问题       $ $     \ \ min _ { x \ \ in   C }   \ \ frac { 1 } { 2 }   d _ D ^ 2 ( x )     $ $       $ ( d _ D ( x ) ) $ 是 点 x 到 凸集 D 的 距离 ， 采   backward - backward   分割 ， 可 得 迭代 算法       $ $     x _ { k + 1 }   =   P _ C ( P _ D ( x _ k ) )     $ $       该 算法 的 几何 图像 是 ， 交替 像 两个 凸集 C 和 D 进行 投影 ， 直到 收敛 ！       投影 算子 计算       常规 的 计算方法 是 直接 根据 定义 ， 求解 优化 问题 ：       $ $     \ \ min _ y   f ( y )   +   \ \ frac { 1 } { 2 \ \ lambda } | | y   -   x | | ^ 2     $ $                         $ ( dom   f ) $ 有限       $ ( dom   f   =   R ^ n ) $                       f 光滑       投影 梯度 法 、 内点法       梯度 下降               f 不 光滑       投影 次梯度 法       次梯度 法                   二次 函数       $ ( f ( x )   =   1 / 2x ^ T   A   x   +   b ^ T   x   + c   ) $ ，       $ $     prox _ { \ \ lambda   f }   ( v ) =   ( I   +   \ \ lambda   A ) ^ { - 1 } ( y   -   \ \ lambda   b )     $ $       如果   A   =   I ,   且 只有 二次 项 ， 即 $ ( f   =   | |   \ \ cdot   | |   _   2 ^ 2 ) $ ， 那么 投影 算子 表现 为 shrinkage   operator ， 直观 来看 就是 把 做 了 一个 衰减 ！       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ frac { v } { 1   +   \ \ lambda }     $ $       标量 函数       如果 f 是 标量 函数 ， 自变量 是 单 变量 ， 那么 很 容易 求得       $ $     v   \ \ in   \ \ lambda   \ \ partial   f ( x )   +   x   \ \ \ \     x   = prox _ { \ \ lambda   f } ( v )   =   ( 1   +   \ \ partial   f ) ^ { - 1 }   v     $ $       当 $ ( f ( x )   =   | x | ) $ 时 ， 有       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ max ( | v |   -   \ \ lambda   ,   0 )   sgn ( v )     $ $       即软 阈值 （ soft   thresholding ） 操作 ！       纺射集 投影       集合 $ ( C   =   \ \ { x |   Ax   =   b   \ \ } ) $ , 投影 操作 为       $ $     \ \ Pi _ C ( v )   =   v   -   A ^ \ \ dagger ( Ax   -   b )     $ $       $ ( \ \ dagger ) $ 是   伪逆   。       当 A 是 一个 向量 时 ， 相当于 投影 到 超平面 ，       $ $     \ \ Pi _ C ( v )   =   v   -   \ \ frac { a ^ Tx - b } { | | a | | _   2 ^ 2 }   a     $ $       半 空间       集合 $ ( C   =   \ \ { x |   a ^ Tx   \ \ le   b   \ \ } ) $ , 投影 操作 为       $ $     \ \ Pi _ C ( v )   =   v   -   \ \ frac { ( a ^ Tx - b )   _   +   } { | | a | |   _   2 ^ 2 }   a     $ $       即 要 减掉 向量 在 另外 一边 的 分量 ！       集合 的 支持 函数       集合 C 的 支持 函数 定义 为       $ $     S _ c ( x )   =   \ \ sup _ { y \ \ in   C } y ^ T   x     $ $       结合 解释 是 ， 以 x 为 外法 向量 的 点 就是 支持 点 ！ 支持 函数 与 示性 函数 共轭 $ ( S _ C ^   *   =   I _ C   ) $ 。     根据 Moreau 分解 ， 知       $ $     prox _ { \ \ lambda   S _ c } ( v )   =   v   -   \ \ lambda   \ \ Pi _ C ( v / \ \ lambda )     $ $       注意 最后 一个 式子 的 几何 理解 ！       范数       如果 函数 $ ( f   =   | |   \ \ cdot   | | ) $ ， 那么 对 偶函数 $ ( f ^   *   =   I _ B ) $ ，   B 是 对偶 范数 的 单位 球 ！       $ $     prox _ { \ \ lambda   f } ( v )   =   v   -   \ \ lambda   \ \ Pi _ B ( v / \ \ lambda )     $ $       L2 范数       L2 范数 的 对偶 范数 还是 L2 范数 ， L2 单位 球 就是 欧式 空间 的 单位 球 ！ 所以       $ $     \ \ Pi _ B ( v )   =   \ \ begin { cases }                             v / | | v | |   _   2 ,   \ \ quad   | | v | |   _   2   & gt ;   1 ,   \ \ \ \                             v ,   \ \ quad   | | v | |   _   2   \ \ le   1                             \ \ end { cases }     $ $       所以 L2 范数 的 投影 为       $ $     prox _ { \ \ lambda   f } ( v )   =   \ \ begin { cases }                             ( 1   -   \ \ lambda / | | v | |   _   2 )   v ,   \ \ quad   | | v | |   _   2   & gt ;   \ \ lambda ,   \ \ \ \                             0 ,   \ \ quad   | | v | |   _   2   \ \ le   \ \ lambda                             \ \ end { cases }     $ $       也 就是 将 原始 向量 v 沿着 v 方向 减少 $ ( \ \ lambda ) $ 长度 ， 除非 减到 0 向量 ！ 也 叫做   block   soft   thresholding 操作 ！       L1 范数       L1 范数 的 对偶 范数 是 $ ( L _ { \ \ infty } ) $ ， 对应 的 单位 球是 单位 立方体 ， 投影 为       $ $     \ \ Pi _ B ( v )   =   \ \ begin { cases }                             sgn ( v _ i ) ,   \ \ quad   | v   _   i |   & gt ;   1 ,   \ \ \ \                             v _ i ,   \ \ quad   | v _ i |   \ \ le   1                             \ \ end { cases }     $ $       所以       $ $     v _ { i + 1 }   =   \ \ begin { cases }             v _ i   -   \ \ lambda ,   \ \ quad   v _ i & gt ; \ \ lambda   \ \ \ \             - v _ i   +   \ \ lambda ,   \ \ quad   v _ i & lt ; - \ \ lambda   \ \ \ \             0 ,   \ \ quad   other             \ \ end { cases }     $ $               软 阈值 算法       L1 范数 正则       L1 正则 问题       $ $     \ \ min _ x   f ( x )   +   \ \ lambda   | x | _   1     $ $       利用 近似 梯度 法 ， 令 $ ( g = | \ \ cdot |   _   1 ) $ 有 迭代 算法       $ $     z _ k   =   x _ k   -   \ \ eta     \ \ nabla   f ( x _ k )   \ \ \ \     x _ { k + 1 }   =   prox _ { g }   z _ k     $ $       最后 一步 是 一个 投影 ， 根据 定义       $ $     prox _ { \ \ lambda   g }   ( v )   =   \ \ arg   \ \ min _ x   \ \ lambda   | x |   _   1   +   \ \ frac { 1 } { 2 } | | x   -   v | | ^ 2     $ $       对 上式 微分 得       $ $     v _ i   -   x _ i   \ \ in   \ \ lambda   \ \ partial   | x _ i | ,   i = 1 , ...     $ $       上 式 表明 下降 量 不 超过 $ ( \ \ lambda ) $ ， 如果 $ ( v _ i ) $ 绝对值 大于 $ ( \ \ lambda ) $ 那么 下降 不会 越过 不可 微分 点 ， 可以 按照 正常 的 梯度 下降 ，     但是 如果 小于 ， 那么 只能 下降 到 0 ， 才能 保证 上 式 成立 ！       $ $     x _ i   =   \ \ begin { cases }             v _ i   -   \ \ lambda ,   \ \ quad   v _ i & gt ; \ \ lambda   \ \ \ \             - v _ i   +   \ \ lambda ,   \ \ quad   v _ i & lt ; - \ \ lambda   \ \ \ \             0 ,   \ \ quad   other             \ \ end { cases }     $ $       这 表明 ， 加 了 L1 正则 项 ， 相当于 将 阈值 为 $ ( \ \ lambda ) $ 以内 的 分量 都 置 0 ， 以上 的 都 减小 $ ( \ \ lambda ) $ 。       L2 范数       L21 范数       软 阈值 算法 实现       求解 问题       $ $     \ \ min _ x   \ \ frac { 1 } { 2 }   | | Ax   -   b | | ^ 2   +   \ \ lambda _ 2   | | x | |   _   2 ^ 2   +   \ \ lambda _ 1   | | x | |   _   1     $ $       可以 看到 ， 算法 很快 就 收敛 了 ， 并且 L1 正则 很 容易 得到 稀疏 解 ！               import       numpy       as       np         ##   问题 100 维       #         n       =       100       A       =       np     .     random     .     randn     (     n     *     10     ,       n     )       #   超定 方程       xx       =       np     .     random     .     randn     (     n     )       mask       =       np     .     random     .     rand     (     n     )     & lt ;     0.8       xx       =       xx       *       mask       print       &# 39 ; ground   truth : &# 39 ;     ,       xx       b       =       np     .     matmul     (     A     ,       xx     )       +       np     .     random     .     randn     (     n     *     10     )       *       0.1       #   加入 一些 噪声         & gt ; & gt ; & gt ;       Output     :               ground       truth     :       [       0 .                         0.42171383       -     0.94716965         0.54640995         0.67948742         0.88857648               0.14209921       -     0.70685128         0.43310285       -     1.2423563           0.27323468       -     0.23007387               1.53049499       -     0.03076339       -     0 .                         0.53795911         1.1853585           0.66830622               0.0813316         -     0.39508658         0.75451939         0.53967945       -     0 .                       -     0.79440595               0.10416921       -     0.76577241         2.21847433       -     1.38163076         0.6089114           1.18767332               -     1.28999937         0.65445551       -     0.3248272         -     0.88002173       -     0.82729771         0.47309462               -     0.8384278         -     1.66928355         0.85613791         0.31921217         2.51727067         1.11885762               0.38646877         0.32068998         0 .                         1.02912399       -     0.4607417         -     0.84519518               -     0 .                         0.34949314         0.56150765         0.08035849         1.812666           -     1.23004836               1.65564242         0.23581581       -     0.03529459       -     0.33258733       -     0.65909872       -     1.1317373               -     0.46223132         0.97113475         0 .                         0.17753836       -     0 .                         0 .               0.08929848         0.02685682         0 .                       -     0 .                         1.86521051       -     1.02918525               1.39816556       -     0.32507115         0.20111102       -     0 .                       -     1.81123986         0.18043876               0 .                       -     0.84625861         0.8709556           0 .                         0.65961205         2.35225572               0 .                       -     0.04910171       -     1.35667457       -     1.45385942         0.15419398         1.1789595               0.7340732         -     0 .                         0.85819805         0.57832173         0.49845621         0 .                       -     0.6047393               0.99361454         0.45679531         0.28392374     ]           #   目标 函数       def       f     (     x     ,       lab     =     1     ,       lab2     =     0.01     ) :               return       0.5     *     np     .     sum     ( (     np     .     matmul     (     A     ,       x     )       -       b     )     * *     2       )     +       lab       *       np     .     sum     (     np     .     abs     (     x     ) )       +       lab2     *     np     .     sum     (     x     *     x     )         #   除了 L1 正则 项外 的 梯度       def       grad     (     x     ,       lab2     =     0.01     ) :               return       np     .     matmul     (     A     .     T     ,       np     .     matmul     (     A     ,       x     )       -       b     )       +       lab2     *     x         #   软 阈值 迭代 算法         x       =       np     .     random     .     randn     (     n     )       mu       =       0.001       lamb       =       0.01       lab2       =       0.01         for       i       in       range     (     100     ) :               #   STEP1 ： 梯度 下降               x       - =       grad     (     x     ,       lab2     )       *       mu                 #   STEP2 ： 投影 操作 ， 也 就是 软 阈值 操作               for       j       in       range     (     len     (     x     ) ) :                       if       x     [     j     ]       & gt ;         lamb     :                               x     [     j     ]       - =       lamb                       elif       x     [     j     ]       & lt ;       -       lamb     :                               x     [     j     ]       + =       lamb                       else     :                               x     [     j     ]       =       0.0                 #   打印 中间 迭代 结果               if       i       %       10       = =       0     :                       print       i     ,       &# 39 ; loss = &# 39 ;     ,       f     (     x     ,       lamb     ,       lab2     )         print       x         & gt ; & gt ; & gt ;       Output     :               0       loss     =       7532.16879981               10       loss     =       11.0285759018               20       loss     =       10.9840030328               30       loss     =       10.9842744582               40       loss     =       10.9842770859               50       loss     =       10.9842771092               60       loss     =       10.9842771094               70       loss     =       10.9842771094               80       loss     =       10.9842771094               90       loss     =       10.9842771094               [       0 .                         0.41005474       -     0.93416517         0.52872225         0.66901803         0.87710187               0.1254697         -     0.69410934         0.42094427       -     1.23128115         0.25644887       -     0.22262353               1.51692245       -     0.02018758         0 .                         0.53116499         1.17967392         0.64459272               0.06787308       -     0.38043066         0.74631218         0.52152388         0 .                       -     0.79768153               0.09135038       -     0.75555311         2.20245087       -     1.36570672         0.59778203         1.17212485               -     1.27509079         0.63889237       -     0.31797818       -     0.87161519       -     0.81344888         0.464237               -     0.82969714       -     1.66337076         0.83671452         0.30766759         2.50494207         1.1034511               0.38148637         0.31093457         0 .                         1.02118932       -     0.45228788       -     0.8302418           0 .               0.32929066         0.54006833         0.06742762         1.80640673       -     1.2229943           1.64774712               0.21735476       -     0.02296309       -     0.3222897         -     0.65161403       -     1.10748401       -     0.44978335               0.95982422         0 .                         0.16099326       -     0.00337374         0 .                         0.0845288               0.01798603         0 .                         0 .                         1.84882179       -     1.01734222         1.39139325               -     0.31544124         0.18957113         0 .                       -     1.80153297         0.17600738         0 .               -     0.83960423         0.86347204         0 .                         0.64546066         2.35318802         0 .               -     0.03208243       -     1.34212428       -     1.44773413         0.14815159         1.17458127         0.72367534               0 .                         0.84649559         0.56543654         0.47495661         0 .                       -     0.59702874               0.98713984         0.4507846           0.27214572     ]                 交替 方向 乘子 ADMM       求 目标 可分解 优化 问题       $ $     \ \ min _ { x , z }   f ( x )   +   g ( z )   \ \ \ \     s . t .   \ \ quad   Ax   +   Bz   =   c     $ $       利用 增广 拉格朗 日 乘子       $ $     L _ { \ \ rho } ( x ,   z ,   y )   =   f ( x )   +   g ( z )   +   y ^ T ( Ax   +   Bz   -   c )   +   \ \ rho / 2   | | Ax + Bz - c | | ^ 2     $ $       利用 拉格朗 日 对偶 问题 的 性质 可知 ， 在 强 对偶 条件 下 ， 最优 解为       $ $     x   *   ,   y   *   ,   z   *   =   \ \ arg \ \ max _ y   ( \ \ arg \ \ min _ { x , z }   L ( x , z ,   y )   )     $ $       于是 有 交替 迭代 算法 ( ADMM ) :       $ $     x _ { k + 1 }   =   \ \ arg \ \ min _ x   L _ { \ \ rho } ( x ,   z _ k ,   y _ k )   \ \ \ \     z _ { k + 1 }   =   \ \ arg \ \ min _ z   L _ { \ \ rho } ( x _ { k + 1 } ,   z ,   y _ k )   \ \ \ \     y _ { k + 1 }   =   y _ k   +   \ \ rho   ( Ax _ { k + 1 }   + Bz _ { k + 1 }   -   c )     $ $       其中 前 两个 式子 实际上 是 两个 投影 算子 ， 最后 一个 式子 是 用 梯度 上升 求 对 偶函数 的 最大值 ！       当 A = - B = I 时 ， 可 得到 无约束 最优化 问题 $ ( \ \ min _ x   f ( x )   +   g ( x ) ) $ 的 求解 算法       $ $     x _ { k + 1 }   =   prox _ { \ \ lambda _ k   f } ( z _ k   -   u _ k )   \ \ \ \     z _ { k + 1 }   =   prox _ { \ \ lambda _ k   f } ( x _ { k + 1 }   +   u _ k )   \ \ \ \     u _ { k + 1 }   =   u _ k   +   x _ { k + 1 }   -   z _ { k + 1 }     $ $       多个 函数       优化 如下 问题 ， 其中 这些 函数 都 有 可能 是 不 光滑 的       $ $     \ \ min   f _ 1 ( x )   +   ...   +   f _ m ( x )     $ $       将 问题 转化 为 约束 优化 问题       $ $     \ \ min _ { x   \ \ in   D }     f ( x )   \ \ \ \     f ( x )   =   f _ 1 ( x _ 1 )   +   ...   +   f _ m ( x _ m )   \ \ \ \     x   =   ( x _ 1 ,   ... ,   x _ m ) ,   D =   { ( x _ 1 , ... , x _ m )   |   x _ 1 = ... = x _ m }     $ $       相关 资料           比较 全面 介绍 近似算法 的 书 ； Parikh   N ,   Boyd   S .   Proximal   algorithms [ J ] .   Foundations   and   Trends ®   in   Optimization ,   2014 ,   1 ( 3 ) :   127 - 239 .       近似算法 综述 论文 ： Combettes   P   L ,   Pesquet   J   C .   Proximal   splitting   methods   in   signal   processing [ M ] / / Fixed - point   algorithms   for   inverse   problems   in   science   and   engineering .   Springer ,   New   York ,   NY ,   2011 :   185 - 212 .       分布式 ADMM 经典 论文 ： Boyd   S ,   Parikh   N ,   Chu   E ,   et   al .   Distributed   optimization   and   statistical   learning   via   the   alternating   direction   method   of   multipliers [ J ] .   Foundations   and   Trends ®   in   Machine   Learning ,   2011 ,   3 ( 1 ) :   1 - 122 .       压缩 感知 书籍 ： Foucart   S ,   Rauhut   H .   A   mathematical   introduction   to   compressive   sensing [ M ] .   Basel :   Birkh ä user ,   2013 .      ", "tags": "machine-learning/optimization", "url": "/wiki/machine-learning/optimization/proximal-algorithm.html"},
      
      
      
      {"title": "Optimization Methods for Large-Scale Machine Learning", "text": "    Table   of   Contents           关于                           关于       了解 分布式 梯度 优化 方法 ， 来源于 综述 文章 ：     Leon   Bottou ,   Frank   E   Curtis ,   Jorge   Nocedal ,   Optimization   Methods   for   Large - Scale   Machine   Learning ,   2016 ，     发表 于   ICML2016 。        ", "tags": "machine-learning", "url": "/wiki/machine-learning/optimization-method-for-large-scale-machine-learning.html"},
      
      
      {"title": "OWL-QN: Scalable Training of L1-Regularized Log-Linear Models", "text": "    Table   of   Contents           关于           问题           记号           OWL - QN   算法                 关于       论文 导读 ： Scalable   Training   of   L1 - Regularized   Log - Linear   Models ,   Galen   Andrew ,   Jianfeng   Gao     Microsoft   Research   ,   2007 .       问题       优化 问题 ：       $ $     f   ( x )   =   l ( x )   +   r ( x )   \ \ \ \     r ( x )   =   \ \ sum   | x _ i |       $ $       l1   范数 优化 ， 在 0 处 不可 导 。       记号       左右 偏导       $ $     \ \ partial _ i ^ +   f ( x )   =   \ \ lim _ { \ \ alpha   \ \ rightarrow   0 ^ + }   \ \ frac { f ( x   +   \ \ alpha   e _ i )   -   f ( x ) } { \ \ alpha }     $ $           方向 导数   $ ( f ' ( x ;   d ) ) $   表示 在 方向 d   对应 的 方向 。       符号 函数   $ ( \ \ sigma ( x ) ) $       投影 函数           $ $     \ \ pi _ i ( x ;   y )   =   \ \ begin { cases }                                     x _ i   & amp ;   if   \ \ sigma ( x _ i )   =   \ \ sigma ( y _ i )     \ \ \ \                                     0       & amp ;   \ \ text { otherwise }                                     \ \ end { cases }     $ $       OWL - QN   算法       在 一个 象限 中 ， 二阶 近似 是 成立 的 ， 且 二阶 梯度 矩阵 与 正则 项 无关 。       对 任意 符号 向量   $ ( \ \ xi   \ \ in   \ \ { - 1 ,   0 ,   1   \ \ } ^ n ) $ ， 定义       $ $     \ \ Omega _ { \ \ xi }   =   \ \ {   x   \ \ in   R ^ n   |   \ \ pi ( x ,   \ \ xi )   =   x   \ \ }     $ $       在 象限 $ ( \ \ Omega _ { \ \ xi } ) $ 中 ， 目标 函数 可以 写为       $ $     f ( x )   =   l ( x )   +   C   \ \ xi ^ T   x .     $ $       将 这个 函数 扩展 到 整个 空间 得到 扩展 函数 $ ( f _ { \ \ xi } ) $ ， 对于 这个 函数 ， 可以 用拟 牛顿 法 求解 。       伪 梯度 ： 其实 就是 将 某个 方向 不可 微点 的 梯度 在 该 方向 的 分量 置 为 搜索 方向 的 方向 梯度 ； 或置 0 （ xi = 0 点 ） ， 或 正 梯度 或负 梯度 .       $ $     \ \ Diamond _ i   f ( x )   =   \ \ begin { cases }                                     \ \ partial _ i ^ +   f ( x )   & amp ;   \ \ partial _ i ^ +   f ( x )   & lt ;   0     \ \ \ \                                     \ \ partial _ i ^ -   f ( x )       & amp ;   \ \ partial _ i ^ -   f ( x )   & gt ; 0     \ \ \ \                                     0           & amp ;     \ \ text { otherwise }                                     \ \ end { cases }     $ $               与 正常 L - BFGS 算法 的 改动 地方 。           用伪 梯度 替代 梯度       下降 方向 投影 到 伪 梯度 子 空间 ， 即 和 伪 梯度 不同 符号 的 分量 全部 置 0       采用 约束 线性 搜索 ， 以 保证 搜索 不会 超过 该 象限       计算 向量 y 只 需要 损失 函数 就 可以 ， 不用 正则 项           约束 线性 搜索 ： 在 非 0 值 ， 约束 在 原来 的 象限 和 0 值 搜索 ； 在 0 值 约束 在 负 梯度方向 ！       $ $     \ \ xi _ i   =   \ \ begin { cases }                                     \ \ sigma ( x _ i ^ k )   & amp ;   x _ i ^ k   \ \ neq   0     \ \ \ \                                     \ \ sigma ( -   \ \ Diamond _ i   f ( x ^ k ) )       & amp ;   x _ i ^ k   = 0                                     \ \ end { cases }       \ \ \ \     p ^ k   =   \ \ pi ( d ^ k ;   \ \ xi _ k )     \ \ \ \     x ^ { k + 1 }   =   \ \ pi ( x ^ k   +   \ \ alpha   p ^ k ;   x ^ k )     \ \ \ \     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/qwlqn.html"},
      
      
        
        
      
      {"title": "Facebook 预测广告点击率的实践经验", "text": "    Table   of   Contents           Facebook   预测 广告 点击率 的 实践经验           摘要           引言                         Facebook   预测 广告 点击率 的 实践经验       摘要       在线 广告 允许 广告 客户 只 为 可以 测量 的 用户 反应 出价 和 支付 ， 比如 点击 广告 。   所以 ， 在线 广告 系统 中 点击 预测 系统 是 大多数 在线 应用 的 核心 。 在 Facebook 上 ， 每天 活跃 用户 超过 7.5 亿 ， 活跃 广告 客户 有 100 多万 ， 点击率 预估 在 Facebook 广告 中是 一个 具有 挑战性 的 机器 学习 任务 。 本文 介绍 了 一个 结合 决策树 与 逻辑 回归 的 模型 ， 它 超越 了 其中 任何 一个 单独 的 方法 ， 效果 提升 了 超过 3 ％ ， 这个 提升 对 整个 广告 系统 的 效果 有显 着 的 影响 。 然后 我们 探索 一些 基本参数 如何 影响 我们 系统 的 最终 预测 性能 。 我们 发现 ， 最 关键 的 是 要 有 正确 的 特征 ：   那些 有关 用户 历史 信息 或 广告 领域 的 其他 类型 的 特征 。 一旦 我们 有 正确 的 特征 和 正确 的 模型 （ 决策树 加 逻辑 回归 ） ， 其他 因素 影响 很小 （ 在 大规模 的 情况 下 ， 虽然 很小 的 改善 也 是 重要 的 ） 。 选择 最佳 处理 数据 新鲜度 ， 学习 速率 和 数据 采样 只 稍微 提高 了 一点点 ， 远 不及 添加 一个 高 价值 的 特征 ， 或 选择 正确 的 模型       引言       数字 广告 是 一个 价值 数十亿美元 的 行业 并且 每年 大幅 增长 。 在 大多数 在线 广告 平台 ， 广告 的 分配 是 动态 的 ， 基于 用户 观察 后 的 反馈 为 用户 的 兴趣 量身 打造 的 。 机器 学习 在 预估 候选 广告 给 用户 的 效果 中起 着 核心作用 ， 并 增加 了 市场 效率 。     2007 年 ， Varian   [ 11 ] 和 Edelman [ 4 ] 等 人 的 开创性 论文 描述 了 由 Google 和 Yahoo 的 出价 ( bid ) 和 每次 点击 付费 的 先驱 工作 !   同年 ， 微软 也 是 在 此基础 上 基于 竞价 模型 构建 一个 赞助 搜索 市场 [ 9 ] 。   广告 拍卖 的 效率 取决于   对 点击 预测 的 准确度 和 校准 。   该   点击 预测 系统 需要 具有 鲁棒性 和 自适应性   能够 从 大量 的 数据 中 学习 。   目标   本文 的 目的 是 分享 来自 实验 的 见解   考虑 到 这些 要求 并 执行   对 现实 世界 的 数据 。   在 赞助 搜索 广告 中 ， 用户 查询 用于   检索 候选 广告 ， 显式 或 隐式 地   匹配 查询 。   在 Facebook 上 ， 广告 没有 关联   用 查询 ， 而是 指定 人口 和 兴趣   定位 。   因此 ， 广告 的 数量   有 资格 在 用户 访问 Facebook 时 可以 显示   大于 赞助 搜索 。   为了 处理 大量 的 候选 广告   请求 ， 每当 用户 触发 广告 请求   访问 Facebook ， 我们 将 首先 建立 一个 级联 的 分类器   增加 计算成本 。   在 本文 中 ， 我们 重点 介绍   级联 分类器 的 末级 点击 预测 模型 ，   那 就是 为 最终 集合 产生 预测 的 模型   的 候选 广告 。   我们 发现 一个 结合 决策树 的 混合 模型   逻辑 回归 优于 这 两种 方法 之一   靠 自己 超过 3 ％ 。   这种 改善 有 重大意义   影响 整个 系统 的 性能 。   一些   基本参数 影响 最终 的 预测 效果   -   我们 的 系统 。   如 预期 的 那样 最 重要 的 事情   是 要 有 正确 的 功能 ：   形成 关于 用户 或 广告 主宰 其他 类型 的 功能 ，   功能 。   一旦 我们 有 正确 的 功能 和 正确 的 模式   （ 决策树 加 逻辑 回归 ） ， 其他 因素 发挥作用   小角色 （ 尽管 小小的 改进 很 重要   规模 ） 。   为 数据 新鲜度 选择 最佳 处理 方式 ，   学习 率 模式 和 数据 采样 改善 模型   稍微 ， 虽然 比 添加 高 价值 功能 少得 多 ，   或者 选择 合适 的 模型 开始 。   我们 首先 概述 了 我们 在 Sec -   在 第 3 部分 我们 评估 不同 的 概率 线性     第 2 页     分类器 和 多种 在线 学习 算法 。   在 合约   线性 分类 文本 我们 继续 评估 影响   功能 转换 和 数据 新鲜度 。   受到 了 启发   吸取 的 实际 经验教训 ， 特别 是 数据 新鲜度   和 在线 学习 ， 我们 提出 了 一个 模型 架构 ，   企业 在线 学习 层 ， 而 公平 地 生产   紧凑型 号 。   第 4 节 描述 了 一个 关键 组件   为 在线 学习 层 ， 在线 木匠 ， 一个   可以 生成 实况 的 实验性 基础设施   实时 训练 数据流 。   最后 ， 我们 提出 交易 记忆 和 准确性 的 方法   计算 时间 ， 并 应付 大量 的 培训   数据 。   在 第五 部分 中 ， 我们 描述 了 一些 实用 的 方法 ，   包含 大规模 应用程序 的 时间 和 延迟   在 第 6 节 我们 深入 到 了 训练 数据 之间 的 权衡   数量 和 准确性 。   2 . 实验 安装   为了 达到 严格 和 可控 的 实验 ， 我们   通过 选择 任意 一周 准备 训练 数据   在 2013 年 第四季度 。 为了 保持 不变   培训 和 测试数据 在 不同 的 条件 下 ，   削减 了 与 观察 到 的 类似 的 训练 数据   线上 。   我们 把 存储 的 数据 分成 训练 和   测试 并 使用 它们 来 模拟 在线 数据 ，   在线 培训 和 预测 。   相同 的 培训 / 测试数据   被 用作 本文 所有 实验 的 测试 平台 。   评估 指标 ： 因为 我们 最 关心 的   这些 因素 对 机器 学习 模型 的 影响 ，   我们 直接 使用 预测 的 准确性 而 不是 度量   与 利润 和 收入 有关 。   在 这项 工作 中 ， 我们 使用 Normal -   熵 （ NE ） 和 校准 作为 我们 的 主要 评估   度量 。   归一化 熵 更 准确 ，   熵 相当于 每次 印象 的 平均 对数 损失   除以 每次 印象 的 平均 对数 损失   如果 一个 模型 预测 了 背景 点击率   （ CTR ） 为 每个 印象 。   换句话说 ，   由 背景 的 熵 归一化 的 对数 对数 损失   CTR 。   背景 点击率 是 平均 经验 点击率   的 训练 数据 集 。   这 可能 会 更 多 的 描述   -   将 这个 度量 称为 归一化 对数 （ Normalized   Logarithmic ）   失利 。   值越 低 ， 预测 越 好   由 模型 制作 。   这个 正常化 的 原因 是   背景 CTR 越 接近 0 或 1 ，   更 容易 实现 更好 的 日志 丢失 。   除以 en -   背景 CTR 的 回归 使得 NE 不 敏感   背景 点击率 。   假设 一个 给定 的 训练 数据 集   N 个 具有 标签 的 示例 y   i   2   { 1 ， +   1 } 和 估计 的 概率   -   点击 的 能力   我 在 哪里 我 =   1 ， 2 ， ...   N 。   平均 经验值   CTR 作为 p   NE   =   1   N   P   n   我 =   1   （   1   +   y   我   2   log （ p   i   ） +   1y   i   2   日志 （ 1   p   i   ） ）   （ p ⇤ log （ p ） + （ 1   p ） ⇤ log （ 1   P ） ）   （ 1 ）   NE 基本上 是 计算 相对 信息 的 一个 组成部分 ，   增益 （ RIG ） 和 RIG   =   1   NE   图 1 ： 混合 模型 结构   输入 功能   通过 增强 的 决策树 进行 转换 。   每个 单独 的 树 的 输出 被 视为 一个   将 分类 输入 特征 映射 到 稀疏 线性 分类器 。   提升 的 决策树 被 证明 是 非常 强大 的   功能 转换 。   校准 是 平均 估计 点击率 和   经验性 点击率   换句话说 ， 这是 数字 的 比例   预期 点击 次数 达到 实际 观察 到 的 点击 次数 。   校准 是 一个 非常 重要 的 指标 ， 因为 准确 和   经过 精心 校准 的 CTR 预测 对 成功 至关重要   在线 竞价 和 拍卖 。   校准 不同   从 1 开始 ， 模型 越 好 。   我们 只 报告 校准   在 不 重要 的 实验 中 。   请 注意 ， ROC 区域 （ AUC ） 也 是 相当 不错 的   不 考虑 排名 质量 的 度量   校准 。   在 现实 的 环境 中 ， 我们 预计 ，   词典 是 准确 的 ， 而 不是 仅仅 得到 最佳 的 选择 。   错误 的 排列 顺序 ， 以 避免 潜在 的 不足 交付 或 过度 使用 ，   交货 。   NE 衡量 预测 的 好处 ，   明确 地 反映 了 校准 。   例如 ， 如果 一个 模型 over -   预测 2 倍 ， 我们 应用 全球 乘数 0.5 来 解决   校准 后 ， 对应 的 NE 也 会 有所改善   即使 AUC 保持 不变 。   详见 [ 12 ]   研究 这些 指标 。   3 . 预测 模型 结构   在 本节 中 ， 我们 提出 一个 混合 模型 结构 ：   提升 的 决策树 和 概率   -   抽动 稀疏 线性 分类器 ， 如图 1 所示 。 在 Sec -   我们 显示 决策树 是 非常 强大 的 输入   功能 转换 ， 这 大大增加 了 ac -   概率 线性 分类器 的 精度 。   在 3.2 节 我们   展示 如何 更 新鲜 的 训练 数据 导致 更 准确 的 预处理 ，   新词语 。   这 激发 了 使用 在线 学习 的 想法   方法 来 训练 线性 分类器 。   在 3.3 节中 ，   削减 了 两个 家庭 的 在线 学习 变量   概率 线性 分类器 。   我们 评估 的 在线 学习 计划 是 基于     第 3 页     随机 梯度 下降 （ SGD ） 算法 [ 2 ] 适用 于   稀疏 线性 分类器 。   经过 功能 转换 后 ，   广告 印象 是 以 结构化 向量 x   = 给出 的   （ e   i   1   ， ... ， e   i   n   ） 其中 e   i 是 第 i 个 单位 矢量 ， i   1   ， ... ， i   n   是 n 个 分类 输入 特征 的 值 。   在 里面   训练 阶段 ， 我们 也 假设 我们 得到 了 一个 二进制   指示 点击 或 不 点击 的 标签 y   2   { + 1 , 1 } 。   给定 一个 标记 的 广告 印象 （ x ， y ） ， 让 我们 表示 线性   活动 权重 的 组合   s （ y ， x ， w ） =   y · w   T   x   =   y   ñ   X   J   =   1   w   j ， i   j   ，   （ 2 ）   其中 w 是 线性 点击 分数 的 权重 向量 。   在 最 先进 的 贝叶斯 在线 学习 计划 中   概率 回归 （ BOPR ） 描述 [ 7 ] 的 可能性 和   事先 给出   p （ y   |   x ， w ） =   ✓ s （ y ， x ， w ） ◆ ，   p （ w ） =   ñ   ÿ   k   =   1 时   N （ W · K ; μ K ， 2   k   ） ，   （ t ） 是 标准 的 累积 密度 函数   正态分布 和 N （ t ） 是 密度 函数   标准 正态分布 。   在线 培训 已经 实现   通过 期望 传播 与 时刻 匹配 。   最终 的 模型 由 均值 和 方差 组成   权重 向量 的 近似 后验 分布   W 上 。   BOPR 算法 中 的 推理 是 计算   p （ w   |   y ， x ） 并 将 其 投影 到 最 接近 因子 分解 的 Gaus -   （ w ） 的 sian 近似 。   因此 ， 更新 算法   可以 单独 用 所有 的 更新 方程 来 表示   意义 和 方差 的 非 零组件 x （ 见 [ 7 ] ） ：   μ I   J   μ I   J   +   Y ·   2   我   j   ^ · v ✓   s （ y ， x ， μ ）   ^   ◆ ，   （ 3 ）   2   我   j   2   我   j   · “ 1   2   我   j   ^   2   · w ✓   s （ y ， x ， μ ）   ^   ◆ ＃ ，   （ 4 ）   ^   2   =   2   +   ñ   X   J   =   1   2   我   j   。   （ 5 ）   这里 ， 校正 函数 v 和 w 由 v （ t ） ： = 给出   N （ t ） / （ t ） 和 w （ t ） ： =   v （ t ） · [ v （ t ） +   t ] 。   这个 推断 可以   被 视为 信念 向量 μ 和 的 SGD 方案 。   我们 将 BOPR 与 可能性 函数 的 SGD 进行 比较   p （ y   |   x ， w ） =   sigmoid （ s （ y ， x ， w ） ） ，   其中 sigmoid （ t ） =   exp （ t ） / （ 1   +   exp （ t ） ） 。   由此 产生 的 al -   算法 通常 被 称为 Logistic 回归 （ LR ） 。   推断   -   在 这个 模型 中 计算 对数 的 导数 ，   可能性 和 步行 每个 坐标 依赖 步长   这个 梯度 的 方向 ：   w   i   j   瓦特   I   J   +   Y · ⌘ I   J · G （ S （ Y ， X ， W ） ） ，   （ 6 ）   其中 g 是 所有 非零 com -   并且 由 g （ s ） 给出 ： =   [ y （ y   +   1 ） /   2   y · sigmoid （ s ） ] 。   请 注意 ， （ 3 ） 可以 被 看作 是 一个 每 坐标 梯度 de -   香味 等 （ 6 ） 上 的 平均 矢量 μ ， 其中 步长 ⌘ I   J   是 由 信念 不确定性 自动控制 的 。   在   3.3 小节 将 介绍 各种 步长 函数 ⌘   并 与 BOPR 进行 比较 。   上述 的 基于 SGD 的 LR 和 BOPR 都 是 流式 的   学习者 一个 接 一个 地 适应 训练 数据 。   3.1 决策树 特征 转换   有 两种 简单 的 方法 来 转换 输入 功能   的 线性 分类器 ， 以 提高 其 准确性 。   对于   连续 的 功能 ， 学习 非线性 的 简单 技巧   转化 是 为了 将 这个 特征 和 待处理 的 垃圾箱   dex 作为 一个 分类 特征 。   线性 分类器 有效   学习 特征 的 分段 恒定 非线性 映射 。   学习 有用 的 bin 边界 是 很 重要 的 ， 并且 有   许多 信息 最大 限度 地 做到 这 一点 。   第二个 简单 而 有效 的 转换 包括   构建 元组 输入 功能 。   对于 分类 特征 ，   蛮力 方法 包括 采取 笛卡尔 产品 ，   uct ， 即 创建 一个 新 的 分类 特征   作为 原始 特征 的 所有 可能 值 的 值 。   不   所有 的 组合 都 是 有用 的 ， 而 那些 不是 可以 的   剪掉 了 。   如果 输入 功能 是 连续 的 ， 可以 这样 做   联合 装箱 ， 例如 使用 kd 树 。   我们 发现 ， 推动 决策树 是 一个 强大 的 ， 非常   便捷 的 方式 来 实现 非线性 和 元组 transfor -   我们 刚刚 描述 的 那种 。   我们 对待 每个 指标 ，   个人 树 作为 一个 分类 特征 ， 作为 值   叶子 的 索引 最终 落入 。 我们 使用 1 -   这种 类型 的 特征 的 - K 编码 。   例如 ， 考虑   图 1 中 的 增强 树 模型 有 2 个子 树 ， 其中   第一个 子树有 3 个 叶子 和 第二个 叶子 。   如果   实例 结束 于 第一个 子树 中 的 叶子 2 和 叶子 1 中   第二个 子树 ， 将 整体 输入 到 线性 分类器 中   是 二进制 向量 [ 0 , 1 , 0 , 1 , 0 ] ， 其中 前 3 个 条目   对应 第一个 子树 的 叶子 ， 最后 2 个   那些 第二个 子树 。   推动 决策树 我们   使用 遵循 梯度 增压 机 （ GBM ） [ 5 ] ， 其中   使用 经典 的 L   2   - TreeBoost 算法 。   在 每个 学习   -   迭代 ， 创建 一个 新 的 树来 模拟 残差   以前 的 树木 。   我们 可以 理解 提升 的 决策树   作为 一个 监督 的 特征 编码   将 实值 向量 转换 为 紧凑 二进制 值   向量 。   从根 节点 到 叶 节点 的 遍历 表示   某些 特征 的 规则 。   拟合 一个 线性 分类器   二元 向量 本质 上 是 学习 权重 的 集合   规则 。   提升 的 决策树 以 批处理 方式 进行 培训 。   我们 进行 实验 来 显示 包括 树 的 效果   作为 线性 模型 的 输入 。   在 这个 实验 中   我们 比较 两个 logistic 回归 模型 ，   变换 和 其他 与 平原 （ 非 转换 ）   特征 。   我们 也 只 使用 一个 提升 的 决策树 模型   比较 。   表 1 显示 了 结果 。   树状 特征 转换 有助于 减少 标准化 的 En -   相对 于 标准化 来说 超过 3.4 ％   没有 树 变换 的 模型 的 熵 。   这是 一个   非常 显着 的 相对 改善 。   作为 参考 ，   ical   feature   engineering   experiment   will   shave   off 夫妇   几十 ％ 的 相对 NE 。   有趣 的 是 看到     第 4 页     表 1 ： Logistic 回归 （ LR ）   锡永树 （ 树 ） 做出 强有力 的 组合 。   我们   评估 他们 的 归一化 熵 （ NE ）   相对 于 只有 树 的 模型 。   模型 结构 NE （ 仅限于 树 ）   LR   + 树木   96.58 ％   只有 LR   99.43 ％   树 只   100 ％   （ 参考 ）   图 2 ： 预测 精度 作为 一个 函数   训练 和 测试 之间 的 延迟 以天 为 单位 。   Accu -   活动 表示 为 归一化 熵 相对 于   最坏 的 结果 ， 得到 的 树木 模型   推迟 6 天 。   孤立 使用 的 LR 和 Tree 模型 具有 相似性 ，   rable 预测 准确性 （ LR 是 好 一点 ） ， 但 它 是   他们 的 组合 产生 了 一个 精确 的 飞跃 。   中 的 收益   预测 准确性 显 着 ;   供 大家 参考   特征 工程 实验 只能 减少   归一化 的 熵 由 百分之几 分 之一 。   3.2 数据 新鲜度   点击 预测 系统 通常 部署 在 动态 环境 中 ，   数据分布 随 时间 变化 的 数据 。   我们   研究 训练 数据 新鲜度 对 预测 每个 人 的 预测 效果 ，   formance 。   要 做到 这 一点 ， 我们 在 一个 特定 的 日子 里 训练 模型   并 在 连续 的 几天 进行 测试 。   我们 运行 这些 实验   对于 一个 推动 的 决策树 模型 和 一个 逻辑 的   具有 树 变换 的 输入 特征 的 回归 模型 。   在 这个 实验 中 ， 我们 训练 一天 的 数据 ， 并 进行 评估   连续 六天 计算 归一化   每个 熵 。   结果 如图 2 所示 。   预测 精度 明显降低 ， 因为 两种 模式   训练 和 测试 集 之间 的 延迟 增加 。   对于 这 两个 mod -   可以 看出 NE 可以 减少 近似值   从 每周 的 培训 到 每天 的 培训 只有 1 ％ 。   这些 发现 表明 ， 每天 都 值得 重新 培训   基础 。   一种 选择 是 重复 日常 工作   可能 会 批量 培训 模型 。   所 需要 的 时间   再 培训 推动 的 决策树 根据 不同 的 因素 而 有所不同   如 培训 的 例子 数量 ， 树木 的 数量 ，   每棵 树上 的 叶子 数量 ， cpu ， 内存 等 。 可能 需要   超过 24 小时 ， 建立 一个 数百人 的 助推 模型   数以亿计 的 实例 中 ，   GLE 核心 CPU 。   在 实际 情况 下 ， 可以 完成 培训   在 几个 小时 内 通过 多 核心 的 充分 并发   整体 拥有 大量 内存 的 机器   训练 集 。   在 下 一节 我们 考虑 一个 替代 方案 。   推动 决策树 可以 每天 或 每个 cou -   几天 的 时间 ， 但 线性 分类器 可以 在 近处 训练   通过 使用 一些 在线 学习 的 风味 实时 。   3.3 在线 线性 分类器   为了 最大 限度 地 提高 数据 的 新鲜度 ， 一种 选择 是 训练   在线 分类器 ， 即 直接 作为 标签   广告 印象 到达 。   在 即将 到来 的 第四 部分 中 ，   扫描 一个 可以 实时 生成 的 基础设施   训练 数据 。   在 本节 中 ， 我们 将 评估 几种 方法   为 基于 SGD 的 在线 学习 设置 学习 率 ，   gistic 回归 。   然后 ， 我们 将 最好 的 变体 与 在线 比较   学习 BOPR 模型 。   就 （ 6 ） 而言 ， 我们 探讨 以下 选择 ：   1 . 每 坐标 学习 率 ：   迭代 t 的 ture 被 设置 为   ⌘ t   ， i   =   ↵   +   qP   t   j   =   1   r   2   J ， I   。   是 两个 可调 参数 （ 在 [ 8 ] 中 提出 ） 。   2 . 权重 平方根 学习 率 ：   ⌘ t   ， i   =   ↵   pn   t ， i   ，   其中 n   t ， i 是 具有特征 的 总 训练 实例   我 直到 迭代 t 。   3 . 重量 学习 率 ：   ⌘ t   ， i   =   ↵   不   ， 我   。   4 . 全球 学习 率 ：   ⌘ t   ， i   =   ↵   PT 。   5 . 不断 学习 率 ：   ⌘ t   ， i   = ↵ 。   前 三个 方案 分别 设置 学习 率   特征 。   最后 两个 使用 相同 的 速度 为 所有 功能 。   所有   可调 参数 通过 网格 搜索 （ optima ） 进行 优化   详见 表 2 . ）   我们 将 学习 率 下降 0.00001 连续   学习 。   我们 用 相同 的 数据 训练 和 测试 LR 模型   以上 学习 率 计划 。   实验 结果 是   如图 3 所示 。   从 以上 结果 可以 看出 ， SGD 以 每 坐标 学习 为主   速率 达到 最好 的 预测 精度 ， 用 NE   al -   比 使用 每 重量 学习 率低 5 ％     第 5 页     表 2 ： 学习 速率 参数   学习 率 模式   参数   每 坐标   0.1   =   0.1 ， =   1.0   重量 平方根   =   0.01   每 重量   =   0.01   全球   =   0.01   不变   0.000   =   0.0005   图 3 ： 不同 学习 的 实验 结果   LR 新加坡元 汇率 。   X 轴 cor -   回应 不同 的 学习 率 计划 。   我们   在 左侧 绘制 标定 y -   轴 ， 而 归一化 的 熵 则 用   右边 的 第二 y 轴 。   表现 最差 。   这个 结果 是 符合 结论 的 ，   sion   [ 8 ] 。   新元 与 重量 平方根 和 恒定   学习 率 达到 相似 和 略 差 的 NE 。   该   其他 两种 方案 比 以前 差 很多   版本 。   全球 学习 率 主要 由于 失败   每个 功能 上 的 训练 实例 数量 不 平衡 。   由于 每个 培训 实例 可能 包含 不同 的 功能 ，   一些 受欢迎 的 功能 ， 接受 更 多 的 培训 ，   立场 比 别人 。   根据 全球 学习 率 计划 ，   对于 具有 较 少 实例 的 特征 的 学习 率 ，   折痕 太快 ， 并 防止 收敛 到 最佳 状态   重量 。   虽然 每 重量 学习 费率 计划 ad -   打扮 这个 问题 ， 它 仍然 失败 ， 因为 它 减少 了   所有 功能 的 学习 速度 太快 。   培训 终止   模型 收敛 到 次 优点 的 地方 太早 了 。   这 就 解释 了 为什么 这个 方案 的 性能 最差   在 所有 的 选择 之中 。   有意思 的 是 BOPR 更新 方程   （ 3 ） 平均数 与 每 坐标 学习 最 相似   SGD 的 LR 版本 。   有效 的 学习 率   BOPR 是 针对 每个 坐标 而定 的 ， 取决于   与 每个 个体 相关 的 体重 的 后 验方 差   协调 ， 以及 给予 什么 标签 的 “ 惊喜 ”   模型 会 预测 [ 7 ] 。   我们 进行 了 一个 实验 ，   通过 每个 坐标 SGD 和 BOPR 训练 LR 的 形式 。   我们 训练 LR 和 BOPR 模型   排序 器 ”   Online ' Joiner ”   培训师 ”   功能 { x }   点击 { y }   楷模   { x ， y }   广告   图 4 ： 在线 学习 数据 / 模型 流程   数据 并 评估 下 一次 的 预测 性能   天 。   结果 如表 3 所示 。   表 3 ： 每 坐标 在线 LR 与 BOPR   模型 类型   NE （ 相对 于 LR ）   LR   100 ％   （ 参考 ）   BOPR   99.82 ％   也许 正如 人们 所 期望 的 那样 ， 鉴于 质量 的 相似性   更新 公式 ， BOPR 和 LR 用 SGD 进行 培训   每 坐标 学习 率有 非常 相似 的 预测 ，   无论是 NE 还是 校准 都 可以 提高 性能   （ 未 在 表格 中 显示 ） 。   LR 比 BOPR 的 一个 优点 是 模型 的 大小   是 一半 ， 因为 只有 一个 相关 的 权重   稀疏 特征值 ， 而 不是 均值 和 方差 。   去   等待 实施 ， 更 小 的 模型 大小 可能   导致 更好 的 缓存 局部性 ， 从而 更快 的 缓存 查找 。   在   在 预测 时间 计算 费用 的 条款 ， LR   模型 只 需要 一个 内部 产品 的 功能 vec -   tor 和 权重 矢量 ， 而 BOPR 模型 需要 两个   方差 向量 和 均值 向量 的 内积   与 特征向量 。   BOPR 比 LR 更 重要 的 一个 优点 是 ，   贝叶斯 公式 ， 它 提供 了 一个 完整 的 预测 分布   -   点击 概率 。   这 可以 用来 com -   预测 分布 的 百分位 数 ， 可以   用于 探索 / 开发 学习 计划 [ 3 ] 。   4 . 在线 数据 连接器   前 一节 确定 了 更新 的 训练 数据   导致 预测 准确度 提高 。   它 还 提出 了 一个   线性 分类器 层 的 简单 模型 架构   在线 培训 。   本 部分 介绍 了 一个 实验 系统 ，   用于 训练 线性 分类器 的 实时 训练 数据 ，   通过 在线 学习 更加 困难 。   我们 将 这个 系统 称为   “ 在线 木匠 ” ， 因为 它 的 关键 操作 是 加入   标签 （ 点击 / 不 点击 ） 到 培训 输入 （ 广告 印象 ） 中   一个 在线 的 方式 。   类似 的 基础设施 用于 流   例如 在 Google 广告 系统 [ 1 ] 中 学习 。   在线 木匠 输出 实时 训练 数据流   到 一个 名为 Scribe 的 基础设施 [ 10 ] 。   而 积极 的     第 6 页     标签 （ 点击 ） 是 明确 的 ， 没有 这样 的 事情   用户 可以 按 “ 不 点击 ” 按钮 。   为此 ， 一个   印象 被 认为 是 否定 否定 点击 标签   用户 在 固定 之后 没有 点击 广告 ， 并且 充分 地   看到 广告 后 很长 一段时间 。   的 长度   等待时间 窗口 需要 仔细 调整 。   使用 等待时间 过长 会 延迟 实时 训练 ，   增加 数据 并 增加 分配 给 缓存 的 内存   等待 点击 信号 的 印象 。   太短 了   时间 窗口 导致 一些 点击 丢失 ， 因为   相应 的 印象 可能 已经 被 刷新 ，   作为 未 点击 的 贝壳 。   这 对 “ 点击 覆盖 ” 产生 了 负面影响   所有 点击 成功 加入 展示 的 比例 。   因此 ， 网上 木匠 系统 必须 取得 平衡   新近 度 和 点击率 之间 。   没有 完整 的 点击 覆盖 意味着 实时 培训   -   一套 将 是 有 偏见 的 ： 经验性 的 点击率 是 有点   低于 事实 的 真相 。   这 是因为 一小部分   的 标记 为 未 点击 的 展示 次数 将会 是 la -   如果 等待时间 足够 长 ， 可以 点击 。   然而 在实践中 ， 我们 发现 很 容易 减少 这种 情况   对 等待 窗口 百分比 的 小数点 偏差   大小 ， 导致 可 管理 的 内存 要求 。   在   此外 ， 这个 小 偏差 可以 测量 和 纠正 。   可以 找到 更 多 关于 窗口 大小 和 效率 的 研究   在 6 点钟 ] 。   在线 木匠 被 设计 来 执行 分布式   流到 流 加入 广告 展示 和 广告 点击 uti -   将 请求 ID 作为 连接 的 主要 组件   谓词 。   每次 用户 每次 生成 一个 请求 ID 时 ，   在 Facebook 上 形成 一个 触发 刷新 的 动作   他们 所 接触 到 的 内容 。   原理图 数据 和 模型   显示 了 在线 加工者 随后 在线 学习 的 流程   如图 4 所示 。 初始 数据流 是 当 用户 产生 的   访问 Facebook 并 向 排名 提出 请求 ，   没有 广告   广告 被 传回 给 用户 的 设备   并行 使用 每个 广告 和 相关 的 功能   排名 印象 被 添加 到 印象 流中 。   如果 用户 选择 点击 广告 ， 该 点击 将 被 添加   到 点击 流 。   实现 流到 流 的 连接   系统 利用 一个 HashQueue 组成 一个 First - In -   先出 队列 作为 缓冲 窗口 和 快速 的 散列图   随机 访问 标签 展示 次数   一个 HashQueue   在 键值 对 上 有 三种 操作 ： 排队 ，   出列 和 查找 。   例如 ， 要 排队 一个 项目 ， 我们   将 项目 添加 到 队列 的 前面 并 在 中 创建 一个 键   散列 映射 ， 其值 指向 队列 的 项目 。   只有 完整 的 加入 窗口 过期 后 ， 标签 才 会 显示   印象 被 发射 到 训练 流 。   如果 没有 点击   它 将 作为 负面 标记 的 例子 发射 出去 。   在 这个 实验 设置 中 ， 教练员 不断 学习   从 培训 流程 中 发布 新款 车型 ，   对于 Ranker 而言 。   这 最终 形成 一个 严密 的 封闭   循环 的 机器 学习 模型 的 变化 ，   可以 捕获 分布 或 模型 性能 ，   了解 到 ， 并 在短期内 继续 纠正 。   实验 一个 重要 的 考虑 因素   实时 训练 数据 生成 系统 是 需要 的   建立 防止 异常 的 保护 机制   腐败 在线 学习 系统 。   让 我们 来 简单 介绍 一下   例 。   如果 点击 流 由于 某些 原因 而 变得 陈旧   数据 基础设施 问题 ， 网上 木匠 将 产生 train -   这些 数据 具有 非常 小 的 甚至 为 零 的 经验性 点击率 。   由此 ， 实时 教练 将 开始   错误 地 预测 非常低 ， 或者 接近 零 概率   单击 。   广告 的 预期 价值 自然 取决于   估计 的 点击 概率 ， 以及 一个 结果   错误 地 预测 非常低 的 点击率 是 系统 可能   展示 广告 展示 次数 减少 。   异常 检测   -   重刑 机制 可以 在 这里 帮助 。   例如 ，   从 网上 木匠 matically 断开 在线 教练   如果 实时 训练 数据分布 突然 改变 。   5 . 包含 内存 和 延迟   5.1 增强 树 的 数量   模型 中 的 树越 多 ， 所 需 的 时间 就 越长   做 一个 预测 。   在 这个 部分 ， 我们 研究 了 这个 效果   估计 精度 的 推动 树 数量 。   我们 改变 树 的 数量 从 1 到 2   000 ， 并 训练   模型 在 一天 的 数据 ， 并 测试 预测 per -   在 第二天 的 表演 。   我们 只 限于 此   每棵 树上 有 12 片 叶子 。   与 之前 的 实验 类似 ，   我们 使用 归一化 熵 作为 评估 指标 。   该   实验 结果 如图 5 所示 。   图 5 ： 增强 次数 的 实验 结果   树木 。   不同 系列 对应 不同 的 子 系列   楷模 。   x 轴 是 助推 树 的 数量 。   Y 轴 是 归一化 的 熵 。   热带植物 减少 ， 因为 我们 增加 了 增加 树木 的 数量 。   然而 ， 增加 树木 的 收益 会 减少 重新 生长 ，   转 。   NE 的 改进 几乎 都 来自 前 500 名   树木 。   最后 一千 棵 树 使 NE 降低 不到 0.1 ％ 。   此外 ， 我们 看到 子 模型 的 归一化 熵   2 棵 树 开始 退化 后 ， 1000 棵 树 。   之所以 这样 做 ，   现象 是 过度 配合 。   由于 子 模型 的 训练 数据   2 比子 模型 0 和 1 小 4 倍 。   5.2 提升 功能 重要性   特征 计数 是 另 一个 模型 特征 ，   在 估算 精度 和 计算 之间 取舍 ，   性能 。   为了 更好 地 理解 功能 的 效果   算上 我们 首先 将 特征 重要性 应用 于 每个 特征 。   为了 衡量 我们 使用 的 功能 的 重要性   统计 促进 特征 重要性 ，     第 7 页     确定 归因于 特征 的 累计 损失 减少 量 。   在 每个 树 节点 结构 中 ， 选择 一个 最佳 特征   分割 以 最大化 平方 误差 减少 。   由于 一个 功能 ，   可以 用于 多种 树木 ， （ 增强 特征   重要性 ） 为 每个 功能 是 通过 求和 来 确定 的   所有 树木 的 特定 特征 的 总 减少 量 。   通常 情况 下 ， 少数 特征 贡献 了 主   -   而 其余 特征 具有 解释性 权力   只有 边际 贡献 。   我们 看到 这种 模式   当 绘制 功能 的 数量 与 他们 的 cumu -   图 6 中 的 特征 重要性 。   图 6 ： 提升 功能 重要性   X 轴 cor -   响应 功能 的 数量 。   我们 画 功能   在 左侧 小学 对数 的 重要性   y 轴 ， 而 累积 特征 的 重要性 是   与 右边 的 副 y 轴 一起 显示 。   从 以上 的 结果 可以 看出 ， 前十名 的 特点 是   负责 总 功能 重要性 的 大约 一半 ，   而 最近 的 300 个 功能 贡献 不到 1 ％ 的 功能   重要性 。   基于 这个 发现 ， 我们 进一步 做 实验   只 保留 了 前 10 , 20 , 50 , 100 和 200 个 功能 ，   并 评估 表现 如何 影响 。   的 结果   实验 如图 7 所示 。 从图 中 我们 可以 看出   可以 看到 归一化 的 熵 有 类似 的 递减   返回 属性 ， 因为 我们 包含 更 多功能 。   下面 我们 将 对 这个 实用性 进行 一些 研究   的 历史 和 背景 特征 。   由于 数据 sen -   敏度 的 性质 和 公司 政策 ， 我们 不 能够   揭示 在 我们 实际 使用 的 功能 细节 。 有些 EX -   充足 的 上下文 特征 可以 是 一天 的 当地 时间 ， 天   周等 历史 特征 可以 是 累计数   的 点击 广告 ， 等等 。   5.3 历史 特点   在 升压 模式 中 使用 的 功能 可 分为   分为 两种 类型 ： 上下文 特征 和 历史 特点 。   语境 功能 的 价值 完全 取决于 CUR -   租 有关 的 上下文 信息 ， 其中 广告 是   示出 的 ， 诸如 由 用户 或 CUR - 所 使用 的 设备   租 页面 ， 用户 上 。 相反 ， 历史   功能 依赖于 广告 或 用户 以前 的 互动 ，   例如 ， 通过 广告 的 点击率 在 上周 ， 或   平均 点击 通过 用户 的 速度 。   图 7 ： 结果 与 顶部 fea - 推进 模型   功能 。 我们 利用 校准 在 左侧 革命制度党   玛丽 y 轴 ， 虽然 示出 归一化 的 熵   与 右手 侧 次级 y 轴 。   在 这 一部分 ， 我们 研究 如何 系统 的 性能   取决于 两个 类型 的 特征 。 首先 ， 我们 检查   这 两种 类型 的 特点 的 相对 重要性 。 我们 这样 做 是 通过   按 重要性 排序 的 所有 功能 ， 然后 计算 出 per -   在 第 k 个 重要 特征 历史 特色 centage 。   结果 在 图 8 中由该 结果 所示 ， 我们 可以 看到   图 8 ： 结果 历史风貌 百分比 。   X 轴 对应 于 特征 数 。 Y 轴   给 的 历史 特点 在 顶部 K - 百分比   重要 特征 。   该 历史风貌 提供 了 相当 多 的 explana -   保守党 功率 比 上下文 特征 。 排名 前 10 的 特征 或   -   按 重要性 dered 都 是 历史 特色 。 之间   排名 前 20 位 的 特点 ， 也 有 只有 2 上下文 特征 ， 尽管   历史 特征 占据 的 特征 大致 75 ％ 在   此 数据 集 。 为了 更好 地 理解 的 比较 值   从 各 类型 合计 ， 我们 班 列车 2 个 助推 功能   荷兰 国际 集团 车型 只有 上下文 特征 ， 只有 历史   功能 ， 那么 这 两个 型号 的 完整 比较   模型 的 所有 功能 。 结果 示于 表 4 。   从表中 ， 我们 可以 再次 确认 ， 在 他 的   -   汇总   torical 功能 发挥 出比 上下文 特征 发挥 更大 的 作用 。     第 8 页     表 4 ： 具有 不同 类型 的 推进 fea - 模型   功能   特征 NE 的 类型 （ 相对 于 语境 ）   所有   95.65 ％   历史 的   96.32 ％   上下文   100 ％   （ 参考 ）   没有 只 上下文 特征 ， 我们 衡量 4.5 ％ 的 损失   预测 精度 。 相反 ， 没有 上下文   功能 ， 我们 遭受 的 预测 精度 小于 1 ％ 的 损失 。   应该 注意 到 ， 上下文 功能 都 非常 的 IM   portant 处理 冷启动 问题 。 对于 新 用户 和   广告 ， 上下文 特征 是 必不可少 的 一种 合理   点击率 预测 。   在 下 一步 ， 我们 评估 与 他 的   -   只有 在 训练 的 模型   在 连续 torical 功能 或 上下文 特征   周来 测试数据 新鲜度 的 特征 依赖 。   该   结果 示于 图 9 。   图 9 ： 结果 datafreshness 用于 不同 类型   的 功能 。 X 轴 是 而 y 轴上 的 评估 日期   是 归一化 熵 。   从图 中 我们 可以 看到 ， 与 情境 模型   功能 更 依赖 比 历史数据 新鲜度   特征 。   这 是 与 我们 的 直觉 线 ， 因为 历史 fea -   功能 形容 长期 积累 的 用户 行为 ，   比 上下文 特征 更加 稳定 。   6 . 海量 训练 数据 期 应对   Facebook 的 广告 印象 数据 的 一整天 可以 包含   大量 实例 。 请 注意 ， 我们 不能   透露 实际 数字 ， 因为 它 是 保密 的 。 但 小   一天 的 有 价值 的 数据 的 部分 可以 有 几百   数以百万计 的 实例 。 用于 控制 的 一种 常用 技术   培训 成本 降低 了 培训 的 数据量 。   在 这 一节 中 ， 我们 评估 两种 技术 下 采样   数据 ， 均匀 子 采样 和 负 向下 采样 。   在   每次 我们 训练 了 一套 增强型 树 模型 与 600 棵 树 情况   并 同时 使用 校准 和 标准化 评估 这些   熵 。   6.1 统一 的 二次 抽样   培训 行 统一 欠 采样 是 一个 诱人 的 AP -   proach 减少 数据量 ， 因为 它 是 既 容易   实现 并且 可以 使用 所 产生 的 模型 与   -   出来 的 二次 抽样 训练 数据 和 修改 都   非 二次 抽样 的 测试数据 。 在 这 一部分 ， 我们 评估 了 一组   大约 成倍增加 二次 采样率 。   对于   每 一个 速度 ， 我们 培养 了 增强型 树 模型 ， 在 这个 采样   率 从 基部 数据 集 。 我们 改变 二次 采样率   在 { 0.001 , 0.01 ， 0.1 ％ ， 0.5 ％ ， 1 } 。   对 数据量 的 结果 在 图 10 中 它 是 在 示   图 10 ： 数据量 试验 的 结果 。   该   X 轴 对应 于 训练 实例 数 。   我们 利用 校准 在 左侧 主   y 轴 ， 而 归一化 的 熵 被 示为 具有   右手 侧 次级 y 轴 。   与 我们 的 直觉 线 更 多 的 数据 带来 更好 的 per -   formance 。 此外 ， 数据 表明 体积 dimin -   ishing 在 预测 准确度 方面 的 回报 。 只用   数据 的 10 ％ ， 则 归一化 的 熵 仅 1 ％ reduc -   重刑 在 相对 于 整个 训练 数据 集 的 性能 。   在 这个 采样率 校准 没有 显示 性能   减少 。   6.2 负下 采样   一流 的 失衡 已经 研究 了 许多 研究 人员 和   已 被 证明 对 perfor - 显著 影响   学习 模式   -   曼斯 。 在 这 一部分 中 ， 我们 调查   使用 负 向下 采样 的 解决 类 不 平衡   问题 。   我们 经验 与 不同 的 负 试验   下 采样率 来 测试 的 预测 精度   学习 模型 。 我们 而 变化 { 0.1 ， 0.01 ， 0.001 ， 0.0001 } 的 速率 。   实验 结果 示于 图 11 。   从 结果 中 我们 可以 看到 ， 负 下降 SAM -   pling 率 对 的 性能 显著 影响   训练 模型 。 最佳 的 性能 与 neg - 实现   ative 下行 采样率 设置 为 0.025 。   6.3 型号 重新 校准   负 采样 可以 加快 培养 和 提高   模型 的 性能 。 需要 注意 的 是 ， 如果 一个 模型 在 数据 训练     第 9 页     图 11 ： 阴性 实验 结果 比较 下降   采样 。 X 轴 对应 于 不同 nega -   略去 下行 采样率 。 我们 利用 校准 的   左手 侧主 y 轴 ， 而 归一化 的   熵 被 示出 具有 右手 侧 次级   y 轴 。   设置 有 负下 采样 ， 也 校准 预   文辞 的 采样 空间 。 例如 ， 如果 aver -   取样 前 年龄 的 点击率 是 0.1 ％ ， 我们 做 一个 负 0.01   下 采样 ， 经验 CTR 将 成为 约 10 ％ 。   我们 需要 重新 校准 的 实际 流量 实验 模型   并 取回 与 q 中 的 0.1 ％ 的 预测 =   p   P   + （ 1P ） /   w 的   其中 p 是 下 采样 空间 和 瓦特 的 预测   负 采样率 。   7 . 讨论   我们 从 实验   -   提出 了 一些 实用 的 经验教训   荷兰 国际 集团 与 Facebook 的 广告 数据 。 这 一直 激励 着 一个 有 前途 的   对于 点击 预测 混合 模型 架构 。   • 数据 新鲜度 的 问题 。 值得 一 至少 再 培训   日常 。 在 本文 中 ， 我们 更进一步 讨论   各种 在线 学习 方案 。 我们 还 提出   基础设施 ， 使 生成 的 实时 培训   数据 。   • 转型 实值 输入 功能 与 提升   决策树 显著 提高 预测 AC -   概率 线性 分类 的 curacy 。   这 激励 着   该 串接 升压 混合 模型 架构   决策树 和 稀疏 线性 分类 。   • 最 优惠 的 在线 学习 方法 ： LR 每个 坐标   学习 率 ， 这 最终 是 类似 的 在 per -   与 BOPR   formance ， 并 执行 胜过 一切   正在 研究 其他 LR   SGD 方案 。 （ 表 4 ， 图 12 ）   我们 已经 描述 的 技巧 来 维持 内存 和 延迟 CON 组   tained 在 大规模 机器 学习 应用   • 我们 已经 提出 的 数量 之间 的 权衡   提高 决策树 和 准确性 。 有利 的 是 ，   保持 树木 小 ， 以 保持 计算 的 数量   记忆 包含 。   • 提振 决策树 给 做 的 一种 便捷 方式   通过 功能 重要 手段 特征选择 。   一   可 积极 地 减少 的 活性 特征 的 数量   而 只有 适度 的 伤害 预测 精度 。   • 我们 分析 了 使用 历史 fea - 的 影响   功能 与 上下文 特征 的 组合 。 对于 广告   和 用户 提供 的 历史 ， 这些 功能 提供 了 优于   预测 性能比 背景 特征 。   最后 ， 我们 已经 讨论 了 欠 采样 培训 方式   数据 ， 都 均匀 地 而且 更 有趣 在 偏置   办法 只有 负面 例子 子 采样 。  ", "tags": "machine-learning/paper-translate", "url": "/wiki/machine-learning/paper-translate/2018-01-01-ctr-facebook-2014.html"},
      
      
      
      {"title": "Parameter Server for Distributed Machine Learning", "text": "    Table   of   Contents           关于           一个 优化 问题                 关于       论文 ： Parameter   Server   for   Distributed   Machine   Learning ， NIPS2014 。       一个 优化 问题       近似 梯度 法 ：       最小化 下述 问题       $ $     f ( w )   +   h ( w )     $ $       定义 近似 操作 ：       $ $     \ \ text { Prox } _ { \ \ gamma } ( x )   =   \ \ arg   \ \ min _ { y   \ \ in   \ \ mathbb { X } }   h ( y )   +   \ \ frac { 1 } { 2   \ \ gamma }   | |   x   -   y   | | ^ 2     $ $       f 是 可微 的 ， h 有 可能 不可 微 ， 那么 ， 对于 学习 率   $ ( \ \ gamma _ t   & gt ;   0 ) $   迭代 方程 是 ： ？       $ $     w ( t   +   1 )   =   \ \ text { Prox } _ { \ \ gamma _ t }   [ w ( t )   -   \ \ gamma _ t   \ \ nabla   f ( w ( t ) ) ]     \ \ text { for   }   t   \ \ in   \ \ mathbb { N }     $ $       一个 博客 ：     http : / / blog . csdn . net / lanyanchenxi / article / details / 50448640             异步 更新 ， 机器 学习 算法 具有 容错 能力      ", "tags": "machine-learning", "url": "/wiki/machine-learning/ps.html"},
      
      
      {"title": "Pattern Recognition and Machine Learning - Bishop", "text": "    Table   of   Contents           关于           引言           曲线拟合           第 11 章   采样 方法           基本 采样 方法           拒绝 采样                                 关于       Bishop 写得 PRML 无疑 是 机器 学习 领域 的 权威 著作 ， 是 加强 机器 学习 理论知识 必看 的 书籍 之一 。     这里 是 我 在 阅读 这 本书 时 记录 的 笔记 和 自己 的 一些 初步 思考 。       引言       曲线拟合       设 曲线拟合 的 目标 变量 为 $ ( t ) $ ， $ ( N ) $ 个 训练 集 为     $ (   \ \ mathbf { x }   =   ( x _ 1 ,   ... ,   x _ N ) ^ T   ) $ ， 对应 的 目标值 为     $ (   \ \ mathbf { t }   =   ( t _ 1 ,   ... ,   t _ N ) ^ T   ) $ 。 假设 估计 误差 服从     高斯分布     $ $     p ( t | x ,   \ \ mathbf { w } ,   \ \ beta )   =   \ \ mathcal { N }   ( t |   y ( x ,   \ \ mathbf { w } ) ,   \ \ beta ^ { - 1 } )       $ $     这里 的 $ ( \ \ mathbf { w } ) $ 是 待 估计 的 参数 ， $ ( \ \ beta ^ { - 1 } ) $ 对应 于     估计 误差 的 方差 。 此时 ， 按照 最大 似然 准则 估计 的 参数 就是 最小 二乘     法 的 结果 ， 也 就是 最小化 均方 误差 。 而 $ ( \ \ beta ^ { - 1 } ) $ 的 最大 似然 估计 为     模型 预测值 与 实际 值 的 均 方差 。     $ $     \ \ frac { 1 } { \ \ beta }   =   \ \ frac { 1 } { N }   \ \ sum _ { n = 1 } ^ N   [ y ( x _ n , \ \ mathbf { w } _ { ML } )   -   t _ n ] ^ 2 .     $ $       如果 我们 对 参数 的 具有 一定 先验 知识 ， 可以 进一步 采用 最大 后验 概率 估计 ，     得到 更好 的 结果 。 作为 一个 特例 ， 如果 认为     $ $     p ( \ \ mathbf { w } | \ \ alpha )   =   \ \ mathcal { N } ( \ \ mathbf { w }   |   0 ,   \ \ alpha ^ { - 1 }   \ \ mathbf { I } )       \ \ \ \                                             =   ( \ \ frac { \ \ alpha } { 2 \ \ pi } ) ^ { ( M + 1 ) / 2 }   \ \ exp ( - \ \ frac { \ \ alpha } { 2 }   \ \ mathbf { w } ^ T   \ \ mathbf { w } ) .     $ $     那么 此时 的 最大 后验 概率 估计 为 带 $ ( L _ 2 ) $ 正则 项 的 估计 ，     它 最小化     $ $     \ \ frac { \ \ beta } { 2 }   \ \ sum _ { n = 1 } ^ N   [ y ( x _ n ,   \ \ mathbf { w } )   - t _ n ] ^ 2   +   \ \ frac { \ \ alpha } { 2 }   \ \ mathbf { w } ^ T   \ \ mathbf { w }     $ $     也就是说 正则 项是 对模型 参数 的 一种 先验 知识 ， $ ( L _ 2 ) $ 正则 项 代表 高斯 先验 。     那 $ ( L _ 1 ) $ 代表 laplace 先验 ( 待 求证 ) ？       第 11 章   采样 方法           计算 期望   $ ( E [ f ]   =   \ \ int   f ( z ) p ( z )   dz ) $       通过 采样 一系列 $ ( z _ i   ~   p ( z ) ) $   , 从而 近似 地用 有限 和 来 近似 上述 期望 $ ( \ \ hat { f }   =   \ \ frac { 1 } { L } \ \ sum _ { l = 1 } ^ Lf ( z _ l ) ) $       估计 误差 可以 用 上述 统计 量 的 方差 来 刻画           基本 采样 方法           一维 分布 变换 关系   $ ( p ( y )   =   p ( z )   \ \ frac { dz } { dy } ) $       分布 f ( y ) 可以 通过 从 [ 0 , 1 ] 间 均匀 随机变量 z , 通过 变换 $ ( F ^ { - 1 } ( z ) ) $ 得到 ,   F 是 y 的 累积 分布 函数       指数分布 $ ( y   =   F ^ { - 1 } ( z )   =   - \ \ frac { 1 } { \ \ lambda }   ln ( 1 - z ) ) $       对于 多维 分布 , 通过 雅克 比 矩阵   $ ( p ( y _ 1 ,   ... ,   y _ M )   =   p ( z _ 1 ,   ... ,   z _ M ) | \ \ frac { \ \ partial ( z _ 1 , ... , z _ M ) } { \ \ partial ( y _ 1 , ... , y _ M ) } | ) $       高斯分布 生成 :   Box - Muller 方法       $ ( z _ 1 , z _ 2 ) $ 独立 采样 自 服从 ( - 1 , 1 ) 之间 的 均匀分布       只 保留 在 单位 圆内 的 点 , 即 满足 $ ( z _ 1 ^ 2   +   z _ 2 ^ 2   \ \ le   1 ) $ , 从而 得到 单位 圆内 的 均匀分布       构造 新 的 变量 $ ( y _ 1   =   z _ 1 ( \ \ frac { - 2   ln   z _ 1 } { r ^ 2 } ) ^ { 1 / 2 } ,   y _ 2   =   z _ 2   ( \ \ frac { - 2   ln   z _ 2 } { r ^ 2 } ) ^ { 1 / 2 } ) $ .   其中 r 是 半径 ,   那么 y1 和 y2 就是 两个 独立 的 服从 标准 正太 分布 的 随机变量       待 证明 , 貌似 证明 挺 复杂 的               一般 的 高斯分布 $ ( \ \ mu ,   \ \ Sigma ) $ , 将 协方差 矩阵 分解 为 $ ( LL ^ T ) $ , 那么 对 独立 的 标准 正太 分布 向量 z , 做 变换 $ ( \ \ mu   +   L   z ) $   即可 得到 目标 分布           拒绝 采样  ", "tags": "machine-learning", "url": "/wiki/machine-learning/prml.html"},
      
      
      {"title": "Pixel RNN & CNN", "text": "  关于           参考 论文 ：       Den   Oord   A   V ,   Kalchbrenner   N ,   Kavukcuoglu   K ,   et   al .   Pixel   Recurrent   Neural   Networks [ C ] .   international   conference   on   machine   learning ,   2016 :   1747 - 1756 .       Conditional   Image   Generation   with   PixelCNN   Decoders                   Pixel   RNN           对 自然 图像 建模 ， 是 无 监督 学习 中 的 一个 重要 任务 。       Piexel   RNN   将 此 问题 当做 一个 序列 预测 的 问题 ， 即 利用 前面 的 像素 ( 作为 上下文   Context ) 预测 后 一个 像素 的 离散 概率 ，     建立 条件 概率模型 。       采用 残差 连接 ， 12 层       预测 离散 值 ， 直接 采用 一层   softmax   ， 作为 一个多 分类 的 问题 。       上下文 可以 是 自 左上角 到 右下角 ， 顺序 的 上下文 ； 也 可以 是 多 尺度 的 上下文 。           图像   $ ( x _ { i , j } ,   i , j   =   1 , 2 , ... , n ) $   作为 一个 序列   $ ( x _ i ,   i = 1 , ... , n ^ 2 ) $ ，     联合 概率       $ $     p ( x _ 1 ,   .. ,   x _ n )   =   \ \ Pi _ { i = 1 } ^ { n ^ 2 }   p ( x _ i | x _ 1 , ... , x _ { i - 1 } )     $ $           并且 ， 不同 通道 （ RGB 三个 通道 ） 不仅 依赖 前面 的 序列 ， 还 依赖 与 其他 通道 。 分布 的 计算 可以 并行 ？ ！ ！ ！           Pixel   CNN       能够 建模 任意 向量 的 条件 概率 ； 如果 对 图像 的   class   label   建模 ， 可以 生成 多样 的 真实 场景 下 的 不同 动物 ， 目标 ， 场景 等 ；     如果 对 图像 的 CNN   embedding 向量 建模 条件 概率 ， 可以 生成 一个 没有 见到 过 的 人 的 其他 侧面 、 姿势 下 的 画像 ！       Pixel   CNN   也 可以 作为 图像   autoencoder   的 一个 强大 的 解码器 ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pix-rnn-cnn.html"},
      
      
      {"title": "PMML: 用XML描述机器学习模型", "text": "    Table   of   Contents           关于           通用 结构           Header           MiningBuildTask           DataDictionary           DataField                   TransformationDictionary           MODEL - ELEMENT           MiningModel           functionName           Segmentation                   RegressionModel                   Extension           其他           modelName           基本 数据类型                         关于       PMML 是 用 XML 来 描述 数据挖掘 模型 的 一种 通用 可 交换 格式 ， 利用 PMML 可以 将 各种 工具 生成 的 模型 很 方便 的 发布 到 生产 环境 ！     目前 著名 的   sklearn   和   R 中 的 模型 ， 都 支持 导出 为 PMML 格式 ！       http : / / dmg . org / pmml /         通用 结构           采用 XML 格式 ， 只有 一个 根 节点                   & lt ; PMML       version =     & quot ; 4.1 & quot ;               xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 1 & quot ;               xmlns : xsi =     & quot ; http : / / www . w3 . org / 2001 / XMLSchema - instance & quot ;     & gt ;       & lt ; / PMML & gt ;                     一个 PMML 文件 可以 包含 多个 模型 ， 一个 应用 系统 可以 通过 名字 选择 模型 ， 否则 选择 第一个 模型           Header       可以 包含     copyright ,   description ,   Applilcation ,   name ,   version ,   Annotation   。     例子 ：               & lt ; Header       copyright =     & quot ; www . tracholar . com & quot ;     / & gt ;                 MiningBuildTask       包含 任意 XML 值 ， 描述 训练 模型 的 配置 ， 不是   PMML   必须 。       DataDictionary       定义 用于 模型 输入 的 数据 的 类型 ， 范围 等 。     可以 用 在 多个 模型 当中 。         DataField               name     必须 唯一 ！         opType     操作 类型 ， 表明 是 连续 ， 离散 特征         dataType     数据类型     https : / / www . w3 . org / TR / xmlschema - 2 /   ， 外加 几个 新 类型     timeSenconds ,   dateDaysSince [ aYear ] ,   dateTimeSecondsSince [ aYear ]           Value             TransformationDictionary       对 数据 变换 :           Normalization ： 归一化 ， 输入 可以 是 连续 或 离散 值       Discretization ： 将 连续 值 变成 离散 值       Value   mapping ： 将 离散 值 映射 为 离散 值       Text   Indexing ： 对 给定 的 term 赋予 一个 频率 值 ？       Functions ： 函数 映射       Aggregation ： 聚合 函数 ， 例如 求和       Lag ： 使用 之前 的 值 ？           MODEL - ELEMENT       MiningModel       functionName       PMML   定义 了 5 中 挖掘 函数 ， 每个 模型 可以 有 一个 属性     functionName     用来 指定 functionName 。     可以 取值 为 ：           associationRules       sequences       classification       regression       clustering       timeSeries       mixed           Segmentation             multipleModelMethod     多个 模型 融合 方法 ：         majorityVote     投票         modelChain     模型 链 ， 前 一个 模型 的 结果 作为 后 一个 模型 的 输入         average     平均       selectFirst ,   selectAll ,   majorityVote ,   modelChain ,   etc               Segment ，   id   1 - based   index ： 为什么 要 有 一个     & lt ; True / & gt ;             RegressionModel       回归 模型           MiningSchema ： 定义 建模 的 字 段       Output ： 定义 一个 模型 的 输出 结果 值 ， 使用 OutputField 定义 名字 ， predictedValue 字段 表明 输出 的 是   raw   predicted   value .       Target           Extension       其他       modelName       用来 指定 模型 民资       基本 数据类型       NUMBER ,   INT - NUMER ,   REAL - NUMBER ,   PROB - NUMBER ,   PERCENTAGE - NUMBER ,   FIELD - NAME ,   ARRAY  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pmml.html"},
      
      
      {"title": "PyTorch", "text": "    Table   of   Contents           张量 定义           自动 微分           参考                 张量 定义       张量 定义 跟 numpy 的 数组 定义 类似               import       torch       torch     .     randn     (     5     ,     6     )       torch     .     ones     (     5     ,     1     )                 自动 微分         torch . Tensor     的 属性     requires _ grad     如果 设 为     True   ,   那么 会 自动 跟踪 所有 的 运算 。       停止 跟踪 梯度 方法     . detach ( )                 & gt ; & gt ; & gt ;       import       torch       & gt ; & gt ; & gt ;       & gt ; & gt ; & gt ;       x       =       torch     .     ones     (     2     ,       2     ,       requires _ grad     =     True     )       & gt ; & gt ; & gt ;       y       =       x       * *       2       +       4       & gt ; & gt ; & gt ;       y       tensor     ( [ [     5 .     ,       5 .     ] ,                       [     5 .     ,       5 .     ] ] ,       grad _ fn     = & lt ;     AddBackward     & gt ;     )                 每 一个 Tensor 都 有 一个   . grad _ fn   属性 , 指向 创建 这个 张量 的 函数 , 除了 用户 自己 创建 的               & gt ; & gt ; & gt ;       out       =       y     .     mean     ( )       & gt ; & gt ; & gt ;       out       tensor     (     5 .     ,       grad _ fn     = & lt ;     MeanBackward1     & gt ;     )                 当 你 调用     . backward ( )     方法 时 , 所有 的 梯度 将会 自动 计算 , 并 放在     . grad     属性 中 。               & gt ; & gt ; & gt ;       out     .     backward     ( )       & gt ; & gt ; & gt ;       x     .     grad       tensor     ( [ [     0.5000     ,       0.5000     ] ,                       [     0.5000     ,       0.5000     ] ] )                 当 输出 是 向量 时 , 需要 指定     . backward ( v )     参数 才能 得到 雅克 比 矩阵 。               & gt ; & gt ; & gt ;       x       =       torch     .     ones     (     4     ,       requires _ grad     =     True     )       & gt ; & gt ; & gt ;       out       =       x       *       2       & gt ; & gt ; & gt ;       out     .     backward     (     torch     .     tensor     ( [     1     ,       1     ,       1     ,       1     ] ,       dtype     =     torch     .     float     ) )       & gt ; & gt ; & gt ;       x     .     grad       tensor     ( [     2 .     ,       2 .     ,       2 .     ,       2 .     ] )                 禁止 跟踪 梯度               with       torch     .     no _ grad     ( ) :               print     ( (     x       * *       2     )     .     requires _ grad     )                 参考             https : / / pytorch . org / tutorials / beginner / blitz / autograd _ tutorial . html           https : / / pytorch . org / tutorials / beginner / blitz / neural _ networks _ tutorial . html        ", "tags": "machine-learning", "url": "/wiki/machine-learning/pytorch.html"},
      
      
      {"title": "Rank 常用方法和模型", "text": "    Table   of   Contents           关于           逻辑 回归           rankSVM           lambda   rank                 关于       排序 是 机器 学习 算法 在 业界 用 得 比较 多 的 场景 ， 很多 问题 也 可以 转化 为 排序 的 问题 。       逻辑 回归       例如 点击率 预估 ， 用户 偏好 等 。 排序 问题 被 当做 二 分类 预测 问题 ， 用 概率 值 进行 排序 ， 一般 会 关注   AUC   指标 。       rankSVM       直接 优化   AUC ， 并 借鉴   SVM   大 间隔 的 思想 ， 构造   rank   loss :       $ $     loss   =     $ $       lambda   rank  ", "tags": "machine-learning", "url": "/wiki/machine-learning/rank.html"},
      
      
        
        
      
      {"title": "An Empirical Evaluation of Thompson Sampling", "text": "    Table   of   Contents           关于           摘要           算法           展示 广告           新 文章 推荐                 关于           论文 :   An   Empirical   Evaluation   of   Thompson   Sampling ,   2011       参考 博客     http : / / lipixun . me / 2018 / 02 / 25 / ts         参考 书籍 :   PRML           摘要           UCB 方法 :   强 理论 保证       P .   Auer ,   N .   Cesa - Bianchi ,   and   P .   Fischer .   Finite - time   analysis   of   the   multiarmed   bandit   problem .   Machine   learning ,   47 ( 2 ) : 235 – 256 ,   2002 .       T . L .   Lai   and   H .   Robbins .   Asymptotically   efficient   adaptive   allocation   rules .   Advances   in   applied   mathematics ,   6 : 4 – 22 ,   1985 .               贝叶斯 优化 版 的 Gittins :   在 给定 先验 分布 直接 优化 最大 回报       John   C .   Gittins .   Multi - armed   Bandit   Allocation   Indices .   Wiley   Interscience   Series   in   Systems   and   Optimization .   John   Wiley   & amp ;   Sons   Inc ,   1989 .               probability   matching :   Thompson   sampling ,   根据 概率 选择 某个 臂 ,   容易 实现 ,   但 缺乏 理论 分析       William   R .   Thompson .   On   the   likelihood   that   one   unknown   probability   exceeds   another   in   view   of   the   evidence   of   two   samples .   Biometrika ,   25 ( 3 – 4 ) : 285 – 294 ,   1933 .                   算法           上下文 $ ( x ) $ ,   动作 集合 A ,   回报 r ,   多臂 老虎机 问题 实际上 是 单步 MDP ,   上下文 实际上 就是 状态       过去 的 经验   D 由 三元组   $ (   ( x _ i ,   a _ i ,   r _ i )   ) $   构成 ,   由 函数 $ ( P ( r | a , x , \ \ theta ) ) $ 建模 , 其中 $ ( \ \ theta ) $ 是 模型 参数       设 模型 参数 的 先验 分布 为 $ ( P ( \ \ theta ) ) $       那么 根据 贝叶斯 法则 , 后验 分布 $ ( P ( \ \ theta | D )   \ \ propto   \ \ Pi _ i   P ( r _ i | a _ i , x _ i , \ \ theta )   P ( \ \ theta )   ) $       如果 我们 知道 理想 的 参数值 $ ( \ \ theta ^ *   ) $ , 那么 我们 应该 选择 使得 期望 收益 最大 的 动作 $ ( \ \ max _ a   E ( r | a , x , \ \ theta ^ * )   ) $       但是 实际上 最优 的 参数 是 未知 的 ,   可以 最大化 经验 分布 下 的 期望 收益 ( 即   exploitation )   $ ( \ \ max   E ( r | a , x )   =   \ \ int   E ( r | a , x , \ \ theta )   P ( \ \ theta | D )   d \ \ theta ) $       如果 在 探索 的 设置 下 , 则 是 根据 是 最优 动作 的 概率 随机 选择 一个 动作 。 即 选择 动作 a 的 概率 等于           $ $     \ \ int   I [ E ( r | a , x , \ \ theta )   =   \ \ max _ { a ' }   E ( r | a ' , x , \ \ theta ) ]   P ( \ \ theta | D )   d \ \ theta     $ $           实际 实现 的 时候 , 积分 是 通过 随机 采样 来 实现 的 , 每次 从 后验 分布 $ ( P ( \ \ theta | D ) ) $ 采样 一组 参数 , 然后 根据 这组 参数 来 选择 最优 动作 a                       在 标准 的 多臂 老虎机 任务 中 ,   汤普森 采样 相当于 每个 臂 使用 了 不同 的 beta 先验                       汤普森 采样 方法 对 先验 分布 参数 的 鲁棒性 :   十分 鲁邦       汤普森 采样 是 渐进 最优 的 ,   比 UCB 具有 更 低 的 regret       EE 的 核心思想 是 :   提升 选择 那些 我们 不是 很 确定 的 动作 的 概率       乐观 汤普森 采样 :   如果 采样 的 score 小于 均值 ( 即该 动作 当前 的 期望 回报 ) ,   那么 就 截断 到 均值 。       替换算法 1 中 的 $ ( E _ r ( r | x _ t , a _ t , \ \ theta ^ t ) ) $ 为   $ ( max (   E _ r ( r | x _ t , a _ t , \ \ theta ^ t ) ,   E _ r ( r | x _ t ,   a _ t )   ) ) $                               后验 塑形 :   改变 后验 分布 , 增加 探索 能力 。 例如 把 beta 分布 的 a 和 b 两个 参数 都 除以 $ ( \ \ alpha ) $ , 将 在 不 改变 期望 的 情况 下 增加 方差 , 从而 增加 探索 能力 。       试验 结果显示 , 虽然 在 渐进 区域 ( T 很大 的 区域 ) 改变 后验 分布 反而 使得 regret 比 标准版 的 TS 效果 要 差 , 但是 在 非 渐进 区域 , 可以 得到 更好 的 regret 的 。       启示 :   可以 在 早期 的 时候 将 $ ( \ \ alpha ) $ 设置 为 小于 1 , 减少 探索性 , 加快 收敛 , 在 后期 的 时候 逐步 增加 到 1                               时延 的 影响 :   因为 实际 系统 中 , 反馈 往往 存在 时延 。 作者 通过 一个 模拟 仿真 , 证实 汤普森 采样 比 UCB 在 时延 变长 的 时候 更加 鲁棒 。 ( ratio 指 的 是 UCB 的 regret / 汤普森 采样 的 regret , 该值 越大 , 说明 ts 比 ucb 越好 )                   展示 广告           假设 模型 参数 的 先验 分布 ,   以 正则 化 逻辑 回归 为例 , 假设 参数 先验 为 高斯分布       后验 分布 并 不是 高斯分布 , 所以 采用 拉普拉斯 近似 , 将 众数 作为 高斯 的 均值 , 二阶 导 作为 协方差 矩阵 ( 只取 了 对角 项 , 没有 要 交叉 项 )       拉普拉斯 近似 具体操作 过程 可以 参考 PRML       仿真 的 结论 是 ,   TS 比 UCB 和 e - greedy 都 要 好 , 但是 没有 探索 的 方法 效果 也 不差 !   作者 认为 一个 可能 的 解释 是 ,   不同 的 上下文 本身 有 带有 一定 的 探索 。       BOPR :   BOPR 则 将 概率函数 直接 用 高斯分布 的 类似 形式 , 都 不用 近似 。 参考 :   Web - scale   Bayesian   click - through   rate   prediction   for   sponsored   search   advertising   in   Microsoft ’ s   Bing   search   engine                   新 文章 推荐           yahoo 的 一个 案例 , 略      ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/thompson-sampling.html"},
      
      
      {"title": "BERT4Rec", "text": "    Table   of   Contents           关于           出发点           -   RNN 结构 的 问题 :   从左到右 的 单向 结构限制 了 历史 序列 表征 ,   所以 要 上 双向 的 , BERT                 关于           论文 :   BERT4Rec :   Sequential   Recommendation   with   Bidirectional   Encoder   Representations   from   Transformer           出发点       -   RNN 结构 的 问题 :   从左到右 的 单向 结构限制 了 历史 序列 表征 ,   所以 要 上 双向 的 , BERT  ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/bert4rec.html"},
      
      
      {"title": "Graph Embedding", "text": "    Table   of   Contents           关键 文献                 关键 文献           DeepWalk   2014       关键 想法 :   通过 random   walk 从 Graph 中 采样 语料 ( 类似 于 NLP 中 的 句子 ) , 然后 利用 word2vec 学习 node 向量               LINE   2015       关键 想法 :   从 embedding 向量 建模 1 阶 相似 和 2 阶 相似 , 然后 优化 得到 向量 ,   没有 采样 语料 ,   借鉴 了 word2vec 思想 的 精髓 ,   而 是不是 将 问题 转化 为 word2vec 能 解决 的 问题               node2vec   2016       关键 想法 :   改进 deepwalk 的 采样 方法 , 既 考虑 深度 有 考虑 宽度               metapath2vec   2017       关键 想法 :   处理 异质 网络 ,   有 多种类型 的 节点 ,   random   walk 考虑 节点 和 关系 的 类型 , 只能 沿着 指定 的 节点 - 关系 路线 ( 即 meta - path ) 进行 采样               Billion - scale   Commodity   Embedding   for   E - commerce   Recommendation   in   Alibaba ,   阿里 2018       关键 想法 :   将 side - information 引入 embedding 改进 冷启动 的 问题 , 模型 还是 浅层 , 是否 可以 用 深层 ( 如 DNN , 原则上 是 可以 的 )               NetSMF   WWW2019 ,   微软 、 清华                    ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/graph-embedding.html"},
      
      
      {"title": "PUSH优化", "text": "    Table   of   Contents           Notification   Volume   Control   and   Optimization   System   at   Pinterest           论文 摘要                   Near   Real - time   Optimization   of   Activity - based   Notifications           相关 文献                 Notification   Volume   Control   and   Optimization   System   at   Pinterest           Pinterest           论文 摘要           通知 系统 的 几个 关键问题 :       什么 时候 投放       通过 哪个 通道 投放       投放 的 频率 和 投放量               挑战       目标 函数 应该 包含 长期 效应 , 不仅仅 是 短期 效应       模型 对 容量 增长 的 非线性 效应       算法 必须 对 大规模 可 扩展                   系统           Weekly   Notification   Budget       Notification   Service       每天 轮询 每 一个 用户 , 决定 是否 给 他 投放 , 生成 投放 内容       Budget   Pacer ,   一个 KV 系统 , userid 做 key , value 是 用户 预算 额度       even   pacing ,   最小化 用户 疲劳 ,   比如 一周 预算 为 3 ,   那么 就 隔 一天 投放 一次 。 本质 上 是 均匀 分配 预算       更 智能 的 pace :   周末 投放 更 多一些 ?               RANKER ,   对于 可以 投放 的 用户 ,   从 一系列 的 通知 中 选择 最好 的 一个 投放 。 ML 模型 在 这个 环节 预测 CTR 等 业务 关键 指标       delivery ,   投放 时间 策略 决定 什么 时候 投放 ,   投放 后 跟踪 用户 响应 反馈               Volume   Optimization   Workflows       之前 的 工作 :   如果 一个 用户 有 很 高 的 概率 会 和 某种 类型 的 通知 交互 , 那么 就 给 他 头 更 高频 地 投放 这种 类型 的 通知       如果 CTR 排名 前 10 % , 可以 每 3 天发 一次 这种 类型 的 push ; 如果 20 % , 则 延长 到 14 天发 一次 ;   间隔 区间 通过 手动 AB 测试 选择               问题 :   不能 从 指标 上 区分 是 通知 质量 变 好 带来 的 , 还是 通知 的 数量 变多 带来 的 ?                           建模 概率   $ ( p ( a | u , k _ u ) ) $ ,   a 代表 这 一天 是否 活跃 ,   u 代表 某个 用户 ,   $ ( k _ u ) $ 代表 用户 的 配额 ,   优化     $ $     \ \ max _ { k _ u }   p ( a | u ,   k _ u )   \ \     s . t .   \ \ sum _ { u }   k _ u   \ \ le   K     $ $           效用 建模 ,   $ ( p ( a | u )   =   p ( a _ o | u )   +   ( 1 - p ( a _ o | u ) ) p ( a _ n | u ) ) $ ,   第二项 表示 用户 没有 通过 其他 途径 打开 APP , 而是 通过 通知 打开 APP 的 的 概率 , 可以 近似 认为 是 通知 的 效用 ( 因为 这个 公式 假定 了 不同 通道 之间 是 独立 的 , 然而 并 不是 这样 )       建模 长期 效用       将 用户 是否 退订 考虑 进去 , 将 活跃 的 概率 分解 为 两 部分 , 即 退订 后 未来 长时间 的 活跃 概率 , 加上 没有 退订 当前 的 活跃 概率 ; 这里 将 退订 和 不 退订 的 活跃 概率 分别 用 不同 时期 的 活跃 概率 是 为了 即 兼顾 长期 利益 , 又 能 考虑 短期 收益 , 是 个 混血儿     $ $     p ( a | u ,   k _ u )   =   p ( s _ { unsub } | u ,   k _ u )   p ( a _ L | u ,   k _ u ,   s _ { unsub } )   +   ( 1 - p ( s _ { unsub } | u ,   k _ u ) )   p ( a | u ,   k _ u ,   s _ { sub } )     $ $               三个 模型 都 是 个 二 分类 问题       无偏 数据 收集 ,   随机 对 一组 用户 设定 投放 次数 , 收集 反馈 作为 数据       前 两个 模型 的 样本 构建 都 没 啥 问题 , 注意 构建 退订 模型 的 时候 , k 要 选择 分配 的 通知 预算 , 而 不是 实际 收到 的 数目 , a 代表 活跃 , 而 不是 点击       第 3 个 模型 是 长期 活跃 , 作者 选 了 第 4 周 之后 的 一周 是否 活跃 构造 label           Near   Real - time   Optimization   of   Activity - based   Notifications           LinkedIn       push 文案 有点像 推荐 理由       简单 的 点击率 预估 模型       点击率 评估 , 将 点击率 排序 分成 10 个桶 ( 每个 桶 人数 一样 多 ) , 然后 评估 每个 桶 里面 预测 的 PCTR 和 实际 的 CTR 之间 的 差异       通道 关闭 模型 很难 优化 , 因为 不 知道 是因为 哪条 push 或者 哪 几条 push 导致用户 关闭 的       push 最优 解是 p 超过 某个 阈值 , 或者 每个 人有 个 个性化 阈值       需要 仔细阅读 , 暂时 先 放过           相关 文献           Rupesh   Gupta ,   Guanfeng   Liang ,   and   R ó mer   Rosales .   2017 .   Optimizing   Email   Volume   For   Sitewide   Engagement .   In   Proceedings   of   the   2017   ACM   on   Conference   on   Information   and   Knowledge   Management ,   CIKM   2017 ,   Singapore ,   November   06   -   10 ,   2017 .   1947 – 1955       Rupesh   Gupta ,   Guanfeng   Liang ,   Hsiao - Ping   Tseng ,   Ravi   Kiran   Holur   Vijay ,   Xi -   aoyu   Chen ,   and   R ó mer   Rosales .   2016 .   Email   Volume   Optimization   at   LinkedIn .   In   Proceedings   of   the   22nd   ACM   SIGKDD   International   Conference   on   Knowledge   Discovery   and   Data   Mining ,   San   Francisco ,   CA ,   USA ,   August   13 - 17 ,   2016 .   97 – 106      ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/notification-volume-control-pinterest.html"},
      
      
      {"title": "Recommender System Handbook 读书笔记", "text": "    Table   of   Contents           关于                 关于       推荐 系统 手册 读书笔记  ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/recommender-system-handbook.html"},
      
      
      {"title": "Top-K Off-Policy Correction for a REINFORCE Recommender System", "text": "    Table   of   Contents           论文 简介                 论文 简介           推荐 是 根据 推荐 算法 曝光 展示 的 样本 及 反馈 数据 训练 模型 , 是 有 偏 的       主要 贡献       将 REINFORCE 算法 应用 到 百万 级别 的 动作 空间 任务       利用   off - policy   纠正 数据 的 偏差       展示 探索 的 价值              ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/youtube-rl-rec-2019.html"},
      
      
      {"title": "关于推荐栏目", "text": "    Table   of   Contents           关于                 关于               本 栏目 主要 收集 以下 资源           国际 会议 中 跟 推荐 有关 的 论文 笔记       工业界 各 公司 的 一些 方案 调研                   paper   list     https : / / github . com / hongleizhang / RSPapers            ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/readme.html"},
      
      
      {"title": "推荐新技术跟踪调研", "text": "    Table   of   Contents           背景           方法           排序 模型           序列 模型                   引入 强化 学习           引入 知识 图谱           模型 压缩           评论 数据 的 利用           GAN   的 应用           -   SGD 优化                 背景       跟踪 推荐 学术界 、 工业界 相关 新 技术 调研 , 把握 技术 动向 。       方法           学术界 的 信息 来源 主要 是 公开 的 论文 以及 关键 论文 的 被 引 , 微博 大脑 推荐       工业界 的 信息 来源 主要 是 一些 公众 号 和 论文           排序 模型           xDeepFM :   Combining   Explicit   and   Implicit   Feature   Interactions   for   Recommender   Systems           序列 模型           attention 流派 :       阿里   DIN :   将 用户 的 某种 集合 ( 一般 是 session 集合 , 点击 过 的 item ) 用 attention 加权 做 Pooling       STAMP :   Short - Term   Attention / Memory   Priority   Model   for   Session - based   Recommendation ,   KDD2018   将 session 的 item 加权 Pooling , 结果 与 最后 一个 item 以及 候选 item 做 了 个 三 向量 内积                   引入 强化 学习           Supervised   Reinforcement   Learning   with   Recurrent   Neural   Network   for   Dynamic   Treatment   Recommendation           引入 知识 图谱           Leveraging   Meta - path   based   Context   for   Top - N   Recommendation   with   A   Neural   Co - Attention   Model ,   KDD2018         关键 IDEA :   除了 用 user 和 item 向量 之外 , 引入 meta - path 上下文 。   meta - path 上下文 构造方法 :   首先 构造 一个 图谱 , 以 电影 推荐 为例 , 节点 有 用户 ( U ) , 电影 ( M ) , 导演 ( D ) ; 节点 间 的 关系 有 用户 间 的 朋友 关系 , 用户 与 电影 之间 看过 关系 , 电影 与 导演 间 的 导演 关系 。 这些 节点 和 关系 构成 一张 异构 图 , 利用 random   walk 按照 某种 特定 的 顺序 采样 ( UMUDM ) , 这个 顺序 即   meta - path , 采样 得到 的 节点 序列 就是 一个 上下文 , 这种 方法 在 其他 文献 中 也 有用 到 。 从 一个 user 出发 到 某个 item 结束 可以 有 很 多种 序列 , 根据 不同 的 meta - path 也 能 采样 出 不同 的 序列 , 这些 节点 序列 通过 embedding , CNN ( 或 RNN 或 self - attention ) , Pooling ( max - pooling 或 attention   Pooling ) 可以 得到 一个 上下文 向量 , 这样 就 将 这个 信息 引入 和 推荐 系统 。                               Explainable   Reasoning   over   Knowledge   Graphs   for   Recommendation ,   AAAI2019       关键 IDEA : 也 是 采样 多个 从 user 到 item 的 路径 , 然后 将 节点 和 关系 都 做 embedding , 构造 向量 序列 用 LSTM 做 Encoder , 多个 路径 的 预测 概率 最后 做 一个 Pooling               RippleNet :   Propagating   User   Preferences   on   the   Knowledge   Graph   for   Recommender   Systems       关键 IDEA :         item 向量 还是 直接 embedding       user 向量 构造 是从 用户 session 开始 , 从 session 的 item 开始 在 知识 图谱 中向 邻居 扩散 , 每 一次 扩散 都 有 一个 输出 向量 , 最终 的 用户 向量 是 这些 输出 向量 之 和       每 一次 传播 的 输出 向量 是 所有 节点 向量 的 加权 和 ,   权重 取决于 来源 节点 、 关系 、 target   item 的 关系 得分                       知识 图谱 的 处理 ,   图 卷积 GNN           模型 压缩           Ranking   Distillation :   Learning   Compact   Ranking   Models   With   High   Performance   for   Recommender   System       基本 IDEA :   用 一个 小 模型 来 学习 大 模型 的 输出 ,   最小化 二者 的 KL 距离                   评论 数据 的 利用               个人观点 :   评论 数据 对 推荐 是 有 一定 作用 , 但是 评论 数据 的 信噪比 很 低 , 在 CTR 模型 中 ,   在 已有 的 数据 基础 上 直接 增加 评论 数据 可能 不见得 有 提升 , 但是 评论 中 可以 抽取 出 item 的 一些 显示 feature ( 比如 电影 中 的 某个 演员 演技 很赞 ,   特效 很 好 ,   值回 票价   etc ) , 如果 这些 feature 能 在 展示 环节 漏出 , 应该 能够 有效 地 提升 点击率               DeepCoNN :   用户 由 用户 写 的 评论 表示 ,   item 由 item 的 所有 评论 表示 , 把 这 两 部分 文档 分别 用 CNN 做 encode , 然后 用 FM 做 匹配 ( 最后 一层 是 用户 向量 和 item 向量 用 FM 算 出来 的 loss ) 。           ref :   Joint   Deep   Modeling   of   Users   and   Items   Using   Reviews   for   Recommendation ,   2017                                   TransNets :   在 DeepCoNN 基础 上 增加 了   multi - task   learning   模块           ref :   TransNets :   Learning   to   Transform   for   Recommendation                   Dual   Attention   CNN   model   ( D - ATT ) :             ref :   对 每个 词学 一个 attention 权重 , 从而 将 重要 的 词 提权 , 不中要 的 词 降权 , 从而 提高 信噪比 。   attention   就是 一种 利用 数据 自己 学习 提高 信噪比 的 方法 ! !       全局 attention :   对 所有 的 词 向量 做 加权       local   attention :   搞 一个 滑动 窗 , 只 在 这个 窗内 做 加权               Multi - Pointer   Co - Attention   Networks   for   Recommendation ,   KDD2028       利用 attention 学到 硬 attention 向量 ( 即 只有 一个 维 1 其他 为 0 , 所以 也 叫 pointer ) , 然后 做 Pooling                           GAN   的 应用           Neural   Memory   Streaming   Recommender   Networks   with   Adversarial   Training ,   KDD2018 ,   360       关键点 :         利用 一个 有 外部 存储 的 neural   memory   networks ,   同时 建模 用户 的 长期 兴趣 和 短期 兴趣       利用 一个 基于 GAN 的 负 采样 方案 ,   来 优化 流式 推荐 模型 。 传统 负 样本 采样 ( 随机 采 或者 基于 popularity 的 采样 ) 采 出来 的                       流式 训练 的 关键点 :   -   SGD 优化          ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/survey2019.html"},
      
      
      {"title": "推荐理由", "text": "    Table   of   Contents           Introduction                 Introduction  ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/explain-review.html"},
      
      
      {"title": "推荐论文集快速浏览", "text": "    Table   of   Contents           论文 列表           A   Survey   on   Session - based   Recommender   Systems           历史           未来 的 方向           参考                   基于 规则 的 算法           Aprior   算法           FP - Tree           并行 FP           序列 FP           改进 应用           序列 模式 挖掘                   推荐 理由           不同 的 解释 形式           可 解释 的 推荐 方法           可 解释 推荐 的 评估           应用           未来 的 方向                   Deep   Learning   based   Recommender   System :   A   Survey   and   New   Perspectives           BiNE :   Bipartite   Network   Embedding           Session - based   Recommendation   with   Graph   Neural   Networks           Graph   Neural   Networks   for   Social   Recommendation                 论文 列表             https : / / github . com / hongleizhang / RSPapers             A   Survey   on   Session - based   Recommender   Systems           基于 session 的 推荐 系统       基于 内容 的 推荐 ( 找 相似 内容 ) 和 基于 协同 过滤 ( 利用 用户 行为 找 相似 的 人 / 物 ) , 偏 静态 , 无法 快速 捕获 用户 的 实时 兴趣 点 的 变化       将 session 作为 推荐 的 基本 单位 而 不是 将 用户 作为 基本 单位       session 的 概念 :   A   session   is   a   set   of   items   ( e . g . ,   referring   to   any   objects ,   e . g . ,   products ,   songs   or   movies )   that   are   collected   or   consumed   in   one   event   ( e . g . ,   a   transaction )   or   in   a   certain   period   of   time   or   a   collection   of   actions   or   events   ( e . g . ,   listening   to   a   song )   that   happened   in   a   period   of   time   ( e . g . ,   one   hour ) .       即 在 一个 事件 ( 比如 交易 ) 中 被 获取 或 消费 的 一系列 item 的 集合 ,   或者 在 一段时间 内 发生 的 动作 或 事件 的 集合       session 推荐 系统 : Given   partially   known   session   information ,   e . g . ,   part   of   a   session   or   recent   historical   sessions ,   an   SBRS   aims   to   predict   the   unknown   part   of   a   session   or   the   future   sessions   based   on   modelling   the   complex   relations   embedded   within   a   session   or   between   sessions .       即 根据 session 的 一部分 来 预测 未知 的 部分 、 或者 未来 的 session 事件 以及 session 之间 的 关联       两个 方向 :   推荐 下 一个 / 多个 item ;   推荐 下 一个 session       基于 session 推荐 :   基于 session 上下文 预测 target , 有时 也 加入 item 特征 和 user 特征                       item 是 基本 单位 , 也 是 基于 session 的 推荐 系统 的 主要 角色 , 其他 的 要么 是 描述 item 的 特征 , 要么 是 组织 item 的 结构 如 session       每个 item 都 被 多种 特征 所 描述 :   item 的 类别 , 价格 , 位置 等等       大多数 情况 下 , item 的 相关性 建模 都 基于 他们 的 共现       关键 挑战       inner - session   challenge       inter - session   challenge       outer - session   challenge               session 上下文 :   时间 、 地点 、 天气 、 季节 、 用户 ;   这里 将 用户 信息 看做 上下文 信息       参考 :       Duc - Trong   Le ,   Yuan   Fang ,   and   Hady   W   Lauw .   2016 .   Modeling   sequential   preferences   with   dynamic   user   and   context   factors .   In   Joint   European   Conference   on   Machine   Learning   and   Knowledge   Discovery   in   Databases .   Springer ,   145 – 161 .       Gediminas   Adomavicius   and   Alexander   Tuzhilin .   2011 .   Context - aware   recommender   systems .   In   Recommender   systems   handbook .   Springer ,   217 – 253       Longbing   Cao .   2015 .   Coupling   learning   of   complex   interactions .   Information   Processing   & amp ;   Management   51 ,   2   ( 2015 ) ,   167 – 186               多样性 挑战       特征值 的 多样性 , 不同 值 出现 的 频率 不大一样       特征 类型 的 多样性       item 的 多样性 , 一些 出现 频率 高二 另外 一些 很少 出现       session 的 多样性 , 不同 上下文 对 当前 session 的 相关性 不 一样 , 不 知道 是 个 什么 鬼       上下文 的 多样性 , 很多 不同 类型 的 上下文 因子 , 时间 、 地点                   历史           最早 始于 1980 年 [ 1 ] ,       两个 阶段 :         1990s - 2010s ,   model - free 阶段 ;   模式 挖掘 , 关联 规则 挖掘 , 序列 挖掘       2010s - 今 ,   Model - base 阶段 ;   时间 序列 相关 模型 , 马尔科夫 链 , RNN , DNN               研究 社区 关注 ,   [ 2 - 4 ]       what   to   recommend ,   购物篮 数据 ,   事件 历史数据 ( movielens ,   POI )       how   to   recommend ,   建模 session 内 的 依赖       item 的 顺序 在 某些 场景 非常 重要 , 比如 基因 数据 , 但是 在 另外 一些 场景 价值 比较 有限 , 比如 加 购物车 的 顺序 。 顺序 关系 比较 重要 的 场景 可以 使用 马尔科夫 链 、 RNN 等 捕获 序列 关系 的 模型       不 考虑 顺序 的 模型 , 发觉 共现 规律       一阶 依赖 与 高阶 依赖 ,   一阶 依赖 : 一阶 马尔科夫 链 、 因子 机 ;   高阶 依赖 :   神经网络       session 间 依赖 ,   将 上 一个 session 也 输入 模型       item 依赖 模型 , 建模 来自 于 不同 session 的 item 间 的 依赖 :   因子 机 模型       集合 依赖 模型 , 将 session 中 的 item 看做 一个 整体       特征 级别 的 依赖 ,   功能 互补 的 item 经常出现 在 同一个 session 当中 。   基于 内容 的 推荐 , 解决 冷启动 的 问题 ,   协同 过滤 。 早期 的 推荐 系统 研究 较 多 , 基于 session 的 推荐 研究 较 少       技术 类别 :       model - free   方法 :         基于 模式 / 规则 的 方法 ,   关联 挖掘 ,   牛奶 和 面包 通常 一起 购买 ,   挖掘 无序 数据       基于 序列 模式 的 方法 ,   挖掘 有序 数据 规律               model - based 方法 :       马尔科夫 链       因子 机 方法       神经网络 模型 方法 ,   也 叫   embedding 模型 和 表示 学习 模型                           不同 方法 的 比较           model - free 方法 : 简单 容易 实现 , 对于 复杂 的 数据 和 关系 挖掘 力不从心       model - based 方法 : 能 处理 复杂 的 数据 和 关系 挖掘 , 上限 很 高                   基于 模式 / 规则 的 方法           频率 模式 挖掘 ,   Aprior 、   FP - Tree ,   如果 P ( i | s ) 概率 高于 某个 阈值 , 就 可以 认为     是 一个 高频 模式       序列 模式 挖掘               基于 模型 的 方法       马尔科夫 链 ,   频率 统计 估计 概率       马尔科夫 embedding 模型 ,   解决 马尔科夫 链 稀疏 的 问题 , 不是 用 统计 频率 来 估计 转移 概率 , 而是 用 embedding 向量 的 欧式 距离 建模 概率   $ ( P ( i _ 1   \ \ rightarrow   i _ 2 )   =   exp ( - | | v _ { i _ 1 }   -   v _ { i _ 2 } | | ^ 2 )   ) $ ,   问题 :   破坏 了 非 对称性 ? ?       因子 机 模型 ,   为 每个 用户 建立 因子 模型 ,   $ ( A ^ { | U |   \ \ times   | I |   \ \ times   | I | } ) $ ,   每个 元素 代表 某个 用户 从 1 个 item 转移 到 另 一个 item 的 转移 概率 。       Tucker   Decomposition ,   $ ( A   =   C   \ \ times   V _ U   \ \ times   V _ { I _ j }   \ \ times   V _ { I _ k } ) $ ,   C 是 核心 张量 ,   $ ( V _ U ) $   是 用户 特征 矩阵 ,     $ ( V _ { I _ j } ) $ 和 $ ( V _ { I _ k } ) $ 分别 代表 最后 的 item 矩阵 和 下 一个 item 矩阵 。 乘法 分解       Canonical   Decomposition ,   加法 分解 。   $ ( a   _   { u ,   i   _   j ,   i   _   k }   =   ( v _ u ,   v _ { i _ j } )   +   ( v _ u ,   v _ { i _ k } )   +   ( v _ { i _ j } ,   v _ { i _ k } ) ) $               神经网络 模型 方法       浅层 网络 ,   item2vec ,   user2vec ,   在 隐 空间 匹配       深层 网络 ,   RNN 做 序列 推荐 ,   GRU2Rec ,   DNN 推荐 ,   CNN                           未来 的 方向           用户 通用 偏好       利用 用户 显式 偏好 , 长期 偏好 和 短期 偏好       更 多 上下文 因子 ,       噪声 和 无关 的 item ,   用户 点击 item 的 行为 具有 太 多 随机性 了 , 怎样 将 随机性 去除 , 而 只 将 有 规律 的 信号 建模 出来 ?   attention ,   Pooling       多 步 推荐 ,   Encoder - Decoder   框架 ?       cross - session   information ,   相当于 偏 长期 一点 的 依赖       cross - domain   information ,   看 了 电影 ,   听 对应 的 歌曲 ,   transfer   learning       非 IID 的 时变 问题           参考       [ 1 ]   Ahmad   M   Ahmad   Wasfi .   1998 .   Collecting   user   access   patterns   for   building   user   profiles   and   collaborative   filtering .   In   Proceedings   of   the   4th   international   conference   on   Intelligent   user   interfaces .   ACM ,   57 – 64     [ 2 ]   Bal á zs   Hidasi ,   Alexandros   Karatzoglou ,   Oren   Sar - Shalom ,   Sander   Dieleman ,   Bracha   Shapira ,   and   Domonkos   Tikk .   2017 .   DLRS   2017 :   Second   Workshop   on   Deep   Learning   for   Recommender   Systems .   In   Proceedings   of   the   Eleventh   ACM   Conference   on   Recommender   Systems .   ACM ,   370 – 371 .     [ 3 ]   Alexandros   Karatzoglou   and   Bal á zs   Hidasi .   2017 .   Deep   Learning   for   Recommender   Systems .   In   Proceedings   of   the   Eleventh   ACM   Conference   on   Recommender   Systems .   ACM ,   396 – 397 .     [ 4 ]   Alexandros   Karatzoglou ,   Bal á zs   Hidasi ,   Domonkos   Tikk ,   Oren   Sar - Shalom ,   Haggai   Roitman ,   Bracha   Shapira ,   and   Lior   Rokach .   2016 .   RecSys ’   16   Workshop   on   Deep   Learning   for   Recommender   Systems   ( DLRS ) .   In   Proceedings   of   the   10th   ACM   Conference   on   Recommender   Systems .   ACM ,   415 – 416 .     [ 5 ]   Shoujin   Wang   and   Longbing   Cao .   2017 .   Inferring   implicit   rules   by   learning   explicit   and   hidden   item   dependency .   IEEE   Transactions   on   Systems ,   Man ,   and   Cybernetics :   Systems   ( 2017 )     [ 6 ]   Wei   Wei ,   Xuhui   Fan ,   Jinyan   Li ,   and   Longbing   Cao .   2012 .   Model   the   complex   dependence   structures   of   financial   variables   by   using   canonical   vine .   In   CIKM ’ 12 .   1382 – 1391 .     [ 7 ]   Jia   Xu   and   Longbing   Cao .   2018 .   Vine   Copula - Based   Asymmetry   and   Tail   Dependence   Modeling .   In   PAKDD ’ 2018 ,   Part   I .   285 – 297 .       基于 规则 的 算法       Aprior   算法           item 集合   I   ,   也 就是 要 推荐 的 东西 的 集合 , 比如 商品 集合 , poi 集合 ,   I       session 集合   S ,   每条 记录 s 代表 一个 item   list , 表明 他们 之间 存在 某种 关联 , 比如 同时 在 一个 订单 中 出现 ,   同时 在 用户 的 一个 session 中 出现 等等 。   s 是   I 的 子集       支持 度   $ ( support ( A   = & gt ;   B )   =   P ( A   \ \ union   B )   ) $ ,   A 和 B 都 是 I 的 子集 ,   支持 度高 的 规则 可以 用来 做 推荐 ,   例如 A 是 用户 已经 点击 过 的 item 集合 , 如果 support ( A   = & gt ;   B ) 很大 , 那么 就 可以 认为 B 是 要 给 用户 推荐 的 item 集合 。 联合 概率       置信度   $ ( confidence ( A   = & gt ;   B )   =   P ( B   |   A )   =   \ \ frac { P ( A   \ \ union   B ) } { P ( A ) } ) $ ,   条件 概率       强 关联 规则 ,   满足 最小 支持 度 和 最小 置信度 的 关联 规则       由于 任何 ( k - 1 ) 非 频繁 项集 都 不是 k 频繁 项集 的 子集 , 所以 在 构建 的 时候 可以 减枝 , 在 生产 k 项集 的 时候 , 可以 只 考虑 k - 1 频繁 项集 的 扩展 集合 即可           FP - Tree           论文 : Mining   frequent   patterns   without   candidate   generation ,   韩家 伟       并行 版本 实现 :   parallel   fp - growth   for   query   recommendation ,   Spark   ml   库 使用 的 方法       Frequent   Pattern   Tree       关键 是 构建 FP 树       遍历 整个 数据 集 ,   统计 出 每个 item 的 次数 , 只 保留 超过 最小 支持 度 的 item       将 每条 记录 中 的 item 按照 item 的 全局 频次 排序       将 排序 号 的 列表 插入 到 FP 树中 ,   直到 所有 数据 插入 完成 ,   得到 一个 包含 了 数据 集 所有 统计 信息 的 数据结构               FP 树 包含 了 用于 频繁 模式 挖掘 的 所有 信息 ,   是 完备 的       FP 树 构建 复杂度 分析       只 需要 扫描 数据库 两次 ,   1 次 统计 每个 item 的 次数 ,   1 次 构建 树       时间 复杂度 是   O ( | transaction | )               FP 树 的 高度 不 超过 每个 transaction 中 频繁 item 的 数目 最大值       FP 树 节点 的 数目 不 超过 全部 transaction 中 频繁 item 的 数目 总和                       根据 FP 树 寻找 频繁 集 算法   FP - Growth ,   扫描 数据库 , 构建 主 FP 树 , 然后 执行 一下 算法 , 初始 后缀 为 空集       对 FP 树中 的 每个 item , 寻找 以 item 结尾 的 所有 路径 构造 子 FP 树 , 每个 节点 的 次数 也 调整 为 item 的 次数 。 相当于 从 原始 数据库 中 只 筛选 出 包含 item 的 transaction , 并且 去掉 频率 比 item 低 的 其他 元素 , 构建 FP 树 。 去掉 频度 低 的 是 为了 避免 重复 统计 。       在 每个 子 FP 树 递归 使用 该 算法 , 寻找 频繁 子串 , 直到 FP 树为 空 , 或者 item 的 所有 路径 上 的 频率 之 和 不 超过 最小 支持 度 。 这些 频繁 子串 与 后缀 拼接 , 得到 完整 的 频繁 子串                           并行 FP           论文 :   parallel   fp - growth   for   query   recommendation ,   Spark   ml   库 使用 的 方法 ,   Google   China ,   2008           序列 FP           论文 :   PrefixSpan -   Mining   Sequential   Patterns   Efficiently   by   Prefix - Projected   Pattern   Growth       。 。 。           改进 应用           PV 时间 加权 来 计算 规则 的 加权 频率       引入 用户 个性化 :   用户 聚类       对 关联 规则 加权 , 用 规则 中 item 的 某种 权重 , 都 是 手工 设计 的       将 协同 过滤 和 关联 规则 结合 起来           序列 模式 挖掘           Effective   next - items   recommendation   via   personalized   sequential   pattern   mining ,   2012           推荐 理由           Explainable   Recommendation :   A   Survey   and   New   Perspectives       推荐 理由 的 方法 :     https : / / zhuanlan . zhihu . com / p / 21497757         基于 内容 的 推荐 理由 :         将 item 的 一些 特征 ( 分类 、 标签 、 价格 、 品牌 、 导演 etc ) 作为 推荐 理由 ;         挑选 高质量 评论 :               热门 推荐 「 热门 指标 】 排名 前 【 名次 】 名 的 【 热门 物品 】 」 , 定期 生成 榜单       协同 过滤 :   「 购买 此 商品 的 用户 也 购买 了 这些 」 , 「 和 你 口味 相似 的 用户 都 买 了 」       基于 知识 :   「 由于 你 【 用户 的 需求 或 偏好 】 ， 所以 你 可能 选择 【 某 物品 】 」               方法 分类 :       生成 解释 的 类型 :   文本 、 视觉       生成 解释 的 模型 :   矩阵 分解 、 主题 模型 、 图 模型 、 deep   learning 、 知识 图谱 、 关联 规则 、 post - hoc   models   etc               例子 :       基于 浅层 模型 的 文本 解释 :   Explicit   factor   models   for   explainable   recommendation   based   on   phrase - level   sentiment   analysis .         基于 深层 模型 的 文本 解释 :   Interpretable   Convolutional   Neural   Networks   with   Dual   Local   and   Global   Attention   for   Review   Rating   Prediction       基于 深层 模型 的 视觉 解释 :   Visually   explainable   recommendation               可 解释性 和 有效性 :   这是 一对 矛盾 ,   但是 现在 在 深度 学习 的 帮助 下 ,   变得 可以 同时 兼得           不同 的 解释 形式           形式       个性化 文本       词云       高亮 图片 的 某个 区域 进行 解释       生成 喜欢 该 item 的 社交 朋友 列表       统计 直方图 、 饼 图 等 形式 可视化 评分 分布 和 item 的 优缺点               user - based 解释 :   推荐 给 用户 的 原因 是因为 与 他 相似 的 一组 用户 对 这个 item 评分 很 高 ,   将 相似 用户 的 评分 可视化 ( 直方图 等 可视化 手段 ) 。 由于 用户 对 「 相似 用户 」 并 不 了解 , 所以 这种 方法 说服力 比较 有限 , 如果 换成 朋友 会 更 有 说服力       item - based 解释 :   推荐 给 用户 的 原因 是因为 他 之前 买 了 某个 item 。 因为 用户 对 购买 过 的 item 比较 熟悉 , 这个 方法 比 user - based 的 方法 更 有 说服力       content - based 解释 :   解释 原因 是因为 item 的 某个 feature , 比如 价格 、 品牌 etc ,   而 这个 feature 与 用户 的 某个 偏好 属性 匹配 上 了 。       人口 统计学 解释 :   「 80 % 的 20 - 30 岁 用户 都 买 了 这个 商品 」       文本 解释 :         feature - level 与 基于 内容 的 解释 相似 ,   区别 是 这些 feature 不是 直接 从 商品 的 属性 中 提取 的 ,   而是 从 用户 评论 中 提取 的 ;         评论 文本 中 抽取 feature :   短语 级别 的 语义 分析 工具 :   feature – opinion – sentiment :   Do   users   rate   or   review ? :   Boost   phraselevel   sentiment   labeling   with   review - level   sentiment   classification ,   通过 这种 方式 ,   基于 抽取 的 feature ,   以词 云 的 方式 可视化 这些 feature 作为 解释       Sentence - level :       基于 模板 + 关键词 :   「 该 产品 XXXX , 你 可能 会 喜欢 」 , 正向 和 负向 的 解释 都 可以 提升 说服力 、 转化率 etc       利用 LSTM 直接 从 评论 中 生成 解释 文本 :   Automatic   generation   of   natural   language   explanations       从 大量 评论 中 抽取 summary , 生成 解释 :   Neural   rating   regression   with   abstractive   tips   generation   for   recommendation                       视觉 解释 :       将 图片 中 用户 感兴趣 的 部分 区域 高亮 :   Visually   explainable   recommendation               社交 解释 :                         混合 解释 ,   输出 的 一致性           可 解释 的 推荐 方法               矩阵 分解 的 解释 方法 :   问题 : 隐 因子 缺乏 可 解释性           显式 矩阵 分解 :   Zhang   et   al .   [ 2014a ]   基本 思想 是 , 在 传统 的 矩阵 分解 的 基础 上 , 假定 隐 空间 的 其中 一部分 维度 和 显性 因子 关联 , 可以 通过 一个 线性 变化 V 从子 隐 空间 变换 到 显 因子 空间 。 用 这 一部分 显 因子 来 做 推荐 解释 。 显 因子 是从 大量 评论 通过 文本 语义 分析 提取 的 、 加上 商品 的 属性 etc       构建   user - item - feature   立方体 , 利用 张量 分解 来 做   Chen   et   al .   [ 2016 ]         多任务 学习 : Wang   et   al .   [ 2018b ]   学习 两个 任务 , 一个 是 推荐 任务 , 一个 是 option 建模                         【 2018 - 推荐 理由 】 Explainable   Recommendation   via   Multi - Task   Learning   in   Opinionated   Text   Data               主题 模型 :   Hidden   Factor   and   Topic   ( HFT )   model ,   将 item 和 user 的 每个 隐 向量 联系 到 LDA 中 的 一个 主题 ,   使用 了 一个 softmax 函数               基于 图 模型 的 解释 :             基于 树 模型 的 解释 ,   利用 GBDT 提取 显式 交叉 特征 ,   进行 解释       深度 学习 的 推荐 解释 :       从 用户 评论 用 CNN 提取 特征 , 预测 评分 进行 推荐 。 CNN 的 时候 用 了 attention 加权 Pooling , 权重 可以 用于 解释 关注点                               生成 自然语言 的 解释 :     https : / / zhuanlan . zhihu . com / p / 33956907         评论 + 辅助 信息 自动 生成 自然语言 解释 :   Automatic   Generation   of   Natural   Language   Explanations       KG   embedding   推荐 的 解释 :       Personalized   PageRank       将 推荐 看做 图谱 中 「 购买 」 关系 下 最近 的 节点       Rippnet ,   推荐 解释 可以 看做 在 图 中 找到 从 user 到 item 的 一条 路径               data   Mining :   关联 规则 :   啤酒 - 尿布 解释       Post   Hoc   Explanation :   事后 解释 ,   已经 推荐 了 这个 item , 然后 从事 先 可能 的 解释 中 选择 一个 , 这些 解释 来自 于 前面 说 的 解释 方法           可 解释 推荐 的 评估           离线 评估 :         评分 预测 :   MAE ,   RMSE       top - n 推荐 :   precision @ n 、 recall @ n 、 f1 @ n 、 NDCG @ n 、 MRR ,   DCG 中 定义 正例 相关性 为 1 , 其他 为 0 , 按照 位置 折扣 加权 , 对前 n 个 求和 , 对 每个 用户 , 用 最大 的 DCG ( 正例 都 在 负例 前面 ) 归一化 , 然后 在 所有 用户 中 平均 得到 NDCG @ n       Hit   Ratio ,   AUC               在线 评估 :       点击率 、 转化率 、 其他 商业 指标               推荐 解释 的 离线 评估 :       explainability   precision ( EP ) 定义 为 推荐 的 top   n   个 item 中有 多少 比例 可 解释       explainability   recall   ( ER )   定义 为 推荐 的 top   n 中 可 解释 的 item 占 所有 可 解释 的 item 的 比例 ?       生成 文本 质量 :   BLEU ,   ROUGE       可读性 指标 :                 推荐 解释 的 在线 评估 :       跟 一般 的 推荐 评估 指标 一致 ,   看 点击率 、 转化率               仿真 与 用户 研究 , 利用 众包 平台 打标 , 小规模 用户 调研       Qualitative   Evaluation   by   Case   Study           应用           电商 推荐       poi 推荐       社交 推荐       多媒体 推荐           未来 的 方向           解释 深度 学习 推荐 系统       基于 知识 ( 图谱 ) 增强 的 可 解释 推荐       多种 信息 类型 的 可 解释 推荐 : 图片 、 文本 、 行为 etc       解释 的 自然语言 生成 NLG :         缺乏 容易 评估 解释 效果 的 离线 指标       解释 需要 随 时间 变化 , 因为 用户 的 行为 、 偏好 、 关注点 等 随 时间 变化       多种 解释 的 聚合 :   选 最优 的   or   聚合 所有 的           Deep   Learning   based   Recommender   System :   A   Survey   and   New   Perspectives           DL 推荐       视频 推荐 YouTube :   Deep   neural   networks   for   youtube   recommendations .       Google   play :   wide & amp ; deep       新闻 推荐 Yahoo : Embedding - based   News   Recommendation   for   Millions   of   Users                   BiNE :   Bipartite   Network   Embedding           将 不同 实体 的 二分 图 ( user - item 图 , 只有 user 和 item 之间 有 链接 ) 做 embedding       作者 认为 按照 node2vec 的 做法 子 在 全同 实体 的 图 可以 , 但是 在 不同 实体 的 图 不是 最优 的       一个 好 的 embedding 方法 应该 可以 根据 embedding 向量 重构 原始 的 图       建模 显式 关系 ( 边 ) 和 隐式 关系       两个 节点 之间 共现 的 概率 建模 为 内积 经 sigmoid 函数 归一化 的 值       用 embedding 向量 表达 的 共现 概率 与 实际 的 共现 概率 之间 的 差异 用 KL 聚类 来 度量           $ $     P ( i ,   j )   =   \ \ frac { w _ { ij } } { \ \ sum _ { ij }   w _ { ij } }   \ \ \ \     \ \ hat { P } ( i ,   j )   =   \ \ frac { 1 } { 1   +   exp ( -   u _ i ^ T   v _ j ) }   \ \ \ \     min   KL ( P ,   \ \ hat { P } )   =   -   \ \ sum _ { ij }   w _ { ij }   log   \ \ hat { P } ( i , j )     $ $           user - user 二阶 相似 度 、 item - item 二阶 相似 度           $ $     w _ { ij } ^ U   =   \ \ sum _ { k \ \ in   V } w _ { ik }   w _ { jk } \ \ \ \     w _ { ij } ^ V   =   \ \ sum _ { k \ \ in   U } w _ { ik }   w _ { jk }     $ $           而用 embedding 向量 来 表达 这 两个 二阶 共现 概率 需要 用到 softmax , 可以 利用 负 采样 近似 , 即 对 1 个 样本 采样 k 个 其他 不共现 样本 、 计算 这 ( k + 1 ) 个 样本 ( 不 ) 共现 概率 的 乘积       最终 的 目标 是 联合 优化 一阶 和 二阶 共现 概率           Session - based   Recommendation   with   Graph   Neural   Networks       Graph   Neural   Networks   for   Social   Recommendation  ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/intro-paper.html"},
      
      
      {"title": "知识图谱与推荐系统", "text": "    Table   of   Contents           deepwalk           line           metapath2vec           Meta - Graph                 deepwalk           DeepWalk :   Online   Learning   of   Social   Representations ,   KDD2014       解决 的 问题 :   将 图 中 的 节点 转换 为 向量 , 节点 之间 的 关系 通过 边来 刻画       解决 的 方法 :   在 图 中用 random   walk 采样 出 一些 列 的 序列 , 得到 语料 , 然后 放到 word2vec 里面 训练       算法 :       随机 初始化 每个 定点 对应 的 embedding 向量       重复 一下 步骤 gamma 次       O   =   shuffle ( V )       for   each   vi   in   O   do       使用 random   walk 采样 一个 从 vi 出发 的 顶点 序列       利用 skip - gram 算法 更新 模型 参数                                   line           LINE -   Large - scale   Information   Network   Embedding ,   2015       解决 的 问题 :   大规模 , 异构 网络 , 有 向 无 向 , 有权 无权 ;   二阶 相似性 ;   边 采样 解决 权重 的 高 方差       一阶 近似       顶点 i 和 j 的 概率   $ (   P ( v _ i ,   v _ j )   =   \ \ frac { 1 } { 1   +   exp ( -   u _ i ^ T   u _ j ) }     )       顶点 i 和 j 的 经验 概率   $ (   \ \ hat { P } ( v _ i ,   v _ j )   =   \ \ frac { w _ { ij } } { W }   ) $   wij 是 边 的 权重 ,   W 是 所有 边 的 权重 和       两个 概率 的 KL 距离 给出 目标 函数   $ ( - \ \ sum _ { ( i , j ) \ \ in   E }   w _ { ij }   log   P ( v _ i ,   v _ j ) ) $               二阶 相似 :   两个 定点 有 相似 的 邻居 ( 上下文 )       每个 顶点 有 两个 向量 , 一个 作为 定点 的 向量 , 一个 作为 上下文 的 向量       上下文 与 定点 存在 边 的 概率 用 softmax 表示 , 用负 采样 近似 。 相当于 一阶 近似 中 的 sigmoid 近似 , 同时 考虑 了 负 样本       优化 目标 还是 用 的 一阶 邻居 , 那 跟 一阶有 啥 本质区别 吗 ? ?               边 采样 :   没有 直接 用 $ ( w _ { ij } ) $ 去 乘以 梯度 , 而是 用 它 作为 边 采样 概率 ,   为什么 这样 就 可以 降低 梯度 方差 ? 梯度 方差 到底 是 啥 ?   梯度 的 方差 是 与 所有 边 的 梯度 加权 平均值 的 方差 , 如果 用 加权 梯度 , 确实 会 增大 梯度 方差 , 如果 只是 做 加权 采样 确实 每次 梯度 都 是 乘以 1 就 不会 有 额外 的 波动           metapath2vec           metapath2vec -   Scalable   Representation   Learning   for   Heterogeneous   Networks       解决 的 问题 , 之前 的 embedding 方案 是 针对 相同 实体 的 方案 , 本文 解决 对 不同 实体 的 embedding       异构 网络 随机 游走 的 问题 : 倾向 于 出现 频次 高 的 关系 类型 ; 解决 方法 , 限定 随机 游走 只能 在 给定 的 meta - path 上 , meta - path 指 的 是 关系 路径 , 比如   用户 - item - 用户   表明 从 一个 用户 触发 , 下 一个 智能 游走 到 ( 看过 的 , 如果 限定 了 关系 ) item 上 , 不能 走 到 其他 类型 的 节点       metapath2ve c++   共现 概率 归一化 也 限定 到 一种 关系 对应 的 类型 的 节点 上           Meta - Graph           Meta - Graph   Based   Recommendation   Fusion   over   Heterogeneous   Information   Networks       核心思想 :       选出 L 条 meta - path ,   得到 L 个 user - item 的 相似 度       用 矩阵 分解 每个 相似 度 矩阵       将 L 个 user 和 item 的 隐 向量 拼接 得到 特征向量 , 放到 FM 中 学习       FM 中 的 权重 可以 划分 到 2L 组 , 对应 2L 个 隐 向量 ( L 个 用户 向量 , L 个 item 向量 ) , 对 每个 向量 做 group   lasso , 用于 筛选 meta - path              ", "tags": "machine-learning/recommend", "url": "/wiki/machine-learning/recommend/kg-recommend.html"},
      
      
      
      {"title": "Recurrent neural network based language model", "text": "    Table   of   Contents           历程           模型                 历程           Bengio   采用 神经网络 做 统计 语言 模型 。 前馈 神经网络   +   固定窗 长度     Yoshua   Bengio ,   Rejean   Ducharme   and   Pascal   Vincent .   2003 .   A     neural   probabilistic   language   model .   Journal   of   Machine   Learning     Research ,   3 : 1137 - 1155       Goodman   在 Bengio   的 基础 上 进行 发展 ， 发现 这种 简单 模型 比 混合 了 其他 多种 方法 的 模型 都 要 好 。     Goodman   Joshua   T .   ( 2001 ) .   A   bit   of   progress   in   language   modeling ,     extended   version .   Technical   report   MSR - TR - 2001 - 72 .       Schwenk   发现 基于 神经网络 的 模型 ， 能够 显著 提升 语音 识别 任务 ， 在 几个 任务 中 比 最好 的 系统 都 要 好 。     Holger   Schwenk   and   Jean - Luc   Gauvain .   Training   Neural   Network     Language   Models   On   Very   Large   Corpora .   in   Proc .   Joint   Conference     HLT / EMNLP ,   October   2005 .           这种 方法 唯一 的 缺点 是 ， 需要 采用 固定 的 窗 长度 ， 一般 在 5 - 10 。     递归 神经网络 理论 上 能够 记忆 任意 长 的 信息 ， 解决 了 这个 问题 。       另外 一种 能够 实现 长期 依赖 的 方法 ： 随机 梯度 下降 ？     Yoshua   Bengio   and   Patrice   Simard   and   Paolo   Frasconi .   Learning     Long - Term   Dependencies   with   Gradient   Descent   is   Difficult .     IEEE   Transactions   on   Neural   Networks ,   5 ,   157 - 166 .       模型       首先 采用 一个 简单 的 递归 神经网络 ， 也 叫 Elman 网络 ：     Jeffrey   L .   Elman .   Finding   Structure   in   Time .   Cognitive   Science ,     14 ,   179 - 211       输入 $ ( x ( t ) ) $ 为 当前 词 向量 $ ( w ( t ) ) $ 和 上 一 时刻 隐层 状态 $ ( s ( t - 1 ) ) 连接成 的 新 向量 ， 这里 用   +   表示 链接 ， 不是 求和 ：     用 隐层 的 状态 来 代表 上下文 信息 。       $ $     x ( t )   =   w ( t )   +   s ( t - 1 )     \ \ \ \     s _ j ( t )   =   sigmoid ( \ \ sum _ i   x _ i ( t )   u _ { ji } )     \ \ \ \     y _ k ( t )   =   softmax ( \ \ sum _ j   s _ j ( t )   v _ { kj } )     $ $       模型 训练 ： 标准 的   BP   +   SGD ， 一 开始 学习 率 $ ( \ \ alpha = 0.1 ) $ ， 每 一个 epoch 之后 ， 在 验证 集上 检验 ， 如果 验证 集 的 对数 似然比 增加 了 ，     就 继续 训练 ， 如果 没有 明显 的 改善 ， 就 将 学习 率 减半 $ ( \ \ alpha _ { new }   =   \ \ alpha   /   2   ) $ 。 如果 之后 仍然 没有 明显 的 改善 ， 就 停止 训练 。     一般 在 10 - 20 个 epoch 就 能 收敛 。       作者 的 模型 没有 明显 的 过 拟合 ， 即使 在 使用 正则 项 的 情况 下 ， 也 没有 明显 的 收益 。       误差 是 基于 交叉 熵 计算 的 ， 即 交叉 熵 的 导数 ：       $ $     err ( t )   =   desired ( t )   -   y ( t )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/recurrent-neural-network-based-language-model.html"},
      
      
        
        
      
      {"title": "Asynchronous Methods for Deep Reinforcement Learning", "text": "    Table   of   Contents           简介           要点                 简介       强化 学习 经典 论文 , 异步 实现 方法 。 Deepmind   & amp ;   Google 。       要点           之前 的   Deep   RL   算法 需要 经验 回放 ( experience   replay ) 解决 样本 时间 相关性 的 问题 ,   例如 DQN ;         经验 回放 有 两个 问题 :   1 是 需要 大 内存 存放 经验 ( 通常 非常 大 ) ,   2 是 用 经验 回放 , 那么 线上 策略 与 学习策略 不 一致 , 也 就是 要用 off - policy 学习 方式 。       异步 并行 的 学习 方式 可以 很 好 的 解决 样本 的 时间 相关性 问题 , 因为 同时 运行 很多 个 episode , 每 一个 都 是 独立 的 , 不 存在 时间 相关性 。       使用 了 单机 多卡 ( GPU ) 的 并行 方案       asynchronous   advantage   actor - critic   ( A3C ) 效果 最优       The   General   Reinforcement   Learning   Architecture   ( Gorila )       多机 分布式 异步 训练       每个 actor 独立 训练 和 使用 经验 回放       每 一个 learner 从 所有 的 经验 中 采样 , 计算 DQN   loss       梯度 异步 与 PS 交互 , 更新       模型 参数 定期 同步 到 每 一个   actor - learner       100   actor - learner ,   30   parameter   server ,   130 台 机器               使用 n 步 TD 的 方法 更新 参数      ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/a3c.html"},
      
      
      {"title": "Explore Exploit 探索与利用", "text": "  Explore   和   Exploit   是 做 推荐 的 常用 问题 , 由于 推荐 的 点击率 / 转化率 模型 是 由 历史数据 产生 的 , 对 新 的 item 点击率 估计 误差 会 很大 , 需要 对 曝光 不足 的 item 进行 探索 。       考虑 多臂 老虎机 问题 , 即 有 N 个 老虎机 , 每个 臂 有 概率   Pi   会 有 奖励 , 并且 这个 概率 不随 时间 变化 。       显然 , 最优 策略 是 选择 概率   Pi   最大 的 那个 臂 不停 地摇 , 每次 期望 回报 最大   $ ( r ^ *   =   \ \ max _ i   p _ i ) $ ,     而 由于 一 开始 不 知道   Pi   , 所以 实际 第 t 步 的 回报 期望 可能 小于 最大值 。 定义   $ ( r _ t ) $ 为 第 t 步 的 回报 , 如果 有 奖励 , 回报 为 1 , 否则 回报 为 0 .     其 期望值 等于 摇 的 那个 臂 对应 的   Pi   的 值 。       可以 定义   regret   为 他们 的 差   $ ( regret _ t   =   r ^ *   -   E   r _ t ) $ 。               ##   多臂 老虎机 仿真       import       numpy       as       np         n       =       10       #   10 个臂       bandit _ p       =       np     .     random     .     rand     (     n     )         def       gen _ data     (     i     ) :               assert       i       & lt ;       n               if       np     .     random     .     rand     ( )       & lt ;       bandit _ p     [     i     ] :                       return       1.0         #   reward               return       0.0       #   no   reward                 在 这个 问题 中 , 一般 有 这些 经典 的 探索 方法 :           随机 策略 : 随机 选择 一个 动作 , 简称 瞎猜 , 每 一个 动作 都 有 概率 。 所以   $ (   regret   =   \ \ max _ i   p _ i   -   \ \ bar { p _ i }   ) $                     def       random _ select     (     ctx     ) :               return       np     .     random     .     randint     (     ctx     [     &# 39 ; n &# 39 ;     ] )                     确定性 策略 : 每次 都 选择 当前 最优 的 动作 , 如果 最优 动作 很多 个 , 随机 选 一个                   #   稍作 修改 , 前 n 次 依次 选择 每 一个 动作       def       determine _ select     (     ctx     ) :               if       ctx     [     &# 39 ; step &# 39 ;     ]       & lt ;       n     :                       return       ctx     [     &# 39 ; step &# 39 ;     ]               return       np     .     argmax     (     ctx     [     &# 39 ; cum _ reward _ action &# 39 ;     ]       /       (     ctx     [     &# 39 ; cum _ action &# 39 ;     ] ) )                     $ ( \ \ epsilon ) $ - 贪心 策略 : 即以   $ ( \ \ epsilon ) $   的 概率 选择 随机 策略 , 以   $ ( 1   -   \ \ epsilon ) $ 选择 确定性 策略 ,   $ ( \ \ epsilon ) $   随着 时间 衰减                   def       epsilon _ greedy     (     ctx     ) :               epsilon       =       1       /       (     1       +       np     .     sqrt     (     ctx     [     &# 39 ; step &# 39 ;     ] ) )               if       np     .     random     .     rand     ( )       & lt ;       epsilon     :       #   random                       return       random _ select     (     ctx     )               else     :                       return       determine _ select     (     ctx     )                     naive   策略 :   前 N 步 随机 探索 ,   从 第   N + 1   步 开始 , 采用 确定性 策略                   def       naive _ select     (     ctx     ) :               if       ctx     [     &# 39 ; step &# 39 ;     ]       & lt ;       500     :                       return       random _ select     (     ctx     )               else     :                       return       determine _ select     (     ctx     )                     softmax   随机 策略 : 即 对 每 一个 动作 的 累积 平均 回报 做 softmax 归一化 , 作为 每个 动作 的 选择 概率                   def       softmax _ select     (     ctx     ,       n0       =       0.1     ) :               if       ctx     [     &# 39 ; step &# 39 ;     ]       & lt ;       n     :                       return       ctx     [     &# 39 ; step &# 39 ;     ]               avg _ reward       =       ctx     [     &# 39 ; cum _ reward _ action &# 39 ;     ]       /       (     ctx     [     &# 39 ; cum _ action &# 39 ;     ] )               p       =       np     .     exp     (     avg _ reward       /       n0       )               p       =       p       /       np     .     sum     (     p     )               return       np     .     random     .     multinomial     (     1     ,       p     ) [     0     ]                     thompson   采样 :   认为 每个 动作 的 Pi   服从 Beta ( a ,   b ) 先验 分布 ,   而 每个 动作 的 历史 平均 回报 后验 分布 则 服从   Beta ( a   +   win ,   b   +   loss )   分布 , 确定性 策略 相当于 按照 后验 分布 的 均值 / 众数   选择 最优 策略 , 然而 前期 由于 探索 不 充分 , 这种 估计 方差 很大 , 必然 要 引入 随机性 进行 探索 , Thompson 的 方法 是从 后验 分布 中 采样 一个 值 代替 对 Pi 的 估计 , 然后 按照 确定性 策略 选择 动作 。                   def       thompson _ select     (     ctx     ) :               win       =       ctx     [     &# 39 ; cum _ reward _ action &# 39 ;     ]               loss       =       ctx     [     &# 39 ; cum _ action &# 39 ;     ]       -       win               p       =       np     .     zeros     (     n     )               for       i       in       range     (     n     ) :                       #   期望 概率   p _ i   =   ( 1   +   win [ i ] )   /   ( 1   +   trials [ i ] ) ,   现在 不是 用 期望 的 概率 , 而是 引入 一定 的 随机性 , 随机 从 p _ i 服从 的 后验 beta 分布 中 采样 一个 结果                       p     [     i     ]       =       np     .     random     .     beta     (     1       +       win     [     i     ] ,       1       +       loss     [     i     ] )               return       np     .     argmax     (     p     )                     UCB 方法 :   因为 利用 历史 平均 回报 对 Pi 估计 不准 , 所以 可以 用 估计 的 置信区间 的 上界   $ ( \ \ bar { r _ i }   +   \ \ sqrt { \ \ frac { 2   ln   T } { n _ i } } ) $   作为 对 Pi 的 乐观 估计 。           然后 采用 确定性 策略 。 其中   T   是 总 的 步数 ,   n _ i   是 第 i 个臂 被 选择 的 次数 。                   def       ucb _ select     (     ctx     ) :               if       ctx     [     &# 39 ; step &# 39 ;     ]       & lt ;       n     :                       return       ctx     [     &# 39 ; step &# 39 ;     ]               cum _ n       =       ctx     [     &# 39 ; step &# 39 ;     ]       +       1.0               up _ bound       =       ctx     [     &# 39 ; cum _ reward _ action &# 39 ;     ]       /       (     ctx     [     &# 39 ; cum _ action &# 39 ;     ] )       +       np     .     sqrt     (     1.0       *       np     .     log     (     cum _ n     )       /       ctx     [     &# 39 ; cum _ action &# 39 ;     ] )               return       np     .     argmax     (     up _ bound     )                     仿真 代码                   import       matplotlib . pyplot       as       plt       from       numba       import       jit       import       logging         @ jit       def       simulation     (     select _ action     ,       n _       =       3000     ) :               t       =       [ ]               reward       =       [ ]               cum _ reward _ per       =       0.0               cum _ reward _ action       =       np     .     zeros     (     n     )               cum _ action       =       np     .     zeros     (     n     )               for       i       in       range     (     n _     ) :                       t     .     append     (     i     )                       ctx       =       {     &# 39 ; n &# 39 ;       :       n     ,       &# 39 ; cum _ reward _ action &# 39 ;       :       cum _ reward _ action     ,       &# 39 ; cum _ action &# 39 ;       :       cum _ action     ,       &# 39 ; step &# 39 ;     :       i     }                       action       =       select _ action     (     ctx     )                       cum _ action     [     action     ]       + =       1                       r       =       gen _ data     (     action     )                       action _ n       =       cum _ action     [     action     ]                       cum _ reward _ action     [     action     ]       + =       r                       cum _ reward _ per       + =       r                       reward     .     append     (     cum _ reward _ per     )               return       np     .     array     (     t     ) ,       np     .     array     (     reward     )         @ jit       def       run     (     select _ action     ,       rnd _       =       100     ,       n _       =       3000     ) :               t       =       [ ]               avg _ reward       =       0               for       i       in       range     (     rnd _     ) :                       t     ,       reward       =       simulation     (     select _ action     ,       n _     )                       avg _ reward       =       avg _ reward       *       i       /       (     i       +       1     )       +       reward       /       (     i     +     1     )                         logging     .     info     (     & quot ; simulate       % d       round .& quot ;       %       (     i     ) )               return       t     ,       np     .     max     (     bandit _ p     )       -       avg _ reward       /     (     1       +       np     .     arange     (     n _     ) )           def       main     ( ) :               n _       =       2000               t     ,       r       =       run     (     random _ select     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                 t     ,       r       =       run     (     epsilon _ greedy     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                 t     ,       r       =       run     (     naive _ select     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                 t     ,       r       =       run     (     thompson _ select     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                 t     ,       r       =       run     (     ucb _ select     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                 t     ,       r       =       run     (     softmax _ select     ,       n _       =       n _     )               plt     .     plot     (     t     ,       r     )                   plt     .     legend     ( [     &# 39 ; random &# 39 ;     ,       &# 39 ; epsilon   greedy &# 39 ;     ,       &# 39 ; naive &# 39 ;     ,       &# 39 ; thompson   select &# 39 ;     ,       &# 39 ; ucb   select &# 39 ;     ,       &# 39 ; softmax   select &# 39 ;     ] )               plt     .     xlabel     (     &# 39 ; t &# 39 ;     )               plt     .     ylabel     (     &# 39 ; regret &# 39 ;     )               plt     .     show     ( )         if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               main     ( )                    ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/explore-exploit.html"},
      
      
      {"title": "延迟回报分解", "text": "    Table   of   Contents           Bias - Variance   for   MDP                 Bias - Variance   for   MDP       动作 值 函数 的 偏差 和 方差       $ $     q ^ { \ \ pi } ( s , a )   =   \ \ sum     $ $  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rudder.html"},
      
      
      {"title": "强化学习2018年论文阅读笔记", "text": "    Table   of   Contents           Robust   DQN                 Robust   DQN           Stabilizing   Reinforcement   Learning   in   Dynamic   Environment   with   Application   to   Online   Recommendation ,   Shi - Yong   Chen ,   Yang   Yu   etc       南京大学 跟 阿里巴巴 合作 的 论文 ,   KDD18       传统 RL 方法 在 静态 环境 中 比较 适合 , 本文 方法 主要 解决 在 动态 环境 ( 环境参数 分布 会 随 时间 变化 ) 中 , 实现 稳定 的 回报 ( reward ) 估计 :   Robust   DQN ( Double   DQN ) 。       相比 于 Atari 游戏 、 alpha   go , 实际 的 推荐 问题 中 最大 的 不同 是 高度 动态 , 环境参数 和 分布 随 时间 的 变化 , 使得 对 reward 的 估计 方差 很大 , 并且 有 偏 :   因为 reward 的 增大 可能 是 策略 提升 带来 的 , 也 可能 是 时间 变化 带来 的 。       两个 关键 策略 改进       解决 方差 较大 : 用 分层 采样 回放 ( stratified   sampling   replay )   代替 传统 的 经验 回放 。 使用 一个 先验 的 用户 分布 , 根据 这个 先验 分布 采样 每 一个 batch       解决 时变 偏差 : 实现 近似 的   regret   reward               分层 采样 回放       通过 长时间 的 数据 统计 , 选出 一些 用户 分布 稳定 的 属性 : 性别 、 年龄 、 地理信息 , 通过 这些 属性 对 用户 分层       在 回放 的 时候 , 用 固定 比例 的 分层 采样 代替 对 短期 用户 的 均匀 采样               Approximate   Regretted   Reward       利用 一个 离线 模型 的 线 上 回报 估计 环境 随 时间 的 动态 变动 ,   然后 利用 估计 出来 的 环境 变动 补偿 线上 的 DQN       用   $ ( \ \ tile ( r ) _ t   =   r _ t   -   r _ b ) $   代替   $ ( r _ t ) $       r _ b   估计 方法 :   随机 选择 一些 用户 , 利用 离线 训练 好 的 模型 进行 决策 , 然后 实时 统计 这些 用户 的 平均 reward 作为   r _ b               应用 案例 :   Tip   推荐       Q ( S ,   a )   将 动作 ( 20000 种 Tip , 对应 动作 空间 20000 ) embedding 后 , 作为 神经网络 的 输入       $ ( r   =   r _ 1   +   \ \ alpha   r _ 2   +   \ \ beta   r _ 3 ) $         第一 部分 回报 是 点击 回报 $ ( r _ 1   =   I   *   ( 1   +   \ \ rho   *   e ^ { - x } ) $   I 代表 是否 点击 , x 是 当前 tip 展示 的 页面 数量       第二 部分 回报 是 描述 用户 点击 tip 的 偏好 ,   $ ( r _ 2   =   I   *   e ^ { - y } ) $   y 是 用户 最近 100 次 PV 中 对 tip 的 点击 次数       第三 部分 回报 是 用户 交易   $ ( r _ 3   =   c ) $   c 代表 是否 交易                      ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl2018-paper-reading.html"},
      
      
      {"title": "强化学习简介", "text": "    Table   of   Contents           关于           动态 规划 ( DP ) 方法           蒙特卡洛 ( MC ) 方法           Importance   Sampling                   时间 差分 ( TD ) 方法           SARSA 方法           Q - learning           期望 SARSA 方法           Double   Q - learning                   n - step   TD 方法           TD ( $ ( \ \ lambda ) $ ) 方法           策略 梯度 理论                 关于       Reinforcement   Learning :   An   introduction   读书笔记       动态 规划 ( DP ) 方法       当 环境 已知 时 ， 即 状态 转移 概率 $ ( P ( s ' | s ,   a ) ) $ 和 回报 $ ( r ( s ,   a ) ) $ 也 知道 的 情况 下 ， 根据 HJB 方程       $ $     V ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ( s ' ) ]     $ $       因为 求 最大值 操作 的 存在 ， 上述 方程 是 值 函数 V 的 非线性 方程 ， 所以 无法 直接 求解 。     动态 规划 方法 求解 值 函数 方法 有 两种 ： 值 迭代 与 策略 迭代 。       采用 值 迭代 的 理论依据 是非 线性方程 的 迭代 求解 方法 ， 从 HJB 方程 来看 ， 值 函数 V 可以 看做 右边 非线性 算子 的 不动点 ， 容易 验证 当 $ ( \ \ gamma & lt ; 1 ) $ 时 ， 该 非线性 算子 是 压缩 映象 ， 值 迭代 比 收敛 于 不动点 ！ 它 也 可以 看做 往前 一步 的 期望值 作为 目标 ， 进行 迭代 。       $ $     V _ { k + 1 } ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V _ k ( s ' ) ]     $ $       另 一种 方法 是 交替 优化 策略 和 值 函数 ， 好处 是 在 策略 固定 时 ， HJB 方程 没有 max 操作 ， 是 线性方程 ！     策略 迭代 分为 两步         策略 评估   ： 将 选择 动作 的 策略 固定 ， 求解 策略 的 值 函数 ， 因为 策略 固定 ， 非线性 的 HJB 方程 变成 线性方程 了 ！ 所以 可以 采用 线性方程 的 所有 求解 方法 进行 求解 ， 比如 高斯消 元法 、 雅克 比 迭代法 等等 。       $ $     V ^ { \ \ pi } ( s )   =   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ^ { \ \ pi } ( s ' ) ] ,   a = \ \ pi ( a | s )     $ $         策略 提升   ， 对于 上述 评估 出来 的 值 函数 ， 提升 策略       $ $     \ \ pi ' ( s )   =   \ \ arg \ \ max _ a   Q ^ { \ \ pi } ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   r ( s ,   a )   +   \ \ gamma   V ^ { \ \ pi } ( s ' )     $ $       策略 提升 可以 保证 值 函数 序列 是 单调 递增 序列 ！       蒙特卡洛 ( MC ) 方法       当 环境 未知 的 时候 ， 无法 采用 动态 规划 方法 求解 ， 需要 根据 经验 数据 进行 评估 。     蒙特卡洛 法 通过 在线 学习 的 方法 ， 将 整个 动作 序列 执行 至 终态 ， 根据 实际 获得 的 总 回报 $ ( G   =   \ \ sum _ { t = 0 } ^ T   \ \ gamma ^ t   r ( s _ t ,   a _ t ) ) $ 来 进行 策略 评估 ！ 当 执行 多次 之后 ， $ ( G ) $ 的 平均值 可以 作为 该 策略 下 ， 初始状态 s 的 值 函数 。 这个 过程 也 可以 用 迭代 的 方法 描述       $ $     Q ^ { \ \ pi } ( s ,   a )   \ \ leftarrow   Q ^ { \ \ pi } ( s ,   a )   -   \ \ alpha [ Q ^ { \ \ pi } ( s ,   a )   -   G ]   \ \ \ \     \ \ alpha   =   1 / N     $ $       该 迭代 过程 可以 看做 用 观测 到 的 回报 G 作为 学习 目标 的 随机 梯度 下降       $ $     J ( Q ^ { \ \ pi } )   =   \ \ frac { 1 } { 2 } \ \ sum _ i   ( Q ^ { \ \ pi } ( s ,   a )   -   G _ i ) ^ 2     $ $       蒙特卡洛 法 的 特点 ， 必须 等到 动作 序列 执行 完毕 后 ， 才能 进行 评估 。     但是 一个 序列 可以 更新 多个 状态 的 值 ， 利用 马尔科夫 链 的 性质 ， 从 这 条链 中间 任何 一个 状态 开始 ， 都 可以 得到 该 状态 的 一个 值 函数 的 采样 值 ！       为了 有效 地 估计 出 Q 函数 ， 对 每个 状态 - 动作 对 都 需要 产生 多个 样本 ， 因此 初始状态 需要 随机 从 可能 的 状态 - 动作 对 中 随机 选择 ！               由于 所有 的 初始状态 - 动作 都 是 随机 的 ， 所以 初始 动作 比较 无效 ！ 但是 又 不能 按照 当前 的 策略 选择 初始 动作 ， 那样 将会 导致 很多 状态 - 动作 对 没有 样本 ！       为了 解决 这个 问题 ， 可以 限制 $ ( \ \ pi ( a | s )   & lt ;   1   -   \ \ epsilon   +   \ \ epsilon / | A ( s ) | ) $ ， 也就是说 不让 策略 只 选择 最优 的 动作 ， 还 以 一定 的 概率 选择 其他 动作 ， 这样 初始 动作 的 选择 也 可以 采用 当前 最优 策略 来选 了 ！ 但是 ， 这样一来 ， 收敛 后 的 策略 并 不是 最优 策略 了 ， 只是 近 最优 策略 ！       Importance   Sampling       另外 一种 解决方案 是 利用 采样 ( Importance   Sampling ) 实现 off - policy ， 也 就是 评估 的 策略 不是 线上 运行 的 策略 ！ 前面 的 方法 评估 的 策略 就是 线上 运行 的 策略 ， 叫做 on - policy 方法 。       假设 评估 的 策略 是 $ ( \ \ pi ( a | s ) ) $ ， 而线 上 运行 的 策略 是 $ ( b ( a | s ) ) $ ， 那么 对于 动作 - 状态 轨迹 $ ( \ \ tau   =   ( A _ 1 ,   S _ 2 ,   A _ 2 ,   ... ,   S _ T ) ) $ ， 两种 策略 产生 该 轨迹 的 概率 之比为       $ $     \ \ rho _ { t : T - 1 }   =   \ \ Pi _ t ^ { T - 1 }   \ \ frac { \ \ pi ( A _ t | S _ t ) } { b ( A _ t | S _ t ) }     $ $       那么 ， 根据 用 策略 $ ( b ) $ 得到 的 经验 数据 ， 可以 估计 在 策略 $ ( \ \ pi ) $ 下 的 值 函数       $ $     V ( s )   =   \ \ frac { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 }   G _ t } { | B ( s ) | }   \ \ \ \     V ( s )   =   \ \ frac { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 }   G _ t } { \ \ sum _ { t   \ \ in   B ( s ) } \ \ rho _ { t : T ( t ) - 1 } }     $ $       $ ( B ( s ) ) $ 是 状态 s 所处 的 时间 集合 。 前 一种 估计 无偏 但是 高 方差 ， 后 一种 有 偏 但是 低 方差 ， 但是 偏差 会 随着 样本数 增加 而 趋近 于 0 ！ 推荐 后 一种 估计 。       时间 差分 ( TD ) 方法       蒙特卡洛 方法 需要 策略 执行 到 终止 状态 才能 评估 策略 ， TD 方法 只 需要 1 步 ！ 核心思想 在于 自助 法 ， 它 对值 函数 的 估计 是       $ $     G _ t   =   R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } )     $ $       即用 原来 的 值 函数 取代 了 后面 所有 的 回报 ！ 并 采用 常数 学习 率 ，       $ $     V ( S _ t )   \ \ leftarrow   V ( S _ t )   +   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } )   -   V ( S _ t ) ]     $ $       上述 迭代 可以 看做 最小化   $ ( 1 / 2 | | G _ t   -   V ^   *   ( S _ t ) | | ^ 2 ) $   进行 随机 梯度 下降 ！     误差 项 $ ( \ \ delta _ t   =   G _ t   -   V ( S _ t ) ) $ 称作   TD   error 。       SARSA 方法       Sarsa 方法 就是 用 TD 方法 对 动作 值 函数 Q ( s , a ) 进行 学习 ！       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   Q ( S _ { t + 1 } ,   A _ { t + 1 } )   -   Q ( S _ t ,   A _ t ) ]     $ $       可以 看到 ， Sarsa 方法 用到 了 两个 状态 和 两个 动作 ， 这 两个 动作 都 是 采样 自 Q 函数 策略 。 如果 动作 选取 有 探索 ， 那么 Q 就 不是 最优 解 ， 一般 可以 随着 学习 的 推进 ， 不断 减小 探索 到 某个 很小 的 值 ， 可以 得到 近 最优 解 。       SARSA 可以 看做 用   $ ( Q ( S _ { t + 1 } ,   A _ { t + 1 } )   ) $   来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ ！       $ $     Q ^   *   ( S _ t ,   A _ t )   =   R _ { t + 1 }   +   \ \ gamma   V ^   *   ( S _ { t + 1 } )   \ \ approx   R _ { t + 1 }   +   \ \ gamma   Q ( S _ { t + 1 } ,   A _ { t + 1 } )     $ $               Q - learning       也 是 一种 TD 学习 方法 ， 而且 是 off - policy ， 与 SARSA 不 一样 的 是 ， 它 只用 到 了 一个 初始 动作 ， 不 需要 根据 Q 函数 策略 采样 的 第二个 动作 ， 这是 最大 的 区别 ！ 也 是 Q - learning 可以 用 off - policy 学到 最优 策略 的 关键 ！       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   \ \ max _ a   Q ( S _ { t + 1 } ,   a )   -   Q ( S _ t ,   A _ t ) ]     $ $       Q - learning 可以 看做 用   $ ( \ \ max _ a   Q ( S _ { t + 1 } ,   a )   ) $   来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ ！       $ $     Q ^   *   ( S _ t ,   A _ t )   =   R _ { t + 1 }   +   \ \ gamma   V ^   *   ( S _ { t + 1 } )   \ \ approx   R _ { t + 1 }   +   \ \ gamma   \ \ max _ a   Q ( S _ { t + 1 } ,   a )     $ $       期望 SARSA 方法       SAESA 方法 用 当前 策略 的 采样 动作 $ ( A _ { t + 1 } ) $ 的 动作 值 函数 近似 $ ( V ^   *   ( s ' ) ) $ ， 因此 是 有 偏 而且 方差 很大 ！     Q - learning 则 用 $ ( \ \ max _ a   Q ( s ' ,   a ) ) $ 近似 $ ( V ^   *   ( s ' ) ) $ ， 也 是 有 偏 的 方差 也 大 ， 而且 因为 取 max 导致 有 过高估计 的 问题 。     期望 SARSA 则 用 当前 Q 函数 在 当前 策略 下 的 期望值   $ ( \ \ sum _ { a }   \ \ pi ( a | s ' )   Q ( s ' ,   a ) ) $   近似 $ ( V ^   *   ( s ' ) ) $ ， 因为 用到 期望值 ， 所以 可以 减少 方差 ！ 迭代 方程 是       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   \ \ sum _ a   \ \ pi ( a | S _ { t + 1 } )   Q ( S _ { t + 1 } ,   a )   -   Q ( S _ t ,   A _ t ) ]     $ $       Double   Q - learning       解决 求 max 操作 的 过高估计 问题 ， 用 两个 Q 函数 ， 交替 学习 ， 每次 学习 时 ， 用 Q2 来 估计 $ ( V ^   *   ( S _ { t + 1 } ) ) $ 作为 目标 ， 对 Q1 进行 梯度 下降       $ $     Q _ 1 ( S _ t ,   A _ t )   \ \ leftarrow   Q _ 1 ( S _ t ,   A _ t )   -   \ \ alpha [ R _ { t + 1 }   +   \ \ gamma   Q _ 2 ( S _ { t + 1 } ,   a )   -   Q _ 1 ( S _ t ,   A _ t ) ]   \ \ \ \     a   =   \ \ arg \ \ max _ a   Q _ 1 ( S _ { t + 1 } ,   a )     $ $       n - step   TD 方法       在 单步 TD 方法 中 ， 指向 前 看 了 一步 就 得出 $ ( V ^ { \ \ pi }   ( S _ t ) ) $ 的 估计值 ， 实际上 也 可以 看多步 ， 这样 可以 估计 更加 准确       $ $     G _ { t : t + n }   =   R _ { t + 1 }   +   ...   +   \ \ gamma ^ { n - 1 }   R _ { t + n }   +   \ \ gamma ^ n   V ( S _ { t + n } )     $ $       可以 看出 单步 TD 方法 就是 n = 1 的 情况 ， 而 蒙特卡洛 方法 可以 看做   $ ( n   \ \ rightarrow   \ \ infty ) $   的 极限 ！     n - step   TD 估计 偏差 来自 于 最后 一项 ， 用 当前 的 值 函数 估计 最优 值 函数 ， 因为 前面 的 系数 是 $ ( \ \ gamma ^ n ) $ 是 指数 衰减 的 ， 所以 n 步 TD 方法 的 偏差 随 指数 衰减 ！       n - step   TD 方法 迭代 方程 是       $ $     V ( S _ t )   \ \ leftarrow   V ( S _ t )   +   \ \ alpha [ G _ { t : t + n }   -   V ( S _ t ) ]     $ $                 SARSA   ： 将 SARSA 对 Q 的 估计 用 n 步 回报 替换 就 可以 得到 n 步 SARSA 方法       $ $     Q ( S _ t ,   A _ t )   \ \ leftarrow   Q ( S _ t ,   A _ t )   -   \ \ alpha \ \ left (   [ R _ { t + 1 }   +   ...   +   \ \ gamma ^ { n - 1 } R _ { t + n }   +   \ \ gamma ^ n   Q ( S _ { t + n } ,   A _ { t + n } ) ]   -   Q ( S _ t ,   A _ t )   \ \ right )     $ $         Q - learning   ： 同理 将 Q - learning 对 Q 的 估计 用 n 步 回报 替换 就 可以 得到 n 步 Q - learning 方法 ！       TD ( $ ( \ \ lambda ) $ ) 方法       基本 出发点 ： 将 n 步 回报 加权 平均 作为 对 回报 的 估计 ， 距 现在 越久 的 权重 越小 。       $ $     G _ t ^ { \ \ lambda }   =   ( 1 - \ \ lambda ) \ \ sum _ { n = 1 } ^ { \ \ infty }   \ \ lambda ^ { n - 1 }   G _ { t : t + n }     $ $       可以 看出 当 $ ( \ \ lambda   =   0 ) $ 时 ， 就是 单步 TD 方法 ， 当 $ ( \ \ lambda   =   1 ) $ 时 ， 就是 蒙特卡洛 方法 。       后 向 更新 算法 ， 迭代 地 利用   TD   error 进行 更新       $ $     Z _ t ( s )   =   \ \ begin { cases }             \ \ gamma   \ \ lambda   Z _ { t - 1 } ( s ) ,   \ \ quad   s   \ \ neq   S _ t   \ \ \ \             \ \ gamma   \ \ lambda   Z _ { t - 1 } ( s )   +   1 ,   \ \ quad   s   =   S _ t             \ \ end { cases }   \ \ \ \     \ \ delta _ t   =   R _ { t + 2 }   +   \ \ gamma   V _ t ( S _ { t + 1 } )   -   V _ t ( S _ t )   \ \ \ \     V _ t ( S _ t )   \ \ leftarrow   V _ { t - 1 } ( S _ t )   +   \ \ alpha   \ \ delta _ t   Z _ t ( S _ t )     $ $       策略 梯度 理论       直接 建模 策略 函数       $ $     \ \ nabla   J ( \ \ theta )   =   \ \ sum _ s   \ \ mu _ { \ \ pi } ( s )   \ \ sum _ a   q _ { \ \ pi } ( s ,   a )   \ \ nabla _ { \ \ theta }   \ \ pi ( a | s , \ \ theta )   =   E _ { \ \ tau   \ \ sim   \ \ pi _ { \ \ theta } }   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tau )   r ( \ \ tau )     $ $       两种 表述 ， 前面 一个 等式 是 状态 - 动作 表述 ， 后 一个 等式 是 状态 - 动作 序列 表述 。 策略 梯度 法 实际上 相当于 用 回报 作为 样本 权重 的 极大 似然 估计 。       REINFORCE ：   蒙特卡罗 策略 梯度         $ $     \ \ nabla   J ( \ \ theta )   =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t \ \ sum _ a     q _ { \ \ pi } ( S _ t ,   a )   \ \ nabla _ { \ \ theta }   \ \ pi ( a | S _ t , \ \ theta )         \ \ right ]     \ \ quad   \ \ text { ( 对 状态 采样 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   q _ { \ \ pi } ( S _ t ,   A _ t )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )         \ \ right ]             \ \ quad   \ \ text { ( 对 动作 采样 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   G _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )         \ \ right ]             \ \ quad   \ \ text { ( 用 采样 的 回报 替换 q 函数 ) } \ \ \ \     =   E _ { \ \ pi } \ \ left [       \ \ gamma ^ t   ( G _ t   -   V ( S _ t ) )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )   \ \ right ]     $ $       等 二个 等式 ， 用到 了 关系       $ $     Eq _ { \ \ pi } ( S _ t ,   A _ t ) \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )     =   \ \ sum _ a     \ \ pi ( a | S _ t ) q _ { \ \ pi } ( S _ t ,   a ) \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta )     $ $       最后 一个 等式 是因为 被 剪掉 的 部分 期望值 为 0 .       因此 ， 可以 利用 单个 样本 $ ( ( S _ t ,   A _ t ) ) $ 估计 策略 梯度 ， 得到 策略 梯度 随机 梯度 上升 迭代 公式       $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ { t }   +   \ \ alpha   \ \ gamma ^ t   G _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta _ t )     $ $       这里 的 $ ( G _ t ) $ 采用 的 是 单条 链 的 最终 回报 ， 所以 是 用 蒙特卡罗 法 估计 策略 梯度 ！       用 $ ( \ \ delta _ t   =   G _ t   -   V ( S _ t ;   w ) ) $ 替换 $ ( G _ t ) $ 可以 减小 梯度 估计 的 方差 ， 并且 使用 TD 方法 估计 $ ( G _ t   =   R _ { t + 1 }   +   \ \ gamma   V ( S _ { t + 1 } ;   w ) ) $ 。 这 就是     Actor   Critic     算法 ：       $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ { t }   +   \ \ alpha   \ \ gamma ^ t   \ \ delta _ t   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi ( A _ t | S _ t , \ \ theta _ t )     $ $          ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl-intro-book.html"},
      
      
      {"title": "强化学习算法实现(I)", "text": "    Table   of   Contents           环境 构建           值 迭代           策略 迭代           蒙特卡罗 法           Q - Learning                 环境 构建       MDP 的 环境 是 指 转移 概率 $ ( P ( s ' |   s ,   a ) ) $ 和 回报 函数 $ ( r ( s ,   a ) ) $ ！     在 环境 已知 的 动态 规划 算法 中 ， 用于 迭代 算法 当中 。 在 环境 位置 的 TD 方法 中 用于 模拟 。       构建 一个 简单 的 环境 ， 有 nS 个 状态 ， 0 ， 1 ， ... ， nS - 1 ； 其中 nS - 1 是 终止 状态 。     该 环境 下 一共 两个 动作 ： 0 向 左 运动 ， 1 向 右 运动 ， 每个 动作 都 有 概率 p0 不动 ， p1 的 概率 会往 反 方向 运动 ,   1 - p0 - p1 概率 正常 运动 。                   0       1       2       ...       9                       ← · →       ← · →       ← · →       ← · →       终点                           import       numpy       as       np         nS       =       10       nA       =       2         # 不要 改 这个 参数       Done       =       nS       -       1       p0       =       0.1       p1       =       0.1       P       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       #   转移 概率       R       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       -       1.0       #   回报 都 是 - 1       gamma       =       1         for       s       in       range     (     nS     ) :               if       s       = =       Done     :       #   终止 态 转移 概率 都 为 0                       continue               for       a       in       range     (     nA     ) :                       inc       =       a       *       2       -       1       #   步长                                       P     [     s     ,       a     ,       s     ]       + =       p0       #   不 动                       P     [     s     ,       a     ,       max     (     0     ,       s       -       inc     ) ]       + =       p1       #   反 方向                       P     [     s     ,       a     ,       max     (     0     ,       s       +       inc     ) ]       + =       1       -       p0       -       p1       #   正常 运动                 值 迭代       迭代法 求解 非线性 方程 ， 值 迭代 迭代 方程       $ $     V ( s )   =   \ \ max _ a   \ \ sum _ { s ' } P ( s ' | s ,   a )   [ r ( s ,   a )   +   \ \ gamma   V ( s ' ) ]     $ $               V       =       np     .     zeros     (     nS     )       for       it       in       range     (     1000     ) :               converage       =       True               for       s       in       range     (     nS     ) :                       if       s       = =       Done     :         #   终止 状态 不 迭代                               V     [     s     ]       =       0                               continue                         #   HJB   非线性 方程                       maxV       =       max     (     sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     )                                                       )       for       a       in       range     (     nA     )                                             )                         if       V     [     s     ]       ! =       maxV     :       #   测试 值 迭代 是否 收敛                               converage       =       False                               V     [     s     ]       =       maxV               if       converage     :                       break         print       &# 39 ; iteral   steps : &# 39 ;     ,       it       print       V         & gt ; & gt ; & gt ;       iteral       steps     :       57       [     -     12.65306123       -     11.40306123         -     9.99681123         -     8.57102998         -     7.14280732           -     5.71427949         -     4.28571351         -     2.85714276         -     1.42857142           0 .                     ]                 策略 迭代           策略 迭代 分 两步       第一步 固定 策略 ， 求解 值 函数 ， 叫 策略 评估 ， 因为 策略 固定 了 ， HJB 方程 由 非线性 方程 变成 线性方程 ， 可以 采用 迭代 解法 或者 高斯消 元法 求解 ；       第二步 叫 策略 提升 ， 对值 函数 构造 新 的 更优 策略 ！                           pi       =       np     .     zeros     (     nS     ,       dtype     =     int     )       # 初始 策略 全部 往 左         for       it       in       range     (     100     ) :               V       =       np     .     zeros     (     nS     )               #   策略 评估 ， 解 线性方程 ， 雅克 比 迭代法               for       _       in       range     (     100     ) :                       converage       =       True                       for       s       in       range     (     nS     ) :                               if       s       = =       Done     :                                       V     [     s     ]       =       0                                       continue                                 #   HJB   线性方程                               a       =       pi     [     s     ]                               v       =       sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     ) )                                 if       V     [     s     ]       ! =       v     :                                       converage       =       False                                       V     [     s     ]       =       v                       if       converage     :                               break                 #   策略 提升               converage       =       True               for       s       in       range     (     nS     ) :                       maxA       =       np     .     argmax     ( [     sum     (     P     [     s     ,       a     ,       ss     ]     *     (     R     [     s     ,       a     ,       ss     ]       +       gamma       *       V     [     ss     ] )       for       ss       in       range     (     nS     ) )       for       a       in       range     (     nA     ) ] )                       if       maxA       ! =       pi     [     s     ] :                               converage       =       False                               pi     [     s     ]       =       maxA               print       &# 39 ; iter &# 39 ;     ,     it     ,       &# 39 ; pi   = &# 39 ;     ,     pi                 if       converage     :                       break         print       &# 39 ; pi   = &# 39 ;     ,       pi       print       &# 39 ; V   = &# 39 ;     ,       V         & gt ; & gt ; & gt ;       iter       0       pi       =       [     0       0       0       0       0       0       0       1       1       0     ]       iter       1       pi       =       [     0       0       0       0       0       1       1       1       1       0     ]       iter       2       pi       =       [     0       0       0       1       1       1       1       1       1       0     ]       iter       3       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       4       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       pi       =       [     1       1       1       1       1       1       1       1       1       0     ]       V       =       [     -     12.65306123       -     11.40306123         -     9.99681123         -     8.57102998         -     7.14280732           -     5.71427949         -     4.28571351         -     2.85714276         -     1.42857142           0 .                     ]                 蒙特卡罗 法       当 环境 未知 时 ， 即 转移 概率 未知 ， 无法 利用 HJB 方程 求解 值 函数 。 蒙特卡罗 法 通过 运行 到 结束 得到 回报 ， 去 更新 动作 值 函数 — — Q 函数 ！               Q       =       np     .     zeros     ( (     nS     ,       nA     ) )       pi       =       np     .     random     .     randint     (     0     ,       nA     ,       nS     )         def       go _ next     (     s     ,       a     ) :               r       =       np     .     random     .     rand     ( )               i       =       0               p       =       0               while       True     :                       if       r       & lt ;       p       +       P     [     s     ,       a     ,       i     ] :                               return       i                       p       + =       P     [     s     ,       a     ,       i     ]                       i       + =       1               return       len     (     P     [     s     ,       a     ] )         alpha       =       0.01       for       it       in       range     (     1000     ) :               if       it       %       50       = =       0     :                       print       &# 39 ; iter &# 39 ;     ,       it     ,       &# 39 ; pi = &# 39 ;     ,       pi                 #   策略 评估 ： 根据 目前 策略 仿真 一条 状态 - 动作 路径 ， 更新 Q 函数               for       s       in       range     (     nS     ) :                       for       a       in       range     (     nA     ) :                               #   仿真 一条 状态 - 动作 路径                               history       =       [ ]                               ss       =       s                               while       ss       ! =       Done     :                                       ss _ next       =       go _ next     (     ss     ,       a     )                                       history     .     append     ( (     ss     ,       a     ,       R     [     ss     ,     a     ,     ss _ next     ] ,       ss _ next     ) )                                       ss       =       ss _ next                                       a       =       pi     [     ss     ]       # 更新 动作                                       # print   ss                                 Gt       =       0                               #   对 出现 的 所有 ( s ,   a ) 对 更新 Q 函数 ， 复用 这 条 路径                               for       i       in       reversed     (     range     (     len     (     history     ) ) ) :                                       ss     ,       aa     ,       rr     ,       _       =       history     [     i     ]                                       Gt       =       gamma       *       Gt       +       rr                                       Q     [     ss     ,       aa     ]       + =       alpha       *       (     Gt       -       Q     [     ss     ,       aa     ] )                 #   策略 提升 ： 根据 更新 后 的 Q 函数 ， 更新 策略               for       s       in       range     (     nS     ) :                       pi     [     s     ]       =       np     .     argmax     (     Q     [     s     ,       : ] )       print       &# 39 ; V = &# 39 ;     ,       np     .     max     (     Q     ,       axis     =     1     )       print       &# 39 ; pi = &# 39 ;     ,       pi         & gt ; & gt ; & gt ;       iter       0       pi     =       [     1       0       1       0       0       1       1       0       1       0     ]       iter       50       pi     =       [     0       1       0       1       0       1       1       0       1       0     ]       iter       100       pi     =       [     0       1       0       1       0       1       1       0       1       0     ]       iter       150       pi     =       [     1       0       1       1       0       1       1       0       1       0     ]       iter       200       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       250       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       300       pi     =       [     1       0       1       1       0       1       1       1       1       0     ]       iter       350       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       400       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       450       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       500       pi     =       [     0       1       1       1       1       1       1       1       1       0     ]       iter       550       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       600       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       650       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       700       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       750       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       800       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       850       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       900       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       iter       950       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]       V     =       [     -     12.60595021       -     11.20499113         -     9.82580041         -     8.41347716         -     6.9761059           -     5.5099145           -     4.19408818         -     2.89429492         -     1.45856045           0 .                     ]       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]                 Q - Learning       Q - learning 是 TD 方法 的 一种 ， 也 是 用 得 最 多 的 一种 ， 因为 他 可以 很 方便 off - policy 学习 ， 不 需用 借助 重要性 采样 。     相比 蒙特卡罗 法 可以 不 需要 等到 运行 结束 就 可以 更新 Q 函数 。               Q       =       np     .     zeros     ( (     nS     ,       nA     ) )       pi       =       np     .     random     .     randint     (     0     ,       nA     ,       nS     )         def       go _ next     (     s     ,       a     ) :               r       =       np     .     random     .     rand     ( )               i       =       0               p       =       0               while       True     :                       if       r       & lt ;       p       +       P     [     s     ,       a     ,       i     ] :                               return       i                       p       + =       P     [     s     ,       a     ,       i     ]                       i       + =       1               return       len     (     P     [     s     ,       a     ] )         alpha       =       0.01       epsilon       =       0.9       #   探索       for       it       in       range     (     100     ) :               if       it       %       10       = =       0     :                       print       &# 39 ; iter &# 39 ;     ,       it     ,       &# 39 ; epsilon = &# 39 ;     ,       epsilon     ,       &# 39 ; V [ 0 ] = &# 39 ;     ,       max     (     Q     [     0     ] )                 #   根据 目前 策略 仿真 一条 状态 - 动作 路径 ， 并 同时 更新 Q 函数               for       s       in       range     (     nS     ) :                       if       s       = =       Done     :                               continue                       for       a       in       range     (     nA     ) :                               #   仿真 一条 状态 - 动作 路径                               ss       =       s                               while       ss       ! =       Done     :                                       ss _ next       =       go _ next     (     ss     ,       a     )                                       Gt       =       R     [     ss     ,     a     ,     ss _ next     ]       +       gamma       *       max     (     Q     [     ss _ next     ,       : ] )                                       Q     [     ss     ,       a     ]       + =       alpha       *       (     Gt       -       Q     [     ss     ,       a     ] )         #   Q - learning   迭代 步骤                                       ss       =       ss _ next                                         a       =       pi     [     ss     ]       # 选择 动作                                       if       np     .     random     .     rand     ( )       & lt ;       epsilon     :       #   探索                                               a       =       np     .     random     .     randint     (     0     ,       nA     )                                       # print   ss                 epsilon       =       max     (     0.01     ,       epsilon       *     0.99     )         for       s       in       range     (     nS     ) :               pi     [     s     ]       =       np     .     argmax     (     Q     [     s     ,       : ] )         print       &# 39 ; V = &# 39 ;     ,       np     .     max     (     Q     ,       axis     =     1     )       print       &# 39 ; pi = &# 39 ;     ,       pi         & gt ; & gt ; & gt ;       iter       0       epsilon     =       0.9       V     [     0     ]     =       0.0       iter       10       epsilon     =       0.813943867508       V     [     0     ]     =       -     5.50430863461       iter       20       epsilon     =       0.736116243838       V     [     0     ]     =       -     9.58192560014       iter       30       epsilon     =       0.665730336049       V     [     0     ]     =       -     11.372140159       iter       40       epsilon     =       0.602074582713       V     [     0     ]     =       -     12.0806787351       iter       50       epsilon     =       0.544505460424       V     [     0     ]     =       -     12.3537691746       iter       60       epsilon     =       0.492440978152       V     [     0     ]     =       -     12.5909546081       iter       70       epsilon     =       0.44535479364       V     [     0     ]     =       -     12.657326121       iter       80       epsilon     =       0.402770892387       V     [     0     ]     =       -     12.5917214982       iter       90       epsilon     =       0.36425877541       V     [     0     ]     =       -     12.7107205284       V     =       [     -     12.70433436       -     11.41177551         -     9.8564413           -     8.59335991         -     7.20906867           -     5.70400336         -     4.41177384         -     2.82674456         -     1.35446586           0 .                     ]       pi     =       [     1       1       1       1       1       1       1       1       1       0     ]        ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/rl-python-a.html"},
      
      
      {"title": "最优控制简介", "text": "    Table   of   Contents           关于           最优控制 问题           动态 规划                 关于       An   introduction   to   Mathematical   Optimal   Control   Theory   读书笔记       最优控制 问题       受控 微分 动力系统       $ $     \ \ begin { align }     \ \ dot { x } ( t )   & amp ; =   f ( x ( t ) ,   \ \ alpha ( t ) ) ,   t & gt ; 0   \ \ \ \     x ( 0 )   & amp ; =   x ^ 0     \ \ end { align }     $ $       Payoffs   函数       $ $     P [ \ \ alpha ( \ \ cdot ) ]   =   \ \ int _ 0 ^ T   r ( x ,   \ \ alpha )   dt   +   g ( x ( T ) )     $ $       g 是 终态 回报 ， 最优控制 就是 寻找 是 上式 最大 的 控制 函数 ！       当 f 是 线性 函数 时 ， 即 系统 是 线性 控制系统 ， 存在 bang - bang 控制 是 最优控制 ！ 即 $ ( | \ \ alpha |   =   1 ) $       最优控制 实际上 相当于 系统 模型 已知 的 强化 学习 问题 ！ 并且 是 确定性 系统 ， 该 系统 用 微分方程 描述 ！       动态 规划       值 函数 $ ( V ( x ,   t ) ) $       $ $     V ( x ,   t )   =   \ \ sup _ { \ \ alpha   \ \ in   A }   P _ { x ,   t }   =   \ \ int _ t ^ T   r ( x ,   \ \ alpha )   ds   +   g ( X ( T ) )     $ $       值 函数 满足 哈密顿 - 雅克 比 - 贝尔曼 方程 HJB       $ $     V _ t   +   \ \ max _ { a   \ \ in   A } {   f ( x ,   a )   \ \ cdot   \ \ nabla _ x   V   +   r ( s ,   a )     }   =   0     $ $       解释 ： 即 在 最优控制 下 ， 单位 时间 值 函数 的 减小 量 等于 回报 ！       动态 规划 求解 步骤 是 ， 先 根据 HJB 方程 求 出值 函数 ， 然后 根据 值 函数 设计 控制策略 ！  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/optimal-control.html"},
      
      
      {"title": "策略梯度理论", "text": "    Table   of   Contents           关于           主要 结论           策略 梯度 基本 理论           几个 概念           策略 梯度 公式                         关于       策略 梯度 理论 早期 经典 论文 笔记 ： Sutton   R   S ,   McAllester   D   A ,   Singh   S   P ,   et   al .   Policy   gradient   methods   for   reinforcement   learning   with   function   approximation [ C ] / / Advances   in   neural   information   processing   systems .   2000 :   1057 - 1063 .       其中   RS   Sutton   就是 《 Reinforcement   Learning   -   An   Introduction 》 的 作者       主要 结论           策略 函数 的 梯度 可以 用 经验 数据 中 估计 出来 的 动作 - 值 函数 ( 即 Q 函数 ) 或者 优势 函数 ( Advantage   function ， 即 A 函数 ) 近似 。       证明 了 策略 梯度 近似 可以 收敛 到 局部 最优 策略 ， 这 也 是 策略 梯度 相比 于值 函数 近似 方法 的 优势 ， 非线性 函数 的 值 函数 近似 方法 无法 保证 收敛 到 局部 最优 ！ （ 不过 线性 函数 还是 可以 的 ）           策略 梯度 基本 理论       用 $ ( \ \ rho ) $ 表示 策略 的 性能 测度 ， 即 期望 回报 ， 用 $ (   \ \ theta   ) $ 表示 策略 函数 的 参数 ， 即 $ (   \ \ pi   =   \ \ pi _ { \ \ theta } ( a |   s )       ) $ 。 那么 策略 梯度 为       $ $     \ \ Delta   \ \ theta   \ \ approx   \ \ alpha   \ \ frac { \ \ partial   \ \ rho } { \ \ partial   \ \ theta }     $ $       因此 ， 当 策略 梯度 趋于 0 时 ， 可以 保证 $ (   \ \ rho   ) $ 达到 极小值 ！       相比 值 函数 近似 方法 ， 策略 梯度 参数 的 很小 变动 只会 带来 很小 的 策略 上 的 改变 ， 而值 函数 近似 方法 可能 带 啦 跳跃式 的 改变 ！       几个 概念       策略 的 长期 期望 单步 回报       $ $     \ \ rho ( \ \ pi )   =   \ \ lim _ { n   \ \ rightarrow   \ \ infty }   \ \ frac { 1 } { n }   E [ r _ 1   +   r _ 2   +   ...   +   r _ n |   \ \ pi ]   =     \ \ sum _ s   d ^ { \ \ pi } ( s   ) \ \ sum _ a   \ \ pi ( s ,   a )   R ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   \ \ sum _ { kt1 } ^ { \ \ infty } E [     r _ t   - \ \ rho ( \ \ pi ) | s _ 0 = s ,   a _ 0 = a ,   \ \ pi ]     $ $           $ (   R ( s ,   a )   =   E [ r _ { t + 1 }   |   s _ t   =   s ,   a _ t   =   a ]   ) $ 是 状态 s 下 采取 动作 a 的 单步 期望 回报 。       $ (   d ^ { \ \ pi } ( s   )   =   \ \ lim _ { n   \ \ rightarrow   \ \ infty }   Pr ( s _ t   = s   | s _ 0 ,   \ \ pi )   ) $   是 在 该 策略 下 状态 s 的 稳态 分布 。           这里 应该 是 认为 这个 马尔科夫 链是 没有 终止 态 的 ， 所以 期望 回报 会 等于 稳态 分布 的 单步 平均值 。       策略 的 长期 折扣 回报       $ $     \ \ rho ( \ \ pi )   =   E [ \ \ sum _ { t = 1 } ^ { \ \ infty }     \ \ gamma ^ { t - 1 }   r _ t | s _ 0 ,   \ \ pi ]   =     \ \ sum _ s   d ^ { \ \ pi } ( s   ) \ \ sum _ a   \ \ pi ( s ,   a )   R ( s ,   a )   \ \ \ \     Q ^ { \ \ pi } ( s ,   a )   =   E [ \ \ sum _ { k = 1 } ^ { \ \ infty }     \ \ gamma ^ { k - 1 }   r _ { t + k } | s _ t = s ,   a _ t = a ,   \ \ pi ]     $ $           $ (   d ^ { \ \ pi } ( s   )   =   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   Pr ( s _ t   = s   | s _ 0 ,   \ \ pi )   ) $   是 在 该 策略 下 状态 s 的 累积 折扣 次数           这 一种 定义 是 比较 常用 的 一种 ， 马尔科夫 链 可以 有 终止 状态 。       策略 梯度 公式       $ $     \ \ frac { \ \ partial   \ \ rho } { \ \ partial   \ \ theta }   =   \ \ sum _ s   d ^ { \ \ pi } ( s )   \ \ sum _ a   \ \ frac { \ \ partial   \ \ pi ( s ,   a ) } { \ \ partial   \ \ theta }   Q ^ { \ \ pi } ( s ,   a )     $ $  ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/pg-function-approximation.html"},
      
      
      {"title": "阿里强化学习应用", "text": "    Table   of   Contents          ", "tags": "machine-learning/reinforcement-learning", "url": "/wiki/machine-learning/reinforcement-learning/ali-ebook.html"},
      
      
      
        
        
      
      {"title": "关于搜索栏目", "text": "    Table   of   Contents           关于                 关于           本 栏目 主要 收集 以下 资源       国际 会议 论文 笔记       工业界 各 公司 的 一些 方案 调研              ", "tags": "machine-learning/search", "url": "/wiki/machine-learning/search/readme.html"},
      
      
      
      {"title": "sklearn - python 机器学习库", "text": "    Table   of   Contents           预处理 模块           LabelBinarizer           LabelEncoder                   决策树           树 的 可视化                   特征 离散 化                 预处理 模块         LabelBinarizer         在 one - vs - all 里面 经常 用 ， 将 数值 或者 字符 类型 的 label ， 转换 为 一个 向量 ， 向量 每 一维 对应 其中 一个 label 值 ，     就 像 one - hot 编码 。               & gt ; & gt ; & gt ;       from       sklearn       import       preprocessing       & gt ; & gt ; & gt ;       lb       =       preprocessing     .     LabelBinarizer     ( )       & gt ; & gt ; & gt ;       lb     .     fit     ( [     1     ,       2     ,       6     ,       4     ,       2     ] )       LabelBinarizer     (     neg _ label     =     0     ,       pos _ label     =     1     ,       sparse _ output     =     False     )       & gt ; & gt ; & gt ;       lb     .     classes _       array     ( [     1     ,       2     ,       4     ,       6     ] )       & gt ; & gt ; & gt ;       lb     .     transform     ( [     1     ,       6     ] )       array     ( [ [     1     ,       0     ,       0     ,       0     ] ,                     [     0     ,       0     ,       0     ,       1     ] ] )                 属性   classes _   保存   fit   后 的 类 列表 ， 提供 正 变换   transform   ， 逆变换   inverse _ transform   。         LabelEncoder         用来 将 数值 类型 或者 字符串 类型 编码 为 数字   0   到   n _ classes - 1 。               & gt ; & gt ; & gt ;       from       sklearn       import       preprocessing       & gt ; & gt ; & gt ;       le       =       preprocessing     .     LabelEncoder     ( )       & gt ; & gt ; & gt ;       le     .     fit     ( [     1     ,       2     ,       2     ,       6     ] )       LabelEncoder     ( )       & gt ; & gt ; & gt ;       le     .     classes _       array     ( [     1     ,       2     ,       6     ] )       & gt ; & gt ; & gt ;       le     .     transform     ( [     1     ,       1     ,       2     ,       6     ] )       array     ( [     0     ,       0     ,       1     ,       2     ]     ...     )       & gt ; & gt ; & gt ;       le     .     inverse _ transform     ( [     0     ,       0     ,       1     ,       2     ] )       array     ( [     1     ,       1     ,       2     ,       6     ] )                 决策树       树 的 可视化       利用   tree . export _ graphviz   函数 ， 可以 将 树 导出 为 图 文件 。 借助于   IPython   可以 在 notebook 里面     将 树 可视化 显示 出来 。 需要 安装   graphviz   命令行 工具 ， 在 mac 里面 可以 通过 brew 安装 ， 命令 是   brew   install   graphviz   。     如果 你 在 终端 里面 输入   dot   ， 没有 提示 找 不到 命令 ， 那么 就 安装 好 了 。               from       IPython . display       import       Image       from       StringIO       import       StringIO       import       pydot       from       sklearn       import       tree         def       tree2png     (     clf     ,       cols     ) :               dot _ data       =       StringIO     ( )               tree     .     export _ graphviz     (     clf     ,       out _ file     =     dot _ data     ,       feature _ names     =     cols     ,     filled     =     True     ,       rounded     =     True     ,                                                                     special _ characters     =     True     )               g       =       pydot     .     graph _ from _ dot _ data     (     dot _ data     .     getvalue     ( ) ) [     0     ]               return       Image     (     g     .     create _ png     ( ) )                 特征 离散 化       从 scikit - learn   0.1 . 7 版本 开始 可以 通过 树 分类器 的   . apply       方法 ， 获取 输出 叶子 节点 的 序号 ， 然后 利用   onehot   转换器 转换 为     稀疏 的 离散 特征 。       利用 决策树 分类器 对 特征 进行 离散 化 的 例子               from       sklearn       import       tree       from       sklearn . tree       import       DecisionTreeClassifier         clf       =       DecisionTreeClassifier     (     min _ samples _ leaf     =     10     )       clf     .     fit     (     X _ train     ,       Y _ train     )           from       sklearn . preprocessing       import       OneHotEncoder       onehot       =       OneHotEncoder     ( )         ##   因为 决策树 apply 输出 是 一个 一位 数组 ， 所以 要 reshape       ##   如果 是 randomforest 或者 gbdt ， 那么 输出 的 是 一个二维 数组 ，       ##   每 一个 决策树 有 一个 输出 。 这里 都 是 说 二 分类器 。       leaf _ numbers       =       clf     .     apply     (     X _ train     )     .     reshape     ( (     -     1     ,     1     ) )       onehot     .     fit     (     leaf _ numbers     )       onehot _ features       =       onehot     .     transform     (     leaf _ numbers     )        ", "tags": "machine-learning", "url": "/wiki/machine-learning/sklearn.html"},
      
      
      {"title": "sklearn 源码阅读计划", "text": "    Table   of   Contents           关于           基础 类   base . py           线性 模型           base . py           逻辑 回归                   特征 选取           参考                 关于       sklearn 是 python 的 机器 学习 库 ， 来源于 Google   summer   的 一个 项目 ， 个人 认为 是 一个 设计 地 非常 好 的 软件 ，     对模型 的 抽象 很 不错 。 另一方面 ， 在 学习 理论 的 同时 ， 自己 写 代码 是 一个 加深 理解 的 好 方式 ， 而 阅读 源码 也 是     一种 很 好 的 方式 ， 而且 还 能 学习 别人 的 实现 和 优化 技巧 。       这个 源码 阅读 计划 可能 持续 较长 ， 内容 会 总结 在 wiki 中 ， 最后 形成 文章 在 我 的 博客 中 出现 。       基础 类   base . py       sklearn 使用 5 个 基础 类 ， 对模型 的 功能 进行 抽象 。 用户 可以 基于 这些 类 实现 自己 的 模型 ， 使得 可以 像 操作 sklearn     里面 的 其他 模型 那样 调用 。 基础 类 放在 包   sklearn . base   中           BaseEstimator ， 要求 构造方法 能够 设置 模型 参数 ， 并且 实现     get _ params     和     set _ params     两个 方法 。       ClassifierMixin ， 实现   score   方法 ， 返回   accuracy _ score ， 所有 的 分类器 的 Mixin 类       RegressorMixin ， 实现   score   方法 ， 返回   r2 _ score ， 所有 回归 器 的 Mixin 类       ClusterMixin ， 实现   fit _ predict   方法 ， 返回   聚类 标签 ， 所有 聚类器 的 Mixin 类       BiclusterMixin ， 两聚类 Mixin 类       TransformerMixin ， 实现   fit _ transform   方法 ， 转换器 Mixin 类       MetaEstimatorMixin ， MetaEstimator   Mixin 类           estimator 有个 属性   _ estimator _ type   ， 用来 表示 他 是 分类器 还是 回归 器 。 这是 通过 两个 Mixin 类来 实现 的 。       在   base   模块 中 ， 还 提供 一个   clone   函数 ， 用于 克隆 一个 现有 的 estimator 。       线性 模型         base . py                   LinearModel     线性 模型 的 基础 类           继承     BaseEstimator   ， 提供 抽象 方法     fit     需要 子类 实现 。       它 实现 了     predict     方法 ， 用来 预测 ， 该 方法 直接 输出     _ decision _ function     的 结果 ！       它 的 核心 就是 这个     _ decision _ function   ， 代码 很 简单     safe _ sparse _ dot ( X ,   self . coef _. T ,   dense _ output = True )   +   self . intercept _   。     就是 对 特征向量   X   做 一个 仿射变换     y   =   W   X   +   b   。 此外 ， 还有 一个     _ set _ intercept     用来 设置 偏置 $ ( b   =   \ \ bar { y }   -   w   \ \ bar { x } ) $ 。       在 实现 的 时候 ， 会 对 数据 做 中心化 处理 ， 有 一个 私有 的 中心化 函数     _ conter _ data   。                     LinearClassifierMixin     线性 分类器   Mixin ， 只 处理 预测           核心 的 函数 是     decision _ function     返回 样本 到 超平面 的 有 向 距离 ， 和 上面 的 函数 一样 （ ？ 为什么 分开 写 ） 。         predict     如果 是 两 分类 ， 返回     ( scores   & gt ;   0 ) . astype ( np . int )   ， 多 分类 则 返回 距离 最大 的 那个 index     scores . argmax ( axis = 1 )   ，     最后 会 将 结果 映射 会 label 标签 的 值 。         _ predict _ proba _ lr     LR 的 私有 方法 ， 输出 概率 ！ 代码 采用   inplaced   优化 空间 ， 可以 看   这个 代码                       SparseCoefMixin     稀疏 系数   Mixin   类 。 稀疏 稀疏 和 普通 系数 互相 转换 。 L1   正则 化 需要 继承 这个 类             densify     方法 将 稀疏 系数 转为 普通 向量 。         sparsify       转为 稀疏 系数                     LinearRegression     线性 回归 模型 ， 继承     LinearModel ,   RegressorMixin   。             residues _     采用     @ property     修饰 实现 只读 属性 ！ 0.19 要 废弃 这个 属性           fit     方法       是 核心 。 对于 稀疏 数据 ，                   逻辑 回归       特征 选取       参考             官方 文档           github 源码        ", "tags": "machine-learning", "url": "/wiki/machine-learning/sklearn-source.html"},
      
      
      {"title": "softmax近似", "text": "    Table   of   Contents           背景                 背景           问题 : 多 分类 如果 类别 太多要 对 softmax 近似           方法           importance   sampling :   sampled   softmax   [ 1 ]       noise   contrastive   estimation   [ 2 ]       经验 分布 采样                   Bengio ,   Yoshua   and   Senecal ,   Jean - Se   ́ bastien .   Adaptive   importance   sampling   to   accelerate   training   of   a   neu -   ral   probabilistic   language   model .   IEEE   Trans .   Neural   Networks ,   19 ( 4 ) : 713 – 722 ,   2008 .           Gutmann ,   Michael   and   Hyva   ̈ rinen ,   Aapo .   Noise -   contrastive   estimation   of   unnormalized   statistical   models ,   with   applications   to   natural   image   statistics .   Journal   of   Machine   Learning   Research ,   13 : 307 – 361 ,   2012 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/softmax-approx.html"},
      
      
        
        
      
      {"title": "升入理解TensorFlow", "text": "    Table   of   Contents           架构 与 设计           关键 模块 篇           数据处理 方法           TensorFlow 编程 框架           TensorBoard                   模型 托管   Tensorflow   Serving                 架构 与 设计           数据流 图 ,   前 向 图 和 后 向 图 ,   前 向 和 后 向 计算 图是 分开 的       节点 ,   操作   operator :   数学 运算       变量   Variable ,   有 内部 状态       占位 符   placeholder ,   用于 输入 数据       梯度 值       更新 参数 操作       更新 后 的 参数               有 向 边 :         数据 边 , 传输数据       控制 依赖 , 控制 执行 顺序               执行 原理 :       创建 散 列表 , 节点 名字 做 key , 入度 做 value       创建 可 执行 节点 队列 ,   将 入度 为 0 的 节点 放入 队列 并 从 散 列表 中 删除       依次 执行 队列 中 的 节点 ,   每次 执行 都 将 执行 节点 的 输出 指向 的 所有 节点 的 入度 减 1 ,   更新 到 散 列表 中       重复 2 - 3 直到 所有 节点 都 计算 完毕               数据 载体 :   张量       Tensor ,   张量 实现 的 时候 是 通过 句柄 的 方式 ,   实现 复用 ,   通过 引用 计数 实现 内存 释放 .       属性 :     name   ,     dtype   ,     graph   ,     op   ,     shape   ,     value _ index   ( 张量 在 该 前置 操作 中 所有 输出 值中 的 索引 )       方法 :     eval   ,     get _ shape   ,     set _ shape   ,     consumers   ( 张量 的 后置 操作 )         创建 ,     tf . constant     以及 操作     tf . add     都 可以 创建 张量       执行 ,     sess . run     和     tensorf . eval                 SparseTensor ,   稀疏 张量 ,   包含     indices     ( N ,   ndims ) ,     values     ( N ) ,     dense _ shape     ( ndims )   三个 属性       创建 ,     tf . SparseTensor ( indices = [ [ 1 ] , [ 3 ] , [ 8 ] ] ,   values = [ 1 , 1 , 1 ] ,   dense _ shape = [ 10 ] )         操作 ,                         模型 载体 :   操作       计算 节点       属性 :     name   ,     type   ,     inputs   ,     control _ inputs   ,     outpus   ,     device   ,     graph   ,     traceback   ( 调用 栈 )       典型 操作       算术 :     add   ,     multiply   ,     mod   ,     sqrt   ,     sin   ,     trace   ,     fft   ,     argmin     已及 numpy 类似 的 矩阵 操作       数组 :     size   ,     rank   ,     split   ,   reverse   ,     cast   ,     one _ hot   ,     quantize         梯度 裁剪 :     clip _ by _ value   ,     clip _ by _ norm   ,     clip _ by _ global _ norm         逻辑 控制 和 调试 :     identity   ,     logical _ and   ,     equal   ,     less   ,     is _ finite   ,     is _ nan         数据流 控制 :     enqueue   ,     dequeue   ,     size   ,     take _ grad   ,     apply _ grad         初始化 操作 :     zeros _ initializer   ,     random _ normal _ initializer   ,     orthogonal _ initializer         神经网络 操作 :     convolution   ,     pool   ,     dropout         随机 运算 :     random _ normal   ,     random _ shuffle         字符串 运算 :     string _ to _ hash _ bucket   ,     reduce _ join   ,     substr   ,     encode _ base64         图像处理 :     encode _ png   ,     resize _ images   ,     rot90   ,     hsv _ to _ rgb   ,     adjust _ gamma                         存储 节点 :   变量       变量 a 其实 由     ( a )   ,     Assign   ,     read   ,     initial _ value     四个 节点 组成 ,     tf . add ( a ,   b )   操作 实际上 读 的 是     read     子 节点 的 值 ,   而     tf . global _ variables _ initializer     实际上 是 将     initial _ value     传入     Assign     节点 , 实现 初始化 的       变量 操作 ,   支持 两种 初始化       用户 指定 的 初始值       VariableDef ,   用 protobuff 定义 的 变量 初始化 ,   用于 继续 训练 场景                       数据 节点 :   Placeholder         tf . placeholder     ,     tf . sparse _ placeholder                         运行 环境   :   会话       普通 会话 ,     sess   =   tf . Session ( ) ,   sess . run ,   sess . close         通过     tf . ConfigProto     设置 会话 配置 ,   包括 但 不 限于 ,   GPU 使用 ,   分布式 环境 RPC 地址   etc         with   语句 可以 不用 指定 session , 就 可以 直接 调用     eval     方法 计算 张量 的 值       交互式 会话     tf . InteractiveSession     会 默认 的 将     eval     的 会 话 设置 为 当前 会话         reset     方法 用于 会话 的 资源 释放               训练 工具 :   优化 器         _ use _ locking     在 并发 更新 的 时候 是否 加锁       子类 实现     _ apply _ dense     和     _ apply _ sparse     方法 ,   这 两个 方法 都 返回 数据流 图上 的 操作         minimize   方法 调用     compute _ gradients     和     apply _ gradients     方法         gate _ gradients     梯度 计算 的 异步 / 同步控制       更 自由 地 控制 优化 过程                           grads _ and _ vars       =       optimizer     .     compute _ gradients     (     loss     )       for       i     ,       (     g     ,       v     )       in       enumerate     (     grads _ and _ vars     ) :                 if       g       is       not       None     :                         grads _ and _ vars     [     i     ]       =       (     tf     .     clip _ by _ norm     (     g     ,       5     ) ,       v     )       #   裁剪 梯度       train _ op       =       optimizer     .     apply _ gradients     (     grads _ and _ vars     )                 关键 模块 篇       数据处理 方法           输入 流水线       创建 文件名 列表       python 列表   或者     tf . train . match _ filenames _ once                 创建 文件名 队列         tf . train . string _ input _ producer                 Reader   和   Decoder       CSV 文件 ,     tf . TextLineReader   ,     tf . decode _ csv         TFRecords   文件 ,     tf . TFRecordReader   ,     tf . parse _ single _ example         自由 格式 ,     tf . FixedLengthRecordReader   ,     tf . decode _ raw                 创建 样本 队列       使用     tf . train . start _ queue _ runners     启动 后台 线程 读取 队列       使用 后台 线程 协调 器     tf . train . Coordinator     管理 线程       创建 批量 数据     tf . train . shuffle _ batch                         模型 参数       参数 创建 、 初始化 、 更新       tf . Variable     实现       模型 文件 存储 和 恢复     tf . train . Saver     实现       初始化 方法     tf . global _ variables _ initilizer     和     tf . variables _ initilizer ( var _ list )   ,     var _ list     是 变量 集合 ,   创建 变量 时 , 可以 通过     collections     参数 指定 不同 的 集合 ,   默认 是     GraphKeys . GLOBAL _ VARIABLES   ,   即 全局变量 集合 ,   如果 显式 指定     trainable = True   , 那么 会加 到     TRAINABLE _ VARABLES     集合 ,   内置 5 类 变量 集合 ( 最新 版本 支持 更 多 了 )         GraphKeys . GLOBAL _ VARIABLES           GraphKeys . LOCAL _ VARIABLES           GraphKeys . TRAINABLE _ VARABLES           GraphKeys . MODEL _ VARIABLES           GraphKeys . MOVING _ AVERAGE _ VARIABLES           GraphKeys . REGULARIZATION _ LOSSES                 更新 模型 参数       赋值 ,     tf . assign   ,     tf . assign _ add   ,     tf . assign _ sub   ,   注意 等于号 没法 实现 变量 赋值 ,   它 只是 创建 了 一个 新 的 张量 ,   并 没有 改变 变量 的 值               使用     tf . train . Saver     保存 和 恢复 模型         saver   =   tf . train . Saver ( { ' w '   :   W } )     要 保存 的 模型 ,   通过 变量 名字 来 标示 和 恢复         saver . restore                 变量 作用域     tf . variable _ scope   ,     reuse     参数 表明 是否 可以 复用 ,     initializer   指定 该 变量 作用域 下 统一 的 初始化 方法               命令行 参数         argparse     和     tf . app . flags   ,   后者 的 好处 :   自动 生成 使用 方法 信息 ,   自动 生成 帮组 信息 ,   自动 生成 错误信息                   TensorFlow 编程 框架           显式 创建 数据流 图     tf . Graph ( )   ,     with   tf . Graph ( ) . as _ default ( )     语句 添加 作用域       ps - worker :   模型 分发 , 参数 更新 由 PS 实现 ;   模型 推断 和 梯度 计算 由 worker 实现       分布式 模型 脚本 需要 从 命令行 参数 获取 集群 配置 参数         tf . ClusterSpec   ,     tf . train . Server         同步 的 梯度 更新 比 异步 的 更 快 ?       同步 训练 :   梯度 聚合 器       异步 训练 :   靠 内部 的 锁 机制 实现       supervisor :   管理 模型 训练       定期 保存 模型 到 checkpoint       重启 的 时候 从 checkpoint 文件 恢复 , 继续 训练       异常 发生 时 , 清理 现场       执行 步骤       创建 Supervisor 实例 , 传入 checkpoint 文件 路径 和 日志 路径       获取 会话 实例   session       检查点 服务 ,   定期 保存       汇总 服务 ,   汇总 日志 ,   追加 到 logdir       步数 计数器               使用 会话 实例 执行 训练 ,   并 检查 停止 条件                           TensorBoard             tf . Summary           audio   ,     image   ,     scalar   ,     histogram   ,     merge _ all           FileWriter           add _ summary   ,     add _ event   ,     add _ graph                         名字 作用域 与 抽象 节点 ,   抽象 节点 是 一个 子图 ,   通过 name _ scope 可以 将 该 scope 下 的 所有 节点 自动 汇聚 到 一个 子图       汇总 数据   summary . proto       折线图     tf . summary . scalar ( name ,   tensor )   ,   汇总 操作 都 会 放到 KEY :     GraphKeys . SUMMARIES   下       分布图     tf . summary . histogram ( name ,   tensor )         图像     tf . summary . image ( name ,   tensor ,   max _ outputs )   ,   tensor   是 4 阶 张量   [ batch _ size ,   height ,   width ,   channels ] ,   channels   可取 1 ( 灰度 图 ) ,   3 ( 彩色 图 ) ,   4 ( 带 Alpha 通道 的 彩色 图 )       音频     tf . summary . image ( name ,   tensor ,   sample _ rate ,   max _ outputs )     tensor   是 3 阶 张量   [ batch _ size ,   frames ,   channels ]   或者 2 阶 张量   [ batch _ size ,   frames ] ,   frames 是 音频 的 值 - 1 到 1 之间     https : / / magenta . tensorflow . org /         可视化 高维 数据 ,   embeddings :   支持 t - SNE 和 PCA 两种 降维 方式           模型 托管   Tensorflow   Serving           流水线       持续 训练 ,   即 在线 学习       模型 服务 ,   gRPC 协议       客户端 访问               自动 感知 模型 更新      ", "tags": "machine-learning/softpackage", "url": "/wiki/machine-learning/softpackage/understand-tensorflow-book.html"},
      
      
      {"title": "张量索引", "text": "    Table   of   Contents           1 维 索引   tf . gather           多维 索引   tf . gather _ nd                 1 维 索引   tf . gather             tf . gather ( tensor ,   i )   =   tensor [ i ]   ,   i 是 一个 标量         tf . gather ( tensor ,   [ i1 ,   i2 ,   ... ,   ik ] )   =   [ tensor [ i1 ] ,   tensor [ i2 ] ,   ... ,   tensor [ ik ] ]           tf . gather ( x ,   ind )   =   y     更 高维索引 ,   相当于 将 ind 中 每 一个 元素 i 替换成 x [ i ] ,     y [ ij ... k ]   =   x [ ind [ ij .. k ] ]   , 可以 看到 , 它 永远 只 在 x 的 第一 维上 索引                   In       [     31     ] :       tf     .     gather     (     tf     .     constant     ( [     0     ,     1     ,     2     ,     3     ,     4     ] ) ,       tf     .     constant     (     2     ) )     .     eval     ( )       Out     [     31     ] :       2           In       [     37     ] :       x       =       tf     .     constant     ( [ [     2     ,     3     , ] , [     0     ,     1     ] , [     3     ,     6     ] ] )       In       [     38     ] :       tf     .     gather     (     x     ,       tf     .     constant     (     1     ) )     .     eval     ( )       Out     [     38     ] :       array     ( [     0     ,       1     ] ,       dtype     =     int32     )         In       [     39     ] :       tf     .     gather     (     x     ,       tf     .     constant     ( [     1     ,       2     ] ) )     .     eval     ( )       Out     [     39     ] :       array     ( [ [     0     ,       1     ] ,                     [     3     ,       6     ] ] ,       dtype     =     int32     )                 多维 索引   tf . gather _ nd             tf . gather ( tensor ,   i )   =   tensor [ i ]   ,   i 至少 要是 一个 向量 ,   实现 一个 多维 索引                   x       =       tf     .     constant     ( [ [     2     ,     3     , ] , [     0     ,     1     ] , [     3     ,     6     ] ] )       tf     .     gather     (     x     ,       tf     .     constant     ( [     1     ,       0     ] ) )     .     eval     ( )         #   x [ 1 , 0 ]   =   0                       tf . gather _ nd ( x ,   ind )     相当于 将 ind 最后 一个 维度 上 的 向量 作为 一个 多维 索引                   x       =       tf     .     constant     ( [ [     2     ,     3     , ] , [     0     ,     1     ] , [     3     ,     6     ] ] )       tf     .     gather     (     x     ,       tf     .     constant     ( [ [     1     ] ,       [     2     ] ] ) )     .     eval     ( )       #   [ x [ 1 ] ,   x [ 2 ] ]   =   [ [ 0 ,   1 ] ,   [ 3 ,   6 ] ]        ", "tags": "machine-learning/softpackage", "url": "/wiki/machine-learning/softpackage/tensorflow-gather.html"},
      
      
      
      {"title": "tensorflow google 开源机器学习库", "text": "    Table   of   Contents           TensorFlow   白皮书           基本概念           优化           工具                   核心 图 数据结构           基本 数据类型           优化           tf . distrib . learn   框架           CPU   vs   GPU           TIPS                 TensorFlow   白皮书       第一代 分布式 机器 学习 框架 ：   DistBelief 。     第二代 ： TensorFlow ， 通用 的 计算 框架 ！       基本概念           TensorFlow   的 计算 被 表达 为 一个 有向图 ( graph ) — — 计算 图 ， 它 由 很多 节点 ( Node ) 构成       每 一个 节点 有 0 个 或者 多个 输入 ， 0 个 或者 多个 输出 ， 表达 了 一个 计算 操作 实例       正常 边上 流动 的 值 被称作 张量 ( tensor )       特殊 边 ： control   dependencies ： 没有 数据流 过 这些 边 ， 用来 控制 依赖 关系 的       操作 （ Operation ） ： 对 计算 的 抽象 ， 例如 矩阵 乘法 ， 加法 等 。 操作 可以 有 属性 ， 所有 的 属性 必须 被 指定 ， 或者 在 图 构建 的 时候 能够 推断 出来       内核 （ Kernel ） ： 操作 的 一种 特殊 实现 ， 能够 在 特定 的 设备 （ 如 CPU ， GPU ） 上 运行       会话 （ Session ） ： client 程序 与   TensorFlow   系统 交互 的 方式 ，   一般 创建 一次 ， 然后 调用     run     方法 执行 计算 图 的 计算 操作       变量 （ Variable ） ： 大多数   tensor   在 一次 计算 后 就 不 存在 了 ， 变量 在 整个 计算 图 计算 过程 中 ， 可以 一直 保持 在 内存 。 Variable   操作 返回 一个 句柄 ， 指向 该 类型 的 可变 张量 。 对 这些 数据 的 操作 可以 通过 返回 的 句柄 进行 ， 例如   assign ,   assignadd 操作 。 一般 用来 保存 模型 参数 ！       实现 ： 单机 ， 分布式 。 client   通过   session   提交 计算 任务 ， master 通过   worker   执行 计算 操作                           设备 ： device ， 如 CPU 或者 GPU ； 每 一个   worker   关联 一个 或 多个 设备 。 每个 设备 都 一个 一个 类型 ， 和 一个 名字 ， 如     / job : localhost / device : cpu : 0       或者     / job : worker / task : 17 / device : gpu : 3   。 其他 设备 类型 可以 通过 注册 的 机制 加入 ！               单机 执行           多机 执行 ： 多机 通信 方式 ： TCP   or   RDMA       容错 ： 一旦 检测 到 错误 ， 就 重新 开始 ； 变量 （ Variable ） 会 定期 的 保存   chekpoint 。       梯度 计算 ： 会 创建 一个 子图 ， 计算 梯度       控制流 ： 支持   条件 跳转 ， switch ， 以及 循环       输入 节点 ： client 通过 feed 灌入 数据 ， 或者 直接 定义 输入 节点 直接 访问 文件 （ 效率 更好 ）       队列 ： 让子 图 异步 执行 的 特性 ！ FIFO 队列 ， shuffle   队列       容器 （ container ） ： 用于 存 长期 可变 状态 ， 例如 变量           优化           Common   Subexpression   Elimination ， 公共 子 表达式 消除       通过 控制流 ， 延迟 recevier 节点 的 通信 ， 减少 不必要 的 通信 资源 消耗       异步   kernel       采用 深度 优化 的 库 ： BLAS ， cuBLAS ， convolutional ， Eigen （ 已 扩展 到 支持 任意 维度 的 张量 ）       有损压缩 ， 通信 的 时候 采用 有损压缩 传递数据       数据 并行 ， 模型 并行 ！           工具           TensorBoard ： 训练 过程 可视化 ， 计算 图 结构 的 可视化           核心 图 数据结构       class   tf . Graph       基本 数据类型           constant                   a       =       tf     .     constant     (     5.0     )       b       =       tf     .     constant     (     6.0     )       c       =       a       *       b       with       tf     .     Session     ( )       as       sess     :               print       sess     .     run     (     c     )               print       c     .     eval     ( )                 #   just   syntactic   sugar   for   sess . run ( c )   in   the   currently   active   session !                     Session         tf . InteractiveSession ( )         default   session               Variables ， 用来 表示 模型 参数 。       “ When   you   train   a   model   you   use   variables   to   hold   and     update   parameters .   Variables   are   in - memory   buffers     containing   tensors ”   -   TensorFlow   Docs .                           W1       =       tf     .     ones     ( (     2     ,     2     ) )       W2       =       tf     .     Variable     (     tf     .     zeros     ( (     2     ,     2     ) ) ,       name     =     & quot ; weights & quot ;     )       R       =       tf     .     Variable     (     tf     .     random _ normal     ( (     2     ,     2     ) ) ,       name     =     & quot ; random _ weights & quot ;     )                 使用 函数   tf . initialize _ all _ variables ( )   参数 初始化 ， 初始值 在 定义 的 时候 给出 。     如果 要 对 变量 作用域 里面 的 所有 变量 用 同一个 初始化 方法 ， 可以 在 定义 作用域 的 时候 指定 。     参考   https : / / www . tensorflow . org / versions / r0 . 7 / how _ tos / variable _ scope / index . html # initializers - in - variable - scope                 with       tf     .     variable _ scope     (     & quot ; foo & quot ;     ,       initializer     =     tf     .     constant _ initializer     (     0.4     ) ) :               v       =       tf     .     get _ variable     (     & quot ; v & quot ;     ,       [     1     ] )               assert       v     .     eval     ( )       = =       0.4         #   Default   initializer   as   set   above .                 变量 的 更新 ， 使用 方法   tf . add   ,     tf . assign   等 方法               state       =       tf     .     Variable     (     0     ,       name     =     & quot ; counter & quot ;     )       new _ value       =       tf     .     add     (     state     ,       tf     .     constant     (     1     ) )       update       =       tf     .     assign     (     state     ,       new _ value     )       with       tf     .     Session     ( )       as       sess     :               sess     .     run     (     tf     .     initialize _ all _ variables     ( ) )               print       sess     .     run     (     state     )               for       _       in       range     (     3     ) :                       sess     .     run     (     update     )                       print       sess     .     run     (     state     )         0       1       2       3                 利用   tf . convert _ to _ tensor   方法 可以 将 数值 变量 转换 为 张量 。           placeholder 用来 输入 数据 ， 通过   feed _ dict   字典 将 输入 数据 映射 到 placeholder .                   input1       =       tf     .     placeholder     (     tf     .     float32     )       input2       =       tf     .     placeholder     (     tf     .     float32     )       output       =       tf     .     mul     (     input1     ,       input2     )       with       tf     .     Session     ( )       as       sess     :               print       sess     .     run     ( [     output     ] ,       feed _ dict     =     {     input1     : [     7 .     ] ,       input2     : [     2 .     ] } )         tf     .     placeholder     (     dtype     ,       shape     =     None     ,       name     =     None     )                 注意 ， 这里 有个 大坑 ， 这个 字典 的 键 是 一个   op   ， 而 不是 一个 字符串 ！ ！               变量 作用域 ，   variable _ scope   ,     get _ variable _ scope   ,     get _ variable   .       scope . reuse _ variables ( )     可以 使得 该 作用域 的 变量 重复使用 ， 在 RNN 实现 中 很 有用 。         声明 重复 利用 的 时候 ，   get _ variable   的 时候 不是 创建 一个 变量 ， 而是 查询 保存 的 那个 变量 。               word   embedding               优化       当 得到 损失 函数 之后 ， 可以 通过   tf . train . Optimizer   优化 工具 来 进行 优化 ， 实际 优化 的 时候 使用 的 是 他 的 子类 ，     如   GradientDescentOptimizer   ,     AdagradOptimizer   ,   or     MomentumOptimizer   。     优化 obj 通常 有 以下 几个 重要 方法 可以 使用 。             train _ op   =   minimize ( loss )   ， 直接 最小化 损失 函数         compute _ gradients ( loss )   ， 计算 梯度         train _ op   =   apply _ gradients ( grad )   ， 应用 梯度 更新 权值             tf . distrib . learn     框架       一个 高级 机器 学习 框架           模型 基本 接口 ， 与 sklearn 很 像         init   ( )   初始化         fit     拟合         evaluate     评估         predict     预测                   CPU   vs   GPU           Q :   自己 代码 在 实现 上 有 什么 区别 呢 ？           TIPS             tf . reshape ( some _ tensor ,   ( - 1 ,   10 ) )   将 数据 重新 划分 为 10 列 的 元素 ， 第一 维自 适应         tf . device     指定 CPU 或者 GPU         tf . add _ to _ collection ( name ,   value )   将 value 保存 为 名字 为 name 的 共享 集合 中 ， 供 后面 使用 .       tf . get _ collection ( name )   ， 获取 存储 的 值       tensorflow 里面 的 标量 和   shape = [ 1 ]   是 不同 的 ， 请 注意 。           [ 1 ]     https : / / www . tensorflow . org /       [ 2 ]     http : / / cs224d . stanford . edu / lectures / CS224d - Lecture7 . pdf    ", "tags": "machine-learning", "url": "/wiki/machine-learning/tensorflow-google.html"},
      
      
      {"title": "Theano", "text": "    Table   of   Contents           theano 中 的 broadcast           theano 两层 神经网络 代码 例子           tips                 theano 中 的 broadcast       theano 和 numpy 中 的 broadcast 不同 ，     theano 中 需要 对 能够 broadcast 的 变量 进行 编译 前 申明 ，     默认 情况 下 ， theano 中 的   matrix / tensor   和 所有 的   shared   variable   都 不 不 可以     broadcast 的 ，   vector   和   matrix   进行 计算 时 是 可以 进行 broadcast 的 。       shared   variable   可以 在 创建 的 时候 声明 可以 broadcast 。               bval       =       np     .     array     ( [ [     10     ,       20     ,       30     ] ] )       bshared       =       theano     .     shared     (     bval     ,       broadcastable     =     (     True     ,       False     ) )                 theano 两层 神经网络 代码 例子               def       nn _ cost _ func     ( ) :               import       theano               from       theano       import       tensor       as       T               w1       =       T     .     dmatrix     (     &# 39 ; w1 &# 39 ;     )               b1       =       T     .     dvector     (     &# 39 ; b1 &# 39 ;     )               w2       =       T     .     dmatrix     (     &# 39 ; w2 &# 39 ;     )               b2       =       T     .     dvector     (     &# 39 ; b2 &# 39 ;     )               x       =       T     .     dmatrix     (     &# 39 ; x &# 39 ;     )               y       =       T     .     dmatrix     (     &# 39 ; y &# 39 ;     )               h       =       T     .     nnet     .     sigmoid     (     T     .     dot     (     x     ,     w1     )     +     b1     )               yp       =       T     .     nnet     .     softmax     (     T     .     dot     (     h     ,       w2     )     +     b2     )               J       =       T     .     mean     (     T     .     sum     (     y       *       T     .     log     (     yp     ) ,       axis     =     1     ) )               dJ       =       T     .     grad     (     J     , [     w1     ,     b1     ,     w2     ,     b2     ] )               fJ       =       theano     .     function     ( [     x     ,     y     ,     w1     ,     b1     ,     w2     ,     b2     ] ,       J     )               fdJ       =       theano     .     function     ( [     x     ,     y     ,     w1     ,     b1     ,     w2     ,     b2     ] ,       dJ     )               return       fJ     ,       fdJ                 tips           梯度 函数 输出 一个 列表 ， 如果 只有 一个 梯度 的 时候 ， 需要 这样 写                   grad     ,       =       fdJ     (     weights     ,       features     ,       ground _ truth     ,       N     ,       regularization     )        ", "tags": "machine-learning", "url": "/wiki/machine-learning/theano.html"},
      
      
      {"title": "TOPIC MODEL - 主题模型", "text": "    Table   of   Contents           主题 模型 的 意义           Latent   Dirichlet   allocation ( LDA )           参考文献                 主题 模型 的 意义       Topic   modeling   provides   methods   for   automatically   organizing ,   understanding ,   searching ,   and   summarizing   large   electronic   archives .           Discover   the   hidden   themes   that   pervade   the   collection .       Annotate   the   documents   according   to   those   themes .       Use   annotations   to   organize ,   summarize ,   and   search   the   texts .           Latent   Dirichlet   allocation ( LDA )           文档 包含 多个 主题       每 一个 主题 是 在 词上 的 一个 分布 ， 可以 表达 为 词 的 直方图       每 一个 文档 是 多个 主题 的 混合 ， 可以 表达 为 主题 的 直方图       每 一个 词 是从 某个 主题 中 采样 得到       但是 我们 只能 观察 到 文档 ， 其他 的 都 是 隐 变量 ！       我们 的 目标 是 推断出 这些 隐 变量 ！                   参考文献           Probabilistic   Topic   Models ,   ICML2012   Tutorial :     http : / / www . cs . columbia . edu / ~ blei / talks / Blei _ ICML _ 2012 . pdf       2 .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/topic-model.html"},
      
      
      {"title": "UMAP- Uniform Manifold Approximation and Projection for Dimension Reduction", "text": "    Table   of   Contents          ", "tags": "machine-learning", "url": "/wiki/machine-learning/umap.html"},
      
      
      {"title": "User Profiling through Deep Multimodal Fusion - WSDM2018", "text": "    Table   of   Contents           UDMF :   USER   PROFILING   THROUGH   DEEP   MULTIMODAL   FUSION           数据源 embedding                 UDMF :   USER   PROFILING   THROUGH   DEEP   MULTIMODAL   FUSION           为了 将 多种 数据源 的 数据 集成 到 一个 模型 中 ,   作者 搞 了 两个 技巧 :   stacking   和   power - set   combination           一个 正常 的 多层 感知器 ,   应用 到 数据源 D 上时 ,   第 h 层 的 第 i 个 神经 源 的 响应 是     $ $     U ^ h _ i ( D )   =   f \ \ left (       \ \ sum _ j   w _ { ij } ^ { hl }   U _ j ^ l ( D )           \ \ right )     $ $               而 第 0 层 的 输入 则 为     $ $     U ^ 0 _ i ( D )   =   f \ \ left (       \ \ sum _ j   w _ { ij }   D _ j           \ \ right )     $ $           利用 stacking 的 方法 , 将 其他 画像 任务 的 输出 作为 当前 画像 任务 的 输入 ( 见图 2 ) , 并且 可以 实现 在 其他 输入 相同 的 情况 下 , 多次 不断 的 迭代 。 第 q 次 迭代 中 ,   第 0 层 的 输入 可以 记 作     $ $     U ^ { 0q }   _   i ( D )   =   f \ \ left (       \ \ sum _ j   w _ { ij }   D _ j     +   \ \ sum _ z   w _ { iz }   \ \ alpha _ z   t _ z ^ { q - 1 }       \ \ right )     $ $       t 代表 第 z 个 画像 的 输出 , q 代表 轮数 , 第 q 次 的 输入 特征 是 上 一轮 其他 画像 的 值 。 $ ( \ \ alpha _ z ) $   是 一个 示性 函数 , 将 当前 画像 上 一轮 的 值 排除 掉 , 当 z 是 当前 画像 时 , 取值 为 0 , 其他 画像 则 为 1 .                       多个 数据源 首先 作为 特征 融合 , 然后 将 多个 画像 用 stacking 方式 融合                           幂集 融合 , k 个 数据源 $ ( DS   =   \ \ {   D _ 1 ,   D _ 2 ,   ... ,   D _ k   \ \ }   ) $ , 非 空子 集 数目 为 $ ( 2 ^ k   -   1 ) $ , 对 每个 非 空子 集 , 构建 一个 mini - DNN ,   他 的 每个 神经元 会 将 其他 子集 对应 的 mini - DNN 中 相同 的 神经元 的 结果 求和 , 说 得 很 啰嗦 , 看 公式 ( $ ( \ \ mathcal { D } ) $ 表示 非 空子 集 的 集合 )     $ $     U ^ { 0q }   _   i ( \ \ mathcal { D } )   =   f \ \ left (     \ \ sum _ { D   \ \ in   \ \ mathcal { D } }   \ \ sum _ j   w _ { ij }   D _ j     +   \ \ sum _ z   w _ { iz }   \ \ alpha _ z   t _ z ^ { q - 1 }       \ \ right )     $ $               以 2 个 数据源 2 个 画像 为例 , 每个 画像 会 有 3 个子 DNN 网络 , 所以 一共 有 6 个 DNN 网络           在 决策层 , 每个 画像 都 将子 DNN 网络 的 输出 结果 进行 融合 , 比如 简单 投票           数据源 embedding           文本 :       88   Linguistic   Inquiry   and   Word   Count   ( LIWC )   features   ( 最佳 )       Glove       fastText               图像 :       Oxford   Face   API       CNN 输出 的 向量 , 作者 用 的 是 在 ImageNet 上预 训练 的 VGG 输出 的 向量 , 效果 不如 特征描述 子 和 API 特征 有效               关系数据 :       Node2Vec   向量   ( 最佳 )       page   like   向量 , 类似 于 矩阵 分解              ", "tags": "machine-learning", "url": "/wiki/machine-learning/user-profile-wsdm18.html"},
      
      
      {"title": "VSM - From Frequency to Meaning: Vector Space Models of Semantics", "text": "    Table   of   Contents           关于           导言           语义 向量 空间 模型           文档 的 相似性 ： The   Term – Document   Matrix ， Salton   et   al .   ( 1975 )           词 的 相似性 ： The   Word – Context   Matrix           关系 的 相似性 ： The   Pair – Pattern   Matrix           Types   and   Tokens           五个 假设                   Linguistic   Processing   for   VSM           Tokenization           Normalization           Annotation                   Mathematical   Processing   for   Vector   Space   Models           频率 统计           加权 频率 变换           平滑 矩阵           比较 向量           有效 的 比较                   3 个 开源 VSM 系统           应用           其他 方法   to   语义 分析           VSM 的 未来           问题                 关于       向量 空间 模型 ( VSM ) 综述 论文 ： From   Frequency   to   Meaning :   Vector   Space   Models   of   Semantics ,   2010 .       导言           词 的 分布式 假设 （ distributional   hypothesis   ） :               words   that   occur   in   similar   contexts   tend   to   have   similar   meanings   ( Wittgenstein ,   1953 ;   Harris ,   1954 ;   Weaver ,   1955 ;   Firth ,   1957 ;   Deerwester ,   Dumais ,   Landauer ,   Furnas ,   & amp ;   Harsh -   man ,   1990 )               三类 矩阵 ： term – document ,   word – context ,   and   pair – pattern   matrices       event   frequencies   而 不是   adjacency   matrix （ 基于 词典 的 方法   wordnet ）               未来 工作 ： 新 的 矩阵 ， 高阶 张量 ！       应用 （ lead   algorithm ） ：       measuring   semantic   relatedness :   Pantel   & amp ;   Lin ,   2002a ;   Rapp ,   2003 ;   Turney ,   Littman ,   Bigham ,   & amp ;   Shnayder ,   2003 .       measuring   the   similarity   of   semantic   relations :   Lin   & amp ;   Pantel ,   2001 ;   Turney ,   2006 ;   Nakov   & amp ;   Hearst ,   2008 .                     案例         IQ 测试   or   性格 测试 ：   subject - item   matrix !       向量分析 方法 ：   因子分析   ！       向量 空间 模型 里面 向量 的 元素 来自 于     事件 频率 统计   ！ ！             Latent   Semantic   Analysis   ( LSA )     Deerwester   et   al . ,   1990 ;   Lan -   dauer   & amp ;   Dumais ,   1997         Hyperspace   Analogue   to   Language   ( HAL )     ( Lund ,   Burgess ,   & amp ;   Atchley ,   1995 ;   Lund   & amp ;   Burgess ,   1996 )           语义 向量 空间 模型           假设         statistical   semantics   hypothesis   :   statistical   patterns   of   human   word   usage   can   be   used   to   figure   out   what   people   mean .         bag   of   words   hypothesis           distributional   hypothesis           extended   distributional   hypothesis           latent   relation   hypothesis                     文档 的 相似性 ： The   Term – Document   Matrix ， Salton   et   al .   ( 1975 )           行向量 对应 于 一个 term ， 通常 是 一个 词 ； 列 向量 是 一个 document ， 例如 一个 网页 ！         bag     是 指 一个 集合 ， 不同 的 是 可以 有 重复 ， 但是 元素 的 顺序 没有 意义 ， 不同 顺序 是 等价 的 ！     一个 bag 可以 用 一个 向量 来 表示 ， 向量 每个 元素 表示 对应 的 bag 元素 出现 的 次数 ， 例如 { a , a , b , c , c , c , } 可以 表示 为 & lt ; 2 , 1 , 3 & gt ; .     一系列 的 bag 可以 用 一个 矩阵 X 表示 ， 矩阵 的 一列 对应 于 一个 bag 向量 ， 而 一行 对应 于 一个 唯一 的 元素 ， $ ( x _ { ij } ) $ 为 第 j 个 bag 中 元素 i 出现 的 次数 。       term - document 中 ， 一个 文档 被 表达 为 一个   bag   of   word ， 每 一个 列 向量 对应 于 bag 的 向量 表达 。               In   information   retrieval ,   the   bag   of   words   hypothesis   is   that   we   can   estimate   the   relevance   of   documents   to   a   query   by   representing   the   documents   and   the   query   as   bags   of   words .   ( Salton   et   al . ,   1975 )               矩阵 的 每 一列   $ ( x _ { : j } ) $ 代表 文档 $ ( d _ j ) $ 的 一种 向 量化 表达 ， 虽然 没有 考虑 词 的 顺序 、 短语 、 句子 等 语义 结构 ， 但是 仍然 在 搜索引擎 中 工作 的 很 好 ！       而 每 一行   $ ( x _ { i } ) $ 代表 term   $ ( w _ i ) $   的 一种 签名 ！ 可以 用来 度量   term   的 相似性 ！ Deerwester   et   al .   ( 1990 )       一种 解释 ： the   topic   of   a   document   will   probabilistically   influence   the   author ’ s   choice   of   words   when   writing   the   document .   直接 导致 LDA 模型 的 出现 ！           词 的 相似性 ： The   Word – Context   Matrix           行向量 是 词 ， 列 向量 是 上下文 ， context   可以 是 词 、 短语 、 句子 、 段落 、 章节 、 文档 等 更 多 可能性       上下文 可以 参考   Sahlgren ’ s   ( 2006 )   thesis       矩阵 相似 的 行向量 代表 相似 的 词 ！ 但是 主要 的 上下文 通常 是 其他 词 ！       共现 频率   Weaver   ( 1955 )   co - occurrence   frequency ， 用来 消歧意               distributional   hypothesis   in   linguistics   is   that   words   that   occur   in   similar   contexts   tend   to   have   similar   meanings   ( Harris ,   1954 )           关系 的 相似性 ： The   Pair – Pattern   Matrix           行向量 是 词 对 ， 例如   mason : stone   and   carpenter   :   wood ； 列 向量 是 词 对 出现 的 模式 ， 例如   “ X   cuts   Y   ”   and   “ X   works   with   Y   ” .   Lin   and   Pantel   ( 2001 ) ， 用来 判定 模式 的 相似性       用来 推理 ， 一个 句子 是 另 一个 句子 的 解释 。       行向量 ： 词对 的 相似性 ， Turney   et   al .   ( 2003 )                 extended   distributional   hypothesis   ,   that   patterns   that   co - occur   with   similar   pairs   tend   to   have   similar   meanings .   Lin   and   Pantel   ( 2001 )         The   latent   relation   hypothesis     is   that   pairs   of   words   that   co - occur   in   similar   patterns   tend   to   have   similar   semantic   relations   ( Turney ,   2008a )           关系 的 相似性 不能 消减 为 属性 的 相似性 （ word - context   matrix )           高阶 张量 ：       term – document – language   third - order   tensor ： 多 语言 信息检索       word – word – pattern   tensor ： 词 相似性       verb – subject – object   tensor ：                   Types   and   Tokens       token - document   matrix ， 里面 相同 的 词 但是 出现 在 不同 地方 的 词 作为 不同 的 token ；     type - duocument   matrix ， 则 把 相同 词 合并 了 。       前者 可以 用 在 词消 歧义 上 ， 一词 多义 ！       问题 ， 这种 token - document   matrix 完全 看不出 有 什么 意义 啊       五个 假设           Statistical   semantics   hypothesis ： 词 的 统计 模式 可以 用来 表明 含义       Bag   of   words   hypothesis       Distributional   hypothesis       Extended   distributional   hypothesis       Latent   relation   hypothesis           Linguistic   Processing   for   VSM       对 数据 的 预处理 ： tokenize ， normalize （ 将 词 不同 的 形式 归一化 ） ， annotate   the   raw   text （ 将 相同 的 形式 标记 为 不同 的 含义 ： eg   动词 ， 名词 ）       Grefenstette   ( 1994 ) ： 三步走 ： tokenization ,   surface   syntactic   analysis ,   and   syntactic   attribute   extraction .       Tokenization       英语 等 西班牙语 系 可以 通过 天然 的 分割 符 空格 进行 分割 ！     而 汉语 等 非 西班牙语 系则 不同 ！       精确 的 Tokenizer 还 需要 处理 标点符号 ！ 连 字符 ， multi - word   terms （ e . g . ,   Barack   Obama   and   ice   hockey ） 。     停止 词 ， 高频 却 无 意义 的 词 ， 代词 等 。 停止 词表 ： SMART   system   ( Salton ,   1971 )       Normalization           case   folding       stemming           一般而言 ， 归一化 将 导致 精确度 降低 ， 召回 率 提高 。     如果 数据量 少 ， 一定 要 用 归一化 ， 提高 召回 率 ；     但 如果 数据量 很大 ， 精确度 更 重要 ， 可以 不 归一化 ！       Annotation           part - of - speech   tagging       word   sense   tagging       parsing           降低 召回 率 ， 提高 精确度 ！       Mathematical   Processing   for   Vector   Space   Models       Lowe   ( 2001 )   4 步 走 ： 1 、 统计 频率 ， 2 、 频率 变换 （ 加权 ） ， 3 、 平滑 ， 4 、 计算 相似性 。       频率 统计       关键技术 ： Hash   Table ； 数据库 ； 搜索引擎 索引 。       加权 频率 变换           TF - IDF   用 倒 文档 频率 作为 权值       文档 长度 ： 因为 相同 的 情况 下 ， 长 文档 更 容易 被 匹配 到 ！ 因为 词多 ！       term   的 权重 ， 两个 很 相近 的 词 同时 出现 在 一个 文档 中 ， 除了 可以 将 他们 归一化 到 同一个 词 ， 也 可以 减少 他们 的 权重 ！       特征选择 也 可以 看做 一种 加权 手段 ： Forman   ( 2003 )       Pointwise   Mutual   Information （ PMI ， Church   & amp ;   Hanks ,   1989 ;   Turney ,   2001 ）       Positive   PMI （ PPMI ） ： 将 PMI 小于 0 的 值置 0 ！ 当用 word - context 矩阵 度量 语义 相似性 地 时候 ， 效果 更好 ！           假设 word - context   矩阵   F ， 行向量 $ ( f _ i ) $ ， 列 向量 $ ( f _ { : j } ) $ 。 新 矩阵   X   是 PPMI 矩阵 ， 定义 为       $ $     p _ { ij }   =   \ \ frac { f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     p _ { i * }   =   \ \ frac { \ \ sum _ j   f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     p _ { *   j }   =   \ \ frac { \ \ sum _ i   f _ { ij } } { \ \ sum _ i   \ \ sum _ j   f _ { ij } }         \ \ \ \     pmi _ { ij }   =   \ \ log \ \ left (   \ \ frac { p _ { ij } } { p _ { i * }   p _ { *   j } }     \ \ right )         \ \ \ \     x _ { ij }   =   \ \ begin { cases }             pmi _ { ij }   & amp ;   if   pmi _ { ij }   & gt ;   0   \ \ \ \             0         & amp ;     \ \ text { otherwise }     \ \ end { cases }     $ $       PMI   的 问题 ， 对 小 概率 事件 有 偏 ！ 特例 ： 当 i 和 j 统计 依赖 ， $ ( p _ { ij }   =   p _ { i * }   =   p _ { *   j } ) $ ，     那么 PMI 变为   $ ( \ \ log ( 1 / p _ { i * } ) ) $ 。       一种 解决方案 是 （ Pantel   & amp ;   Lin ,   2002a ） ， 对 $ ( f _ { ij } ,   f _ { i * } ,   f _ { *   j } ) $ 进行 平滑 处理       $ $     \ \ delta _ { ij }   =   \ \ frac { f _ { ij } } { f _ { ij }   +   1 }   \ \ frac { \ \ min ( f _ { *   j } ,   f _ { i * } ) } { \ \ min ( f _ { *   j } ,   f _ { i * } )   +   1 }     \ \     newpmi _ { ij }   =   \ \ delta _ { ij }   pmi _ { ij }     $ $       另 一种 解决方案 是 对 概率 进行 拉普拉斯 平滑 ！ 即 对 每 一个 $ ( f _ { ij }   \ \ rightarrow   f _ { ij }   +   k ) $ 。       平滑 矩阵           限制 向量 成分 ： 只 保留 PMI 超过 某个 阈值 的 项 ， 其他 置 0 .       truncated   SVD ： 应用 到 document   similarity 就是   Latent   Semantic   Indexing   ( LSI ) ； 应用 到   word   similarity   就是   Latent   Semantic   Analysis   ( LSA )           rank - k   矩阵 近似 ， 最小化 富 比尼 范数 $ ( | | X   -   \ \ hat { X } | | _ F ) $ ！ ( Golub & amp ; VanLoan , 1996 )           Latent   Meaning :   Deerwester   et   al .   ( 1990 )   and   Landauer   and   Dumais   ( 1997 )   ， 认为 k 个 最大 的 奇异 值 是 隐层 语义 ， 对应 的 两个 矩阵 分别 代表 term 和 document 与 不同 隐 变量 的 相关度 。       Noise   reduction ： 是 对 矩阵 X 的 平滑 ！ Rapp   ( 2003 )       High - order   co - occurrence :   Landauer   and   Dumais   ( 1997 ) ， Lemaire   and   Denhiere   ( 2006 )           Sparsity   reduction ： 类似 于 矩阵 补全 ！               SVD 实现 ：           svdlibc :   http : / / tedlab . mit . edu / ~ dr / svdlibc /   .   Rohde       Brand ’ s   ( 2006 )       Gorrell ’ s   ( 2006 )                   高阶 张量 类似 算法 ： parallel   factor   analysis ， canonical   decomposition ， Tucker   decomposition           其他 平滑 方法 ：       Nonnegative   Matrix   Factorization   ( NMF )   ( Lee   & amp ;   Seung ,   1999 ) ,   Probabilistic   Latent   Semantic   Indexing   ( PLSI )   ( Hofmann ,   1999 ) ,   Iter -   ative   Scaling   ( IS )   ( Ando ,   2000 ) ,   Kernel   Principal   Components   Analysis   ( KPCA )   ( Scholkopf ,   Smola ,   & amp ;   Muller ,   1997 ) ,   Latent   Dirichlet   Allocation   ( LDA )   ( Blei   et   al . ,   2003 ) ,   and   Discrete   Component   Analysis   ( DCA )   ( Buntine   & amp ;   Jakulin ,   2006 ) .               SVD   隐含地 假设 词频 是 高斯分布 ， 然而 并 不是 ， PMI 比 PPMI 更 接近 高斯分布 ！           比较 向量           向量 夹角 余弦 值 ！       距离 的 度量 可以 转换 为 相似 度       距离 度量 ： 欧式 距离 ， 曼哈顿 距离 ，   Hellinger ,   Bhattacharya ,     and     Kullback - Leibler         在   Bullinaria   and   Levy   ( 2007 )   试验 中 ， 余弦 相似 度 效果 最好 ！       其他 度量 ： Dice   and   Jaccard   coe   cients   ( Manning   et   al . ,   2008 ) .           三类 ： Weeds   et   al .   ( 2004 )           high - frequency   sensitive   measures   ( cosine ,   Jensen - Shannon ,   $ ( \ \ alpha ) $ - skew ,   recall ) ,       low - frequency   sensitive   measures   ( precision ) ,   and       similar - frequency   sensitive   methods   ( Jaccard ,   Jaccard + MI ,   Lin ,   harmonic   mean ) .           有效 的 比较           稀疏 矩阵 乘法 优化       将 低于 阈值 的 项 减为 0 ， 也 可以 极大 的 减少 计算 量       分布式 实现 ： MapReduce ， Elsayed ,   Lin ,   and   Oard   ( 2008 )       随机 算法 ：       random   indexing       Locality   sensitive   hashing （ LSH ）                   3 个 开源 VSM 系统               Term - Document   Matrix ：   Lucene .   结合   Nutch ， Solr 可以 做 一个 搜索 系统 了 ！               Word – Context   Matrix :   Semantic   Vectors               Pair – Pattern   Matrix :   Latent   Relational   Analysis   in   S - Space               应用           Term – Document   Matrices ：       文档 检索 ， 跨 语言 检索 ： 截断 SVD 可以 提高 精度 和 召回 ！ ！ ！ 问题 在于 要 解决 大规模 问题 的 计算 量 ！ 其他 技巧 有 协同 过滤 和 PageRank       文档 聚类       文档 分类 ： 主题 ， 语义 ， 垃圾邮件       文章 自动 打分       文档 分割 ： 将 文档 分割 为 几个 不同 的 主题       QA   问答 系统       Call   routing ， 客服 ？               Word – Context   Matrices ：       词 相似性 ： TOEFL       词聚类       词 分类       词典 自动 生成       词消 歧义       上下文 评写 纠错       查询 扩展 ： 搜索引擎 扩展 查询 词为 相近 的 词 ： 使用   session   上下文 和 click   上下文       文本 广告 ： 点击 付费 广告 ： bidterm   扩展       信息提取 （   I     nformation     E     xtraction ) :   名字 实体 识别 （ NER ） ， relation   extraction ,   event   extraction ,   and   fact   extraction               Pair – Pattern   Matrices       关系 相似性       模式 相似性       关系 聚类       关系 分类       关系 搜索       自动 词典 生成       Analogical   mapping ： SAT 测试   a : b : : c : d                   其他 方法   to   语义 分析           概率 语言 模型       词典 ： 图           VSM 的 未来           批评 ： 没有 考虑 词 的 顺序       80 % 的 含义 来自 于 词 ！ ！ ？ Landauer   ( 2002 )           问题             随机 投影           LSH     SIMIHash 等      ", "tags": "machine-learning", "url": "/wiki/machine-learning/vsm.html"},
      
      
      {"title": "What You Get Is What You See: A Visual Markup Decompiler", "text": "    Table   of   Contents           关于           导言           Problem :   Image - to - Markup   Generation           模型   WYGIWYS                 关于       论文 ： What   You   Get   Is   What   You   See :   A   Visual   Markup   Decompiler       导言       OCR 用来 识别 并 提取 结构 信息 ： 不仅仅 要 识别 文字 ， 还要 提取 语义 。     数学 表达式 OCR 系统 ： INFTY 系统 。     需要 联合 处理 图片 和 文字 信息 。       文章 使用 的 模型 是 对模型     attention - based   encoder - decoder   model   ( Bahdanau ,   Cho ,   and   Bengio   2014 )     的 简单 扩展 。           The   use   of   attention   addi -   tionally   provides   an   alignment   from   the   generated   markup   to   the   original   source   image           数据 集 ： IM2LATEX - 100K       在线 效果 演示 ：   http : / / lstm . seas . harvard . edu / latex /         Problem :   Image - to - Markup   Generation           图像 ： $ ( x   \ \ in   \ \ mathcal { X } ) $ ， 例如 $ ( \ \ mathcal { X }   =   \ \ mathbb { R } ^ { H   \ \ times   W } ) $ 。       文本 ： $ ( y   =   ( y _ 1 ,   y _ 2 ,   ... ,   y _ C ) ;   y   \ \   in   \ \ mathcal { Y } ,   y _ i   \ \ in   \ \ Sigma ) $ 。       编译 ： $ ( \ \ mathcal { Y }   \ \ rightarrow   \ \ mathcal { X } ) $ .       需要 学习 一个 反编译器 ！       训练 ： 利用 样本 $ ( ( x ,   y ) ) $ 训练 学习 一个 反编译器 。       测试 ： 利用 模型 预测 的 $ ( \ \ hat { y } ) $ 和 编译 函数 ， 生成 一个 图像 $ ( \ \ hat { x } ) $ ， 要求 生成 的 图像 和 $ ( x ) $ 一致 。           模型   WYGIWYS                   图像 特征 抽取 ： CNN ， 没有 全 连接 层 ， 抽取 的 特征 V 尺寸 为   $ ( D   \ \ times   H '   \ \ times   W ' ) $ ， 分别 是 通道 数 ， 降维后 的 高度 和 宽度 。       编码器 ： 之前 的 ImageCaption 不 需要 这个 编码器 ， 但是 编码器 可以 学到 顺序 关系 ， 这 可以 ：       学习   markup   languages   的 从左到右 的 顺序 关系       使用 周围 的 上下文 去 编码 隐层 表达                   编码器 使用 RNN （ LSTM ） 。 隐层   feature   grid   $ ( \ \ tilde { V } _ { h , w }   =   \ \ text { RNN } ( \ \ tilde { V } _ { h , w - 1 } ,   V _ { h ,   w } ) ) $ ，     即 按行 顺序 编码 ， 对 每 一行 的 初始状态 $ ( \ \ tilde { V } _ { h , 0 } ) $ ， 也 是 通过 学习 得到 （ 怎么 训练 ？ 作为 一个 参数 一起 学 ？ ） ， 叫做   position   embedding ， 可以 表达 图像 所在位置 信息 。           解码器 ： 优点 复杂       通过 上述 编码 后 的 特征   grid   $ ( \ \ tilde { V } ) $ ， 加上 历史 隐层 向量 $ ( h _ { t - 1 } ) $ 学习 一个 注意力 向量 $ ( \ \ alpha _ t ) $       利用 注意力 向量 和 特征 矩阵   $ ( \ \ tilde { V } ) $   学习 一个 有 注意力 的 上下文 向量   $ ( c _ t ) $       利用 当前 隐态 向量   $ ( h _ t ) $   和 带有 注意力 的 上下文   $ ( c _ t ) $   学习 一个 输出 向量   $ ( o _ t ) $ ， 最终 做 softmax 变换 得到 输出 的 词 $ ( y _ t ) $ ！       隐态 更新 采用 常规 的 Decoder 方案 ， 即 上 一 时刻 的 隐态 $ ( h _ { t - 1 } ) $   加上   上 一 时刻 的 输出   $ ( o _ t ,   y _ { t - 1 } ) $                   $ $       $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/im2tex.html"},
      
      
      {"title": "word2vec", "text": "    Table   of   Contents           关于           word2vec   论文           导论           The   Skip - gram   Model           Hierarchical   Softmax           Negative   Sampling           高频词 的 负 采样                   测试数据           短语 学习                   CBOW   论文           Glove                 关于       词 向量 相关 论文 学习 。       word2vec   论文                 Mikolov ,   T .   ,   Sutskever ,   I . ,     Chen ,   K .   ,   Corrado ,   G .   S . ,   & amp ;     Dean ,   J .     ( 2013 ) .   Distributed   representations   of   words   and   phrases   and   their   compositionality .   In   Advances   in   neural   information   processing   systems   ( pp .   3111 - 3119 ) .               问题 ：           相比 直接 通过 VSM 学习 到 的 词 向量 ， 优势 是 什么 ？                   导论               创新 点 ：           针对 连续   skip - gram   模型 ： 采用 一些 介壳 提升 词 向量 质量 和 训练 速度       负 采样 技术 ：   对 高频词 负 采样 可以 显著 提升 训练 时间 （ 2 - 10 倍 的 提升 ） ， 同时 也 能 提高 低频词 的 词 向量 质量 ？                   recursive   autoencoders   ： Richard   Socher ,   Cliff   C .   Lin ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .   Parsing   natural   scenes   and   natural   language   with   recursive   neural   networks .   In   Proceedings   of   the   26th   International   Conference   on   Machine   Learning   ( ICML ) ,   volume   2 ,   2011 .               The   Skip - gram   Model       用 中间 词 预测 周围 的 词 ， 最大化 对数 似然 函数       $ $     \ \ frac { 1 } { T }   \ \ sum _ { t = 1 } ^ T   \ \ sum _ { - c   \ \ le   j   \ \ le   c ,   j   \ \ neq   0 }   \ \ log   p ( w _ { t + j } |   w _ t )   \ \ \ \     p ( w _ O |   w _ I )   =   \ \ frac { \ \ exp ( v ^ ' _ { w _ O } ^ T   v _ { w _ I } ) } { \ \ sum _ { w = 1 } ^ W   \ \ exp ( v ^ ' _ { w } ^ T   v _ { w _ I } )   }     $ $       计算 代价 正比 于 词典 规模   W （ 10 ^ 5 - 10 ^ 7 ） ， 因此 很 费时间 。       Hierarchical   Softmax       Frederic   Morin   and     Yoshua   Bengio   .   Hierarchical   probabilistic   neural   network   language   model .   In   Pro -   ceedings   of   the   international   workshop   on   artificial   intelligence   and   statistics ,   pages   246 – 252 ,   2005 .       只 需要 计算 $ ( \ \ log _ 2W ) $ 个 节点 ！     将 条件 概率 的 计算 ， 变成 多个 分类 概率 的 计算 。 可以 用 一个 二叉树 表示 出来 ， 叶子 结点 对应 词 ， 中间 节点 有 一个 参数 ！     每 一个 内部 节点 可以 看做 一个 二 分类 逻辑 回归 ， 其 参数 就是 二 分类 参数 。 这个 参数 也 要 学习 ！     文中 表示 用 哈夫曼 树 作为 这个 二叉树 可以 简单 提升 性能 。       $ $     p \ \ left (   w _ O   |   w _ I   \ \ right )   =   \ \ prod _ { j   =   1 } ^ { L ( w )   -   1 }   \ \ sigma   \ \ left (   [ n ( w , j + 1 )   =   ch ( n ( w , j ) ) ]   \ \ centerdot   v _ { n ( w , j ) } ^ { \ \ top }   v _ { w _ I }   \ \ right )     $ $       Negative   Sampling           Noise   Contrastive   Estimation   ( NCE ) ：       Michael   U   Gutmann   and   Aapo   Hyva   ̈ rinen .   Noise - contrastive   estimation   of   unnormalized   statistical   mod -   els ,   with   applications   to   natural   image   statistics .   The   Journal   of   Machine   Learning   Research ,   13 : 307 – 361 ,   2012 .       Andriy   Mnih   and   Yee   Whye   Teh .   A   fast   and   simple   algorithm   for   training   neural   probabilistic   language   models .   arXiv   preprint   arXiv : 1206.6426 ,   2012 .                   负 采样 解释 可以 看 2014 年 的 文章 ：   http : / / cn . arxiv . org / pdf / 1402.3722 v1 . pdf         可以 将 每一项 理解 为 一个二元 分类 问题 ， 正 样本 是 词 在 中心词 的 上下文 ， 而负 样本 是 不 在 中心词 上下文 的 词 。     目标 函数 相当于 让 正 样本 出现 以及 k 各负 样本 不 出现 的 联合 概率 最大化 ！       $ $     \ \ log   \ \ sigma ( v _ { w _ O } ' ^ T   v _ { w _ I } )   +   \ \ sum _ { i = 1 } ^ k   \ \ mathbb { E } _ { w _ i   \ \ sim   P _ n ( w ) }   \ \ left [     \ \ log ( - \ \ sigma ( v _ { w _ i } ' ^ T   v _ { w _ I } ' ) )   \ \ right ]     $ $       对于 小 数据 集 k 取   5 - 20   即可 ； 对于 大 数据 集 k 可以 取 小点 2 - 5 .     采样 方法   P   取   unigram   distribution   $ ( U ( w ) ^ { 3 / 4 } ) $ 最好 ， 即 正比 于 词频 的 3 / 4 次 幂 。       高频词 的 负 采样       以 概率 P 丢弃 ！       $ $     P ( w _ i )   =   1   -   \ \ sqrt { \ \ frac { t } { f ( w _ i ) } }     $ $       t   是 阈值 ， 典型值 为 $ ( 10 ^ { - 5 } ) $ ， f 是 词 频率 ！     这种 方式 不但 可以 加快速度 ， 还 能 提高 低频词 的 精度 ！ （ 不是 数据 越多越好 ？ ！ ）       测试数据           相似 推理 任务 ：       语法 相似   syntactic   analogies ：   “ quick ”   :   “ quickly ”   : :   “ slow ”   :   “ slowly ”       语义 相似   semantic   analogies ： “ Germany ”   :   “ Berlin ”   : :   “ France ”   :   ?     结果 如下 图 所示 。                           短语 学习       将 经常出现 在 一起 的 ， 而 不 经常 在 其他 上下文 出现 的 多个 词 作为 一个 token 。 例如 ： New   York   Times ；     但是 ： this   is ， 没有 作为 一个 token ！       CBOW   论文         Mikolov ,   T .   ,   Chen ,   K . ,   Corrado ,   G . ,   & amp ;   Dean ,   J .   ( 2013 ) .   Efficient   estimation   of   word   representations   in   vector   space .   arXiv   preprint   arXiv : 1301.3781 .       周围 词加 和 预测 中心词 ， Hierarchical   Softmax   哈夫曼 树 ！       Glove       Pennington ,   J . ,   Socher ,   R . ,   & amp ;   Manning ,   C .   D .   ( 2014 ,   October ) .   Glove :   Global   Vectors   for   Word   Representation .   In   EMNLP   ( Vol .   14 ,   pp .   1532 - 43 ) .           square   root   type   transformation   in   the   form   of   Hellinger   PCA   ( HPCA )   ( Lebret   and   Collobert ,   2014 )       Mnih   and   Kavukcuoglu   ( 2013 )   also   proposed   closely - related   vector   log - bilinear   models ,   vLBL   and   ivLBL ,   and   Levy   et   al .   ( 2014 )   proposed   explicit   word   embed -   dings   based   on   a   PPMI   metric .           核心思想 ， 直接 建模 共生 矩阵 ！ （ skip - gram ， CBOW 是 直接 建模 上下文 ！ 不能 利用 全局 统计 信息 ？ ）     建模 概率 比率 ， 而 不是 建模 概率 本身 ！       $ $     F ( w _ i ,   w _ j ,   \ \ hat { w } _ k )   =   \ \ frac { P _ { ik } } { P _ { jk } }     $ $       希望 学习 到 线性关系 ？ ！       $ $     F ( w _ i ,   w _ j ,   \ \ hat { w } _ k )   =   F ( w _ i   -   w _ j ,   \ \ hat { w } _ k )   \ \ \ \       =   F (   ( w _ i   -   w _ j ) ^ T   \ \ hat { w } _ k )   =   \ \ frac { P _ { ik } } { P _ { jk } }     $ $       考虑 对称性 ， 即将 共生 矩阵 行列 对换 ， 需要 保持 不变性 ！ 那么 要 F 是 群 ( R ,   + ) 到 ( R + ,   x ) 的 同态 映射       $ $     F ( ( w _ i   -   w _ j ) ^ T   \ \ hat { w } _ k )   =   \ \ frac { F ( w _ i ^ T   \ \ hat { w } _ k ) } { F ( w _ j ^ T   \ \ hat { w } _ k ) }     $ $       因此 ， F 是 指数函数 ！       进而 要求 具有 交换 对称性 ， 可以 增加 bias 实现       $ $     w _ i ^ T   \ \ hat { w } _ k   +   b _ i   +   \ \ hat { b } _ k   =   \ \ log ( X _ { ik } )     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/word2vec.html"},
      
      
      {"title": "xgboost", "text": "    Table   of   Contents           关于           xgboost   论文 导读           introduction           Tree   bossting   in   a   nutsell           分裂 点 寻找 算法           系统 设计                   个人 注解           GBDT 和 XGBOOST 的 联系                   算法 细节           源码 实现           TODO           reference                 关于       xgboost   据说 是 现在 大 数据 竞赛 冠军队 的 标配 ！       xgboost   论文 导读       三个 关键点     -   large - scale     -   sparsity - aware   algorithm   for   sparse   data     -   weighted   quantile   sketch   for   approximate   tree   learning       introduction           机器 学习 和 data - driven 方法 的 成功 依赖于 两 方面 的 发展 ： 1 ， 有效 的 （ 统计 ） 模型   2 ， scalable   算法       tree   boosting   算法       P .   Li .   Robust   Logitboost   and   adaptive   base   class   ( ABC )     Logitboost .   In   Proceedings   of   the   Twenty - Sixth   Conference     Annual   Conference   on   Uncertainty   in   Artificial   Intelligence     ( UAI ’ 10 ) ,   pages   302 – 311 ,   2010 .       C .   Burges .   From   ranknet   to   lambdarank   to   lambdamart :     An   overview .   Learning ,   11 : 23 – 581 ,   2010 .       X .   He ,   J .   Pan ,   O .   Jin ,   T .   Xu ,   B .   Liu ,   T .   Xu ,   Y .   Shi ,     A .   Atallah ,   R .   Herbrich ,   S .   Bowers ,   and   J .   Q .   n .   Candela .     Practical   lessons   from   predicting   clicks   on   ads   at   facebook .     In   Proceedings   of   the   Eighth   International   Workshop   on     Data   Mining   for   Online   Advertising ,   ADKDD ’ 14 ,   2014 .         Netflix   price     J .   Bennett   and   S .   Lanning .   The   netflix   prize .   In     Proceedings   of   the   KDD   Cup   Workshop   2007 ,   pages   3 – 6 ,     New   York ,   Aug .   2007 .               Kaggle   2015   年 29 个 比赛 中 ， TOP3 队伍 中有 17 个用 了 XGBoost ， 其中 8 个用 XGBoost 直接 预测 ， 而 另外 的 用 XGBoost 和 神经网络 进行 集成 。 而 DNN 居然 只有 11 个 ！       KDDCup   2015   TOP10   队伍 都 用 了 XGBoost ！ ！       各种 比赛 中 已 解决 的 任务 包括 ：   store   sales     prediction ;   high   energy   physics   event   classification ;   web   text     classification ;   customer   behavior   prediction ;   motion   detection ;     ad   click   through   rate   prediction ;   malware   classification ;     product   categorization ;   hazard   risk   prediction ;   massive   online     course   dropout   rate   prediction .           XGBoost   创新 点 在于 ：           a   novel   tree   learning   algorithm   is   for   handling   sparse   data ;       a   theoretically   justified   weighted     quantile   sketch   procedure   enables   handling   instance   weights     in   approximate   tree   learning       Parallel   and   distributed   computing     makes   learning   faster   which   enables   quicker   model   exploration       还 可以 使用 out - of - core 计算 ， 使得 在 单机 就 可以 处理 上 亿 （ hundred   million ） 样本                   现有 的 并行 的   tree   boosting   算法 有 ：           B .   Panda ,   J .   S .   Herbach ,   S .   Basu ,   and   R .   J .   Bayardo .     Planet :   Massively   parallel   learning   of   tree   ensembles   with     mapreduce .   Proceeding   of   VLDB   Endowment ,     2 ( 2 ) : 1426 – 1437 ,   Aug .   2009 .       S .   Tyree ,   K .   Weinberger ,   K .   Agrawal ,   and   J .   Paykin .     Parallel   boosted   regression   trees   for   web   search   ranking .   In     Proceedings   of   the   20th   international   conference   on   World     wide   web ,   pages   387 – 396 .   ACM ,   2011 .       J .   Ye ,   J . - H .   Chow ,   J .   Chen ,   and   Z .   Zheng .   Stochastic     gradient   boosted   distributed   decision   trees .   In   Proceedings     of   the   18th   ACM   Conference   on   Information   and     Knowledge   Management ,   CIKM   ’ 09 .                   尚未 解决 的 问题 是 ： out - of - core   computation ,     cache - aware   and   sparsity - aware   learning     -   大神 解决 的 几个 方案 ， 后面 再 膜拜             -   T .   Chen ,   H .   Li ,   Q .   Yang ,   and   Y .   Yu .   General   functional             matrix   factorization   using   gradient   boosting .   In   Proceeding             of   30th   International   Conference   on   Machine   Learning             ( ICML ’ 13 ) ,   volume   1 ,   pages   436 – 444 ,   2013 .             -   T .   Chen ,   S .   Singh ,   B .   Taskar ,   and   C .   Guestrin .   Efficient             second - order   gradient   boosting   for   conditional   random             fields .   In   Proceeding   of   18th   Artificial   Intelligence   and             Statistics   Conference   ( AISTATS ’ 15 ) ,   volume   1 ,   2015 .       Tree   bossting   in   a   nutsell           正则 化 目标 函数           回归 树 的 数学 表示 如下 ， q 是 一个 将 特征向量 x 映射 到 树 的 叶子 节点 ， T 是 叶子 结点 个数 。     每 一个 叶子 结点 对应 一个 连续 值 $ ( w _ i ) $ ， 输出 的 是 q 映射 的 那个 叶子 结点 的 值 。       $ $     F   =   { f ( x )   =   w _ { q ( x ) }   }   ( q   :   R ^ m   →   { 1 , 2 , ... , T } ,   w   ∈   R ^ T )     $ $       树 ensemble 之后 的 输出 是 融合 每 一棵树 的 结果 后 的 输出 ( 直接 求和 ！ ！ ？ ？ )       $ $     \ \ hat { y }   =   \ \ phi ( x _ i )   =   \ \ sum _ { k = 1 } ^ K   f _ k ( x _ i ) ,   f _ k   \ \ in   F     $ $       添加 正则 项后 的 目标 函数 为       $ $     L ( \ \ phi )   =   \ \ sum _ i   l ( y _ i ;   \ \ hat { y } _ i )   +   \ \ sum _ k   \ \ Omega ( f _ k )     \ \ \ \     where   \ \ Omega ( f )   =   \ \ gamma   T   +   \ \ frac { 1 } { 2 }   \ \ lambda   | | w | | ^ 2     $ $       这个 损失 函数 也 在 Regularized   greedy   forest   ( RGF )     model   出现 过 ， 参看 这 篇文章       T .   Zhang   and   R .   Johnson .   Learning   nonlinear   functions     using   regularized   greedy   forest .   IEEE   Transactions   on     Pattern   Analysis   and   Machine   Intelligence ,   36 ( 5 ) ,   2014 .       上面 的 目标 函数 比   RGF   模型 简单 ， 更 容易 并行处理 ？ ！ ！     传统 的 GBM 模型 没有 正则 项 ！           Gradient   Tree   Boosting ， 目标 函数 通过 顺序 加树 进行 优化 ， 在 第 t 额颗 树 ，           $ $     L ^ t   =   \ \ sum _ { i = 1 } ^   l ( y _ i ,   \ \ hat { y } _ i   +   f _ t ( x _ i ) )   +   \ \ Omega ( f _ t )     $ $       将 损失 函数 展开 到 二阶 项 ， 丢掉 常数 项后       $ $     \ \ hat { L } ^ t   =   \ \ sum _ { i = 1 } ^   [ g _ i   f _ t ( x _ i )   +   \ \ frac { 1 } { 2 }   h _ i   f _ i ^ 2 ( x _ i ) ]   +   \ \ Omega ( f _ t )     $ $       例如 ， 损失 函数 取为       $ $     l ( y _ i ,   \ \ hat { y } _ i )   =   ( y _ i   -   \ \ hat { y } _ i ) ^ 2     $ $       那么 ， 对应 的 梯度 和 二阶 导为       $ $     g _ i   =   - 2 ( y _ i   -   \ \ hat { y } _ i )   =   - 2   e _ i     \ \ \ \     h _ i   =   2     $ $       定义 样本 集合 $ ( I _ j   =   {   i   |   q ( x _ i )   =   j   } ) $ ， 即 到达 第 j 个 叶子 结点 的 样本 集合 。     那么 损失 函数 可以 改写 为 对 第 t 颗树 的 叶子 结点 求和 ， 下面 的 权值 w 也 是 指 第 t 颗树 的       $ $     \ \ hat { L } ^ t   =   \ \ sum _ { j = 1 } ^ T   [ ( \ \ sum _ { i   \ \ in   I _ j }   g _ i )   w _ j   +   \ \ frac { 1 } { 2 }   ( \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda )   w _ j ^ 2 ]   +   \ \ gamma   T     $ $       从 上式 可以 求得 在 给定 的 q 函数 下 ， 最佳 的 权值 为       $ $     w _ j ^ *   =   -   \ \ frac { \ \ sum _ { i   \ \ in   I _ j }   g _ i } { \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda }     $ $     对应 的 最优 目标 函数 为       $ $     \ \ hat { L } ^ t ( q )   =   -   \ \ frac { 1 } { 2 }   \ \ sum _ { j = 1 } ^ T   \ \ frac { ( \ \ sum _ { i   \ \ in   I _ j }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ j }   h _ i   +   \ \ lambda }   +   \ \ gamma   T     $ $     这个 值 可以 作为 q 函数 的 score 来 评估 树 的 结构 ， 作用 和 CART 的 不 纯度 gini 系数 一样 。     理论 上 来说 需要 遍历 所有 可能 的 树 ， 实际上 用 启发式 的 方法 ， 从单 叶子 节点 的 树 开始 ， 然后 添加 分支 。     设 分裂 前 的 样本 集为 I ， 分裂 后 左右 子树 的 样本 集 分别 为 $ ( I _ L ,   I _ R ) $ ， 那么 分裂 带来 的 损失 函数 减少 量 为       $ $     L _ { split }   =   \ \ frac { 1 } { 2 }   (   \ \ frac { ( \ \ sum _ { i   \ \ in   I _ L }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ L }   h _ i   +   \ \ lambda }     +     \ \ frac { ( \ \ sum _ { i   \ \ in   I _ R }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I _ R }   h _ i   +   \ \ lambda }   -   \ \ frac { ( \ \ sum _ { i   \ \ in   I }   g _ i ) ^ 2   } { \ \ sum _ { i   \ \ in   I }   h _ i   +   \ \ lambda } )   -   \ \ gamma     $ $     就 像 C4 . 5   和   CART   的 信息 增益 率 和 gini 系数 增加量 那样 ， 作为 该 分裂 点 的 score ， 用来 确定 分裂 点 是否 最优 。           Shrinkage   and   Column   Subsampling     这 两种 技巧 用来 防止 过 拟合           shrinkage ：     J .   Friedman .   Stochastic   gradient   boosting .   Computational     Statistics   & amp ;   Data   Analysis ,   38 ( 4 ) : 367 – 378 ,   2002 .       shrink   将 新 加入 的 权值 乘上 一个 系数 $ ( \ \ eta ) $ ， 为 后面 的 树 提供 一定 的 学习 空间 。       列 采样 来自 随机 森林 ：     L .   Breiman .   Random   forests .   Maching   Learning ,     45 ( 1 ) : 5 – 32 ,   Oct .   2001       列 采样 之前 没 在 Boosting 里面 用过 ， 据说 比行 采样 效果 要 好 。 我 的 理解 是 ， 列 采样 导致 每个 树 学习 的 多样化 ， 行 采样 也 会 有 ， 但是 会少 很多 。     另一方面 ， 也 为 算法 并行 化 提供 了 好处 。       分裂 点 寻找 算法           Basic   Exact   Greedy   Algorithm           在 每 一次 寻找 中 ， 枚举 所有 可能 的 分裂 点 ， 然后 利用 score 确定 最佳 分裂 点 。     代表 的 实现 软件 有 ： sklearn ，   R 的 GBM ，   单机版 的 XGBoost 。     算法 首先 对 特征 进行 排序 ， 然后 依次 访问 数据 ， 并 以此 数据 该维 特征 的 值 作为 分裂 点 ， 计算 score 。           近似 方法     精确 寻找 不 适用 与 分布式 数据 ， 近似 方法 通过 特征 的 分布 ， 按照 百分比 确定 一组 候选 分裂 点 ， 通过 遍历 所有 的 候选 分裂 点来 找到 最佳 分裂 点 。     两种 策略 ： 全局 策略 和 局部 策略 。 在 全局 策略 中 ， 对 每 一个 特征 确定 一个 全局 的 候选 分裂 点 集合 ， 就 不再 改变 ； 而 在 局部 策略 中 ， 每 一次 分裂     都 要 重选一次 分裂 点 。 前者 需要 较大 的 分裂 集合 ， 后者 可以 小 一点 。 论文 中 对比 了 补充 候选 集 策略 与 分裂 点 数目 对模型 的 影响 。     全局 策略 需要 更细 的 分裂 点 才能 和 局部 策略 差不多 。               什么 意思 ：               Notably ,   it   is   also   possible     to   directly   construct   approximate   histograms   of   gradient     statistics   [ 19 ]               [ 19 ]   S .   Tyree ,   K .   Weinberger ,   K .   Agrawal ,   and   J .   Paykin .     Parallel   boosted   regression   trees   for   web   search   ranking .   In     Proceedings   of   the   20th   international   conference   on   World     wide   web ,   pages   387 – 396 .   ACM ,   2011               Weighted   Quantile   Sketch   算法     对 第 k 个 特征 ， 构造 数据 集           $ $     D _ k =   { ( x _ { 1k } ,   h _ 1 )   ,   ( x _ { 2k } , h _ 2 )   ,   ... , ( x _ { nk } , h _ n )   }     $ $     其中 $ ( h _ i ) $ 是 该 数据 点 对应 的 损失 函数 的 二阶 梯度 。 二阶 梯度 在 这里 相当于 样本 的 权值 ， 目标 函数 可以 看做 一个 带权 的 均 方 误差 ( 通过 近似 ， 将 所有 凸函数 形式 的 目标 函数 都 变成 了 和 最小 均方 误差 一样 了 ) 。     重新 改写 目标 函数 为       $ $     \ \ sum _ { i = 1 } ^ n   \ \ frac { 1 } { 2 }   h _ i ( f _ t ( x _ i )   -   g _ i   /   h _ i ) ^ 2   +   \ \ Omega ( f _ t )   +   constant     $ $     定义 序 函数 为 带权 的 序 函数       $ $     r _ k ( z )   =   \ \ frac { 1 } { \ \ sum _ { ( x , h )   \ \ in   D _ k   }   h }   \ \ sum _ { ( x , h )   \ \ in   D _ k ,   x & lt ; z }   h     $ $     它 代表 第 k 个 特征 小于 z 的 样本 比例 （ 带权 的 ） 。 候选 集 的 目标 要 使得 相邻 两个 候选 分裂 点 相差 不 超过 某个 值 $ ( \ \ epsilon ) $ 。       样本 权值 相同 的 时候 ，   quantile   sketch   算法 可以 找到 这些 分裂 点 ：           M .   Greenwald   and   S .   Khanna .   Space - efficient   online     computation   of   quantile   summaries .   In   Proceedings   of   the     2001   ACM   SIGMOD   International   Conference   on     Management   of   Data ,   pages   58 – 66 ,   2001 .       Q .   Zhang   and   W .   Wang .   A   fast   algorithm   for   approximate     quantiles   in   high   speed   data   streams .   In   Proceedings   of   the     19th   International   Conference   on   Scientific   and   Statistical     Database   Management ,   2007 .           对于 带权 的 ， 目前 都 是 通过 对 随机 抽取 的 子集 进行 排序 得到 的 。 缺点 ： 没有 理论 保证 ， 也 存在 一定 错误 概率 。       作者 提出 的 一种   分布式 带权   quantile   sketch   算法 ， 有 概率 上 的 理论 保证 。 在 附录 里面 有 详细 介绍 。           Sparsity - aware   Split   Finding     为了 发现 稀疏 数据 里面 的 模式 ， 为 每 一个 树 的 节点 提供 一个 默认 的 方向 。 如果 该 特征 缺失 ， 就 以 默认 的 方向向 树 的 底部 移动 。     （ 这 不是 相当于 为 空值 人为 地 填充 了 一个 值 补全 么 ？ ） 这个 方向 是 学习 得到 的 ！ （ 好 吧 ， 怎么 学 的 ？ ）     算法 让 所有 缺失 值 的 样本 首先 全部 走到 右子 树 ， 然后 在 非 缺失 值 样本 上 迭代 ， 依次 选取 不同 分裂 点求 出 最佳 score ， 相当于 missing   value   全部 用 最大值 填充 ，     接着 右 让 缺失 值 全部 走 左子 树 ， 然后 依次 选取 不同 分裂 点求 出 最佳 score ， 相当于 missing   value   全部 用 最小值 填充 ，     经过 两次 遍历 后 ， 选出 最佳 score ， 相当于 比 传统 的 方式 多 遍历 一次 ？ ！ （ 那 为什么 速度 还 比 传统 的 快 呢 ？ ）     算法 随非 缺失 值 样本 数目 现行 增长 ， 因为 它 只 在 非 缺失 值 样本 上 迭代 。     在 Allstate - 10K   dataset 上 ， 比 naive 的 算法 快 50 倍 ！           算法 详细 ， 请 看 论文 。       系统 设计           Column   Block   for   Parallel   Learning             最 耗时 的 地方 在于 对 样本 排序 ， 为了 减少 这部分 时间 ， 将 数据 保存 在 内存 单元 block 中 。     在 block 中 ， 数据 以 compressed   column   ( CSC )   保存 ， 每 一列 按照 该列 对应 的 特征 进行 排序 。     因此 ， 这种 数据 只 需要 计算 一次 ， 就 可以 被 反复 使用 。           此外 可以 同时 对 所有 的 叶子 结点 执行   split   finding   算法 ， 寻找 最优 分裂 点 。           什么 意思 ：         We   do   the   split     finding   of   all   leaves   collectively ,   so   one   scan   over   the   block     will   collect   the   statistics   of   the   split   candidates   in   all   leaf   branches           这种 结构 对 近似 搜索算法 也 有用 ， 可以 使用 多个 block ， 每 一个 block 对应 一个 行 的 子集 ，     不同 的 block 还 可以 在 不同 的 机器 上 ， 或者 保存 在 磁盘 上 实现 out - of - core 计算 。       对 每 一列 的 统计 可以 并行 ， 这 导致 了 split   finding   的 并行算法 。     这种 结构 也 支持 列 采样 。       这个 结构 还 没 搞懂 ， 可能 需要 看 一下 代码 。 。 。 。 。 待续           Cache - aware   Access               因为 需要 访问 每行 的 梯度 统计 ， 这种 结构 导致 内存 的 不 连续 访问 ， 这会 使得 CPU   cache 命中率 降低 ，     而 降低 算法 的 运行 速度 ！ （ 靠 ！ 这 都 考虑 到 了 ， 牛 逼 ） ， 需要 合理 选择 block 的 大小 。           216   examples   per   block   balances   the     cache   property   and   parallelization .           Blocks   for   Out - of - core   Computation                 为了 使得 核外 计算 可能 ， 将 数据 分为 多个 block ， 保存 到 磁盘 。     在 计算 的 过程 中 ， 并行 地用 另外 的 线程 将 数据 从 磁盘 预取 到 内存 缓存 中 。     但是 由于 IO 通常 会 花费 更 多 时间 ， 简单 地 预取 还是 不够 ， 我们 采用 下面 两种 技巧 来 优化 ：       Block   Compression ， block 按照 列 压缩 ， 然后 在 读取 的 时候 ， 用 另外 的 线程 解压 。 对于 行 索引 ， 保存 于 block 初始 索引 的 差值 ， 16bit 整数 保存 。       Block   Sharding ，                   个人 注解       GBDT 和 XGBOOST 的 联系       从 损失 函数 来看 ， GBDT 相当于 $ ( H = 2 ,   \ \ lambda   =   0 ,   \ \ gamma   =   0 ) $   的 特殊 情形 ，     此外 陈天奇 对 XGBOOST 并行 实现 的 优化 也 很 牛 ！       算法 细节       源码 实现         dmlc             xgboost 的 spark 版本 是 对 每 一个 分区 单独 训练 ？ ？ ？ ？ 没看 懂                   partitionedData     .     mapPartitions       {                   trainingSamples       = & gt ;                       rabitEnv     .     put     (     & quot ; DMLC _ TASK _ ID & quot ;     ,       TaskContext     .     getPartitionId     ( ) .     toString     )                       Rabit     .     init     (     rabitEnv     .     asJava     )                       var       booster     :       Booster       =       null                       if       (     trainingSamples     .     hasNext     )       {                           val       cacheFileName     :       String       =       {                               if       (     useExternalMemory       & amp ; & amp ;       trainingSamples     .     hasNext     )       {                                   s & quot ;     $ appName     - dtrain _ cache -     $ {     TaskContext     .     getPartitionId     ( )     }     & quot ;                               }       else       {                                   null                               }                           }                           val       trainingSet       =       new       DMatrix     (     new       JDMatrix     (     trainingSamples     ,       cacheFileName     ) )                           booster       =       SXGBoost     .     train     (     trainingSet     ,       xgBoostConfMap     ,       round     ,                               watches       =       new       mutable     .     HashMap     [     String   ,     DMatrix     ]       {                                   put     (     & quot ; train & quot ;     ,       trainingSet     )                               } .     toMap     ,       obj     ,       eval     )                           Rabit     .     shutdown     ( )                       }       else       {                           Rabit     .     shutdown     ( )                           throw       new       XGBoostError     (     s & quot ; detect   the   empty   partition   in   training   dataset ,   partition   ID : & quot ;       +                               s & quot ;       $ {     TaskContext     .     getPartitionId     ( ) .     toString     }     & quot ;     )                       }                       Iterator     (     booster     )               } .     cache     ( )                 TODO           了解 一下 Kaggle   2015 年 TOP3 队伍 解决方案           reference             XGBoost :   A   Scalable   Tree   Boosting   System           chen   tianqi   slide           github   dmlc   xgboost           Greedy   function   approximation :   a   gradient   boosting   machine         The   present   and   the   future   of   the   kdd   cup   competition :   an   outsider ’ s   perspective .      ", "tags": "machine-learning", "url": "/wiki/machine-learning/xgboost.html"},
      
      
      {"title": "【2018-Airbnb】Applying Deep Learning To Airbnb Search", "text": "    Table   of   Contents           关于           摘要 与 导言           模型 演化           SimpleNN           Lambdarank   NN           Decision   Tree / Factorization   Machine   NN           Deep   NN                   错误 的 模型           Listing   ID           Multi - task   learning                   特征 工程           系统工程           超 参数           特征 重要性           参考                 关于           论文 :   Applying   Deep   Learning   To   Airbnb   Search           摘要 与 导言           排序 候选 集 只有 几千个       算法 迭代 :       手工 打分 函数       GBDT ( 很大 提升 , 然后 饱和 )       NN               模型 预测 目标       房主 接受 顾客 预订 的 概率       顾客 给 这段 经历 打 5 分 的 概率       也 是 目前 重要 考虑 的 点 :   顾客 会 订购 的 概率               搜索 的 特点       用户 会 搜索 多次       点击 一些 listing 去 查看 详情       最终 订购               新 模型 通过 老 的 模型 部署 后 产生 的 线 上 数据 进行 训练 ,   得到 一个 打分 函数       特征 工程 ,   系统工程 ,   超 参数 搜索           模型 演化                   评估 指标   NDCG       订购 的 相关性 为 1 ,   其他 都 是 0       左图 显示 了 不同 模型 的 离线 NDCG 上 的 收益 ,   SimpleNN 没有 GBDT 好 ,   DeepNN 最好           SimpleNN           单隐层 ,   32 个 隐层 节点 ,   ReLU 激活 函数 ,   击败 了 GBDT ,   相同 的 特征 ,   最小化   L2   回归 损失 函数 ,   1 订购 ,   0 未 订购   ? ? ?       作用 :   用于 验证 pipeline 和 线 上 系统 正确性           Lambdarank   NN           将   Lambdarank   的 思想 应用 到 NN 上 , 直接 上 代码 了 (   后面 根据 这个 复现 一下   )                           def       apply _ discount     (     x     ) :               &# 39 ; &# 39 ; &# 39 ; Apply   positional   discount   curve &# 39 ; &# 39 ; &# 39 ;                 return       np     .     log     (     2.0     )     /     np     .     log     (     2.0       +       x     )         def       compute _ weights     (     logit _ op     ,       session     ) :                 &# 39 ; &# 39 ; &# 39 ; Compute   loss   weights   based   on   delta   ndcg .               logit _ op   is   a   [ BATCH _ SIZE ,   NUM _ SAMPLES ]   shaped   tensor   corresponding   to   the   output   layer   of   the   network .               Each   row   corresponds   to   a   search   and   each               column   a   listing   in   the   search   result .   Column   0   is   the   booked   listing ,   while   columns   1   through               NUM _ SAMPLES   -   1   the   not - booked   listings .   &# 39 ; &# 39 ; &# 39 ;               logit _ vals       =       session     .     run     (     logit _ op     )                 ranks       =       NUM _ SAMPLES       -       1       -       logit _ vals     .     argsort     (     axis     =     1     )     .     argsort     (     axis     =     1     )                 discounted _ non _ booking       =       apply _ discount     (     ranks     [ : ,       1     : ] )                 discounted _ booking       =       apply _ discount     (     np     .     expand _ dims     (     ranks     [ : ,       0     ] ,       axis     =     1     ) )                 discounted _ weights       =       np     .     abs     (     discounted _ booking       -       discounted _ non _ booking     )                 return       discounted _ weight         #   Compute   the   pairwise   loss       pairwise _ loss       =       tf     .     nn     .     sigmoid _ cross _ entropy _ with _ logits     (                 targets     =     tf     .     ones _ like     (     logit _ op     [ : ,       0     ] ) ,                 logits     =     logit _ op     [ : ,       0     ]       -       logit _ op     [ : ,       i     : ]       )       #   Compute   the   lambdarank   weights   based   on   delta   ndcg       weights       =       compute _ weights     (     logit _ op     ,       session     )       #   Multiply   pairwise   loss   by   lambdarank   weights       loss       =       tf     .     reduce _ mean     (     tf     .     multiply     (     pairwise _ loss     ,       weights     ) )                 Decision   Tree / Factorization   Machine   NN                   在 这个 阶段 主要 线上 模型 是 NN ,       同时 :       用 GBDT 模型 构造 训练 集   Iterations   on   the   GBDT   model   with   alternative   ways   to   sample   searches   for   constructing   the   training   data .       用 FM 学习 query 和 listing 的 相关性 ,   用 32 维 向量               将 GDBT 输出 的 叶子 节点 作为 类别 特征 ,   embedding 输入 到 NN ,   FM 输出 的 相关性 得分 直接 输入 NN           Deep   NN           数据 集 扩大 10 倍 ,   17 亿对 样本 对 ,   NN 的 隐层 数目 增加 到 2 层       特征 :       价格 ,   环境 ( amenities ) ,   历史 订购 次数 ,   etc       智能 定价 价格       listing 和 该 用户 历史 看过 的 listing 的 相似 度   ( 就是 KDD2018   best   paper 那 篇文章 )                   错误 的 模型           两个 流行 的 方法 , 但是 实际 没有 效果           Listing   ID           将   listing   id 作为 特征 ,   然后   embedding ,   输入 到 NN       试 了 很多 版本 ,   加入   listing   id   很 容易 过 拟合       原因 是 listing 订购 数据 太 稀疏 了 , 即使 一年 也 只有 365 次 , 更 不用说 那些 不 热门 的 listing 了           Multi - task   learning           用 长时间 浏览 作为 辅助 任务 , 做 MTL 。       log ( view   duration ) 作为 权重 ?       试验 结果 发现 对 长时间 浏览 任务 有 较大 帮助 , 但是 对 订购 没有 帮助           Xing   Yi ,   Liangjie   Hong ,   Erheng   Zhong ,   Nanthan   Nan   Liu ,   and   Suju   Rajan .   2014 .   Beyond   Clicks :   Dwell   Time   for   Personalization .   In   Proceedings   of   the   8th   ACM   Conference   on   Recommender   Systems   ( RecSys   ’ 14 ) .   ACM ,   New   York ,   NY ,   USA ,       特征 工程           对于 NN ,   是 为了 让 特征 具有 某些 特殊 的 性质 ,   让 NN 能够 学到 复杂 的 计算 逻辑       特征 归一化 ,   均值 方差 归一化 ;     对于 幂率 分布 ,   用 $ ( log ( \ \ frac { 1   +   v } { 1   +   median } ) ) $ 归一化 。 为什么 不用 cdf ?                       数据 不是 平滑 的 分布         Spotting   bugs     大规模 的 数据 中 难免 有 少数 bug 数据 , 简单 的 范围 限制 的 方法 , 只能 找到 一部分 , 还有 一部分 可以 通过 平滑 分布 找到 。 例如 , 对于 某 地区 的 价格 , log 之后 的 分布图 中 哪些 尖锐 的 值 很 有 可能 就是 bug 数据         Facilitating   generalization     DNN 每 一层 的 输出 的 分布 越来越 平滑 , ( 下图 ,   log ( 1   +   relu _ output ) ) 。 作者 认为 , 底层 的 输出 分布 越 平滑 , 上层 神经元 越能 泛化 到 未知 数据 中 。 作者 通过 抖动 测试 印证 了 这 一点 ,   通过 将 测试 集中 的 所有 样本 的 某个 特征 放到 2 倍 , 3 倍 , 观察 NDCG 的 变化 , 发现 观察 到 前所未有 的 稳定性 。 因此 , 作者 认为 要 尽可能 保证 输入 的 特征 分布 的 平滑 。 绝大多数 特征 都 可以 通过 修复 bug + 合适 的 变换 得到 平滑 的 分布 , 还有 一部分 是 需要 特殊 的 特征 工程 的 , listing 用 经纬度 表示 的 地理位置 。 经纬度 的 原始 值 是 不 平滑 的 分布 ( 图 11a   和   图 11b ) , 作者 使用 了 经纬度 的 相对 偏移 , 将 经纬度 的 原点 放在 用户 看到 的 地图 中心 , 用 相对 经纬度 代替 经纬度 , 分布 就 平滑 很多 ( 图 11c   和   图 11   e ) , 作者 还 对 相对 经纬度 取 log 得到 新 的 一组 平滑 分布 特征 。         Checking   feature   completeness     用 listing 的 未来 可 订购 天数 作为 特征 ,   但是 原始 的 入住 天数 分布 不 平滑 ( 图 12   a ) 。 通过 调研 , 作者 发现 另外 一个 影响 因素 :   listing 有 最小 停留时间 要求 , 有 一些 要求 至少 一个月 ! 因此 , 他们 有 不同 的 入住率 , 但是 我们 又 不能 将 这个 作为 特征 放进去 , 因为 它 跟 日期 有关 而且 也 太 复杂 了 。 作者 添加 了 平均 入住 时 长 作为 特征 , 一旦 入住 天数 用 平均 入住 时 长 归一化 后 , 它们 的 比值 竟然 具有 平滑 的 分布 了 ( 图 12   b ) ! !                                             高维 类别 特征       对 附近 城市 的 偏好 是 一个 重要 的 位置 信息 , 然而 在 GDBT 中 需要 耗费 很多 力气 来 做 这特 特征 , 而且 还 没有 考虑 价格 等 关键因素 。 在 DNN 中 , 直接 将 查询 的 城市 与 listing 的 第 12 级 S2   cell 一起 , hash 到 一个 整数 即可 。 例如 , query 城市 为 「 San   Francisco 」 , listing 在 Embarcadero 附近 , 对应 的 S2   cell 是 539058204 ,   hash ( { \" San   Francisco \" ,   539058204 } )   =   71829521   作为 输入 DNN 的 类别 特征 。 我 的 理解 是 , 其实 就是 「 两个 离散 特征 交叉 得到 另外 一个 更 高维 的 离散 特征 」 嘛 !       下图 可视化 了 query = San   Francisco 时 , 不同 位置 的 embedding   values 的 大小 ( 取模 ? ) , 可以 看到 , 不仅 San   Francisco 附近 的 值 很 高 , 而且 在 一些 靠南 的 地方 也 很 高 。                               S2     https : / / s2geometry . io /             系统工程           目的 : 加速 模型 训练 和 在线 打分           系统 架构 :           Java 服务 提供 查询 接口 和 打分 ,   该 服务 同时 记录 日志 , 日志 格式 使用 序列化 后 的   Thrift   对象       日志 通过 Spark 处理 , 得到 训练 数据       模型 训练 使用 的 是   TensorFlow       用 Scala   和   Java 实现 了 一些 工具 评估 模型 和 计算 离线 指标       然后 , 模型 上 传到 服务 中 实现 检索 和 打分       所有 的 都 运行 在 AWS 上                   一 开始 用 的 是 CSV 格式 作为 训练 集 ( 迁移 自 GBDT ) , 通过     feed _ dict     将 数据 喂给 GPU , GPU 使用率 只有 25 % ,   后来 改为   protobuf   和   DataSet     方式 , 速度 快 了 17 倍 , GPU 使用率 也 达到 了 90 % 。           listing 很多 特征 都 是 静态 的 , 每次 读取数据 会 有 大量 磁盘 读取 , 作者 将 这些 特征 作为 listing   id 的 embedding 向量 , 而用 listing   id 作为 输入 特征 , 和 普通 的 embedding 向量 不同 的 是 , 这个 向量 在 训练 的 过程 中 保持 不变 。 相当于 用 内存 做 cache 。       Java   NN   lib ,   在 2017 年 早期 的 时候 , 还 没有 好用 的 Java   NN 打分 的 库 , 作者 自己 写 了 一个 , 降低 了 延时 。 话 说 现在 能 通过 JNI 的   TensorFlow   Java   API 了           超 参数           Dropout   没 啥 用       NN 权重 用   Xavier   初始化 ;   Embedding 用 [ - 1 ,   1 ] 均匀 初始化       Adam 默认 参数 就 很 好 了 , 现在 用 的 是     LazyAdam   , 在 大 embedding 时 更 快 一些       用 固定 的 batchsize = 200 , 对 训练 速度 影响 挺大 的           特征 重要性           GBDT 的 部分 依赖图     https : / / scikit - learn . org / stable / auto _ examples / ensemble / plot _ partial _ dependence . html         Ablation   Test   一次 去掉 一个 特征 ,   观察 对模型 性能 的 影响       Permutation   Test   选择 一个 特征 , 将 测试 集中 的 该 特征 在 整个 测试 集中 重新 随机 排列 , 观察 模型 在 测试 集上 效果 的 差异 。 越 重要 的 特征 , 这个 差异 就 越 大 。 但是 , 实际上 只能 证明 随机 排列 如果 对 效果 影响 不 大 , 特征 没 啥 效果 ; 但是 如果 影响 约 大 , 并 不能 说明 特征 约 重要 , 因为 随机 排列 生成 的 数据 在 真实 实际 并 不 存在 , 特征 间 并 不 独立 。       TopBot   Analysis   作者 自己 开发 的 一个 工具 ,   top - bottom   analyzer 。 对于 一个 query , 将 listing 排序 , 分析 某 一个 特征 在 TOP   listing 的 分布 和 Bottom 的 分布 , 分布 上 如果 有 明显 差异 , 则 说明 模型 对 这个 特征 比较 敏感 。 图 14 就是 一个 例子 , 说明 模型 对 price 比较 敏感 , 对 评论 数量 不 敏感 , 说明 模型 对 评论 数据 的 拟合 不 符合 预期 , 说明 在 评论 数据 使用 上 需要 进一步 研究 。                   参考             https : / / developers . google . com / machine - learning / guides / rules - of - ml /         Daria   Sorokina   and   Erick   Cantu - Paz .   2016 .   Amazon   Search :   The   Joy   of   Rank -   ing   Products .   In   Proceedings   of   the   39th   International   ACM   SIGIR   Conference   on   Research   and   Development   in   Information   Retrieval   ( SIGIR   ’ 16 ) .   459 – 460 .       Peng   Ye ,   Julian   Qian ,   Jieying   Chen ,   Chen - Hung   Wu ,   Yitong   Zhou ,   Spencer   De   Mars ,   Frank   Yang ,   and   Li   Zhang .   2018 .     Customized   Regression   Model   for   Airbnb   Dynamic   Pricing   .   In   Proceedings   of   the   24th   ACM   SIGKDD   Conference   on   Knowl -   edge   Discovery   and   Data   Mining .       Sebastian   Ruder .   2017 .   An   Overview   of   Multi - Task   Learning   in   Deep   Neural   Networks .   CoRR   abs / 1706.05098   ( 2017 ) .   arXiv : 1706.05098     http : / / arxiv . org / abs / 1706.05098         Xing   Yi ,   Liangjie   Hong ,   Erheng   Zhong ,   Nanthan   Nan   Liu ,   and   Suju   Rajan .   2014 .   Beyond   Clicks :   Dwell   Time   for   Personalization .   In   Proceedings   of   the   8th   ACM   Conference   on   Recommender   Systems   ( RecSys   ’ 14 ) .   ACM ,   New   York ,   NY ,   USA ,       Chiyuan   Zhang ,   Samy   Bengio ,   Moritz   Hardt ,   Benjamin   Recht ,   and   Oriol   Vinyals .   2017 . Understandingdeeplearningrequiresrethinkinggeneralization .   https :   / / arxiv . org / abs / 1611.03530      ", "tags": "machine-learning", "url": "/wiki/machine-learning/airbnb-dnn-rank-2018.html"},
      
      
      {"title": "【2018-Airbnb】Customized Regression Model for Airbnb Dynamic Pricing", "text": "    Table   of   Contents           摘要 与 间接           动态 价格 模型           需求 估计           价格 策略 模型                   定价 系统           Booking   Probability   Model           评估 建议 价格           Strategy   Model                   参考                 摘要 与 间接           3 个 模型 组成       每个 listing 订购 概率 ,   二 分类       自定义 损失 函数 的 回归 模型 , 拟合 最优 价格       额外 的 个性化 逻辑               产品 :       Price   Tips   价格 建议       Smart   Pricing   智能 价格                           动态 价格 模型       需求 估计           如果 能够 估计 出 需求 曲线 ( 需求 函数 )   $ ( F ( P )   ) $   P 是 价格 , 那么 , 最大化   $ ( P   F ( P ) ) $   即可 得到 最优 价格 P 。       在   Airbnb 场景 下 , 需求 还 跟 其他 因素 有关 , 所以 作者 估计 的 是 这样 一个 需求 函数   $ (   F ( P ,   t ,   id )   ) $   t 是 时间 , id 是 listing   id 。       时变 因素 :       季节 因素       订购 时间 与 入住 时间 的 距离 , 距离 越短 , 订购 成功 的 概率 越低               Listing - varying ,   也 就是 listing   是 唯一 的 , 可 替代性 比 酒店 弱           预估 的 价格 只有 部分 被 采用               需求 估计 相关 论文 :           PatrickBajari , DenisNekipelov , StephenP . Ryan , andMiaoyuYang . 2015 . Demand   estimation   with   machine   learning   and   model   combination .   National   Bureau   of   Economic   Research   ( 2015 ) .       H .   Varian .   2014 .   Big   data :   New   tricks   for   econometrics .   Journal   of   Economic   Perspectives   28 ,   2   ( 2014 ) ,   3   –   28 .                   价格 策略 模型           模型 :   GBM       目标 函数 :   自定义 函数 ,   借鉴   SVR ,   $ ( \ \ epsilon ) $   提供 一个 范围 区间 ,   认为 最优 价格 是 在 这个 区间 里面 , 因为 实际上 观察 不到 最优 价格           定价 系统       Booking   Probability   Model   - & gt ;   Strategy   Model   - & gt ;   Personalization       Booking   Probability   Model           预估 函数   Pro ( id ,   t ,   ctx ,   P )         特征       Listing   Features :   listing   price   per   night ,   room   type ,   person   capacity ,   the   number   of   bedrooms / bathrooms ,   amenities ,   locations ,   reviews ,   historical   occupancy   rate ,   instant   booking   enabled ,   etc .       Temporal   Features :   seasonality   ( the   day   of   the   year ,   day   of   the   week ,   etc ) ,   the   calendar   availability   ( e . g .   the   gap   between   check   in   and   check   out ) ,   distance   between   ds   and   ds _ night ,   etc .       Supply   and   demand   dynamics :   number   of   available   listings   in   the   neighborhood ,   listing   views ,   searches / contacts   rates ,   etc .               相比 所有 市场 用 一个 GBM , 每个 市场 单独 使用 一个 , 并 采用 不同 的 采样率 会 更好       预估 需求 曲线 的 问题       数据 稀疏 :   很多 listing 价格 不怎么 变 ,   价格 变化 的 listing 变动 也 很少 , 导致 探索 困难 。   也 就是 数据分布 不到 所有 空间 。   这个 也 是 我们 在 做 的 时候 遇到 的 问题       样本 的 唯一性 :   不 存在 同时 处于 两种 价格 的 listing ,   不同 的 listing 又 难以 互相 替代       特征 依赖 :   我们 希望 价格 P 和 其他 特征 是 独立 的 , 然而 很多 其他 特征 是 依赖 价格 的 ,   比如 订购 率 。   这 导致 需求 曲线 预估 的 精度 很差                   评估 建议 价格           无法 观测 到 最优 价格       但是 可以 观察 到 订购 的 样本 的 下界 ( 即 最优 价格 不 低于 订购 价格 ) 和 未 订购 样本 的 上界 ( 即 最优 价格 不 高于 未 订购 价格 )       订购 样本 的 上界 可以 估计 为 订购 价格   P   的 c1 倍 , c1 & gt ; 1 是 超 参数       未 订购 样本 的 下界 可以 估计 为 未 订购 价格   P   的   c2   倍 ,   c2 & lt ; 1 是 超 参数       损失 函数 为 区间 损失 , 即 在 这个 区间 , 损失 为 0 , 否则 损失 为 距离 这个 区间 的 距离           $ $     L   =   \ \ min   \ \ sum _ i   ( L ( P _ i ,   y _ i )   -   f _ { \ \ theta } ( x ) ) ^ +   +   ( f _ { \ \ theta } ( x )   -   U ( P _ i ,   y _ i ) ) ^ +     $ $       L   和   U   分别 代表 下界 和 上界       Strategy   Model       参考             https : / / www . kdd . org / kdd2018 / accepted - papers / view / customized - regression - model - for - airbnb - dynamic - pricing           https : / / zhuanlan . zhihu . com / p / 46288070        ", "tags": "machine-learning", "url": "/wiki/machine-learning/airbnb-dynamic-price.html"},
      
      
      {"title": "【KDD2018-Airbnb】Real-time Personalization using Embeddings for Search Ranking at Airbnb", "text": "    Table   of   Contents           摘要   & amp ;   导言           METHODOLOGY           Listing   Embeddings ( List 指 一个 项目 ? 还是 列表 ? )           User - type   & amp ;   Listing - type   Embeddings           试验                   问题                 摘要   & amp ;   导言           Airbnb   问题 的 特殊性       是 一个 双边 市场 , 既 需要 考虑 考虑 买家 的 体验 , 还 需要 考虑 卖家 的 体验       用户 很少 有 复购 行为               99 % 的 转化 来自 于 实时 搜索 排序 和 推荐       方法 : pairwise   regression ,   下单 的 作为 正例 , 拒绝 的 作为 负例         lambda   rank       联合 优化 买家 和 卖家 的 排序   ? ?       利用 listing   embeddings ,   low - dimensional   vector   representations 计算 用户 交互 的 list 与 候选 list 的 相似性 , 作为 个性化 特征 放到 排序 模型 中               利用 点击 等 行为 学习 短期 兴趣 ; 而 利用 下单 学习 长期 兴趣       问题 :   下 单行 为 过于 稀疏 ,   一个 用户 平均 一年 旅行 1 - 2 次 ;   长尾 用户 只有 一次 下单       解决 方法 :   不是 对 userid 做 embedding , 而是 在 用户 类型 维度 做 embedding , 类型 通过 多 对 一 的 规则 映射 得到                   论文 创新 点 :           实时 个性化                   相关 论文 :           Yahoo       MihajloGrbovic , NemanjaDjuric , VladanRadosavljevic , FabrizioSilvestri , Ri - cardo   Baeza - Yates ,   Andrew   Feng ,             Erik   Ordentlich ,   Lee   Yang ,   and   Gavin   Owens .   2016 .   Scalable   semantic   matching   of   queries   to   ads   in   sponsored             search   advertis -   ing .   In   SIGIR   2016 .   ACM ,   375 – 384 .       MihajloGrbovic , VladanRadosavljevic , NemanjaDjuric , NarayanBhamidipati ,   Jaikit   Savla ,   Varun   Bhagwan ,   and   Doug   Sharp .   2015 .             E - commerce   in   your   inbox :   Product   recommendations   at   scale .   In   Proceedings   of   the   21th   ACM   SIGKDD             International   Conference   on   Knowledge   Discovery   and   Data   Mining .       Dawei   Yin ,   Yuening   Hu ,   Jiliang   Tang ,   Tim   Daly ,   Mianwei   Zhou ,   Hua   Ouyang ,   Jianhui   Chen ,   Changsung   Kang ,             Hongbo   Deng ,   Chikashi   Nobata ,   et   al .   2016 .   Ranking   relevance   in   yahoo   search .   In   Proceedings   of   the   22nd   ACM   SIGKDD .               Etsy       Kamelia   Aryafar ,   Devin   Guillory ,   and   Liangjie   Hong .   2016 .   An   Ensemble - based   Approach   to   Click - Through             Rate   Prediction   for   Promoted   Listings   at   Etsy .   In   arXiv   preprint   arXiv : 1711.01377 .               Criteo       Thomas   Nedelec ,   Elena   Smirnova ,   and   Flavian   Vasile .   2017 .   Specializing   Joint   Representations   for   the   task             of   Product   Recommendation .   arXiv   preprint   arXiv : 1706.07625   ( 2017 ) .               Linkedin       Benjamin   Le .   2017 .   Deep   Learning   for   Personalized   Search   and   Recommender   Systems .   In   Slideshare :       https : / / www . slideshare . net / BenjaminLe4 / deep - learning - for - personalized - search - and - recommender - systems   .       Thomas   Schmitt ,   Fran ç ois   Gonard ,   Philippe   Caillou ,   and   Mich è le   Sebag .   2017 .   Language   Modelling   for             Collaborative   Filtering :   Application   to   Job   Applicant   Matching .   In   IEEE   International   Conference   on             Tools   with   Artificial   Intelligence .               Tinder       SteveLiu . 2017 . PersonalizedRecommendationsatTinder : TheTinVecApproach .   In   Slideshare :         https : / / www . slideshare . net / SessionsEvents / dr - steve - liu - chief - scientist - tinder - at - mlconf - sf - 2017   .               Tumblr       MihajloGrbovic , VladanRadosavljevic , NemanjaDjuric , NarayanBhamidipati ,   and   Ananth   Nagarajan .   2015 .             Gender   and   interest   targeting   for   sponsored   post   advertising   at   tumblr .   In   Proceedings   of   the   21th             ACM   SIGKDD   International   Conference   on   Knowledge   Discovery   and   Data   Mining .   ACM ,   1819 – 1828 .               Instacart       Sharath   Rao .   2017 .   Learned   Embeddings   for   Search   at   Instacart .   In   Slideshare :         https : / / www . slideshare . net / SharathRao6 / learned - embeddings - for - search - and - discovery - at - instacart   .               Facebook       Ledell   Wu ,   Adam   Fisch ,   Sumit   Chopra ,   Keith   Adams ,   Antoine   Bordes ,   and   Jason   Weston .   2017 .             StarSpace :   Embed   All   The   Things !   arXiv   preprint   arXiv : 1709.03856 .                           METHODOLOGY           listing   embedding   短期 实时 个性化       user - type   & amp ;   listing   type   embeddings   长期 个性化           Listing   Embeddings ( List 指 一个 项目 ? 还是 列表 ? )           session 定义 :   $ (   s   =   ( l _ 1 ,   ... ,   l _ M )   ) $   非 中断 点击 序列 ;   新 的 session 定义 为 相继 两次 点击 事件 相隔 30 分钟 以上 。       skip - gram 模型 :   损失 函数           $ $     \ \ mathbf { L }   =   \ \ sum _ { s \ \ in   S } \ \ sum _ { l _ i   \ \ in   s }   \ \ sum _ { - m   \ \ le   j   \ \ le   m ,   i   \ \ ne   0 }   P ( l _ { i + j }   |   l _ i )     $ $       利用 负 采样 近似 ,   正例 集合   $ ( ( l ,   c )   \ \ in   D _ p ) $   l , c 在 同一个 上下文 ;   负例 集合   $ (   ( l ,   c )   \ \ in   D _ n   ) $       $ $     \ \ arg \ \ max _ { \ \ theta }   \ \ sum _ { ( l ,   c )   \ \ in   D _ p }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { -   v '   _   c   v _ l } }   +   \ \ sum _ { ( l ,   c )   \ \ in   D _ n }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { v '   _   c   v _ l } }       $ $           将 订购 的 listing 作为 全局 上下文 :         booking   sessions   最后 有 订购       exploratory   sessions               对于   booking   session ,   认为 最终 订购 的 listing 跟 之前 点击 的 每 一个 listing 都 有 相关性 , 所以 可以 将 它 作为 之前 点击 的 每 一个 listing 的 全局 上下文           $ $     \ \ arg \ \ max _ { \ \ theta }   \ \ sum _ { ( l ,   c )   \ \ in   D _ p }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { -   v '   _   c   v _ l } }   +   \ \ sum _ { ( l ,   c )   \ \ in   D _ n }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { v '   _   c   v _ l } }     +     \ \ sum _ { l }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { -   v '   _   b   v _ l } }     $ $       $ (   v _ b   ) $   是 订购 的 listing       相当于 增加 一些 正例 !           在 搜索 场景 性 , 用户 看到 的 和 搜索 的 大多 是 同一个 小 市场 / 区域 中 的 房屋 , 所以 正例 都 是 在 同一个 市场 中 的 房屋 对 , 而 负例 都 是 随机 采样 的 , 所以 负例 对 大多 不 在 同一个 市场 中 。         这 导致 模型 难以 学到 市场 内 的 相似性 差异 , 所以 可以 额外 加入 一些 同一个 市场 中 的 负例 对 。           $ $     \ \ sum _ { ( l ,   m _ n )   \ \ in   D _ { m _ n } }   \ \ log   \ \ frac { 1 } { 1   +   e ^ { v '   _   { m _ n }   v _ l } }     $ $       $ (   D _ {   m _ n   } ) $   是 采样 自 l 同一个 市场 中 的 负例 对 。           冷启动 问题 :   选出 新 房源 地点 附近 10miles 半径 范围 内 , 相同 房屋 类型 , 相同 价格 带 的 3 个 有 embedding 向量 的 其他 房屋 , 用 它们 的 均值 代表 新 房源 的 向量       listing   embedding 检验 :   向量 维度 32 ,   session 数量 800M !       embedding 的 重点 是 为了 学习 房屋 特点 、 结构 、 类型 、 观感 etc , 等 难以 直接 提取 的 相似性 。 作者 开发 了 一个 评估 工具 : Similarity   Exploration   Tool       学习 到 的 类型 :   船屋 、 树屋 、 城堡 、 小木屋 、 海景房     https : / / youtu . be / 1kJSAG91TrI             User - type   & amp ;   Listing - type   Embeddings       用 用户 的 订购 的 序列 作为 session ,   来 学习 跨 market 的 相似性 。 这种 跨 market 的 相似性 可以 用来 解决 用户 来到 一个 新 的 market 的 时候 给 他 推荐 的 问题 。     但是 这种 方法 由于 数据 稀疏 会 带来 一下 几个 问题 :           大多数 人 的 历史 下单 次数 很少 , 甚至 只有 1 个 , 没 法学       listing 的 长尾 特性 , 导致 长尾 的 listing 的 向量 学得 不好       由于 时间跨度 长 , 原来 假设 的 相邻 两个 listing 是 相似 的 假设 并 成立 , 因为 用户 的 偏好 、 价格 等 因素 以及 发生 了 改变           用   User - type   & amp ;   Listing - type   而 不是 ID       上下文 是   ( User - type ,   Listing - type )   对 ,   购买 的 对 是 正例 ,   没有 购买 的 对 是 负例 ,   被 拒绝 的 对 也 是 负例 。       session 是 同一个 用户 的 购买 序列   user - type ,   listing - type   序列 ,   s   = ( ut1 ,   lt1 ,   ... ,   utM ,   ltM )     这样 可以 复用   word2vec   代码 ,   并 在 同一个 空间 投影 ,   让 向量 距离 具有 可比性 !       试验           session 构建 :   按照 登录 用户 ID ,   将 点击 的 listing   id 按照 点击 时间 排序 ,   然后 按照 30 分钟 不 活跃 划分 成 多个 session   ,   session 数量 800M ,   listing 数量   4.5 M       过滤 掉 在 listing 页面 停留 少于 30s 的 事件 ( 认为 是 噪音 ) ,   保留 至少 2 个 listing 的 session       过 采样   booked   session ,   x5       每天 定期 训练 配置 :       离线 训练 、 评估 、 调参       滑动 窗 构建 数据 集 ,   数月 的 数据       每次 都 是 重新 开始 训练 ( 相同 的 随机数 种子 ) , 随机 初始化 每 一个 向量 ;   发现 比 增量 训练 效果 要 好       每天 都 在 变 的 向量 对 效果 没有 影响 , 因为 最后 使用 的 是 余弦 相似 度 作为 特征       d = 32 ,   主要 是 性能 和 相似 度 检索 的 性能 之间 的 权衡       上下文 窗口 为 5 ,   10 次 遍历 整个 训练 集       修改 了   word2vec         使用   MapReduce   训练 :   300 个 mapper 读数据 ,   1 个 reducer 训练 模型 ,   多线程 训练   ?       Airflow2     http : / / airbnb . io / projects / airflow                 离线 评估       设 最后 一次 点击 的 listing 是 A ,   待 排序 的 listing 列表 是 :   BCDEFG 。 。 。 , 待 排序 的 列表 需要 包含 了 最终 订购 的 listing ,         我们 可以 计算 A 和 这些 listing 的 余弦 相似 度 ,   然后 根据 相似 度 排序 , 从而 得到 订购 的 listing 排序 的 rank ,   rank 越小 , 说明 越靠 前 。       这种 排序 得到 的 订购 listing 的 rank , 取决于 两个 重要 因素 , 一是 当前 距离 最后 订购 之间 还有 几次 点击 ,   显然 越 接近 订购 行为 , 相关性 越强 ;         二是 embedding 向量 的 好坏 ,   这 正是 要 评估 的 东西 。       为了 评估 两种 embedding 向量 好坏 , 可以 固定 距离 最终 订购 之间 的 点击 次数 , 来 比较 排序 ,   排序 越靠 前 ,   说明 学到 的 embedding 向量 跟 最终 的 排序 目标 越 接近 。                                   相似 listing 推荐           现有 的 方法 是 在 给定 的 listing 附近 , 根据 是否 可 定购 、 价格 范围 、 房屋 类型 筛选 出 候选 集 , 然后 调用 搜索 排序 模块       从 同一个 market 中 ,   在 相同 的 日期 内 可 订购 的 房屋 中 ,   利用 embedding 向量 直接 找 K 近邻 ,   用 余弦 相似 度 度量 距离       CTR   提升 了 21 %                   实时 个性化             排序 模型 :         标签 :   $ (   y _ i   \ \ in   \ \ {   0 ,   0.01 ,   0.25 ,   1 ,   - 0.4   \ \ }   ) $   分别 代表 , 只 曝光 , 曝光 后 只有 点击 ,   曝光 后 用户 联系 了 但是 没 订购 ,   1 订购 ,   - 0.4 房东 拒绝 了 。       特征 :   listing   features ,   user   features ,   query   features   and   cross - features       曝光 后 用 一周 的 观测 时间 获得 label ,   只 保留 那些 曝光 列表 中有 订购 的 房屋 的 数据 。       利用   GBDT   进行   pairwise   regression ,   lambda   rank 损失 ,   NDCG   metric 。       Listing   Embedding   Features :         对于 每个 用户 ID ,   用 Kafka 收集 用户 近 两周 的 房屋 id 集合       Hc   近 两周 点击 过 的 listing   id       Hlc   点击 过且 页面 停留时间 超过 60s 的 listing   id       Hs   用户 跳过 的 曝光 靠 前 的 listing   id       Hw   用户 近 两周 加 到 心愿 单 的 listing   id       Hi   用户 近 两周 咨询 过 的 listing   id       Hb   用户 近 两周 订购 过 的 listing   id               将 上述 每 一个 集合 进一步 分成 同 market 的 子集 ,   例如 :   Hc   里面 有   New   York   和   Los   Angeles 两个 市场 , 那么 就 会 分成 两个 子集   Hc ( NY ) ,   Hc ( LA )       以 EmbClickSim 为例 , 说明 这个 特征 的 构造 过程 :       先 计算 每 一个 market 的 embedding 中心 ,   也 就是 按照 Market 对   listing   id 的 vector 做 average   Pooling       计算 候选 的 listing 的 向量 与 每 一个 中心 的 相似 度 ( 距离 ) ,   得到 多个 距离 , 然后 取 相似 度 的 最大值 ( 最小 距离 ) ,   max   pooling               EmbLastLongClickSim   是从 Hlc 中 找到 最近 点击 的 listing   id , 计算 的 相似 度 , 最后 一个 影响 大       UserTypeListingTypeSim   直接 拿 type 的 embedding 向量 计算 的 余弦 相似 度               特征 单 因素 分析 , 固定 其他 特征 , 改变 单个 特征 , 分析 单个 特征 变化 对 排序 分数 影响 ,   ( 跟 我 的 想法 一模一样 )                           NDCU ( Discounted   Cumulative   Utility ) 提升   2.27 % ,   指标 就 不列 了           NDCU   相当于 用 y 作为 Gain ,   计算 的 NDCG ,   而 不是 用   $ ( 2 ^ r   -   1 ) $   作为 Gain   ?                                                       https : / / medium . com / airbnb - engineering / listing - embeddings - for - similar - listing - recommendations - and - real - time - personalization - in - search - 601172f7603e         问题           wor2vec   如何 嵌入 到 MapReduce 中 ? 怎么 实现 ?       NDCU   ?      ", "tags": "machine-learning", "url": "/wiki/machine-learning/airbnb-kdd2018.html"},
      
      
      {"title": "【NIPS2018】ODENet - Neural Ordinary Differential Equations", "text": "    Table   of   Contents           参考           摘要           反 模式 的 自动 微分                 参考           论文 :   Ricky   T .   Q .   Chen ,   Yulia   Rubanova ,   Jesse   Bettencourt ,   David   Duvenaud .   \" Neural   Ordinary   Differential   Equations . \"   Advances   in   Neural   Processing   Information   Systems .   2018 .     arxiv         代码 :     https : / / github . com / rtqichen / torchdiffeq             摘要           用 神经网络 拟合 隐态 的 梯度 ,   通过 ODE   Solver   求解 输出           残差 网络 可以 看做 一个 常 微分方程 初值问题 的 差分 形式       resnet       $ $     h _ { t + 1 }   =   h _ { t }   +   f ( h _ t ,   \ \ theta _ t ) ,   t = 0 ,   1 , 2 , ...   \ \ \ \     h _ 0   =   input     $ $       ode       $ $     \ \ frac { d   h ( t ) } { d   t }   =   f ( h ( t ) ,   \ \ theta ,   t )   \ \ \ \     h _ 0   =   input     $ $       相当于 在 不同 的 时间 上 , 参数 共享 !   所以     参数 / 内存 复杂度 跟 步长 / 层数 无关   !       另外 , 由于 现代 的   ODE   solver   可以 自 适应 调整 以 达到 给定 的 精度 。       此外 ,   可以 应用 到 连续 时间 动态 系统 的 优化 中 !               反 模式 的 自动 微分       假设 标量 损失 函数 $ (   L ( )   ) $ 的 输入 是   ODE   solver   的 输出 ,   也 就是 把   z ( t )   ( 你 可以 理解 为 神经网络 最后 一层 隐 向量 ) 输入 到 损失 函数 中 ( 例如 logloss , rankloss 等 )     $ $     L ( z ( t _ 1 ) )   =   L   \ \ left (   \ \ int _ { t _ 0 } ^ { t _ 1 }   f ( z ( t ) ) ,   t ,   \ \ theta   dt     \ \ right )   =   L ( ODESolver ( z ( t _ 0 ) ,   f ,   t _ 0 ,   t _ 1 ,   \ \ theta ) ) \ \ \ \     \ \ frac { d   z ( t ) } { d   t }   =   f ( z ( t ) ,   \ \ theta ,   t )       $ $       为了 优化 L , 需要 计算 L 对 参数 的 梯度 。       adjoin   $ (   a ( t )   =   \ \ partial { L }   /   \ \ partial { z ( t ) }   ) $   满足 如下 ODE ( 两边 对 t 求导 ) ,   注意 标量 场 L 的 散度 等于 0     $ $     \ \ frac { d   a ( t ) }   { d   t }   =   -   a ( t ) ^ T   \ \ frac { \ \ partial   f ( z ( t ) ,   t ,   \ \ theta ) } { \ \ partial   z }     $ $       这 表明 , 我们 可以 从 a ( t1 ) 反向 计算 积分 得到 a ( t0 )     $ $     a ( t _ 0 )   =   \ \ int _ { t _ 1 } ^ { t _ 0 }   -   a ( t ) ^ T   \ \ frac { \ \ partial   f ( z ( t ) ,   t ,   \ \ theta ) } { \ \ partial   z }   dt     $ $     这个 类似 于 多层 神经网络 的 BP 过程 。       L 对 参数 $ ( \ \ theta ) $ 的 梯度     $ $     -   \ \ frac { d   L }   { d   \ \ theta }   =   \ \ int _ { t _ 0 } ^ { t _ 1 }   a ( t ) ^ T   \ \ frac { \ \ partial   f ( z ( t ) ,   t ,   \ \ theta   ) } { \ \ partial   \ \ theta }   dt     $ $               算法 参考文献 :     Optimization   and   uncertainty   analysis   of   ODE   models   using   second   order   adjoint   sensitivity   analysis    ", "tags": "machine-learning", "url": "/wiki/machine-learning/odenet.html"},
      
      
      {"title": "个性化价格推荐", "text": "    Table   of   Contents           paper           基本 思想                 paper       Personalized   Pricing   Recommender   System       基本 思想           将 用户 分为 三类 ： 标准 类型   Standard ,   折扣 类型   Discount ,   不 关注 类型   Indifferent                       价格       标准       折扣       不 关注                       标准 价格       买       买       不买               折扣 价格       买       不买       不买                       每 一类 人 的 购买 行为 对应 一个 回报 ， 对 标准 和 折扣 是 支付 金额   $ ( \ \ alpha ) $ 和 $ ( \ \ beta ) $ ， 而 对于   Indifferent   类型 ， 购买 的话 会 将 产品 转卖 ， 导致 需要 的 用户 买不到 ， 存在 一定 损失 ， 这里 将 购买 的 收益 置 0 ， 不 购买 相比 购买 存在 正 的 收益   $ ( \ \ gamma   & lt ; & lt ;   \ \ alpha ) $                       响应       标准       折扣       不 关注                       买       $ ( \ \ alpha ) $       $ ( \ \ beta ) $       0               不买       0       0       $ ( \ \ gamma ) $                       观测 歧意 ：       对于 采用 标准 价格 用户 ， 其中 观测 为 支付 的 用户 为 标准 用户 ， 但是 无法 区分 折扣 用户 和 不 关注 用户 ；       如果 采用 折扣 价格 ， 则 不 支付 的 用户 是 不 关注 用户 ， 但是 无法 区分 标准 用户 和 折扣 用户 。               解决 方法 ： 多 步 分类器 。       prescreening   预赛 选 过程 ， 过滤 掉 明显 是   Indifferent   类型 的 用户 ， 减轻 类别 不 均衡 。 即 通过 预测 用户 是否 会 下单 ， 如果 下单 率 特别 低 ， 那么 就是   Indifferent   用户       standard   stage ： 区分 标准 用户 和 其他 两类 用户 。       正 样本 （ 即 标准 用户 ） ： 展示 标准 价格 并且 以 标准 价格 下单 的 用户       负 样本 ： 展示 标准 价格 但是 并 没有 下单 的 用户 ； 展示 折扣 价格 但是 并 没有 下单 的 用户               discounted   classifier ： 在 非标准 用户 中 区分 折扣 用户 和 不 关注 用户       正 样本 ： 展示 折扣 价格 下单 的 非标准 用户       负 样本 ： 展示 折扣 价格 未 下单 的 非标准 用户                              ", "tags": "machine-learning", "url": "/wiki/machine-learning/personal-price-recommender.html"},
      
      
      {"title": "人工智能介绍(不定期更新)", "text": "    Table   of   Contents           关于           人工智能 具体 是 做 什么 ?           数据挖掘           机器 视觉 / 图像处理           自然语言 处理           语音 交互           自动 驾驶           游戏 AI                   人工智能 的 价值           知识 储备 与 获取           个人观点 : 对 未来 的 展望                 关于       本文 主要 面向 哪些 想 了解 以及 想 从事 人工智能 相关 领域 研究 、 工作 的 入门 人士 , 在 踏入 这个 领域 之前 , 通过 3W ( 「 是 什么 」 , 「 为什么 」 , 「 怎么办 」 ) 了解 一下 人工智能 的 概貌 。       人工智能 具体 是 做 什么 ?       目前 从事 人工智能 的 有 两 部分 人 , 一部分 在 学术界 , 以 大学 和 研究所 教授 为 代表 , 另 一部分 在 企业 当中 , 以 各种 算法 工程师 为 代表 。 目前 来看 , 人工智能 的 几个 比较 热门 的 主题 主要 有 :     1 .   数据挖掘 ( Data   Mining )     2 .   机器 视觉 ( CV )     3 .   自然语言 处理 ( NLP )     4 .   语音 交互     5 .   自动 驾驶     6 .   游戏 AI       数据挖掘       数据挖掘 属于 特别 古老 , 而且 在 工业界 落地 特别 成熟 的 一个 领域 了 , 而且 现在 几乎 每个 人 都 受益 于 它 。 数据挖掘 机器 学习 跟 早期 的 数据分析 很 相似 , 通过 从 数据 中 找到 某种 规律 , 用 这种 规律 来 改善 产品 等 的 用户 体验 , 实现 个性化 。       具体 而言 , 它 的 应用 场景 包括     1 .   个性化 推荐 ,   比如 各种 APP 里面 的 \" 猜 你 喜欢 \" , \" 看了又看 \" , \" 为 你 推荐 \" , 淘宝 、 京东 、 美团 、 亚马逊 等 电商 相关 的 公司 ;   头条 、 腾讯 等 新闻媒体 的 个性化 咨询 推荐 。   https : / / tech . meituan . com / 2018 / 03 / 29 / recommend - dnn . html       2 .   个性化 搜索 ,   你 知道 吗 ? 即使 你 跟 别人 在 百度 中 同时 搜索 相同 的 内容 , 出现 的 内容 和 排序 结果 也 是 不 完全 一样 的 , 利用 数据挖掘 可以 实现 千人 千面 的 排序 来 提高 用户 体验 。 现在 Google 、 百度 、 淘宝 等 搜索 都 使用 了 个性化 排序 , 提升 用户 体验 。     3 .   个性化 广告 ,   目前 你 在 社交 媒体 上 和 网页 上 看到 的 绝大多数 广告 , 都 是 个性化 的 , 比如 百度 、 Google 广告 、 微信 朋友圈 广告 。     4 .   打车 调度 ,   滴滴 用 数据挖掘 的 方法 , 对 平台 上 的 订单 进行 分发 , 提升 总体 的 接单 成功率 。   https : / / www . leiphone . com / news / 201808 / 7ZbAz8REosn3L8kT . html       5 .   外卖 配送 ,   美团 外卖 用 数据挖掘 的 方法 ,   估计 订单 配送 时 长 。     https : / / tech . meituan . com / 2018 / 12 / 13 / machine - learning - in - distribution - practice . html       6 .   智能 定价 ,   Airbnb 用 算法 为 房东 根据 时间 不同 智能 地 调整 价格 。   https : / / www . kdd . org / kdd2018 / accepted - papers / view / customized - regression - model - for - airbnb - dynamic - pricing         机器 视觉 / 图像处理       机器 视觉 是 指用 计算机 算法 去 理解 图像 , 多年 以前 , 普通人 唯一 能 扯 得 上 关系 的 也 只有 PS , 然而 随着 近几年 的 发展 , 机器 视觉 方面 的 应用 已经 深入 到 日常 的 很多 方面 了 。           图像处理 软件 ,   Photoshop 等 软件 已经 集成 了 很多 视觉 方面 的 算法 , Adobe 公司 近几年 也 一直 在 投入 很多 精力 研发 新 的 更 智能 的 图像处理 算法 。       自动 抠 图 :     https : / / sites . google . com / view / deepimagematting         图像 风格 转换 :     https : / / zhuanlan . zhihu . com / p / 26746283         图像 超 分辨 重建 ,   将 720p 的 图片 变成 4K 高清 图片 :     https : / / zhuanlan . zhihu . com / p / 25532538                 图像识别 与 检测 ,   现在 到 银行 办事 , 或者 在 APP 上 认证 ,   都 可以 基于 人脸识别 的 方式 进行 检验 。   https : / / faceid . com /         自动 驾驶 中 的 识别 ,   需要 及时 地 从 摄像头 捕获 的 图片 中 识别 出 障碍 和 行人 。   https : / / yq . aliyun . com / articles / 228194         AI 相机 ,   各种 美颜 算法 , 瘦脸 等 算法 相比 也 不 陌生 了 。       各种 AI 换脸 ,   搜 一下   deepfake   有 惊喜 哦 。           自然语言 处理       自然语言 处理 指 的 是 计算机 通过 算法 理解 人们 日常 的 语言 。 我们 每天 用 的 翻译 工具 、 输入法 等 软件 都 涉及 到 自然语言 处理 相关 的 算法 。           联想 输入法 ,   现在 的 输入法 越来越 智能 了 , 背后 是 自然语言 处理 算法 的 提升 , 代表 公司 就是 搜狗 。       机器翻译 ,   google 翻译 不用 多 讲 了       智能 问答 / 客服 ,   相信 你 或多或少 都 用 过 一些 网站 的 智能 客服 ,   淘宝 、 京东 等 电商 都 有 ,   一些 简单 的 问题 通过 这些 机器人 客服 就 可以 解决 。 此外 , 各种 聊天 机器人 和 助手 , 比如 Siri 要 理解 你 的话 , 也 是 先 转成 文本 后 , 然后 通过 自然语言 处理 算法 来 处理 。 GMail 还 搞 了 一个 只能 生成 邮件 回复 的 工具 , 背后 也 是 自然语言 处理 的 算法 在 驱动 。       搜索引擎 ,   搜索引擎 应该 是 最 成功 的 应用 自然语言 处理 的 商业 产品 了 ,   代表 产品 是 百度 和 Google 。       垃圾邮件 过滤 ,   这个 也 是 很 古老 的 自然语言 处理 算法 的 一个 应用 场景 了 , 不过 现在 关注 的 人 不 多 了 。           语音 交互       语音 交互 应该 也 是 近几年 火 起来 的 ,   从 Siri 、 Google   Now 、 亚马逊   Echo , 再 到 国内 各种 智能 音箱 和 AI 手机 助手 。 目前 还 处于 幼儿期 , 不够 成熟 , 但 未来 潜力 很大 , 结合 自然语言 处理 可以 搞 出 很多 大 新闻 。 著名 的 科技 导师 李开复 就是 做 语音 识别 的 。           语音 识别 ,   国内 以 科大 讯 飞为 龙头 , BAT 各家 也 都 有       AI 助手 / 客服 ,   中国移动 跟 讯 飞 合作 搞 了 机器人 客服 ,   各大长 的 AI 音箱 。       同声翻译 ,   目前 虽然 还 不 成熟 ,   未来 还是 有 可能 商用 的 。       语音 降噪 / 语音 合成 ,   有些 降噪 耳机 中 也 用到 了 先进 的 降噪 算法 ,   语音 合成 就 不必 说 了 ,   很多 电子书 阅读 软件 都 可以 体验 到 ,   智能 客服 输出 的 语音 也 是 合成 的 。           自动 驾驶       自动 驾驶 应该 是 目前 的 风口 行业 了 ,   最早 是 由 Google 带 起来 的 ,   国内 厂商 百度 跟进 , 然后 一大堆 自动 驾驶 公司 冒出来 了 , 很多 汽车 制造商 也 加入 了 竞争 。 自动 驾驶 要 用到 上面 几乎 所有 的 AI 技术 , 包括 视 ( 机器 视觉 ) 听 ( 语音 识别 ) 说 ( 语言 合成 ) 读 ( 自然语言 理解 ) 写 ( 自然语言 合成 ) 。       游戏 AI       游戏 AI 随着 游戏 的 诞生 就 有 了 , 早期 的 游戏 AI 很菜 , 所以 玩 单机游戏 都 称之为 「 虐 电脑 」 , 但是 现在 就 不 一定 了 哦 。 这个 标志性 事件 是从 AlphaGo 开始 的 , 在 这 之前 AI 只能 在 一些 简单 游戏 上 胜过 人类 选手 ,   但是 从这以后 , 各种 游戏 AI 横空出世 , 甚至 魔兽 这种 复杂 的 游戏 AI 也 出来 了 。   https : / / www . zhihu . com / question / 41176911         人工智能 的 价值       取代 或 部分 替代 人类 的 工作 ,   完全 替代 很 美好 ,   但 一般 需要 很长 的 过程 ,   最后 也 不 一定 能 实现       知识 储备 与 获取           数学       高等数学       线性代数       概率论 与 统计       凸 优化       数值 计算 ( 可 选 )       泛函 ( 可 选 )               计算机       C / C++ / Java   ( 选 一门 就行 )       Python ,   AI 御用 编程语言       数据结构 预算法       数据库 基础 ( SQL 要会 写 )       数据挖掘 / 机器 学习 ,   Andrew   Ng 的 公开课 就行 ,   推荐 教材   周志华 《 机器 学习 》       自然语言 处理 ( cs224n )       数字图像处理 / 分析 / 理解 , 计算机 视觉 ( cs231n )       语音 信号处理 / 识别 / 合成       强化 学习 ( cs234 ,   cs294 )               相关 新闻 源       爱 可可 的 新浪 微博 , 可 邮件 订阅       新智元       机器 之心       reddit ,   r / machinelearning     https : / / www . reddit . com / r / MachineLearning /         知乎 「 机器 学习 」 相关 话题               国际 会议       NIPS       AAAI       ICML       CVPR       KDD               研究组 :       DeepMind       OpenAI               实践       Kaggle 比赛     https : / / www . kaggle . com         阿里 云 天池 大 数据 比赛                   个人观点 : 对 未来 的 展望       未来 10 年内 , 会 用 基本 的 机器 学习 算法 优化 业务 问题 将 成为 IT 行业 的 标配 ,   未来 很多 工作 会   部分 地   被 AI 替代 ,   目前 推荐 、 搜索 和 广告 基本上 已 被 完全 替代 了  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ai-brief.html"},
      
      
      {"title": "优化算法", "text": "    Table   of   Contents           梯度 下降           动量 方法           Nesterov   accelerated   gradient           Adagrad           Adadelta   ( Google   Inc ,   New   York   Univ ,   Matthew   D . Zeiler )           RMSprop   ( Hinton )           Adam ,   adaptive   moment   estimation .           Additional   strategies   for   optimizing   SGD           Batch   normalization                   Reference                 梯度 下降           批量 梯度 下降       随机 梯度 下降   SGD       mini - batch   梯度 下降           mini - batch 梯度 下降 的 问题     -   学习 率 难以 选择 ， 过 小 收敛 太慢 ， 过 大会 导致 震荡     -   自动 降低 学习 率 需要 预先指定 条件 ， 不是 自 适应 的     -   算法 对 所有 的 参数 采用 相同 的 学习 率     -   难以 跳出 鞍点 ， 这是 致命 的 问题       优化 算法       动量 方法       可以 加速 相关 方向 的 收敛 和 抑制 不 相关 方向 的 震荡 。 动量 实际上 是 对 梯度 的 指数 平滑     $ $     v _ t   =   \ \ gamma   v _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ nabla _ \ \ theta   J ( \ \ theta ) .   \ \     \ \ theta   =   \ \ theta   -   \ \ eta   v _ t .     $ $       Nesterov   accelerated   gradient       采用 预测 的 点 的 梯度 ， 而 不是 当前 梯度 。     $ $     v _ t   =   \ \ gamma   v _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ nabla _ \ \ theta   J ( \ \ theta   -   \ \ gamma   v _ { t - 1 } ) .   \ \     \ \ theta   =   \ \ theta   -   \ \ eta   v _ t .     $ $       Adagrad       解决 了 两个 问题 ， 自 适应 学习 率   和   对 不同 频次 特征 采用 不同 的 学习 率 ， 适应 于 稀疏 特征   。     更新 权值 的 方程 为     $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ t   -   \ \ frac { \ \ eta } { \ \ sqrt { G _ t   +   \ \ epsilon } }   \ \ odot   g _ t .     $ $     $ ( G _ t ) $ 是 对 过去 的 梯度 的 平方 的 累积 ， 因此 学习 率 一直 在 减少 。       Adadelta   ( Google   Inc ,   New   York   Univ ,   Matthew   D . Zeiler )       改善 Adagrad 单调 递减 的 学习 率 ， 它 不是 累计 所有 的 梯度 ， 而是 设置 了 一个 固定 的 时间 窗 $ ( w ) $ .     平滑 平方 误差 $ ( E [ g ^ 2 ] _ t ) $     $ $     E [ g ^ 2 ] _ t   =   \ \ gamma   E [ g ^ 2 ] _ { t - 1 }   +   ( 1 - \ \ gamma )   g _ t ^ 2 .       $ $     权值 更新 方程 为     $ $     \ \ Delta   \ \ theta _ t   =   -   \ \ frac { \ \ eta } { \ \ sqrt { E [ g ^ 2 ] _ t   + \ \ epsilon } }   \ \ odot   g _ t .     $ $     平滑 步长 的 平方     $ $     E [ \ \ Delta   \ \ theta ^ 2 ] _ t   =   \ \ gamma   E [ \ \ Delta   \ \ theta ^ 2 ] _ { t - 1 }   +   ( 1 - \ \ gamma )   \ \ Delta   \ \ theta ^ 2 _ t .     $ $     利用 这个 重新 设计 学习 率 使得 单位 一致     $ $     \ \ Delta   \ \ theta _ t   =   -   \ \ frac { \ \ sqrt { E [ \ \ Delta   \ \ theta ^ 2 ]   +   \ \ epsilon } } { \ \ sqrt { E [ \ \ theta ^ 2 ] _ t   +   \ \ epsilon } }   \ \ odot   g _ t .     $ $       RMSprop   ( Hinton )       第一种 Adadelta 。       Adam ,   adaptive   moment   estimation .       算法     $ $     m _ t   =   \ \ beta _ 1   m _ { t - 1 }   +   ( 1 - \ \ beta _ 1 )   g _ t .   \ \     v _ t   =   \ \ beta _ 2   v _ { t - 1 }   +   ( 1 - \ \ beta _ 2 )   g _ t ^ 2 .     $ $       $ $     \ \ hat { m } _ t   =   \ \ frac { m _ t } { 1   -   \ \ beta _ 1 ^ t } .   \ \     \ \ hat { v } _ t   =   \ \ frac { v _ t } { 1   -   \ \ beta _ 2 ^ t } .     $ $       更新 方程     $ $     \ \ theta _ { t + 1 }   =   \ \ theta _ t   -   \ \ frac { \ \ eta } { \ \ sqrt { \ \ hat { v } _ t   +   \ \ epsilon } }   \ \ hat { m } _ t .     $ $       Additional   strategies   for   optimizing   SGD       每 一次 的 循环 都 要 打散 数据 的 顺序 。 Zaremba   and   Sutskever 训练 LSTM 的 时候 ，     发现 按照 一定 顺序 反而 能 提高 性能 ？       Batch   normalization           可以 使用 更 高 的 学习 率       移除 或 使用 较 低 的 dropout       降低 L2 权重           Reference             http : / / sebastianruder . com / optimizing - gradient - descent /           https : / / arxiv . org / pdf / 1502.03167 v3 . pdf           http : / / blog . csdn . net / happynear / article / details / 44238541        ", "tags": "machine-learning", "url": "/wiki/machine-learning/optimization.html"},
      
      
      {"title": "先验知识的编码", "text": "    Table   of   Contents           先验 知识 的 编码           跟踪 自由落体           跟踪 行人                         先验 知识 的 编码       论文 ： Label - Free   Supervision   of   Neural   Networks   with   Physics   and   Domain   Knowledge       利用 无 监督 样本 学习 ， 将 先验 信息 编码 到 损失 函数 中 ！       跟踪 自由落体           模型 不是 去 拟合 每 一帧 目标 的 高度 ， 而是 要求 模型 预测 的 结果 满足 物理 定律 。       输入 是 图像 序列 x ， 输出 一个 数值 序列 y ， 即 每 一帧 目标 的 高度       对 每 一个 图像 x ， 应用 函数 f ， 得到 预测 结果 f ( x )       用 一个 a = 9.8 的 抛物线 这些 结果 序列 ， 得到 $ ( \ \ hat { y } ) $       损失 函数 用 拟合 的 结果 和 预测 结果 的 残差 表示           $ $     g ( x ,   f ( x ) )   =   \ \ sum _ { i = 1 } ^ N   | \ \ hat { y } _ i   -   f ( x )   _   i |     $ $       利用 随机 梯度 下降 优化 损失 函数 （ 可以 使用 正常 的 正则 项 ） ， 得到 最佳 $ ( f ^   *   ) $ 。           实验 结果表明 ： 这种 方法 的 测试 集上 相关性 90 % ， 监督 学习 方法 94.5 % ， 随机 猜 只有 12.1 % ， 这个 实验 证明 ， 有效 地 将 物理 规律 编码 进 损失 函数 ， 利用 无 标注 样本 就 能够 有效 地 学习 模型 ！           跟踪 行人           水平 移动 的 行人 位置 ， 没有 了 二次 项 ， 认为 是 匀速运动 。       增加 了 方差 鼓励 ， 鼓励 输出 序列 的 方差 不为 0 ， 防止 平凡 解       增加 边界 约束 ， 防止 过大 的 reward ！       发现 无 监督 的 guide   learning 比 监督 学习效果 还好 ， 因为 监督 的 样本 太 少 ， 过 拟合 了 。      ", "tags": "machine-learning", "url": "/wiki/machine-learning/prio-knowledge.html"},
      
      
      {"title": "多目标识别", "text": "    Table   of   Contents           多 目标 检测                 多 目标 检测       论文 ： MULTIPLE   OBJECT   RECOGNITION   WITH   VISUAL   ATTENTION ， Jimmy   Lei   Ba ， Volodymyr   Mnih ， Koray   Kavukcuoglu   @ DeepMind                   每 一步 ， 产生 一个   glimpse   的 坐标   $ ( l _ n ) $   和   图像 块   $ ( x _ n ) $ ， 其中 初始 的   l   由 原始 图像 下 采样 后 ， 经过   context   network   提供 的 。      ", "tags": "machine-learning", "url": "/wiki/machine-learning/multi-object-recognition.html"},
      
      
      {"title": "小样本单样本学习算法", "text": "    Table   of   Contents                   self - train   Learning       transductive   Learning       PU   Learning           co - train     tri - train       tsvm  ", "tags": "machine-learning", "url": "/wiki/machine-learning/pulearning.html"},
      
      
      {"title": "少样本学习", "text": "    Table   of   Contents           关于           问题           基于 模型 的 方法           基于 度量 的 方法           孪生 网络                         关于           人类 非常 擅长 通过 极少量 的 样本 识别 一个 新 物体 ， 比如 小孩子 只 需要 书中 的 一些 图片 就 可以 认识 什么 是 “ 斑马 ” ， 什么 是 “ 犀牛 ” 。 在 人类 的 快速 学习 能力 的 启发 下 ， 研究 人员 希望 机器 学习 模型 在 学习 了 一定 类别 的 大量 数据 后 ， 对于 新 的 类别 ， 只 需要 少量 的 样本 就 能 快速 学习 ， 这 就是   Few - shot   Learning   要 解决 的 问题 。         https : / / mp . weixin . qq . com / s / sp03pzg - Ead - sxm4sWyaXg             问题           flow - shot 学习 和 one - shot 学习 本质 上 的 问题 是 , 给 你 C 个 类别 , 但是 每个 类别 只有 K 个 样本 , 现在 有个 新 的 样本 , 要 你 识别 这个 样本 是 哪个 类别 ,   这种 就 叫 C - way   K - shot 学习       一般 的 分类 问题 , 每个 类别 的 样本 数目 K 一般 很大 , 几百 到 几万 ,   这里 一般 只有 几个 , 所以 很难 正常 建立 一个 分类 模型 去 识别 是 哪个 类别       这个 类似 于 人脸 匹配 的 任务 , 数据库 中 只有 人脸 的 1 张 图片 , 有个 新 的 人脸 来 了 , 要 识别 是 谁           基于 模型 的 方法           没看 懂 , 比较复杂           基于 度量 的 方法           对 样本 间 距离 建模 , 同类 靠近 , 不同 类 的 远离           孪生 网络           论文 :   Siamese   Neural   Networks   for   One - shot   Image   Recognition       训练 好 的 网络 不但 可以 预测 未见 过 的 样本 但是 见 过 的 类别 的 数据 , 还 可以 预测 没有 见过 类别 的 数据 ?       问题 :   预测 那些 我们 只见 过 一次 的 类别   one - shot   learning       区别 于   zero - shot   learning ,   一次 都 不见               相关 论文 :       Fe - Fei ,   Li ,   Fergus ,   Robert ,   and   Perona ,   Pietro .   A   bayesian   approach   to   unsupervised   one - shot   learning   of   object   categories .   In   Computer   Vision ,   2003 .   Proceedings .   Ninth   IEEE   International   Conference   on ,   pp .   1134 –   1141 .   IEEE ,   2003 .       Fei - Fei ,   Li ,   Fergus ,   Robert ,   and   Perona ,   Pietro .   One - shot   learning   of   object   categories .   Pattern   Analysis   and   Ma -   chine   Intelligence ,   IEEE   Transactions   on ,   28 ( 4 ) : 594 –   611 ,   2006 .       Lake ,   Brenden   M ,   Salakhutdinov ,   Ruslan ,   Gross ,   Jason ,   and   Tenenbaum ,   Joshua   B .   One   shot   learning   of   simple   visual   concepts .   In   Proceedings   of   the   33rd   Annual   Con -   ference   of   the   Cognitive   Science   Society ,   volume   172 ,   2011 .               基本 方法       不是 降 图片 作为 多 分类 来 做 , 而是 将   图片 对   当做 二 分类 问题       对 每 一对 有 标注 的 图片 ( a ,   b )   用 同一个 CNN 抽取 特征向量 va ,   vb       计算 加权 距离     | alpha ( va   -   vb ) |   ( L1 ,   L2   距离 ) ,   alpha 是 这个 模型 的 参数 , 用来 给 不同 维度 加权       用   sigmoid ( 加权 距离 )   作为 这 两个 图片 属于 同一 类别 的 概率       利用 二 分类 损失 函数 进行 训练               数据 集 的 构造       有 标注 的 同一 类别 的 图片 不能 太 少 , 否则 无法 构造 相同 类别 的 图片 对       成 对 构造 同类 和 不同 类别 的 图片 对 , 也 就是 最终 模型 预测 的 正负 例是 1 : 1 , 其他 比例 影响 是 什么 ?       测试 集 的 构造 , 选取 一类 图片 的 两张 , 把 其中 一张 用来 预测 , 其他 类别 抽出 一张 , 除了 用来 预测 的 那 1 张外 , 剩下 的 C 张 用作 支撑 集 , 让 模型 预测 那 一张 是 跟 支撑 集 里面 的 哪个 类比 一致 !                   $ $     y   \ \ sigma ( \ \ alpha ^ T ( v _ a   -   v _ b ) )   +   ( 1 - y ) \ \ sigma ( - \ \ alpha ^ t ( v _ a   -   v _ b ) )     $ $           其他 损失 函数 ,   相似 损失 直接 用 距离 ,   不像 似 损失 要求 距离 大于 某个 数值     $ $     y   D ( a ,   b )   +   ( 1 - y )   ( max ( 0 ,   m   -   D ( a ,   b ) ) ) ^ 2     $ $      ", "tags": "machine-learning", "url": "/wiki/machine-learning/few-shot-learning.html"},
      
      
      {"title": "尚未解决的问题集", "text": "    Table   of   Contents           关于           问题 列表                 关于       记录 尚未 解决 的 问题 ， 带 解决 。       问题 列表           论文 ： Natural   Language   Processing   ( almost )   from   Scratch ， page   15 ,   求和 的 递推 公式       评估 指标 ： BLEU ？       Beam - search   decoder     ✅       2014 - QA ： Dependency   tree ， 问题 ： 词 之间 的 这个 树结构 怎么 得到 的 ？       WER 词 错误率 ， CTC      ", "tags": "machine-learning", "url": "/wiki/machine-learning/question.html"},
      
      
      {"title": "强化学习", "text": "    Table   of   Contents           关于           Markov   Decision   Process           Reinforcement   Learning           model - based   Learning                   深度 强化 学习 tutorial @ ICML2016           Value - Based   Deep   RL           Q - Learning ：           DQN ： 利用   agent   自身 经验 构建 样本 ！           Double   DQN           Prioritised   replay           Duelling   network           Deep   Policy   Networks                   相关 资料                 关于       强化 学习 是 未来 很 重要 的 方向 ！ 参考   Alpha   Go ！       Markov   Decision   Process           A   set   of   states   $ ( s   \ \ in   S ) $       A   set   of   actions   $ ( a   \ \ in   A ) $       A   transition   function   $ ( T ( s ,   a ,   s ' ) ) $       转移 概率   $ ( P ( s ' |   s ,   a ) ) $       也 叫   model   或者   dynamics               A   reward   function   $ ( R ( s ,   a ,   s ' ) ) $       通常 只是 状态 的 函数 $ ( R ( s ) ,   R ( s ' ) ) $               一个 初始状态       一个 终止 状态           MDP   不是 一个 确定性 的 搜索 问题 ， 一种 方法 是     期望值 最大 搜索     ？ ！       马尔科夫 性说 的 是 未来 的 决策 与 过去 无关 ， 至于 当前 状态 和 策略 有关 ！       （ 最优 ） 策略 ： $ ( \ \ pi ^ *   :   S   \ \ rightarrow   A ) $       状态 转移 函数   $ ( T ( s ,   a ,   s ' )   =   p ( s ' |   s ,   a ) ) $       discounting   reward :   $ ( r _ 0   +   \ \ gamma   r _ 1   +   \ \ gamma ^ 2   r _ 2   +   ... ) $       当   $ ( \ \ gamma   & lt ;   1 ) $ 时 ， discounting   reward   有 界   $ ( R _ { max } / ( 1 - \ \ gamma ) ) $ ！       或者 采用   finite   horizon ， 当 迭代 到   T   步时 ， 停止 ！               MDP   quantities :           Policy   =   对 每 一个 状态 选择 一个 action       Utility   =   sum   of   ( discounted )   rewards                   状态   value   function ：   $ ( V ^ *   ( s ) ) $           q - state   ( s ,   a ) ,   $ ( Q ^ *   ( s , a ) ) $           最优 策略   $ ( \ \ pi ^ *   ( s ) ) $               递归 定义 ：           $ ( V ^   *   ( s )   =   \ \ max _ a   Q ^   *   ( s ,   a ) ) $       $ ( Q ^   *   ( s ,   a )   =   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ *   ( s ' )   \ \ right ] ) $       $ ( V ^   *   ( s )   =   \ \ max _ a   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ *   ( s ' )   \ \ right ] ) $                   Time - limited   value :   定义 $ ( V _ k ( s ) ) $   为 状态 s 下 ， 最多 k 步下 的 最优 value           Value   iteration :       $ ( V _ 0 ( s )   =   0 ) $       $ ( V _ { k + 1 } ( s )   \ \ leftarrow   \ \ max _ a   \ \ sum _ { s ' }   ( s ,   a ,   s ' )   \ \ left [   R ( s ,   a ,   s ' )   +   \ \ gamma   V _ k ^ *   ( s ' )   \ \ right ] )   ) $       重复 1 - 2 直到 收敛 ！       复杂度 ， 每次 迭代   $ ( O ( S ^ 2A ) ) $       收敛 到 唯一 最优 值 ！ 贝尔曼 算子 在   $ ( \ \ gamma & lt ; 1 ) $ 时时 压缩 算子 ， 所以 必 收敛 到 不动点 。               Policy   iteration       随机化 策略 $ ( \ \ pi ) $       Policy   evaluation ： 对 给定 的 策略   $ ( \ \ pi ) $ ， 利用 迭代 或者 线性方程 求解 的 方法 计算 该 策略 下 的 值 函数 ， 即 求解 下面 第一个 方程 。 因为 该 方程 是 一个 关于 值 函数 的 线性方程 ， 所以 对于 有限 状态 的 情况 可以 直接 求解 线性方程 ， 或者 利用 迭代 求解 。       Policy   improvement ： 对 上述 值 函数 ， 利用 贝尔曼 方程 求 出 最优 策略 。 重复 2 - 3 多次 直到 收敛 ， 收敛 条件 是 策略 不 改变 了 。           实际上 ， 它 是 在 交替 迭代 策略 和 值 函数 。 这种 方法 可以 保证 每次 迭代 值 函数 单调 不减 ， 又 因为 有 界 所以 收敛 。                   $ $     V ^ { \ \ pi _ i } ( s )   =   r ( s ,   \ \ pi _ i ( s ) )   +   \ \ gamma   \ \ sum _ { s '   \ \ in   S }   p ( s ' | s ,   \ \ pi _ i ( s ) )   V ^ { \ \ pi _ i } ( s ' )   \ \ \ \     \ \ pi _ { i + 1 } ( s )   =   \ \ arg   \ \ max _ a   r ( s ,   a )   +   \ \ gamma   \ \ sum _ { s '   \ \ in   S }   p ( s ' | s ,   a )   V ^ { \ \ pi _ i } ( s ' )     $ $       Reinforcement   Learning       仍然 是 一个 MDP 过程 ， 仍然 寻找 最优 决策 ！     但是 不 知道 状态 转移 函数 T   和   回报 函数   R ！           MDP ： Offline       RL ： Online           model - based   Learning       通过 经验 ， 学习 一个 近似 模型 ， 然后 求解 ！           Step1 ： 学习 经验 MDP 模型 ：       对 每 一个个 ( s ,   a ) 统计 s '       归一化 得到 $ ( \ \ hat { T } ( s ,   a ,   s ' ) ) $     3 .                   深度 强化 学习 tutorial @ ICML2016       AI   =   RL   +   DL           值 函数       策略       状态 转移 模型           使用 深度 学习 建模 值 函数 ， 策略 和 模型 ！       Value - Based   Deep   RL       Q - Networks       $ $     Q ( s ,   a ,   w )   \ \ approx   Q ^ *   ( s ,   a )     $ $       Q - Learning ：       最优   Q - value   应该 遵循   Bellman   方程       $ $     Q ^   *   ( s ,   a )   =   \ \ mathbb { E } _ { s ' }   \ \ left [   r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ) ^   *   | s ,   a     \ \ right ]     $ $       其中   s   表示 状态 ， a   表示 agent 对 环境 做出 的   action ！     将 方程 右边 当做 目标 ， 用 神经网络 学习 ！     损失 函数 ：       $ $     l   =   ( r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w )   -   Q ( s ,   a ,   w )   ) ^ 2     $ $       问题 ： 1 .   训练样本 不是   iid ；   2 .   目标 不 稳定 ！       DQN ： 利用   agent   自身 经验 构建 样本 ！       $ $     l   =   \ \ left ( r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w ^ - )   -   Q ( s ,   a ,   w )   \ \ right ) ^ 2     $ $       在 某 一次 replay   的 更新 中 ， $ ( w ^ - ) $ 是 固定 的 ！ replay 结束 后 ， 将线 上 的 权值 $ ( w ) $ 更新 到 $ ( w ^ - ) $       Double   DQN           当前 的   Q - network   w   用来 选择   action       老 的   Q - network   w -   用来 评估   action           $ $     a ^ *   =   \ \ arg \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w )   \ \ \ \     l   =   \ \ left (   r   +   \ \ gamma   Q (   s ' ,   a ^   *   ,   w ^ -   )   -   Q ( s ,   a ,   w )     \ \ right ) ^ 2     $ $       Prioritised   replay       按照   TD - error   对   replay   memory   中 的 样本 进行   importance   sampling 。       $ $     \ \ delta   =   \ \ left |   r   +   \ \ gamma   \ \ max _ { a ' }   Q ( s ' ,   a ' ,   w ^ - )     -   Q ( s ,   a ,   w )   \ \ right |   \ \ \ \     P ( i )   =   \ \ frac { p _ i ^ { \ \ alpha } } { \ \ sum _ k   p _ k ^ { \ \ alpha } }   \ \ \ \     p _ i   =   \ \ delta _ i   +   \ \ epsilon     $ $       Duelling   network       将   Q   函数 分解 为 状态值 函数 与   advantage   function （ 不 知道 怎么 翻译 ）   之 和 。       $ $     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   A ( s , a ;   \ \ theta ,   \ \ alpha )     $ $       上式 V 和 A 之间 是 不定 的 ， 可以 相差 一个 任意 常数 ， 不 影响 结果 。 为此 ， 有 两种 解决方案 ， 减 最大值 和 平均值 。     平均值 方案 更加 稳定 ， 因为 V 只 需要 跟踪 平均 波动 ， 而 不是 最大 波动 。       $ $     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   \ \ left (   A ( s , a ;   \ \ theta ,   \ \ alpha )   -   \ \ max _ { a ' \ \ in   \ \ mathcal { A } }   A ( s , a ' ;   \ \ theta ,   \ \ alpha )   \ \ right )     \ \ \ \     Q ( s , a ;   \ \ theta ,   \ \ alpha ,   \ \ beta )   =   V ( s ;   \ \ theta ,   \ \ alpha )   +   \ \ left (   A ( s , a ;   \ \ theta ,   \ \ alpha )   -   \ \ frac { 1 } { | \ \ mathcal { A } | }   \ \ sum _ { a ' }   A ( s , a ' ;   \ \ theta ,   \ \ alpha )   \ \ right )     $ $               Deep   Policy   Networks       用 神经网络 建模 策略 函数       $ $     a   =   \ \ pi ( a |   s ,   \ \ mathbf { u } )   =   \ \ pi (   s ,   \ \ mathbf { u } )     $ $       目标 函数 为 total   discounted   reward       $ $     J ( u )   =   \ \ mathbf { E } ( r _ 1   +   \ \ gamma   r _ 2   +   \ \ gamma ^ 2   r _ 3   +   ...   |   \ \ pi ( . ,   u ) )     $ $       令 $ ( \ \ tao   =   ( s _ 1 ,   a _ 1 ,   ... ,   s _ t ,   a _ t ) ) $ 代表 状态 - 动作 路径 ， 用 $ ( r ( \ \ tao ) ) $ 代表 每个 路径 的 折扣 reward ， 那么 期望 回报 函数       $ $     J ( \ \ theta )   =   \ \ int   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao     $ $       对 参数 $ ( \ \ theta ) $ 求导 ， 由于 回报 函数 与 参数 无关 ， 所以 梯度 只 作用 与 策略 函数       $ $     \ \ nabla _ { \ \ theta }   J ( \ \ theta )   =   \ \ int   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao   = \ \ int     \ \ pi _ { \ \ theta } ( \ \ tao )   \ \ nabla _ { \ \ theta }   \ \ log   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )   d \ \ tao   =   E _ { \ \ tao   ~   \ \ pi _ { \ \ theta } ( \ \ tao ) }   \ \ nabla _ { \ \ theta }   \ \ pi _ { \ \ theta } ( \ \ tao )   r ( \ \ tao )     $ $       利用 马尔科夫 性 ，       $ $     \ \ log   \ \ pi _ { \ \ theta } ( \ \ tao )   =   \ \ log   p ( s _ 1 )   +   \ \ sum _ { t = 1 } ^ T   \ \ left [ \ \ log   \ \ pi _ { \ \ theta } ( a _ t | s _ t )   +   \ \ log   p ( s _ { t + 1 } | s _ t ,   a _ t )   \ \ right ]     $ $       代入 上式 可得       $ $     \ \ nabla _ { \ \ theta }   J ( \ \ theta )   =   E _ { \ \ tao   ~   \ \ pi _ { \ \ theta } ( \ \ tao ) }   \ \ sum _ { t = 1 } ^ T     \ \ nabla _ { \ \ theta } \ \ log   \ \ pi _ { \ \ theta } ( a _ t | s _ t )     r ( \ \ tao )     $ $       如果 把 策略 函数 看做 在 状态 s 下 选择 动作 a 的 概率 ， 回报 是 该 样本 的 权重 ！ 即 加权 极大 似然 估计 ！       相关 资料           强化 学习 书籍 ：   https : / / webdocs . cs . ualberta . ca / ~ sutton / book / ebook / the - book . html         Tutorial :   Deep   Reinforcement   Learning ，   ICML   2016 .   David   Silver ,   Google   Deepmind .       CS234 :   Reinforcement   Learning     http : / / web . stanford . edu / class / cs234 / index . html         Berkeley   课程 ： CS   294 :   Deep   Reinforcement   Learning .     http : / / rll . berkeley . edu / deeprlcourse /           http : / / ai . berkeley . edu / course _ schedule . html        ", "tags": "machine-learning", "url": "/wiki/machine-learning/reinforcement-learning.html"},
      
      
      {"title": "文本摘要相关算法汇总", "text": "    Table   of   Contents           关于           关键 短语 提取 ： review           Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information           TextRank           KeyCluster           Topical   PageRank                 关于       汇总 文本 摘要 相关 的 模型 、 算法 即 评估 指标       关键 短语 提取 ： review       论文 ： Automatic   Keyphrase   Extraction :   A   Survey   of   the   State   of   the   Art ， Kazi   Saidul   Hasan   and   Vincent   Ng               定义 ： 自动 选择 出 文档 中 的 重要 的 、 表达 主题 的 短语 ！ 确定 文档 中 最 有 表达能力 的 少量 关键词 ！           Peter   Turney .   2000 .   Learning   algorithms   for   keyphrase   extraction .   Information   Retrieval ,   2 : 303 – 336 .       Takashi   Tomokiyo   and   Matthew   Hurst .   2003 .   A   lan -   guage   model   approach   to   keyphrase   extraction .   In   Proceedings   of   the   ACL   Workshop   on   Multiword   Ex -   pressions ,   pages   33 – 40 .       Zhiyuan   Liu ,   Peng   Li ,   Yabin   Zheng ,   and   Maosong   Sun .   2009b .   Clustering   to   find   exemplar   terms   for   keyphrase   extraction .   In   Proceedings   of   the   2009   Conference   on   Empirical   Methods   in   Natural   Lan -   guage   Processing ,   pages   257 – 266 .       Zhuoye   Ding ,   Qi   Zhang ,   and   Xuanjing   Huang .   2011 .   Keyphrase   extraction   from   online   news   using   binary   integer   programming .   In   Proceedings   of   the   5th   In -   ternational   Joint   Conference   on   Natural   Language   Processing ,   pages   165 – 173 .       Xin   Zhao ,   Jing   Jiang ,   Jing   He ,   Yang   Song ,   Palakorn   Achanauparp ,   Ee - Peng   Lim ,   and   Xiaoming   Li .   2011 .   Topical   keyphrase   extraction   from   Twitter .   In   Proceedings   of   the   49th   Annual   Meeting   of   the   Association   for   Computational   Linguistics :   Human   Language   Technologies ,   pages   379 – 388 .       Zhiyuan   Liu ,   Wenyi   Huang ,   Yabin   Zheng ,   and   Maosong   Sun .   2010 .   Automatic   keyphrase   extrac -   tion   via   topic   decomposition .   In   Proceedings   of   the   2010   Conference   on   Empirical   Methods   in   Natural   Language   Processing ,   pages   366 – 376 .                   应用 ： 文档 快速 、 高精度 的 检索 ； 提升 自然语言 处理 的 其他 任务 ： 文本 摘要 ， 文本 分类 ， 观点 挖掘 ， 文档 索引 ；           Yongzheng   Zhang ,   Nur   Zincir - Heywood ,   and   Evangelos   Milios .   2004 .   World   Wide   Web   site   summariza -   tion .   Web   Intelligence   and   Agent   Systems ,   2 : 39 – 53 .       Ga   ́ bor   Berend .   2011 .   Opinion   expression   mining   by   exploiting   keyphrase   extraction .   In   Proceedings   of   the   5th   International   Joint   Conference   on   Natural   Language   Processing ,   pages   1162 – 1170 .                   影响 关键词 提取 的 几个 主要 因素 ：           长度 ： 长 文档 的 候选词 更 多       结构 一致性 ： 科技 文档 的 结构 非常 一致 ， 可以 利用 abstract 提取 关键词 ！       主题 的 变化 ： 科技 文档 的 主题 在 同一个 文档 中 基本 不变 ， 但是 对话 则 经常 随 时间 变化 ！       主题 想 关心 ： 非正式 文档 的 多个 主题 可能 并 不 相关 。                   关键词 提取 方法 ：           利用 一些 启发式 方法 提取 一个 关键词 列表       利用 监督 或者 无 监督 学习 确定 一个 关键词 是否是 正确 的 关键词                   候选词 选择 ：           启发式 的 规则 ， 减少 候选词 数目       停止 词 列表 ： 移除 停止 词       保留 特定 词性 的 词 ： 名词 、 形容词 、 动词 etc       利用 其它 信息 ： 允许 维基百科 词条 的   n - gram       保留 满足 特定 词法 模式 的   n - gram       其它 减枝 技术               监督 学习 方法 ： Task   Reformulation ， feature   design       Task   Reformulation ：       二 分类 标注 ： 给定 一个 关键词 和 文档 ， 预测 该 关键词 是否是 该 文档 的 关键词 ； 缺点 是 不能 确定 哪些 词 更 有 表达能力 ！       Peter   Turney .   1999 .   Learning   to   extract   keyphrases   from   text .   National   Research   Council   Canada ,   In -   stitute   for   Information   Technology ,   Technical   Report   ERB - 1057 .       Peter   Turney .   2000 .   Learning   algorithms   for   keyphrase   extraction .   Information   Retrieval ,   2 : 303 – 336 .       Ian   H .   Witten ,   Gordon   W .   Paynter ,   Eibe   Frank ,   Carl   Gutwin ,   and   Craig   G .   Nevill - Manning .   1999 .     KEA   :   Practical   automatic   keyphrase   extraction .   In   Pro -   ceedings   of   the   4th   ACM   Conference   on   Digital   Li -   braries ,   pages   254 – 255 .               排序 方法 ： pairwise 的 排序 方法 效果 明显 优于 二 分类 的 方法     KEA   ！       Xin   Jiang ,   Yunhua   Hu ,   and   Hang   Li .   2009 .   A   ranking   approach   to   keyphrase   extraction .   In   Proceed -   ings   of   the   32nd   International   ACM   SIGIR   Confer -   ence   on   Research   and   Development   in   Information   Retrieval ,   pages   756 – 757 .                           特征 设计 ： 1 .   文档 内 特征 ； 2 .   文档 外 特征           统计 特征 ：       tf * idf , 不 解释       第一次 出现 的 位置 距 文档 开头 的 归一化 距离 ； 通常 关键词 出现 在 文档 的 头部       短语 在 训练 集中 作为 关键词 的 次数       其他 统计 信息 ： 短语 长度 ， 短语 跨度 （ 第一次 出现 和 最后 一次 出现 的 距离 ）               结构特征 ： 短语 出现 在 科技 文档 不同 章节 的 频率 ， 出现 在 网页 metadata 的 频率 etc       句法 特征 ： 当有 其他 特征 时 ， 这 类 特征 没 啥 用       将 短语 编码 为   POS   序列 ， 例如 编码 为   动词 - 名词 ； 形容词 - 名词   etc       词法 后缀 序列 ， 貌似 只有 拉丁语系 才 有 ，   full - tion ,   less - tion   etc               维基百科 类 特征 ： 是否 作为 维基百科 词条 ？ etc       是否 作为 搜索 关键词 ？       两个 候选词 的 语义 相关性 特征 ！                   无 监督 学习 方法 ：           Graph - Based   Ranking :       一个 词是 重要 的 ： 1 ， 与 大量 其他 候选词 是 相关 的 ； 2 ， 候选词 是 重要 的 ！       词 的 关系 通过 共生 矩阵 来 描述 （ 实际上 现在 可用 过词 向量 来 描述 啦 ）       一个 文档 的 词用 一个 图来 描述 ， 图 的 节点 是 词 ， 边 的 权重 是 词 的 关系 。 一个 节点 的 score 由 他 的 邻居 的 score 决定 ！ 选出 TOP 个 节点 即可 ！       TextRank ： Rada   Mihalcea   and   Paul   Tarau .   2004 .   TextRank :   Bringing   order   into   texts .   In   Proceedings   of   the   2004   Conference   on   Empirical   Methods   in   Natural   Language   Processing ,   pages   404 – 411 .       缺点 在于 选取 的 词 无法 覆盖 文档 的 全部 主要 信息 ！               Topic - Based   Clustering       将 候选词 按 主题 聚类       KeyCluster :   利用 维基百科 和 共生 矩阵 聚类 相似 的 词 ， 对 每 一个 类 （ 主题 ） 选出 最靠近 中心 的 词 ！ 缺点 在于 并 不是 所有 的 主题 都 重要 ！ 这种 方法 给 每 一个 主题 相同 的 权重 ！       Topical   PageRank ： 利用 textrank 对 每个 主题 内 的 词 排序 ， 词 的 最终 score 是 在 各个 主题 中 的 score 的 加权 和 ， 权重 是 该 主题 在 文档 中 的 概率 ！       CommunityCluster ： 保留 重要 主题 下 的 所有 候选词 ！               Simultaneous   Learning       Language   Modeling               评估       典型 方法 ：       to   create   a   mapping   between   the   keyphrases   in   the   gold   standard   and   those   in   the   system   output   using   exact   match       score   the   output   using   evaluation   metrics   such   as   precision   ( P ) ,   recall   ( R ) ,   and   F - score   ( F ) .               BLEU ， METEOR ,   NIST ,   and   ROUGE   解决 精确 匹配 的 问题       R - precision                   Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information       论文 ： Keyword   Extraction   from   a   Single   Document   using   Word   Co - occurrence   Statistical   Information ， Y .   MATSUO ， M .   Ishizuka ， 2003       基本 思想 ： 只 需要 单个 文档 （ 长 文档 ） ， 首先 提取 高频词 ， 如果 一个 词 与 高频词 的 共现 关系 通过 卡方 检验 ， 就 认为 是 可能 的 关键词 。       TFIDF ： 在 该 文档 经常出现 ， 但是 在 整个 语料 中 出现 得 不 那么 频繁 的 词 ！           automatic   term   recognition ， automatic   indexing ， automatic   keyword   extraction       本文 的 方法 只 需要 一篇 文档 ， 不 需要 语料       选出 高频词 ， 统计 高频词 出现 的 频率       统计 词 与 高频词 的 共现 矩阵               如果 一个 词 经常 与 一个 高频词 子集 共现 ， 那么 这个 词 就 有 高 的 概率 是 关键词 ！ 这种 偏差 用卡方 统计 量 来 度量       如果 一个 词 w 跟 高频词 的 任何 子集 都 没有 特殊 的 共现 关系 ， 那么 共现 矩阵 中 w 的 分布 期望值 应该 就是 高频词 本身 的 分布 。     反之 ， 则 实际 分布 与 这种 期望 分布 存在 较大 偏差 ， 可以 用卡方 统计 量 度量 这种 偏差 。           $ $     \ \ chi ^ 2 ( w )   =   \ \ sum _ { g   \ \ in   G }   \ \ frac { ( freq ( w ,   g )   -   n _ w   p _ g ) ^ 2 } { n _ w   p _ g }     $ $       这里 w 是 某个 待 检验 的 词 ， $ ( g   \ \ in   G ) $   是 高频词 ， G 是 高频词 组成 的 集合 。 $ ( n _ w ) $ 是 w 在 共现 矩阵 中 出现 的 总数 ，     $ ( p _ g ) $ 是 高频词 g 在 高频词 中 的 归一化 频率 。           优化 ：       针对 长短句 不同 带来 的 共现 偏差 ， 重新 定义 $ ( p _ g ) $ 为   g 出现 的 句子 中词 的 数目 / 文档 的 总词 数目 ;   $ ( n _ w ) $ 定义 为 w 出现 过 的 句子 中 的 总词 数目 ！       增加 鲁棒性 ， 防止 某个 词 只 跟 某 一个 特定 的 高频词 高度 共现 ， 方法 是 减去 这个 高度 共现 的 部分 ， 即 最大值 。                   $ $     \ \ chi ' ^ 2 ( w )   =   \ \ chi ^ 2 ( w )   -   \ \ max _ { g   \ \ in   G }   \ \ frac { ( freq ( w ,   g )   -   n _ w   p _ g ) ^ 2 } { n _ w   p _ g }     $ $           词聚类 ： 将 高频词 相似 的 词聚类 ！ 词 的 相似 度 基于     Jensen - Shannon   divergence     度量 ！ 然后 采用 pairwise 聚类 ， 利用 交互 信息量 度量 。           TextRank       论文 ： TextRank :   Bringing   Order   into   Texts ， Rada   Mihalcea   and   Paul   Tarau ， 2004       带权   PageRank       $ $     WS ( V _ i )   =   ( 1 - d )   +   d   \ \ sum _ { V _ j   \ \ in   IN ( V _ i ) }   \ \ frac { w _ { ji } } { \ \ sum _ { V _ k   \ \ in   OUT ( V _ j ) }   w _ { jk } }   WS ( V _ j )     $ $       WS   是 定点 的   PageRank   score 。 随机 初始化 ， 然后 迭代 收敛 ！           将 文本 作为 一个 图 ： 每 一个 词 做 顶点 ， 如果 两个 词 出现 在 同一个 上下文 窗 （ 大小 认为 设定 ， 试验 中窗 大小 为 2 比较 好 ） ， 那么 就 有 一条 边 。 只 使用 形容词 和 名词 ！       多个 词 通过 后处理 得到 ， 例如 两个 词 A ， B 都 在 TOPN 中 ， 并且 这 两个 词 相邻 ， 那么 AB 就是 一个 新 的 关键词       关键词 评估 指标 ， P ， R ， F1       句子 的 相似性 通过 公共 词 数目 定义 ， 评估 指标   ROUGE           $ $     similarity ( S _ i ,   S _ j )   =   \ \ frac { | \ \ { w _ k |   w _ k   \ \ in   S _ i   ,   w _ k   \ \ in   S _ j   \ \ } | } { \ \ log { | S _ i | }   +   \ \ log { | S _ j | } }     $ $       KeyCluster       论文 ： Clustering   to   Find   Exemplar   Terms   for   Keyphrase   Extraction ， Zhiyuan   Liu ,   Wenyi   Huang ,   Yabin   Zheng   and   Maosong   Sun           关键词 的 几个 目标 ： 可 解释性 ； 相关性 ； 对 主题 的 覆盖率       无 监督 聚类 方法 ：       首先 将 文档 的   term   按照 语义 聚类 ， 每 一类 用 一个 代表   term   表达 ， 每 一个 类 的 中心   term               算法 流程 ：       候选词 选择 ： 过滤 停止 词 等 无 意义 词       计算   term   间 的 语义 相关性 度量 ： Wikipedia 的 tfidf ， pmi ， ngd       基于 相关性 将 term 聚类 ： Hierarchical   Clustering ， Spectral   Clustering ， Affinity   Propagation       使用 每个 类 的 代表 词 提取 关键词 ： 选出   代表 词   的 组合 短语 作为 关键词                   Topical   PageRank       论文 ： Automatic   Keyphrase   Extraction   via   Topic   Decomposition       基本 思想 ， pagerank 的 时候 ， 只 关注 某 一个 主题 ， 求 出 每个 term 在 该 主题 先 的 rank 后 ， 然后 按照 文档 的 主题 分布 加权 得到 最终 的 rank 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/text-sum.html"},
      
      
      {"title": "智能运营相关资料整理", "text": "    Table   of   Contents           关于           论文           案例                 关于       智能 运营 是 指用 人工智能 相关 算法 优化 在 电子商务 和 传统 公司 运营 相关 的 业务 指标 ， 在 当下 的 运营 中 ， 主流 还是 以 人工 策略 为主 ， 相信 在 未来 一定 会 成为 技术 主导 的 事情 ， 这里 总结 一下 相关 论文 和 案例 。       论文           价格 敏感度 建模 ： Modeling   Consumer   Preferences   and   Price   Sensitivities   from   Large - Scale   Grocery   Shopping   Transaction   Logs           案例  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-fo-operator.html"},
      
      
      {"title": "最大熵模型", "text": "    Table   of   Contents           PPT   记录           最大 熵 模型   -   多元 逻辑 回归           最大 熵 模型           reference                 PPT   记录           conditional   or   discriminative   probabilistic   models   in   NLP   ,   IR ,   and   Speech       联合 概率   or   条件 概率 ： 生成 模型   or   判别 模型           最大 熵 模型   -   多元 逻辑 回归       可以 证明 ， 多元 逻辑 回归 是 在 存在 约束 的 条件 下 ， 最大化 输出 的 分布   $ ( \ \ hat { y }   =   p ( y | x ) ) $   的 熵 。       LogisticRegressionMaxEnt . pdf   。       对于 链接 中 的 证明 ， 有个 问题 是 ， 那个 关键 的 约束 关系 是 通过 逻辑 回归 的 式子 ， 利用 极大 似然 导出 的 ，     后面 又 用来 证明 从 最大 熵 原理 推导 出 具体 表达式 ， 这 不是 循环论证 么 ？ ！       结论 ： 最大 熵 模型 就是 多元 逻辑 回归 ， 他 也 是 建模 条件 概率 ， 而 不是 后验 概率 或者 联合 概率 ， 是 判别 模型 ；     通常 我们 说 建模 后验 概率 （ 贝叶斯 的 方法 ） 实际上 就是 建模 联合 概率 ， 是 生成 模型 。       最大 熵 模型       给定 数据 集   $ ( ( x ,   y ) ) $ ， 其 经验 分布 为 $ ( \ \ tilde { p } ( x ,   y ) ) $ 。 $ ( f ( x ,   y ) ) $   是 样本空间 到 实数 集合 的 映射 ， 成为 特征 ！     目标 是 学习 得到 条件 分布 $ ( p ( y | x ) ) $ ！ 为此 ， 可以 让 该 条件 分布 满足 最大 熵 原理 即       $ $     \ \ max   H ( p )   =   -   \ \ sum _ { x , y }   \ \ tilde { p } ( x )   p ( y | x )   \ \ log   p ( y | x )   \ \ \ \     s . t .   p ( f _ i )   =   \ \ tilde { p } ( f _ i ) ,   i = 1 , ... , n     \ \ \ \     \ \ sum _ { y }   p ( y | x )   =   1           \ \ \ \     $ $     这里 ：     $ $     p ( f _ i )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x ) p ( y | x )   f _ i ( x , y )   \ \ \ \     \ \ tilde { p } ( f _ i )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x ,   y )   f _ i ( x , y )     $ $       上面 $ ( p ( f _ i ) ,   \ \ tilde { p } ( f _ i ) ) $   分别 表示 特征 $ ( f _ i ) $ 在 样本 中 的 期望值 和 在 条件 分布 $ ( p ) $ 下 的 期望值 ，     约束条件 要求 这 两者 相同 。 如果 没有 这个 约束 ， 那么 最大 熵 的 分布 就是 均匀分布 （ 闭 区间 ） 、 指数分布 （ 半 无限 区间 ， 均值 约束 ） 和 正态分布 （ 无限 区间 ， 均值 和 方差 约束 ） 了 ！       利用 拉格朗 日 对偶原理 ， 可 得 拉格朗 日 函数 ：       $ $     L ( p ,   \ \ lambda ,   \ \ gamma )   =   -   \ \ sum _ { x , y }   \ \ tilde { p } ( x )   p ( y | x )   \ \ log   p ( y | x )   +   \ \ sum _ i   \ \ lambda _ i   ( p ( f _ i )   -   \ \ tilde { p } ( f _ i ) )   + \ \ gamma   ( \ \ sum _ { y }   p ( y | x )   -   1 )     $ $       由   KKT   条件 ， $ ( \ \ partial   L   /   \ \ partial   p ( y | x )   =   0 ,   \ \ partial   L   /   \ \ partial   \ \ gamma   =   0 ) $ 可 得 ：       $ $     p ^ *   ( y | x )   =   \ \ frac { 1 } { Z _ { \ \ lambda } ( x ) }   \ \ exp ( \ \ sum _ i   \ \ lambda _ i   f _ i ( x , y ) )     \ \ \ \     Z _ { \ \ lambda } ( x )   =   \ \ sum _ y   \ \ exp ( \ \ sum _ i   \ \ lambda _ i   f _ i ( x , y ) )     $ $       而 其 对偶 问题 优化 的 对象 变为       $ $     \ \ Phi ( \ \ lambda )   =   - \ \ sum _ x   \ \ tilde { p } ( x ) \ \ log   Z _ { \ \ lambda } ( x )   +   \ \ sum _ i   \ \ lambda _ i   \ \ tilde { p } ( f _ i )     $ $       这个 目标 函数 可以 看做 似然 对数 ！ 其中 条件 概率 采取 上述 指数 模型 建模 ！ （ 和 逻辑 回归 模型 一致 ）       $ $     L _ { \ \ tilde { p } } ( p )   =   \ \ sum _ { x , y }   \ \ tilde { p } ( x , y )   \ \ log ( y | x )     $ $       和 逻辑 回归 的 关系 ： 取 特征 映射 为 $ ( f _ i ( x ,   y )   =   f _ i ( x ) ) $ 即 样本 的 第 $ ( i ) $ 个 特征 分量 ！       reference           stanford   NLP   ppt     http : / / nlp . stanford . edu / pubs / maxent - tutorial - slides - 6 . pdf        ", "tags": "machine-learning", "url": "/wiki/machine-learning/maxent.html"},
      
      
      {"title": "机器学习 - 周志华", "text": "    Table   of   Contents           关于           绪论           线性 模型           类别 不 平衡 问题           集成 学习           集成 学习 基本 思想           Hoeffding   不等式 的 推导           Boosting                         关于       南 大 周志华 老师 写得 机器 学习 是 国内 少有 的 中文 机器 学习 教材 ， 很多 人 推荐 ，     所以 看看 。       绪论           NFL 定理           线性 模型           最大 熵 模型 与 多元 逻辑 回归 的 关系 ？ 没 啥 区别                     https : / / www . quora . com / What - is - the - relationship - between - Log - Linear - model - MaxEnt - model - and - Logistic - Regression           https : / / stackoverflow . com / questions / 21241602 / maximum - entropy - model - and - logistic - regression                     类别 不 平衡 问题       三种 策略 ： （ 假设 负 样本 比正 样本 多 很多 ， 实际 遇到 的 问题 基本上 都 是 这种 问题 ）           -   对负 样本 欠 采样 ， 简单 的 欠 采样 可能 会 丢失 关键 信息 ， 代表性 算法   EasyEnsemble   [ Liu   ,   2009 ]   利用 集成 学习 的 机制 ， 将 反例 划分 为 若干个 集合 ， 供 不同 的 学习 使用 ， 这样 在 每 一个 学习 奇 看来 ， 都 是 欠 采样 ， 但 全局 来看 却 不会 丢失 信息 。     -   对 正 样本 过 采样 ， 简单 的 重复 样本 会 导致 严重 的 过 拟合 ， 代表性 算法   SMOTE   [ Chawla ,   2002 ]   通过 对 训练 集正 样本 进行 插值 来 产生 额外 的 正例 。     -   resale ， 直接 基于 原始 训练 机 进行 学习 ， 在 预测 的 时候 采用 阈值 移动 的 策略 ， 将 正负 样本 的 比例 因素 考虑 进去 。       集成 学习       集成 学习 基本 思想       利用 很多 个 独立 的 （ 或者 不同 的 ） 弱 分类器 ， 进行 投票 得到 一个 强 的 分类器 。 该 理论 可以 由 下面 的 推导 得到       设 每个 弱 分类器 的 性能 为 $ ( \ \ epsilon ) $ ， 即     $ $     P ( h _ i ( x )   \ \ neq   f ( x ) )   =   \ \ epsilon     $ $     这里 $ ( h _ i ) $ 是 弱 分类器 的 判决 函数 ， $ ( f ) $ 是 要 学习 的 未知 函数 。     那么 $ ( T ) $ 个 这样 的 分离器 采用 简单 投票 策略 的 分类器 $ ( H ) $ 判决 错误 ， 要求 一半 以上 的 判错 ， 概率 为     $ $     P ( H ( x )   \ \ neq   f ( x ) )   =   \ \ sum _ { k = 0 } ^ { T / 2 }   \ \ binom { T } { k }   ( 1 - \ \ epsilon ) ^ k   \ \ epsilon ^ { T - k }     \ \ \ \                                         \ \ le   \ \ exp ( - \ \ frac { 1 } { 2 } T ( 1 - \ \ epsilon ) ^ 2 )     $ $     后面 这个 式子 基于   Hoeffding   不等式         可以 看出 ， 只要 T 充分 大 ， 就 可以 使得 误差 足够 小 ！ ！     个体 学习 器 的 准确性 和 多样性 确实 一个 矛盾体 ， 准确性 高 了 之后 ， 要 增加 多样性 就 会 牺牲 准确性 。       两种 类型 ：     -   个体 学习 期 之前 强 依赖 ， 需要 串行 ， Boosting     -   不 强 依赖 ， 可 并行 ， Bagging   和   随机 森林       Hoeffding   不等式 的 推导       Hoeffding 不等式 是 ， 如果 随机变量 $ ( X _ i ) $ 独立 ， 切 都 在 0 到 1 之间 ， 那么 有     $ $     P ( \ \ bar { X }   -   E   \ \ bar { X }   & gt ;   t )   \ \ le   e ^ { - 2   n   t ^ 2 }   \ \     P ( \ \ bar { X }   -   E   \ \ bar { X }   & lt ;   - t )   \ \ le   e ^ { - 2   n   t ^ 2 }     $ $     $ ( \ \ bar { X } ) $   是 这 n 个 随机变量 的 均值 。         Hoeffding   引理   ： 随机变量 的 指数函数 的 期望       如果 随机变量 $ ( X ) $ 满足 均值 为 0 ， 且 $ ( a   \ \ le   X   \ \ le   b ) ， 那么 其 指数函数 的 期望 的 上界 为     $ $     E   e ^ { \ \ lambda   X }   \ \ le   \ \ exp ( \ \ frac { 1 } { 8 }   \ \ lambda ^ 2 ( b - a ) ^ 2 )     $ $     该 引理 证明 可以 参考 维基百科 ， 它 的 基本思路 是 将 闭 区间 上 的 指数函数 扩大 为 链接 两端 点 的 线性 函数 ，     然后 对 两边 求 期望 ， 这样 讲 期望 去掉 了 。 剩下 的 就是 求 去掉 期望 后 的 函数 的 上界 （ 指数 界 ） 的 问题 了 。         Markov   不等式   ： 随机变量 概率 和 期望 的 不等式       随机变量 $ ( X ) $ 大于 0 ， 那么 对 正数 $ ( a ) $ 有     $ $     P ( X   \ \ ge   a )   \ \ le   E ( X ) / a     $ $     证明 很 简单 ， 将 概率 转换 为 示性 函数 的 期望 表示 即可 。 因为     $ $     I ( X   \ \ ge   a )   \ \ le   X   /   a     $ $     两边 求 期望 就是 了 。       利用 这个 引理 和   Markov   不等式 可以 证明 前面 的 不等式 ，     $ $     P ( \ \ bar { X }   -   E   \ \ bar { X }   & gt ;   t )   =   P ( e ^ { s ( \ \ bar { X }   -   E   \ \ bar { X } ) }   & gt ;   e ^ { st } )   \ \ \ \                             \ \ le   e ^ { - st }   E ( e ^ { s ( \ \ bar { X }   -   E   \ \ bar { X } ) } )       \ \ \ \                             \ \ le   e ^ { - st }   e { \ \ frac { 1 } { 8   n }   s ^ 2   }     $ $     注意 最后 一个 不等式 是 将 前 一个 式子 展开 成 n 个 乘积 后 ， 再 缩放 的 。     因为 上式 对 所有 的 s 都 成立 ， 所以 可以 取 一个 最小 的 作为 它 的 上界 ，     利用 二次 函数 的 性质 可得 上界 为 Hoeffding   不等式 的 右边 。       利用 Hoeffding   不等式 ， 令 其中 的 独立 随机变量 为 $ ( y _ i   =   I ( h _ i ( x )   =   f ( x ) ) ) $ ， 即 每个 分类器     是否 判决 正确 。 设 $ ( \ \ bar { y } } ) $ 是 这 T 个 变量 的 均值 ， 那么 有     $ $     P ( H ( x )   \ \ neq   f ( x ) )   =   P ( \ \ bar ( y )   & lt ;   0.5 )   =   P ( \ \ bar { y }   -   E   \ \ bar { y }   & lt ;   0.5   -   \ \ epsilon )   \ \ \ \             \ \ le   \ \ exp ( - 2   T   ( 0.5 - \ \ epsilon ) ^ 2 )     $ $     这 就是 前面 集成 学习 基本 理论 里面 的 那个 不等式 。       Boosting  ", "tags": "machine-learning", "url": "/wiki/machine-learning/ml-zzh.html"},
      
      
      {"title": "机器学习中的基本概念", "text": "    Table   of   Contents           关于           基础 统计 分布           几种 常见 的 分布           正态分布 的 和           卡方 分布           student - t   分布           F 分布           卡方 分布 、 t 分布 、 F 分布 的 联系           参考                   统计 检验           方差 统计           卡方 统计           F   classif           F   regression                   特征 变换           特征 Hash                         关于       记录 机器 学习 中 的 基本 问题 和 概念 。       基础 统计 分布       几种 常见 的 分布       包括 正态分布 、 泊松 分布 、 指数分布 等 。 略 ， 后面 可能 会 写 。       正态分布 的 和       两个 服从 正太 分布 的 随机变量   $ ( X _ 1 ,   X _ 2 ) $ ，   只要 其 联合 分布 为 正态分布 ， 那么 和 也 为 正态分布 。       卡方 分布       设 $ ( X _ 1 , ... , X _ n ) $   iid ， 服从 标准 正态分布 ， 那么 平方和 $ ( X _ 1 ^ 2 +...+ X _ n ^ 2 ) $ 服从 自由度 为 n 的 卡方 分布 。       卡方 分布 的 和 ：   $ ( X _ 1 , X _ 2 ) $ 独立 ， $ ( X _ 1   \ \ sim   \ \ chi _ m ^ 2 ,   X _ 2   \ \ sim   \ \ chi _ n ^ 2 ) $ ， 那么   $ ( X _ 1 + X _ 2   \ \ sim   \ \ chi _ { n + m } ^ 2 ) $       student - t   分布       $ ( X _ 1 , X _ 2 ) $ 独立 ， 且 $ ( X _ 1 \ \ sim \ \ chi _ n ^ 2 ,   X _ 2   \ \ sim   N ( 0 , 1 ) ) $ ， 那么 $ ( X _ 2   /   \ \ sqrt { X _ 1 / n } ) $ 服从 自由度 为 n 的 t 分布 。     一个 例子 ， 从 正态分布 总体 采样 的 n + 1 个 样本均值 对 样本 标准差 归一化 后 的 值 服从 自由度 为 n 的 t 分布 。       F 分布       $ ( X _ 1 \ \ sim \ \ chi _ n ^ 2 ,   X _ 2 \ \ sim \ \ chi _ m ^ 2 ) $ 且 统计 独立 ， 那么 $ ( m ^ { - 1 }   X _ 2   /   ( n ^ { - 1 }   X _ 1 ) ) $ 服从 自由度 为 $ ( ( m , n ) ) $ 的 F 分布       由于 F 分布 是 两个 卡方 分布 的 比值 ， 而卡方 分布 是 正态分布 的 平方和 ， 常 出现 在 方差 之中 ， 所以 F 分布 在 方差分析 之中 被 大量 使用 。       卡方 分布 、 t 分布 、 F 分布 的 联系       设   $ ( X _ 1 , ... , X _ n , Y _ 1 , ... , Y _ m ) $ 独立 同 分布 ( iid ) ， 服从 标准 正态分布 ， 记   $ ( \ \ bar { X }   =   ( X _ 1 +...+ X _ n ) / n ) $ ，     $ ( S ^ 2 = \ \ sum _ i   ( X _ i   -   \ \ bar { X } ) ^ 2 / ( n - 1 ) ) $ ， 则 ：                         $ ( ( n - 1 ) S ^ 2 ) $ 服从 自由度 为 n - 1 的 卡方 分布       $ ( \ \ sqrt { n } \ \ bar { X } / S ) $ 服从 自由度 为 n - 1 的 t 分布       $ ( [ S _ Y ^ 2 / ( m - 1 ) ] / [ S _ X ^ 2 / ( n - 1 ) ] ) $ 服从 自由度 为 ( m - 1 , n - 1 ) 的 F 分布           参考           陈希孺 ， 概率论 与 数理统计 ， 中国 科学技术 大学 出版社         Cochran   theaream             统计 检验       方差 统计       去掉 方差 太小 的 特征 。       卡方 统计       卡方 检验 ， 也 称 独立性 检验 ， 拟合 优度 检验 。 使用 要求 ， 自变量 为 正值 。       sklearn   中 用来 检验 正值 特征 与 目标 是否 独立 ， 从而 进行 特征选择 。       例如 变量   X   为 性别 （ 男 0 ， 女 1 ） ， 变量   Y   为 是否 为 左撇子 （ 否   0 ,   是   1 )   。 对于 某个 样本 ，     有列 联表 ：                         男       女       总计                       否       43       44       87               是       9       4       13               总计       52       48       100                   从 数据 中 ， 可以 看到 几个 边缘 分布 ：       $ $     P ( 男 )   =   0.52 ,   P ( 女 )   =   0.48   \ \ \ \     P ( 否 )   =   0.87 ,   P ( 是 )   =   0.13     $ $       如果 两个 变量 是 独立 的 ， 那么 列联表 里面 的 分布 应该 由式       $ $     P ( X ,   Y )   =   P ( X )   P ( Y )     $ $       得到 ， 我们 将 这个 值 作为 期望值 ， 记 作 $ ( E _ { i , j } ) $ ， 而 将 实际 值 记作 $ ( O _ { i , j } ) $ ， 例如 ， 男性 不是 左撇子 的 期望值 为     $ ( E _ { 1 , 1 }   =   100   *   0.52 * 0.87   =   45 ) $ ， 而 观测 值为 $ ( O _ { 1 , 1 }   =   43 ) $ 。       利用 上述 符号 ， 定义 统计 量               $ $     \ \ chi ^ 2   =   \ \ sum _ { i = 1 } ^ r   \ \ sum _ { j = 1 } ^ c   \ \ frac { ( O _ { i , j }   -   E _ { i , j } ) ^ 2 } { E _ { i , j } }     $ $       则 它 近似 服从 自由度 为 $ ( ( r - 1 ) ( c - 1 ) ) $ 的 卡方 分布 （ 理论 呢 ？ ） 。 该 统计 量 越 小 ， 说明 越 符合 独立 分布 ， 因此 ， 变量 间 越 独立 。       如果 其中 一个 是 连续 值 ，   sklearn   中是 将 连续 值 求和 ， 然后 用类 的 分布 概率 乘以 该值 作为 期望值 ， 而 实际 不同 类 求和 的 值 作为 观测 值 ， 然后 求卡方值 。               ##   Y 是 类别 的 one - hot 编码 ， X 是 特征       observed       =       safe _ sparse _ dot     (     Y     .     T     ,       X     )                         #   n _ classes   *   n _ features       feature _ count       =       X     .     sum     (     axis     =     0     )     .     reshape     (     1     ,       -     1     )       class _ prob       =       Y     .     mean     (     axis     =     0     )     .     reshape     (     1     ,       -     1     )       expected       =       np     .     dot     (     class _ prob     .     T     ,       feature _ count     )                 F   classif       ANOVA   F - value       连续变量 与 类别 变量 之间 的 独立性 检验 ， 只 要求 为 正态分布 。       统计 检验 量 是 样本 在 各个 分组 之间 的 差异 ( between - group   mean   square   value ) 与 组内 的 差异 之 和 ( within - group   mean   of   squares ) 的 比值 。     组间 差异 是 指 每个 样本 用 它 所在 组 的 均值 替换 ， 然后 汇总 每个 样本 与 样本均值 的 差 的 平方 ， 最后 除以 自由度 。     而 组内 差异 是 指 直接 计算 每个 样本 与 该组 样本均值 的 差 的 平方 ， 最后 除以 自由度 。       如果 自变量 对 因变量 没有 显著 影响 ， 那么 这个 比值 应该 接近 于 1 ， 反之 将 远大于 1 .       $ $     MSB   =   \ \ frac { 1 } { K - 1 } \ \ sum _ { i , j }   ( \ \ bar { Y _ i }   -   \ \ bar { Y } ) ^ 2     \ \ \ \     MSW   =   \ \ frac { 1 } { K ( N - 1 ) } \ \ sum _ { i , j }   ( Y _ { i , j }   -   \ \ bar { Y _ i } ) ^ 2   \ \ \ \     F   =   MSB   /   MSW     $ $       这里 $ ( K , N ) $ 分别 是 分组 数目 （ 或者 分类 类别 数目 ） 和 每组 样本 数目 ， 分母 其实 是 自由度 。     最后 得到 的 统计 检验 量 服从 自由度 为 ( K - 1 , K ( N - 1 ) ) 的 F 分布 。       参考   wikipedia         F   regression       Univariate   linear   regression   tests       构建 很多 个单 因素 的 线性 回归 检验 .           SST   总 的 平方差 之 和   $ ( \ \ sum   ( Y   -   \ \ bar { Y } ) ^ 2 ) $       SSM   模型 的 平方差 之 和   $ ( \ \ sum   ( Y _ { pred }   -   \ \ bar { Y } ) ^ 2 ) $       SSR   残余 的 平方差 之 和   SST   -   SSM .       MSM   模型 均 方差   SSM   /   SSM 的 自由度       MSR   残余 均 方差   SSR   /   SSR 的 自由度           $ $     R ^ 2   =   SSM   /   SST     \ \ \ \     F   =     MSM   /   MSR     $ $       如果 自变量 与 因变量 没有 明显 的 关系 ， 那么 F 值 应该 很小 。       参考     http : / / homepages . inf . ed . ac . uk / bwebb / statistics / Univariate _ Regression . pdf         特征 变换       特征 Hash       一种 快速 的 空间 效率高 的 向 量化 特征 的 方法 。 Yahoo   研究院 的 Weinberger 与 ICML2009 提出 。       Feature   Hashing   for   Large   Scale   Multitask   Learning         Hash   算法 是 一种 有效 的 特征 降维 的 方法 ， 非 参数估计 。 可以 用来 做   multitask   learning   with   hundreds   of     thousands   of   tasks ！ ！ ！       核 方法   kernel   trick ： $ ( x _ 1 , ... , x _ n   \ \ in   X ) $ ， 半 正定 核       $ $     k ( x _ 1 ,   x _ 2 )   =   ( \ \ phi ( x _ 1 ) ,   \ \ phi ( x _ 2 ) )     $ $       这种 方法 可以 将 原 空间 非线性 决策 边界 变成 变换 后 空间 的 线性 可分 的 界 。 （ SVM ）     相反 的 问题 是 ， 如果 问题 在 原 空间 是 现行 可分 的 （ 通常 是 通过 人工 的 非线性 特征 工程 实现 ） ，     但是 特征 的 维度 很 高 。 作者 提出 一种 和   kernel   trick   互补 的 方法   hash   trick ，     将 高 维空间 $ ( R ^ d ) $ 特征 映射 到 低 维空间 $ ( R ^ m ) $ ， $ ( \ \ phi ( x )   \ \ to   R ^ m ) $ （ Langford   et   al . ,   2007 ;   Shi   et   al . ,   2009 ） 。     并且 有 $ ( m   & lt ; & lt ;   d ) $ 。     不同于     随机 投影 ？     ，   hash   trick   解决 了 稀疏 性 ， 也 不 需要 额外 的 存储空间 存储 核 矩阵 。       hash   内积       hash   trick   在   multi - task   learning   场景 下 很 有用 ， 这里 原始 特征 是 数据 集 的 交叉 积   cross   product 。     每 一个 任务   U   可以 采用 不同 的 hash   函数 $ ( \ \ phi _ 1 ( x ) , ... , \ \ phi _ { | U | } ( x ) ) $ ， 另外 共享 一个 通用 的 hash 函数 $ ( \ \ phi _ 0 ( x ) ) $ 。       以 垃圾邮件 分类 为例 ， 每 一个 用户 需要 有 他 自己 的 个性化 偏好 ， 这里 的 任务 集   U   是 针对 所有 用户 （ 对于   Yahoo   mail   和   Gmail 数目 很大 ） 。     特征 空间 是 大量 的 语言 的 词汇 集合 。       论文 的 主要 贡献 ：           专门 实现 的 任意 内积 hash 函数 ， 可以 应用 到 许多   kernel   method       指数 界 解释       独立 hash 子 空间 使得 大规模   multi - task   learning   空间 使用 很小       实际 的 协同 垃圾邮件 过滤 系统           Hash   函数 ：       hash   函数   $ ( h :   \ \ mathbb { N }   \ \ to   \ \ { 1 , ... , m \ \ } ) $ ， 另 一个 hash 函数 $ ( \ \ xi   :   \ \ mathbb { N }   \ \ to   \ \ { \ \ pm   1 \ \ } ) $ .     对于 向量   $ ( x , x '   \ \ in   l _ 2 ) $ ， 定义 hash   特征 映射       $ $     \ \ phi _ i ^ { ( h ,   \ \ xi ) }   =   \ \ sum _ { j :   h ( j ) = i }   \ \ xi ( j )   x _ j   \ \ \ \     \ \ langle   x ,   x '   \ \ rangle _ { \ \ phi }   =   \ \ langle     \ \ phi ^ { ( h ,   \ \ xi ) } ( x ) ,   \ \ phi ^ { ( h ,   \ \ xi ) } ( x ' )   \ \ rangle     $ $       这里 的 hash 函数 定义 在 自然数 集上 ， 实际上 对 字符串 也 成立 ， 因为 有限 长 字符串 都 可以 表示 为 一个 自然数 。       个人 理解 ： 举例 ， 假设 h 是 一个 求 余函数 ， 那么 特征 映射 后 的 低 维空间 的 每 一个 分量 相当于 将 向量 等 间隔 的 分量 交替 求和 ， 从而 实现 降维 。       引理 ：   hash   kernel   是 无偏 的 ， 如果   $ ( E _ { \ \ phi }   [ \ \ langle   x ,   x '   \ \ rangle _ { \ \ phi } ]   =   \ \ langle   x ,   x '   \ \ rangle ) $ 。     并且 方差   $ ( \ \ sigma _ { x , x ' } ^ 2   =   \ \ frac { 1 } { m }   \ \ sum _ { i \ \ neq   j }   x _ i ^ 2   x _ j ^ { ' 2 }   +   x _ i   x _ i '   x _ j   x _ j ' ) $ ， 如果 原始 向量     都 是 标准化 的 ， 即 二 范数 为 1 ， 那么 方差 为 $ ( O ( { \ \ frac { 1 } { m } } ) ) $ 。       略去 若干 理论 和 证明 ， 后面 有 需要 再 来看 。       Multiple   Hashing       近似 正交 性 ： 对于 multi - task   learning ， 需要 学习 不同 的 权值 ， 当 映射 到 同一个 空间 时 ， 需要 参数 空间 交叉 部分 尽可能少 。     设 $ ( w   \ \ in   R ^ m ) $ 是 $ (   U   \ \   { u } ) $ 中 的 某个 任务 的 参数 向量 ， 对 任意 特征 $ ( x   \ \ in   u ) $ ， $ ( w ) $   和   $ ( \ \ phi _ u ( x ) ) $ 的 内积 很小 。       应用 ： 减少 存储 ， 避免 矩阵 向量 乘法 运算   Locality   Sensitive   Hashing   ？       个性化 ： 每 一个 task 更新 局部 ( local ) 权值 和 全局 ( global ) 权值 。 正交 性 让 我们 可以 将 这些 特征 hash 到 同一个 空间 ， 而 没有 太多 的 重叠 部分 。       邮件 过滤 问题 ： 每个 用户 有 一些 标记 数据 ， 但是 很少 ， 如果 对 每 一个 用户 单独 训练 一个 模型 $ ( w _ u ) $ （ 模型 参数 ？ ） ， 将 缺乏 数据 而 使 模型 不可 用 。     可以 通过 一个 全局 模型 $ ( w _ 0 ) $ 使得 数据 能够 在 多个 分类器 中 共享 。 存储 所有 分类器 需要 $ ( O ( d   ( | U |   +   1 ) ) ) $ 的 空间 复杂度 。     简单 的 方法 是 排除 低频 的 token 。 但 这回 导致 恶意 错误 拼写 的 单词 被 丢弃 ， 而 hash 到 一个 低 维空间 ， 使得 这些 低频词 也 能 对模型 有所 贡献 。     而且 大规模 的 邮件 过滤器 对 内存 和 时间 有 严格要求 ， 因为 用户量 太 大 。 为此 ， 我们 将 权值 向量 $ ( w _ 0 ,   ... ,   w _ { | U } ) $   通过 不同 的 映射 $ ( \ \ phi _ 0 , ... , \ \ phi _ { | U | } ) $ ，     映射 到 低维 特征 空间 $ ( R ^ m ) $ 。 最终 的 权值 为 ：       $ $     w _ h   =   \ \ phi _ 0 ( w _ 0 )   +   \ \ sum _ { u   \ \ in   U }   \ \ phi _ u ( w _ u )     $ $       实际上 ， $ ( w _ h ) $ 可以 在 低 维空间 直接 学习 ， 重来 不 需要 去 计算 高维 向量 。 对于 用户 u 新 的 文档 或者 邮件 x ， 预测 任务 变成 计算 内积 $ ( \ \ langle   \ \ phi _ 0 ( x )   +   \ \ phi _ u ( x ) ,   w _ h   \ \ rangle ) $ 。 hash 操作 将会 带来 内积 的 计算 的 失真 $ ( \ \ epsilon _ d ) $ ， 其他 hash 函数 带来 的 重叠 $ ( \ \ epsilon _ i ) $ 。       $ $     \ \ langle   \ \ phi _ 0 ( x )   +   \ \ phi _ u ( x ) ,   w _ h   \ \ rangle   =   \ \ langle   x ,   w _ 0   +   w _ u   \ \ rangle   +   \ \ epsilon _ d   +   \ \ epsilon _ i     $ $       大规模 协同 过滤 ： $ ( M   =   U ^ T   W ) $ ， U 和 W   $ ( \ \ in   R ^ { nd } ) $ ， 存储 需要 大量 空间 ， 利用 hash 只 需要 两个 m 维 向量 $ ( u ,   w ) $       $ $     u _ i   =   \ \ sum _ { j , k : h ( j , k ) = i }   \ \ xi ( j , k )   U _ { jk } , \ \ \ \     w _ i   =   \ \ sum _ { j , k : h ' ( j , k ) = i }   \ \ xi ' ( j , k )   W _ { jk }     $ $       这里 $ ( ( h , \ \ xi ) ,   ( h ' , \ \ xi ' ) ) $ 是 两组 独立 的 hash   函数 。 近似计算 $ ( M _ { ij } ) $ 的 方法 是 ：       $ $     M _ { ij } ^ { \ \ phi }   =   \ \ sum _ k   \ \ xi ( k , i ) \ \ xi ' ( k , j )   u _ { h ( k , i ) }   w _ { h ' ( k , j ) }     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/general-machine-learning.html"},
      
      
      {"title": "机器学习平台调研", "text": "    Table   of   Contents           关于           阿里巴巴           机器 学习                   微软   Azure           亚马逊           Google           Twitter                 关于       调研 现阶段 的 机器 学习 平台       阿里巴巴       目前 公开 的 平台 是 数加 ：   https : / / data . aliyun . com / product / learn       特点 ：           构建 于 阿里 云 MaxCompute 、 GPU 等 计算 集群 之上       分布式 算法 ， 包括 数据处理 、 特征 工程 、 机器 学习 算法 、 文本 算法 等       基于 MaxCompute 、 GPU 集群 ， 支持 MR 、 MPI 、 SQL 、 BSP 、 SPARK 等 计算 类型 。       机器 学习 支持 的 功能 如下           机器 学习           输入输出 源 ： ODPS 数据表       特征 预处理 ：       采样 ： 随机 采样 ； 加权 采样 （ 某 一列 作为 权值 ） ； 分层 采样       清洗 ： SQL 条件 过滤       缺失 值 填充 ：       归一化 ： log ， MinMax ， ZScore               特征 工程 ：       降维去 噪 ： PCA       规范化       离散 化 ： 等距 等频       异常 值 平滑 ： Zscore 平滑 （ 3sigma ） ， 百分位 平滑 ， 阈值       特征选择 ： 随机 森林 、 GBDT 特征 重要性 ； 线性 模型 特征 重要性 ， 重要性 定义 为   weight   *   std 。 即 学到 的 系数 与 特征 的 标准差 乘积 ？ ！ ( 支持 可视化 )       偏好 计算 ：               统计分析 ：       百分位 统计       相关系数       直方图 统计       特征分析 ： 离散 值 和 连续 值       统计 检验 ： t 检验 ， 卡方 检验               机器 学习 ：       分类 和 回归 算法   +   XGBOOST               文本 分析 和 特征 ：       word2vec       文本 相似性       分词                   微软   Azure           可视化 交互 ， 和 阿里 差不多       支持 脚本 ！ ！   Python   R       更 多 的 数据处理 功能 ， 滤波       OpenCV ？           亚马逊       不够 开放       Google           语音 ， 图像 ， 文本 等 都 有 很 好 的 支持       基于 API 接口 调用           Twitter  ", "tags": "machine-learning", "url": "/wiki/machine-learning/machine-learning-parform.html"},
      
      
      {"title": "机器学习相关资料", "text": "    Table   of   Contents           会议           研究组           课程           Stanford   CS231n           Stanford   CS224d           MIT 深度 学习 导论           MIT 自动 驾驶           MIT 通用 人工智能                   书籍                 会议           NIPS     https : / / nips . cc /         ICML       CVPR       KDD       AAAI       ICLR       IJCAI           研究组           南京大学 - 周志华     https : / / cs . nju . edu . cn / zhouzh /         DeepMind     https : / / deepmind . com / research / publications /         Hinton     http : / / www . cs . toronto . edu / ~ hinton /         Yoshua   Bengio     http : / / www . iro . umontreal . ca / ~ bengioy / yoshua _ en /             课程       Stanford   CS231n       课程 地址 ：   http : / / cs231n . stanford . edu / syllabus . html         李飞飞 主讲 的 深度 学习 在 机器 视觉 的 应用 ， 即使 不 做 CV 也 非常 值得一看 。       Stanford   CS224d       课程 地址 ：   http : / / cs224d . stanford . edu / syllabus . html         经典 课程 ， 主讲 深度 学习 在 NLP 中 的 应用 ， 即使 不 搞 NLP 也 值得一看 。       MIT 深度 学习 导论       课程 地址 ：     http : / / introtodeeplearning . com /         MIT 自动 驾驶       课程 地址 ：     https : / / selfdrivingcars . mit . edu /         MIT 通用 人工智能       课程 地址 ：     https : / / agi . mit . edu /         书籍  ", "tags": "machine-learning", "url": "/wiki/machine-learning/conference-list.html"},
      
      
      {"title": "机器翻译", "text": "    Table   of   Contents           SCORE           BLEU           WER                   机器翻译 ： 联合 训练   alignment   and   translater           低频词           低频词 ： 混合 模型                 SCORE       BLEU       WER       机器翻译 ： 联合 训练   alignment   and   translater       Bahdanau   D ,   Cho   K ,     Bengio   Y   .   Neural   machine   translation   by   jointly   learning   to   align   and   translate [ J ] .   arXiv   preprint   arXiv : 1409.0473 ,   2014 .       Encoder   -   Decoder   当 句子 很长 的 时候 ， 难以 学习 ， 因为 需要 把 一个 很长 的 序列 压缩 为 一个 固定 长度 的 向量 。       attention   向量 是 变长 的 ， 这个 问题 怎么 解决 ？       低频词       论文 ： Luong   M ,   Sutskever   I ,   Le   Q   V ,   et   al .   Addressing   the   Rare   Word   Problem   in   Neural   Machine   Translation [ C ] .   meeting   of   the   association   for   computational   linguistics ,   2014 :   11 - 19 .       低频词 ： 混合 模型       论文 ： Luong   M ,   Manning   C   D .   Achieving   Open   Vocabulary   Neural   Machine   Translation   with   Hybrid   Word - Character   Models [ C ] .   meeting   of   the   association   for   computational   linguistics ,   2016 :   1054 - 1063 .       解决 的 问题 ： encoder   +   decoder   +   attention   模型 ， 对 低频词 效果 不好 ，     常将 低频词 全部 映射 为 一个   的 特殊 词 ， 然后 通过 后续 规则 解决 。       方法 ： 在   encoder   和   decoder   测加 了 一个 对   的   Character   Model ， 这个 模型 和 翻译 模型 联合 优化 ！          ", "tags": "machine-learning", "url": "/wiki/machine-learning/mt.html"},
      
      
      {"title": "树模型", "text": "    Table   of   Contents           关于           树 模型 简史           随机 森林           GBDT           xgboost           一些 待 整理 的 记录           参考                 关于       树 模型 是 一个 很 经典 的 模型 ， 而且 不断 有 新 的 东西 产生 ， 比如 randomforest   和   xgboost 。     因此 有 必要 单独 为 它 做 一个 专题 。       树 模型 简史       随机 森林       GBDT       xgboost       一些 待 整理 的 记录       Leo   Breiman ,   1928   -   2005         -   1954 :   PhD   Berkeley   ( mathematics )         -   1960   - 1967 :   UCLA   ( mathematics )           -   1969   - 1982 :   Consultant           -   1982   -   1993   Berkeley   ( statistics )         -   1984   “ Classification   & amp ;   Regression   Trees ”     ( with   Friedman ,   Olshen ,   Stone )     -   1996   “ Bagging ”     -   2001   “ Random   Forests ”       Linear   discriminant   analysis   ( 1930 ’ s )       -   Logistic   regression   ( 1944 )     -   Nearest   neighbors   classifiers   ( 1951 )       参考           Leo   Breiman ,   RANDOM   FORESTS ,   2011 ,     https : / / www . stat . berkeley . edu / ~ breiman / randomforest2001 . pdf        ", "tags": "machine-learning", "url": "/wiki/machine-learning/tree-model.html"},
      
      
      {"title": "残差网络", "text": "    Table   of   Contents           关于           残差 网络 2015 年 的 论文 导读           摘要           导言           相关 的 研究 工作           Residual   Representations .           Shortcut   Connection                   Deep   Residual   Learning                   何凯明 PPT @ ICML2016           深度 的 演化           深度 频谱           初始化 技巧           Batch   Normalize           Deep   Residual   Network   10 - 100 层           单位 映射 的 重要性           未来 的 方向                   参考                 关于       Residual   Networks   残差 网络 ， 何凯明 ， 孙剑   @ MSRA 。       残差 网络 2015 年 的 论文 导读       摘要           152 层 残差 网络 ， 是   VGG   net 的 8 倍 ， 但是 复杂度 更 低 ， 效果 更好 。       ImageNet   测试 集 错误率 为   3.57 %       COCO   object   detection   dataset   28 %   相对 提升       ILSVRC   & amp ;   COCO   2015   competitions   第一名 ， on   the   tasks   of   ImageNet   detection ,   ImageNet   localization ,     COCO   detection ,   and   COCO   segmentation           导言           深度 卷积 网络 （ CNN ） 是 图像 分类 问题 的 重大突破 ，     它 可以 自动 学习 底层 / 中层 / 高层 特征 。     特征 的 层级 可以 通过 stack 的 方式 （ 增加 深度 ） 得到 提升 。           CNN 的 重要 论文 ：           Y .   LeCun ,   B .   Boser ,   J .   S .   Denker ,   D .   Henderson ,   R .   E .   Howard ,     W .   Hubbard ,   and   L .   D .   Jackel .   Backpropagation   applied   to   handwritten     zip   code   recognition .   Neural   computation ,   1989       A .   Krizhevsky ,   I .   Sutskever ,   and   G .   Hinton .   Imagenet   classification     with   deep   convolutional   neural   networks .   In   NIPS ,   2012 .       P .   Sermanet ,   D .   Eigen ,   X .   Zhang ,   M .   Mathieu ,   R .   Fergus ,   and   Y .   LeCun .     Overfeat :   Integrated   recognition ,   localization   and   detection     using   convolutional   networks .   In   ICLR ,   2014       M .   D .   Zeiler   and   R .   Fergus .   Visualizing   and   understanding   convolutional     neural   networks .   In   ECCV ,   2014 .           近期 研究 表明 ， 堆叠 的 深度 是 至关重要 的 因素 。           K .   Simonyan   and   A .   Zisserman .   Very   deep   convolutional   networks     for   large - scale   image   recognition .   In   ICLR ,   2015       C .   Szegedy ,   W .   Liu ,   Y .   Jia ,   P .   Sermanet ,   S .   Reed ,   D .   Anguelov ,   D .   Erhan ,     V .   Vanhoucke ,   and   A .   Rabinovich .   Going   deeper   with   convolutions .     In   CVPR ,   2015           ImageNet   的 最佳 结果 都 是 很 深 的 模型 ， 从 13 层到 30 层 。 深度 模型 对 其他 的 图像 任务 也 有 帮助 。           K .   He ,   X .   Zhang ,   S .   Ren ,   and   J .   Sun .   Delving   deep   into   rectifiers :     Surpassing   human - level   performance   on   imagenet   classification .   In     ICCV ,   2015 .       S .   Ioffe   and   C .   Szegedy .     Batch   normalization   :   Accelerating   deep     network   training   by   reducing   internal   covariate   shift .   In   ICML ,   2015       O .   Russakovsky ,   J .   Deng ,   H .   Su ,   J .   Krause ,   S .   Satheesh ,   S .   Ma ,     Z .   Huang ,   A .   Karpathy ,   A .   Khosla ,   M .   Bernstein ,   et   al .   Imagenet     large   scale   visual   recognition   challenge .   arXiv : 1409.0575 ,   2014 .           学习 深度 模型 最大 的 问题 在于   vanishing   gradient ， 梯度 消减 ！ 导致 模型 无法 收敛 。     利用 这些 技巧 ， 几十层 深度 的 模型 也 可以 通过 BP 算法 + SGD 进行 训练 。           Y .   Bengio ,   P .   Simard ,   and   P .   Frasconi .   Learning   long - term   dependencies     with   gradient   descent   is   difficult .   IEEE   Transactions   on   Neural     Networks ,   5 ( 2 ) : 157 – 166 ,   1994 .       X .   Glorot   and   Y .   Bengio .   Understanding   the   difficulty   of   training     deep   feedforward   neural   networks .   In   AISTATS ,   2010 .           梯度 消减 的 问题 被 很大 程度 上 通过   normalized   initialization   和   intermediate   normalization   layers   解决 了           Y .   LeCun ,   L .   Bottou ,   G .   B .   Orr ,   and   K . - R .   Muller .   Efficient   backprop .   ¨     In   Neural   Networks :   Tricks   of   the   Trade ,   pages   9 – 50 .   Springer ,   1998 .       A .   M .   Saxe ,   J .   L .   McClelland ,   and   S .   Ganguli .   Exact   solutions   to     the   nonlinear   dynamics   of   learning   in   deep   linear   neural   networks .     arXiv : 1312.6120 ,   2013 .       K .   He ,   X .   Zhang ,   S .   Ren ,   and   J .   Sun .   Delving   deep   into   rectifiers :     Surpassing   human - level   performance   on   imagenet   classification .   In     ICCV ,   2015 .       S .   Ioffe   and   C .   Szegedy .   Batch   normalization :   Accelerating   deep     network   training   by   reducing   internal   covariate   shift .   In   ICML ,   2015 .           随着 层数 的 加深 ， 模型 的 性能 逐渐 饱和 ， 然后 迅速 恶化 。 这个 问题 并 不是 由于 过 拟合 ， 更深 的 模型 导致 更差 的 性能 ！     理论 上 来讲 ， 深层 模型 应该 可以 做到 比 浅层 模型 更好 的 性能 ， 可以 设想 多余 的 层 是 恒等 变换 ， 那么 深层 模型 结果 和 浅层 一样 。     但是 实际上 的 结果 并非如此 。       残差 网络 并 不 直接 拟合 目标 ， 而是 拟合 残差 。 假设 潜在 的 目标 映射 为 $ ( \ \ mathcal { H } ( x ) ) $ ， 我们 让 非线性 层 学习 残差     $ ( \ \ mathcal { F } ( x ) : = \ \ mathcal { H } ( x )   -   x ) $ ， 并 提供 一条 短路 （ 或直 连 ） 通道 ， 使得 输出 为 $ ( \ \ mathcal { F } ( x ) + x ) $ 。     我们 假设 优化 残差 比 原始 映射 要 简单 ！ （ 假设 ！ ！ ！ ！ ）     在 极端 情况 下 ， 可以 让 非线性 层置 0 ， 使得 直接 输出 输入 值 。 （ 我 的 思考 ： 存在 正则 项 的 时候 ， 这个 确实 更优 ， 那 是不是 就 证明 残差 网络 不会 比 浅层 网络 更差 了 呢 ？ ！ ）     短路 连接 在 这里 可以 跳过 一层 或者 多层 。 单位 短路 通道 （ 即 短路 通道 直接 输出 输入 的 值 ） 不 增加 计算 复杂度 也 不 增加 额外 的 参数 。     整个 网络 可以 采用   end - to - end   使用 SGD + BP 算法 ， 可以 采用 现有 的 求解 器 就 能 实现 。           C .   M .   Bishop .   Neural   networks   for   pattern   recognition .   Oxford     university   press ,   1995 .       B .   D .   Ripley .   Pattern   recognition   and   neural   networks .   Cambridge     university   press ,   1996 .           W .   Venables   and   B .   Ripley .   Modern   applied   statistics   with   s - plus .     1999               ImageNet   论文 ：   O .   Russakovsky ,   J .   Deng ,   H .   Su ,   J .   Krause ,   S .   Satheesh ,   S .   Ma ,     Z .   Huang ,   A .   Karpathy ,   A .   Khosla ,   M .   Bernstein ,   et   al .   Imagenet     large   scale   visual   recognition   challenge .   arXiv : 1409.0575 ,   2014 .           CIFAR - 10   论文 ： A .   Krizhevsky .   Learning   multiple   layers   of   features   from   tiny   images .     Tech   Report ,   2009 .               We   present   successfully   trained   models   on   this   dataset   ( CIFAR - 10 )   with     over   100   layers ,   and   explore   models   with   over   1000   layers .     Our   ensemble   has   3.57 %   top - 5   error   on   the   ImageNet   test   set ,   and   won   the   1st   place   in   the   ILSVRC     2015   classification   competition .     The   extremely   deep   representations   also   have   excellent   generalization   performance     on   other   recognition   tasks ,   and   lead   us   to   further   win   the   1st   places   on :   ImageNet   detection ,   ImageNet   localization ,     COCO   detection ,   and   COCO   segmentation   in   ILSVRC   & amp ;   COCO   2015   competitions .           相关 的 研究 工作       Residual   Representations .           VLAD ： H .   Jegou ,   F .   Perronnin ,   M .   Douze ,   J .   Sanchez ,   P .   Perez ,   and     C .   Schmid .   Aggregating   local   image   descriptors   into   compact   codes .     TPAMI ,   2012       Fisher   Vector ： F .   Perronnin   and   C .   Dance .   Fisher   kernels   on   visual   vocabularies   for     image   categorization .   In   CVPR ,   2007 .           这 两种 表达 被 应用 在 图像 检索 和 分类 中 ：               K .   Chatfield ,   V .   Lempitsky ,   A .   Vedaldi ,   and   A .   Zisserman .   The   devil     is   in   the   details :   an   evaluation   of   recent   feature   encoding   methods .     In   BMVC ,   2011               A .   Vedaldi   and   B .   Fulkerson .   VLFeat :   An   open   and   portable   library     of   computer   vision   algorithms ,   2008               在 矢量 量化 中 ， 编码 残差 比 编码 原始 矢量 更加 有效 。               H .   Jegou ,   M .   Douze ,   and   C .   Schmid .   Product   quantization   for   nearest     neighbor   search .   TPAMI ,   33 ,   2011 .               低级 视觉 和 计算机 图形学 中 ， 为了 解决 PDE ， 采用 Multigrid 方法 。 。 。 。 不 懂 ， 所以 略 。               Shortcut   Connection       已 被 研究 多日 了 （ 哈哈哈哈 ） ， 早起 的 多层 感知器 研究 在 输入输出 间 单独 加 了 一个 线性 层 。     在 另外 两篇 论文 中 ， 一些 中间层 直接 连接 到 一个 辅助 的 分类器 ， 通过 这种 方式 减少 梯度 消减 。           C . - Y .   Lee ,   S .   Xie ,   P .   Gallagher ,   Z .   Zhang ,   and   Z .   Tu .   Deeply   supervised     nets .   arXiv : 1409.5185 ,   2014 .       R .   K .   Srivastava ,   K .   Greff ,   and   J .   Schmidhuber .   Highway   networks .     arXiv : 1505.00387 ,   2015           等等 其他 ， 略 。       highway   networks   在 短路 链接 采用 了 门 函数 ， 该门 函数 有 参数 需要 通过 数据 学习 。     当门 关掉 （ 为 0 值 ） 时 ， 网络 就是 传统 的 神经网络 ， 而 不是 残差 网络 。       Deep   Residual   Learning           假设 ： （ 还是 一个 open   question ） 多层 非线性 可以 逼近 复杂 函数 。       当 输入输出 是 相同 的 维度 ， 可以 假设 它 逼近 残差 $ ( \ \ mathcal { H } ( x )   -   x ) $ 。 虽然 逼近 原始 函数 和 逼近 残差 ， 这 两个 函数 都 很 复杂 ，     但是 后者 更 容易 ！ 前面 说 过 ， 如果 这些 加入 的 非线性 层 是 单位 映射 ， 那么 多层 不会 比 浅层 差 。 但是 由于 梯度 消减 ， 多层 非线性 难以 逼近 单位 函数 ，     但是 残差 网络 可以 很 容易 ， 让 非线性 层置 0 即可 。 实际上 ， 单位 映射 往往 不是 最优 的 。 实验 结果表明 ， 残差 部分 学 出来 的 结果 都 比较 小 ，     这 表明 单位 映射 是 一个 很 好 的 先验 条件 。       残差 网络 基本 模块 是 ：           $ $     y   =   \ \ mathcal { F } ( x ,   { W _ i } )   +   x       \ \ \ \     \ \ mathcal { F }   =   W _ 2   \ \ sigma ( W _ 1   x )       \ \ \ \     \ \ sigma   =   ReLU     $ $       如果 输入输出 维度 不同 ， 可以 通过 投影 的 方法 解决 。 $ ( W _ s ) $ 仅仅 用来 解决 维度 匹配 的 问题 ， 如果 维度 相同 ， 单位 映射 就 好 了 。       $ $     y   =   \ \ mathcal { F } ( x ,   { W _ i } )   +   W _ s   x     $ $               在 论文 里面 ， 在 ImageNet 上 最好 的 结果 是 110 层 ， 作者 也 试过 1202 层 ， 发现 训练 集 误差 相近 ， 但是 测试 集 效果 变差 了 ， 作者 认为 是     过 拟合 的 原因 ， 因为 没有 用到 MaxOut [ 1 ] 和 Dropout [ 2 ] 强 正则 化 的 做法 。               I .   J .   Goodfellow ,   D .   Warde - Farley ,   M .   Mirza ,   A .   Courville ,   and     Y .   Bengio .   Maxout   networks .   arXiv : 1302.4389 ,   2013 .           G .   E .   Hinton ,   N .   Srivastava ,   A .   Krizhevsky ,   I .   Sutskever ,   and     R .   R .   Salakhutdinov .   Improving   neural   networks   by   preventing   coadaptation     of   feature   detectors .   arXiv : 1207.0580 ,   2012           何凯明 PPT @ ICML2016       此时 他 已经 来到 Facebook   AI 团队 了 ！       深度 的 演化           AlexNet ,   8   layers   ( ILSVRC   2012 )       VGG ,   19   layers   ( ILSVRC   2014 )       GoogleNet ,   22   layers   ( ILSVRC     2014 )       ResNet ,   152   layers   ( ILSVRC   2015 )               >         200   citations       in     6       months     after       posted     on     arXiv   ( Dec .   2015 )           深度 频谱           5   layers :   easy               10       layers :   initialization ,   Batch       Normalization                       30       layers :   skip         connections                       100     layers :   identity         skip         connections                   初始化 技巧       总结 ， 好 的 初始化 很 重要 ， 当 层数 较 深 （ 20 - 30 ） 时 ， 可能 收敛 更 快 ， 初始化 不好 可能 不 收敛 。           LeCun   et   al     1998         “ Efficient     Backprop ”       Glorot & amp ;     Bengio   2010   “ Understanding     the   difficulty     of     training         deep         feedforward   neural     networks ”           Batch   Normalize           输入 标准化       标准化 每 一层   for   each   mini - batch       极大 地 加速 训练       减少 初值 敏感       增强 正则 化           $ $     \ \ hat { x }   =   \ \ frac { x   -   \ \ mu } { \ \ sigma }   \ \ \ \     y   =   \ \ gamma   \ \ hat { x }   +   \ \ beta     $ $           $ ( \ \ mu ,   \ \ sigma ) $   分别 是   mini - batch   的 均值 和 标准差 ， 是 由 数据 计算出来 的       $ ( \ \ gamma ,   \ \ beta ) $   是 缩放 因子 和 位移 量 ， 需要 模型 学 出来 。       注意 ， 训练 集 的 均值 方差 是从 数据 中 计算 ， 但是 测试 集是 采用 训练 集 计算 的 结果 （ 平均 ） 。           Deep   Residual   Network   10 - 100 层           简单 叠加 会 变差 ！           单位 映射 的 重要性       单位 映射 下 ：       $ $     x _ L   =   x _ l   +   \ \ sum _ { i = l } ^ { L - 1 }   \ \ mathcal { F } _ i ( x _ i )     \ \ \ \     \ \ frac { \ \ partial   E } { \ \ partial   x _ l }   =   \ \ frac { \ \ partial   E } { \ \ partial   x _ L } ( 1   +   \ \ frac { \ \ partial   E } { \ \ partial   x _ l }   \ \ sum _ { i = l } ^ { L - 1 }   \ \ mathcal { F } _ i ( x _ i ) )     $ $       在 单位 映射 下 ， 梯度 可以 以 恒定 比例 传递 过来 ，     如果 不是 ， 一旦 深度 变深 了 ， 要么 衰减 ， 要么 爆炸 ！       加总 之后 ， 还是 单位 映射 好 ， （ 我 觉得 还是 梯度 传递 的 问题 ， 需要 单位 范数 的 映射 才能 不 使得 梯度 消失 和 爆炸 ！ ） pre - active           Kaiming     He ,   Xiangyu   Zhang ,     Shaoqing           Ren ,       & amp ;       Jian         Sun .         “ Identity       Mappings         in     Deep         Residual         Networks ” .     arXiv       2016 .           未来 的 方向           Representation       skipping     1       layer       vs .   multiple         layers ?       Flat     vs .   Bottleneck ?       Inception - ResNet [ Szegedy   et       al     2016 ]       ResNetin     ResNet [ Targ   et     al     2016 ]       Width   vs .   Depth       [ Zagoruyko   & amp ;         Komodakis   2016 ]               Generalization       DropOut ,     MaxOut ,   DropConnect ,         …       Drop     Layer       ( Stochastic   Depth )     [ Huang     et     al     2016 ]               Optimization       Without       residual / shortcut ?                   参考             Deep   Residual   Learning   for   Image   Recognition           Deep         Residual         Networks        ", "tags": "machine-learning", "url": "/wiki/machine-learning/residual-network.html"},
      
      
      {"title": "深度学习", "text": "    Table   of   Contents           关于           DNN   方向           CNN   方向           RNN   方向           递归 神经网络 语言 模型                   DQN   方向                 关于       深度 学习 的 调研 ， 相当于 review   。       DNN   方向       CNN   方向       RNN   方向       递归 神经网络 语言 模型       论文 ：           Mikolov ,   Recurrent   neural   network   based   language   model ,   interspeech   2010 ,   Johns   Hopkins   University           人物 ：           Toma ´ s   Mikolov ,   \" Johns   Hopkins   University \" ,   \" Speech @ FIT ,   Brno   University   of   Technology ,   Czech   Republi \"           DQN   方向  ", "tags": "machine-learning", "url": "/wiki/machine-learning/deep-learning.html"},
      
      
      {"title": "用神经网络做风格迁移", "text": "    Table   of   Contents           关于           神经网络 风格 迁移           实时 风格 迁移                 关于       风格 迁移 就是 将 图片 变成 某种 风格 的 图片 ， 例如 为 你 的 照片 加上 梵高 的 画 的 风格 。       神经网络 风格 迁移       用 神经网络 做 风格 迁移 的 方法 最早 由   Leon   A .   Gatys   提出 ：   Image   Style   Transfer   Using   Convolutional   Neural   Networks ， 2016               基本 思想 是 用 一个 神经网络 ， 对于 原始 照片   p   ， 用 神经网络 中 的 某 一层 特征   $ ( F _ { ij } ^ l ) $   ( 这里 第一个 下标 表示 特征 通道 ， 第二个 下标 表示 空间 维度 ， 将 二维 空间 压缩 为 一维 便于 表述 ， 下同 ) 作为 内容 的 表达 ，     对于 生成 的 照片   x ， 用 神经网络 中 的 同 一层 特征 表达 生成 的 图片 的 内容   $ ( P _ { ij } ^ l ) $ ， 要求 新 生成 的 照片 内容 和 原始 照片 内容 接近 ，     即 损失 函数       $ $     L _ { content } ( p ,   x ,   l )   =   \ \ frac { 1 } { 2 }   \ \ sum _ { ij } ( F _ { ij } ^ l   P _ { ij } ^ l ) ^ 2     $ $       较 小 。       另一方面 ， 需要 生成 的 照片 的 风格 和 图片   a   相似 ， 风格 可以 通过 特征 空间 的   Gram   矩阵 来 表达 。       $ $     G _ { ij } ^ l   =   \ \ sum _ { k }   F _ { ik } ^ l   F _ { jk } ^ l     $ $       通过 图片 x 不同 层 的 Gram 矩阵 和 图片   a   相似 ， 实现 风格 的 相似 。       $ $     E _ l   =   \ \ frac { 1 } { 4N _ l ^ 2M _ l ^ 2 }   \ \ sum _ { ij }   ( G _ { ij } ^ l   -   A _ { ij } ^ l ) ^ 2   \ \ \ \     L _ { style } ( a ,   x )   =   \ \ sum _ { l = 1 } ^ L   w _ l   E _ l .     $ $       $ ( N _ l ,   M _ l ) $ 分别 是 第 l 层 特征 数目 和 空间 维度 ！       通过 内容 和 风格 损失 函数 最小化 ， 实现 内容 和 风格 的   tradeoff 。       $ $     L   =   \ \ alpha   L _ { content } ( p ,   x )   +   \ \ beta   L _ { style } ( a ,   x )     $ $       注意 上述 两个 损失 函数 的 特征 都 是 做 了 max - pooling 后 的 特征 。           一些 讨论 ：       内容 匹配 如果 选用 约 高层 的 表征 ， 保留 的 细节 越 少 ， 感受 约 平滑       太慢 了 ： 用   K40   GPU ， 一张 512x512 图片 的 风格 迁移 也 需要 1 小时 ！                           实时 风格 迁移       论文 ： Perceptual   Losses   for   Real - Time   Style   Transfer   and   Super - Resolution ， Justin   Johnson ,   Alexandre   Alahi ,   and   Li   Fei - Fei ， 2016 .       特点 ：   perceptual   loss   functions ，   three   orders   of   magnitude   faster ， real - time           超 分辨   pixel   loss   function ： 不能 很 好 的 刻画 语义 的 不同 （ 例如 两个 只是 平移 了 一点点 的 图片 ， 像素 的 差异 很大 ， 但是 语义 差异 较 小 ）       解决之道 ： 利用 训练 好 的 深度 神经网络 的 高层 语义 空间 的 特征 表达 ！                       解决 风格 迁移 test 速度慢 的 方法 是 ， 用 一个 神经网络 建模 这种 变换 关系 ， 目标 用 前者 的 损失 函数 ！       包含 两个 网络 ：   image   trans -   formation   network   $ ( f _ W ) $   ，   loss   network   $ ( \ \ phi ) $ ； 前者 是 一个 深度 残差 网络 ， 后者 是 一个 预 训练 的   VGG   网络 。           $ $     W ^ *   =   \ \ arg   \ \ min   _   W   \ \ mathbf { E }   _   { x ,   {   y _ i   } }   \ \ sum _ { i }   \ \ lambda   _   i   l   _   i   ( f _ W   ( x ) ,   y _ i )     $ $       损失 函数 和   Gatys   一样 ！   TV   范数 正则 化 ！  ", "tags": "machine-learning", "url": "/wiki/machine-learning/style-transform.html"},
      
      
      {"title": "知识图谱", "text": "    Table   of   Contents           AAAI   tutorial           知识 图谱 简介           NLP 基础                   跨 语言 知识 图谱 构建 ： 李 涓 子                 AAAI   tutorial       知识 图谱 简介           知识 图谱 ： 将 知识 表达 为 图 的 形式       获取 实体 、 属性 、 关系           图 的 顶点 代表 实体               应用 ： QA ， 决策               工业界 产品 ：           Google   Knowledge   Vault       Amazon   Product   Graph                   数据 来源 ： 结构 数据 和 非 结构化 数据 ， 图片 、 视频 ？           知识 表达 ：       RDF ：     :   r ( s , p , o )       ABox   ( assertions )   versus   TBox   ( terminology )       Common   ontological   primitives       rdfs : domain ,   rdfs : range ,   rdf : type ,   rdfs : subClassOf ,   rdfs : subPropertyOf ,   ...       owl : inverseOf ,   owl : TransitiveProperty ,   owl : FunctionalProperty ,   ...                       语义 网           从 文本 中 获取 知识 的 方法           chunking       polysemy / word   sense   disambiguation   消 歧义       entity   coreference   实体 共止 ： 通过 判定 两个 实体 是否 存在 共指 关系 ？ 如 “ IBM ” 和 “ IBM   Inc . ”       relational   extraction ： 关系 抽取                   基本 问题           实体 识别       实体 的 属性 和 标签       实体 关系                   NLP 基础           Entity   resolution       Entity   linking       Relation   extraction       Coreference   Resolution       Dependency   Parsing       Part   of   speech   tagging           Named   entity   recognition               抽取 方法 ：           规则 ： 高精度 低 召回       监督 学习                   定义   domain               论文 ： Toward   an   Architecture   for   Never - Ending   Language   Learning ，       跨 语言 知识 图谱 构建 ： 李 涓 子           语义 搜索   Semantic   Search       RDF   ：   资源 描述 框架   W3C   标准       要素 ： 资源 、 属性 和 属性 值       RDF 陈述 ： 主体 、 谓语 和 客体               语义 网 ： 数据 用 一个   directed   labeled   graph   描述 ， 每 一个 顶点 对应 一个 资源 ， 每 一个 边 标注 了 一个 属性 类型 。       语义 网 描述 ： RDF ， RFDS       语义 网 查询 ： SOAP            ", "tags": "machine-learning", "url": "/wiki/machine-learning/knowledge-graph.html"},
      
      
      {"title": "知识图谱：实体关系挖掘", "text": "    Table   of   Contents           关于           TransE                 关于       实体 关系 挖掘 相关 的 论文 阅读 笔记 ， 因为 用户 画像 中要 做 用户 、 poi 等 实体 间 的 关系 挖掘 ， 所以 看 了 一些 关系 挖掘 的 论文 。       TransE       论文 ： Bordes   A ,   Usunier   N ,   Garcia - Duran   A ,   et   al .   Translating   embeddings   for   modeling   multi - relational   data [ C ] / / Advances   in   neural   information   processing   systems .   2013 :   2787 - 2795 .  ", "tags": "machine-learning", "url": "/wiki/machine-learning/relation-model.html"},
      
      
      {"title": "神经程序员", "text": "    Table   of   Contents           关于           摘要           模型                 关于       用 神经网络 做 编程 ！ ！           Neural   Programmer :   Inducing   Latent   Programs   with   Gradient   Descent ,   Arvind   Neelakantan ,   Quoc   V   Le ,   Ilya   Sutskever ,   ICLR   2016           摘要       神经网络 虽然 在 很多 领域 如 语音 识别 ， 图像识别 etc 等 领域 取得 了 巨大 的 成功 ， 但是 在 基本 的 算术 和 逻辑 代数 运算 上 ，     神经网络 的 精确 学习 却 很 困难 ！   Neural   Programmer   增加 了 一部分 基本 的 算术 和 逻辑 操作 ， 解决 了 这 一点 。           A   major   limitation   of   these   models   is   in   their   inability   to   learn   even   simple   arithmetic   and   logic   operations .     recurrent   neural   networks   ( RNNs )   fail   at   the   task   of   adding   two   binary   numbers   even   when   the   result   has   less   than   10   bits           往 梯度 中 增加 高斯 噪声 ， 可以 提升 训练 效果 ， 增加 泛化 能力 。       模型       Neural   Programmer   由 3 个 部分 构成 ：           question   Recurrent   Neural   Network   ( RNN )   处理 用 自然语言 输入 的 问题       selector   生成 两个 概率分布 ， 用于 （ soft   select ） 选择 数据 分片 和 操作       history   RNN   记住 历史 选择 的 数据 分片 和 操作                   除了 操作 列表 ， 其他 的 都 可以 通过 梯度 下降 ， 由 数据   ( question ,   data   source ,   answer )   三元组 样本 训练 得到 ！       data   source   以 表格 形式 存在   $ ( table   \ \ in   \ \ mathbb { R } ^ { M   \ \ times   C } ) $       QUESTION   MODULE   是 一个 简单 的   RNN   模块 ， 将 输入 的 词 序列 （ 分布 是 表达 ） 编码 成 一个   d   维 的 向量 q 。     如果 问题 包含 长 句子 ， 采用 一个 双向   RNN 。       预处理 将 数字 单独 拿 出来 ， 放到 一个 列表 中 。       SELECTOR   生成 两个 分布 ， 一个 是 operator 的 概率分布 ， 一个 是 数据 列 的 概率分布 （ 问题 ： 数据 列是 变动 的 ， 怎么办 ？ ）     输入 是 问题 的 编码 向量 q （ d 维 ) 和 输入 历史 的 向量 h [ t ] （ d 维 ） 。       每 一个   operator   编码 为 一个 d 维 向量 ！ 所有 的   operator   构成 一个 矩阵   U 。   operator   选择 表达式 为 ：       $ $     \ \ alpha _ t ^ { op }   =   softmax ( U   tanh ( W ^ { op }   [ q ;   h _ t ] ) )     $ $       数据 列名 采用 问题 编码 RNN 中 的 词 向量 表达 ！ 或者   RNN   phrase   embedding 。     所有 的 列名 构成 一个 矩阵   P ！ 列 选择 表达 为       $ $     \ \ alpha _ t ^ { col }   =   softmax ( P   tanh ( W ^ { col }   [ q ;   h _ t ] ) )     $ $       将 出现 的 数字 单独 拿 出来 ， 对于 比较 操作 ， 需要 知道 比较 的 列 ， 即   pivot ，       $ $     \ \ beta _ { op }   =   softmax ( Z   U ( op ) )     \ \ \ \     pivot _ { op }   =   \ \ sum _ { i = 1 } ^ N   \ \ beta ( i )   qn _ i     $ $  ", "tags": "machine-learning", "url": "/wiki/machine-learning/neural-programmer.html"},
      
      
      {"title": "统计检验", "text": "    Table   of   Contents           关于           独立性 检验           KS （ Kolmogorov – Smirnov ） 检验                 关于       汇总 统计 检验 相关 知识点 。       独立性 检验           卡方 检验       F 检验           KS （ Kolmogorov – Smirnov ） 检验       检验 数据 拟合 优度 。   KS   test         检验 统计 量       $ $     F _ n ( x )   =   \ \ frac { 1 } { n }   \ \ sum _ { i = 1 } ^ n   I _ { [ - \ \ infty ,   x ] } ( X _ i )   \ \ \ \     D _ n   =   \ \ sup _ { x }   | F _ n ( x )   -   F ( x ) |     $ $           Wiener 过程   $ ( W _ t ) $       $ ( W _ 0   =   0 ) $       独立 增量 过程   $ ( W _ { t + u }   -   W _ t ,   u   \ \ ge   0 ) $   独立 于   $ ( W _ s :   s   \ \ le   t ) $       高斯 增量   $ ( W _ { t + u }   -   W _ t   \ \ sim   \ \ mathcal { N } ( 0 ,   u ) ) $       连续 路径 ， 以 概率 1 在 t 空间 连续                   Wiener 过程 作为   random   walk   的 极限 ！ 设   $ ( \ \ xi _ 1 ,   \ \ xi _ 2 ,   ... ) $   iid ， 均值 为 0 ， 方差 为 1 的 随机变量 。 对 任意 正整数 n ， 定义 连续 时间 随机 过程       $ $     W _ n ( t )   =   \ \ frac { 1 } { n }   \ \ sum _ { 1   \ \ le   k   \ \ le   nt }   \ \ xi _ k ,   n   \ \ rightarrow   \ \ infty     $ $       由 中心 极限 定理 可知 ， 对于 充分 大 的 n ， $ ( W _ n ( t )   -   W _ n ( s )   \ \ rightarrow   \ \ mathcal { N } ( 0 ,   t - s ) ) $ 。           Brownian   bridge ， 定义 随机 过程   $ ( B _ t ) $   为           $ $     B _ t   : =   ( W _ t   |   W _ T   =   0 ) ,   t   \ \ in   [ 0 ,   T ]     $ $     其中   $ ( W _ t ) $ 是   Wiener   过程 。           Kolmogorov   分布 ， 定义 随机变量           $ $     K   =   \ \ sup _ { t   \ \ in   [ 0 ,   1 ] } | B ( t ) |     $ $     其中   $ ( B ( t ) ) $   是   Brownian   bridge .       KS 检验 的 检验 统计 量   $ ( \ \ sqrt { n }   D _ n   \ \ rightarrow   \ \ sup _ t   | B ( F ( t ) ) | ) $ ， 即 近似 服从   Kolmogorov – Smirnov   分布 。  ", "tags": "machine-learning", "url": "/wiki/machine-learning/statistic-test.html"},
      
      
      {"title": "翻译: Rules of Machine Learning: Best Practices for ML Engineering", "text": "    Table   of   Contents           关于                 关于       原文     https : / / developers . google . com / machine - learning / guides / rules - of - ml /    ", "tags": "machine-learning", "url": "/wiki/machine-learning/rules-of-ml.html"},
      
      
      {"title": "自动微分", "text": "    Table   of   Contents           论文           要点                 论文       自动 微分 综述 :   Automatic   differentiation   in   machine   learning -   a   survey . pdf       要点           前向 模式       令   dx = r ,   在 计算 图中 向前 冒泡 , 可以 方便 计算 函数 f 在 x 处沿 r 方向 的 方向 导数 。       令 r 为 单位向量 , 就 可以 计算 f 对 任意 自变量 的 偏导 , 如果 有 n 个 自变量 , 那么 要 计算 n 次       适合 从 低维到 高维 的 函数 计算 偏 导数       对 偶数               反向 模式       适合 从 高维 到 低维 的 函数 计算 偏 导数               前向 + 反向   可以 直接 计算 海森 矩阵 与 向量 乘积 ? ? ? ?   [ & gt ;     _         _     ____ ]      ", "tags": "machine-learning", "url": "/wiki/machine-learning/autodiff.html"},
      
      
      {"title": "自编码模型", "text": "    Table   of   Contents           关于           历史           神经网络 预 训练 方案           好 的 特征 表达 ：   交互 信息   到   自 编码           独立 成分 分析 （ ICA ） ： Bell   and   Sejnowski   ( 1995 )           传统 自 编码           Denoise   准则                   Variational   autoencoders   论文 导读           变 分界   variational   bound           The   SGVB   estimator   and   AEVB   algorithm           例子 ： 变分 自 编码                   Stacked   What - Where   Auto - encoders           软 最大值 max 和 argmax           where 的 重要性           结论                   Reference                 关于       自 编码 模型 常用 做 深度 神经网络 预 训练 。       历史       神经网络 预 训练 方案       多层 神经网络 直接 训练 会 因为 局部 最优 问题 ， 导致 初值 敏感 ， 所以 随机 初始化 效果 不好 。     可以 采用 局部 无 监督 准则 进行 逐层 初始化 预 训练 ， 使得 后续 训练 更大 可能 跳出 局部 最优 。           RBM ： Hinton   et   al . ,   2006 ;   Hinton   and   Salakhutdinov ,   2006 ;   Lee   et   al . ,   2008       auto - encoder ： Bengio   et   al . ,   2007 ;   Ranzato   et   al . ,   2007       semi - supervised   embedding ： Weston   et   al . , 2008       kernel   PCA ： Cho   and   Saul ,   2010           RBM   和   自 编码   模型 函数 形式 很 像 ， 但是 训练 和 解释 都 不同 。 一个 很大 不同 是 ， 确定性 的 自 编码 可以 用实 数值 作为 隐层 的 表达 ，     而 随机 的 RBM 采用 二进制 表达 隐层 。 但是 在 实际上 ， 应用 在 深度 网络 中 的 RBM 还是 用 实数 均值 作为 表达 。     autoencoder   的 重构 误差 可以 看做   RBM   的   log - likelihood   gradient   一种 近似 。     RBM   中 的   Contrastive   Divergence   更新 。       如何 构造 一个 好 的 表达 ！ ？ 采用 无 监督 学习 ， 学出 输入 中 的 重要 模式 ！       好 的 特征 表达 ：   交互 信息   到   自 编码       定义 好 的 表达 ： 对 最终 我们 感兴趣 的 任务 是 有用 的 ， 相比 不 采用 这种 表达 ， 它 能够 帮助 系统 更快 地 达到 更 高 的 性能 ！           A   good   representation   is   one   that   will   yield   a   better   performing   classifier           实验 表明 ， 一个 无 监督 准则 的 初始化 ， 可以 帮助 分类 任务 得到 明显 地 提升 。     人类 能够 快速 地 学习 新 的 东西 一个 重要 的 原因 是 已经 获得 了 这个 任务 的 一些 先验 知识 。       学习 一个 输入 $ ( X ) $   的 表达 $ ( Y ) $ ， 实际上 是 学习 条件 概率 $ ( q ( Y | X )   =   q ( Y | X ;   \ \ theta ) ) $ 。 $ ( \ \ theta ) $ 是 要 学习 的 参数 。       一个 基本 要求 是 要 尽可能 保留 输入 的 信息 ， 在 信息论 里面 可以 表达 为 最大化 交互 信息   $ ( \ \ mathbb { I } ( X ;   Y ) ) $ ：   Linsker   ( 1989 )       独立 成分 分析 （ ICA ） ： Bell   and   Sejnowski   ( 1995 )       $ $     \ \ arg   \ \ max _ { \ \ theta }   \ \ mathbb { I } ( X ;   Y )   =   \ \ arg   \ \ max _ { \ \ theta }   -   \ \ mathbb { H } ( X | Y )   \ \ \ \             =   \ \ arg   \ \ max _ { \ \ theta }   \ \ mathbb { E } _ { q ( X ,   Y ) }   [ \ \ log   q ( X | Y ) ]     $ $       对于 任意 分布   $ ( p ( X ,   Y ) ) $ ， 利用   KL   距离 的 性质 可知 ：       $ $     \ \ mathbb { E } _ { q ( X ,   Y ) }   [ \ \ log   p ( X | Y ) ]   \ \ le   -   \ \ mathbb { H } ( X | Y )     $ $       设 这个 分布 通过 参数 $ ( \ \ theta ' ) $ 刻画 ， 那么 优化 下面 这个 式子 相当于 优化 条件 熵 的 下界 ：       $ $     \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ( X ,   Y ;   \ \ theta ) }   [ \ \ log   p ( X | Y ;   \ \ theta ' ) ]     $ $       当 两个 分布 相同 的 时候 ， 可以 得到 精确 的 交互 信息 。 infomax   ICA   中 ， 特征 映射 为   $ ( Y   =   f _ { \ \ theta } ( X ) ) $ 。     那么 $ ( q ( X ,   Y ; \ \ theta )   =   q ( X )   \ \ approx   q ^ 0 ( X ) ) $ ， 即用 样本 集 的 分布 代替 总体 分布 。 优化 问题 变为 ：       $ $     \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   [ \ \ log   p ( X | Y = f _ { \ \ theta } ( X ) ;   \ \ theta ' ) ]     $ $       UFLDL 里面 的   独立 成分 分析   ：     找到 一组 基 向量 使得 变换 后 的 特征 是 稀疏 的 。     数据 必须 ZCA 白化 ， 标准 正交 基维 数 小于 输入 维度 ， 是 一组 不 完备 基 。       $ $     \ \ min   | |   W   x   | | _ 1     \ \ \ \     s .   t .   WW ^ T   =   I     $ $       优化 方法 ： 梯度 下降   +   每 一步 增加 投影 。       $ $     W   =   W   -   \ \ alpha   \ \ nabla _ W   | |   W   x   | | _ 1   \ \ \ \     W   =   ( WW ^ T ) ^ { - 1 / 2 }   W     $ $       传统 自 编码       在 传统   autoencoder ( AE )   中 ， 特征 变换 函数 （ Encoder ） 用 sigmoid 函数 来 近似 ：       $ $     y   =   f _ { \ \ theta } ( x )   =   s ( Wx + b )       \ \ \ \     \ \ theta   =   \ \ {   W ,   b   \ \ }     $ $       特征 重构 （ Decoder ） 变换 也 用 sigmoid 函数       $ $     z   =   g _ { \ \ theta ' } ( y )   =   s ( W '   y   +   b ' )   \ \ \ \     \ \ theta '     =   \ \ {   W ' ,   b '   \ \ }     $ $       损失 函数 为 重构 误差 ：       $ $     L ( x ,   z )   =   \ \ varpropto   -   \ \ log   p ( x   |   z )     $ $           对于 实 数值 $ ( x   \ \ in   \ \ mathbb { R } ^ d ,   X | z   \ \ sim   \ \ mathcal { N } ( z ,   \ \ sigma ^ 2   I ) $ ， 那么 重构 误差 对应 于均方 误差 $ ( L ( x ,   z )   =   C ( \ \ sigma ^ 2 ) | | x - z | | ^ 2 ) $ 。       对于 二进制 变量   $ ( x   \ \ in   \ \ {   0 ,   1   \ \ } ,   X | z   \ \ sim   \ \ mathcal { B } ( z )   ) $ ， 那么 重构 误差 对应 于 交叉 熵 损失 函数 。           常用 的 两种 形式 ： 纺射 + sigmoid   Encoder ； 纺射   Decoder   +   均方 误差 ，   纺射 + sigmoid   Decoder   +   交叉 熵 损失 函数       autoenoder   的 训练 最小化 重构 误差 ， 即 优化 下列 问题 ：       $ $     \ \ arg   \ \ min _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   L ( X ,   Z = g _ { \ \ theta ' } ( f _ { \ \ theta } ( X ) ) )     $ $       等价 于       $ $     \ \ arg   \ \ max _ { \ \ theta ,   \ \ theta ' }   \ \ mathbb { E } _ { q ^ 0 ( X ) }   \ \ log ( p ( X |   Y = f _ { \ \ theta } ( X ) ;   \ \ theta ' ) )     $ $       这 表明 ， 我们 是 在 最大化 X 和 Y 的 交互 信息量 的 下界 ！           training   an   autoencoder   to   minimize   reconstruction   error   amounts     to   maximizing   a   lower   bound   on   the   mutual   information   between   input   X   and   learnt   representation   Y           但是 ， 简单 地 保留 原有 信息 是 不够 的 ！ 比如 简单 地 将 Y 设置 为 X ， 但是 这个 并 没什么 卵用 ！     如果 Y 的 维数 不少 于 X ， 那么 学 一个 单位 映射 就 可以 最大 限度 地 保留 X 中 的 信息 ！     传统 的 autoencoder 方法 采用 不 完备 的 表达 $ ( d '   & lt ;   d ) $ 。     降维后 的 Y 相当于 X 的 有损压缩 表达 。     当 采用 纺射 变换 做 编码 和 解码 ， 而 没有 非线性 变化 ， 那么 就是 PCA ！     但是 引入 非线性 变化 后 ， 将 能够 学到 不 一样 的 特征 表达 ！           The   use   of   “ tied   weights ”   can   also   change   the   solution :   forcing   encoder   and   decoder   matrices   to     be   symmetric   and   thus   have   the   same   scale   can   make   it   harder   for   the   encoder   to   stay   in   the   linear     regime   of   its   nonlinearity   without   paying   a   high   price   in   reconstruction   error .           另外 ， 也 可以 添加 其他 约束 ， 而 不是 更 低 的 维度 。     例如 通过 添加 稀疏 约束 ， 可以 采用 过 完备 的 维度 ， 即 比 输入 更大 的 维度 。       稀疏 表达 ， 稀疏 编码 ： Olshausen   and   Field   ( 1996 )   on   sparse   coding .     稀疏 自 编码 （ A   sparse   over - complete   representations ） ： sparse   representations   ( Ranzato   et   al . ,   2007 ,   2008 ) .       Denoise   准则       目标 ： 还原 部分 腐蚀 的 输入 ， 即 降噪 ！ Denoising       一个 好 的 表达 应该 是 能够 鲁棒 地 表达 腐蚀 后 的 输入 ， 可以 帮助 恢复 任务 ！       输入 加入 噪声 ： 高斯 噪声 （ 连续变量 ） ， 椒盐 噪声 ， 马赛克 噪声 。       几何 解释 ：     流形 学习 。       Variational   autoencoders   论文 导读       论文 ： Auto - Encoding   Variational   Bayes ,   Diederik   P .   Kingma ,   Max   Welling ,   2014               N 个 iid 样本 $ ( x ^ { ( i ) } ) $ ， 连续 或 离散 值 。 假定 这些 数据 从 一些 随机 过程 产生 ！           从 先验 分布 $ ( p _ { \ \ theta ^ *   } ( z ) ) $ 产生 隐 变量 $ ( z ) $ 。       从 条件 概率 $ ( p _ { \ \ theta ^ *   } ( x | z ) ) $ 产生 $ ( x ) $ 。           假定 先验 分布 和 条件 分布 无限 可微 ！     难点 ：           边际 分布 $ ( p ( x ) ) $ 需要 计算 一个 积分 ， 对于 神经网络 等 复杂 模型 ， 难以 求导 ！ EM   算法 失效 ；       大 数据 集 ， batch 优化 没有 效率 ， 需要 随机 梯度 之类 的 优化           工作 ：           有效 地 近似 地 实现   ML   和   MAP 估计 参数 $ ( \ \ theta ) $       给定 x ， 推导 z ， 有效 的 近似算法       有效 的 估计 x 的 边际 分布           建立 识别 模型 $ ( q _ { \ \ phi } ( z | x ) ) $ ， 近似 $ ( p _ { \ \ theta } ( z | x ) ) $ ， 编码 角度 来看 ， 就是 一个 编码器 ！     而   $ ( p _ { \ \ theta } ( x | z ) ) $   作为 解码器 ！       变 分界   variational   bound       边际 分布 的 对数 似然 函数 为       $ $     \ \ log   p ( x ^ { ( 1 ) } ,   ... ,   x ^ { ( N ) } )   =   \ \ sum _ i   \ \ log   p _ { \ \ theta }   ( x ^ { ( i ) } )     $ $       而 其中 每一项 可以 改写 为       $ $     \ \ log   p _ { \ \ theta }   ( x ^ { ( i ) } )   =   D _ { KL } ( q _ { \ \ phi } ( z | x ^ { ( i ) } )   | |   p _ { \ \ theta }   ( x ^ { ( i ) } ) )   \ \ \ \             +   \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )             \ \ \ \     \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )     =   \ \ mathbb { E } _ { q _ { \ \ phi ( z | x ) } } [ - \ \ log   q _ { \ \ phi } ( z | x )   +   \ \ log   p _ { \ \ theta } ( x ,   z ) ]   \ \ \ \             =   -   D _ { KL } ( q _ { \ \ phi } ( z | x ^ { ( i ) } )   | |   p _ { \ \ theta } ( z ) )   +   \ \ mathbb { E } _ { q _ { \ \ phi ( z | x ^ { ( i ) } } }   \ \ left [   \ \ log   p _ { \ \ theta } ( x ^ { ( i ) }   |   z )   \ \ right ] }     $ $       对数 似然 函数 第一项 是 近似 误差 ， 第二项 是 近似 之后 的 似然 函数 ， 或者 数据 i 的 边际 对数 似然 函数 下界 ！     第二项 可以 写为 一个 KL 距离 和 一个 期望 ， 前者 可以 通过 解析 积分 计算 ， 后者 要 采用 近似 估计 ！     而 通常 的   Monte   Carlo   梯度 估计 在 这个 问题 上 方差 太 大 ， 不 适用 与 这里 ！       The   SGVB   estimator   and   AEVB   algorithm       分布 $ ( \ \ tilde { z }   \ \ sim   q _ { \ \ phi ( z | x ^ { ( i ) } } ) $ 通过 一个 可微 的 变换 $ ( g _ { \ \ phi } ( \ \ epsilon ,   x ) ) $ ，     从 一个 noise 变量 $ ( \ \ epsilon   \ \ sim   p ( \ \ epsilon ) ) $ 采样 得到 。       $ $     \ \ mathbb { E } _ { q _ { \ \ phi } ( z | x ^ { ( i ) } ) } [ f ( z ) ]   \ \ approx   \ \ frac { 1 } { L }   \ \ sum _ { l = 1 } ^ L   f ( g _ { \ \ phi } ( \ \ epsilon ^ { ( l ) } ,   x ^ { ( i ) } ) ) ,   \ \ \ \     \ \ epsilon ^ { ( l ) }   \ \ sim   p ( \ \ epsilon )     $ $       mini - batch 方法 ：       $ $     \ \ mathcal { L } ( \ \ theta ,   \ \ phi ;   X )   \ \ approx   \ \ tilde { \ \ mathcal { L } } ^ M ( \ \ theta ,   \ \ phi ;   x ^ M )   \ \ \ \             =   \ \ frac { N } { M }   \ \ sum _ { i = 1 } ^ M   \ \ tilde { \ \ mathcal { L } } ( \ \ theta ,   \ \ phi ;   x ^ { ( i ) } )     $ $       $ ( { X ^ M } ) $   是 随机 从 全部 数据 集 采样 的 M 个 数据 。       auto - encoder 角度 ： 似然 函数 的 第一项 相当于 正则 ， 第二项 是 重构 误差       例子 ： 变分 自 编码       用 多层 感知器 从 输入 学习 到 两个 参数 向量 $ ( \ \ mu ,   \ \ sigma ) $ ，     隐 变量 通过 随机 采样 得到 $ ( z   \ \ sim   \ \ mathcal { N } ( z ;   \ \ mu ,   \ \ sigma ) ) $ 。       解码器 和 编码器 一样 ， 从应 变量 z 通过 多层 感知器 学习 到 两个 参数 $ ( \ \ mu ' ,   \ \ sigma ' ) $ ，     重构 变量 $ ( x '   \ \ sim   \ \ mathcal { N } ( x ' ;   \ \ mu ' ,   \ \ sigma ' ) ) $ 得到 。     对于 贝努利 分布 ， 参数 只有 一个 均值 。       Stacked   What - Where   Auto - encoders       论文 导读 ： Stacked   What - Where   Auto - encoders ，   Junbo   Zhao ,   Michael   Mathieu ,   Ross   Goroshin ,     Yann   LeCun   ,   ICLR   2016 .       What :   就是 polling 后 得到 的 max 值 ， 而 where 是 最大值 所在 的 位置 信息 ， 用来 帮助 解 卷积 器 重构 ！       文章 的 方法 将 编码器 学习 和 监督 学习 进行 联合 训练 学习 ， 作者 认为 （ 深度 比较 深时 ？ ） 用 自 编码 初始化 的 参数 所 携带 的 信息 ，     会 在 调优 的 时候 丢失 ， 导致 预 训练 没 啥 乱 用 ！ 而 解决 的 方法 就是 进行 联合 训练 。     此时 ， 重构 误差 相当于 一种 正则 ！     目标 函数 为 ：       $ $     L   =   L _ { NLL }   +   \ \ lambda _ { L2rec }   L _ { L2rec }   +   \ \ lambda _ { L2M }   L _ { L2M }     $ $       其中 NLL 代表 监督 学习 的 损失 函数 ， 负 对数 似然 函数 ， 对 回归 问题 是 L2 损失 ， 分类 问题 是 交叉 熵 。     重构 损失 函数 为 L2 损失 函数 。 L2rec   代表 输入 和 输出 的 重构 样本 的 重构 误差 ， L2M   代表 中间 编码器 输入 特征 和 解码器 输出 特征 的 重构 误差 。               实现 监督 学习 ， 无 监督 学习 ， 半 监督 学习 的 统一 框架 ！       软 最大值 max 和 argmax       Ross   Goroshin ,   Michael   Mathieu ,   and   Yann   LeCun .   Learning   to   linearize   under   uncertainty .   arXiv   preprint   arXiv : 1506.03011 ,   2015 .               文章 中 ， 取 $ ( \ \ beta = 100 ) $ ！       where 的 重要性               对比 采用 where 信息 做 unpooling 的 重构 结果 和 直接 copy 的 upsampling 的 结果 ， 可以 看出 where 信息 对 重构 输入 至关重要 ！     想想 也 能 想到 啊 ！ 肯定 重要 啊 ！       what 学习 到 的 具有 平移 不变性 ！       结论               从 论文 中 的 结果 来看 ， 添加 重构 误差 项 还是 挺 重要 的 ！     如果 没有 重构 误差 项 ， 错误率 会 增加 ！     训练 中 也 可以 加入   droupout   正则 化 方法 ！       Reference           Hinton ,   G .   E .   and   Salakhutdinov ,   R .   R .   Reducing   the   dimensionality   of   data   with   neural   networks .   Science   2006 .       2010 ,   Pascal   Vincent ,   Yoshua   Bengio ,   Stacked   Denoising   Autoencoders :   Learning   Useful   Representations   in   a   Deep   Network   with   a   Local   Denoising   Criterion .           近期 进展           Richard   Socher ,   Jeffrey   Pennington ,   Eric   H .   Huang ,   Andrew   Y .   Ng ,   and   Christopher   D .   Manning .     Semi - supervised   recursive   autoencoders   for   predicting   sentiment   distributions .   In     EMNLP ,   2011 .       Variational   autoencoders      ", "tags": "machine-learning", "url": "/wiki/machine-learning/auto-encoder.html"},
      
      
      {"title": "语义分割", "text": "    Table   of   Contents           关于           Learning   Hierarchical   Features   for   Scene   Labeling                 关于       用 卷积 网络 做 图像 的 语义 分割 ： 将 图像 中 的 每 一个 像素 标注 到 其 所属 对象 。       Learning   Hierarchical   Features   for   Scene   Labeling  ", "tags": "machine-learning", "url": "/wiki/machine-learning/cnn-segment.html"},
      
      
      {"title": "语音识别技术", "text": "    Table   of   Contents           基于 HMM 的 语音 识别 技术           参考                 基于 HMM 的 语音 识别 技术       主要 参考文献 [ 1 ]               首先 ， 语音 信号 被 分帧 ( 一般 10ms ) 提取 特征 ， 比如   MFCC   特征 ， 得到 一系列 的 特征向量 序列   $ ( Y _ { 1 : T }   =   y _ 1 , ... , y _ T ) $ , 这里 $ ( y _ i ) $ 都 代表 一帧 的 语音 特征 。 语音 识别器 寻找 一个 最佳 词 序列   $ ( w _ { 1 : L }   =   w _ 1 , ... , w _ L ) $ 进行 解码 ， 即 最大化 后验 概率   $ ( P ( w | Y ) ) $ 。     后验 概率 建模 通常 比较 困难 （ 现在 可以 直接 用 深度 学习 建模 啦 ） ， 所以   HMM   的 语音 识别 通过 条件 概率 建模 ，       参考           The   Application   of   Hidden   Markov   Models   in   Speech   Recognition ,   Mark   Gales ,   Steve   Young ,   Cambridge   University ,   2008      ", "tags": "machine-learning", "url": "/wiki/machine-learning/speech-recognition.html"},
      
      
      {"title": "近似最近邻搜索", "text": "    Table   of   Contents           综述           Metric   Tree ，   Spill   Tree                   大规模 最近 邻 搜索           partitioning   trees                         综述       论文 ： An   Investigation   of   Practical   Approximate   Nearest   Neighbor   Algorithms ， Ting   Liu ,   Andrew   W .   Moore ,   Alexander   Gray   and   Ke   Yang ， 2004           最近 邻 搜索 方案 ：       Voronoi   diagrams ： 1 - 2 维 ， F .   P .   Preparata   and   M .   Shamos .   Computational   Geometry .   Springer - Verlag ,       kd 树 、 metric   trees 、 ball - trees ： 中等 维度 10s       J .   H .   Friedman ,   J .   L .   Bentley ,   and   R .   A .   Finkel .   An   algorithm   for   finding   best   matches   in   loga -   rithmic   expected   time .   ACM   Transactions   on   Mathematical   Software ,   3 ( 3 ) : 209 – 226 ,   September   1977 .       J .   K .   Uhlmann .   Satisfying   general   proximity / similarity   queries   with   metric   trees .   Information   Processing   Letters ,   40 : 175 – 179 ,   1991 .       S .   M .   Omohundro .   Efficient   Algorithms   with   Neural   Network   Behaviour .   Journal   of   Complex   Systems ,   1 ( 2 ) : 273 – 347 ,   1987 .                       $ ( 1 + \ \ epsilon ) $ - k 最近 邻 搜索 ： 返回 的 点 最大 距离 不 超过 第 k 个 近邻 距离 的   $ ( 1 + \ \ epsilon ) $   倍 ！           Metric   Tree ，   Spill   Tree           metric   tree   将 样本 按照 二叉树 结构 保存 ， 根 节点 代表 所有 的 样本 ， 它 的 两个 子 节点 将 样本 分割 成 不想 交 的 两 部分 ；     用   v   代表 节点 ， N ( v ) 代表 v 节点 的 样本 ， 左右 子 节点 用   v . lc   v . rc   表示       分割 ： 对于 节点 v ， 找到 两个   pivot   点   v . lpv   v . rpv ， 使得 这 两个 节点 的 距离 是 集合 的 距离 ， 即   $ ( | | v . lpv   -   v . rpv | |   =   \ \ max _ { p1 , p2   \ \ in   N ( v ) }   | | p1   -   p2 | | ) $ ， 最优   pivot   寻找 是 二次 复杂度 ， 可以 近似 用 线性 复杂度 找   pivot ：       先 随机 找 一个点 p       找到   N ( v )   中距 p 最远 的 点 作为   v . lpv       再 找 距 v . lpv   最远 的 点 作为   v . rpv               找到 两个   pivot   点后 ， 然后 将 所有 点点 投影 到 这 两个 点 的 连线 上   $ ( u   =   v . rpv   -   v . lpv ) $ ， 找到 投影 的 中值   A   作为 分割 点 ， 投影 小于 A 的 分到 左子 树 ， 大于 A 的 分到 右子 树 ； 为 计算 效率 计 ， 可以 直接 用 中点 代替 ， 即   $ ( 1 / 2 ( v . rpv   -   v . lpv ) ) $       每 一个 节点   v   保留   N ( v )   的 覆盖 超球 信息 ， 球心   v . center ,   半径   v . r .           搜索 （ MT - DFS ） ： 深度 优先 搜索 ， 如果 待 查找 的 q 投影 在 A 左边 ， 先找 左子 树 ， 反之 先找 右子 树 ； 保留 k 个 已 找到 的 最近 邻 ， 设 这些 点 的 距 q 最大 距离 为 r ； 如果 节点   v   的 所有 点 距离 q 都 大于 r 就 可以 减枝 ， 不再 找 v 和 v 的 子树 。 可以 通过 条件   $ ( | | v . center   -   q | |   -   v . r   \ \ ge   r ) $   判断 ！               metric   tree   的 计算 通过 减枝 减少 寻找 数目 ， 但是 如果 无法 减枝 的 节点 ， 需要 不断 回溯 ， 使得 查找 性能 不高 。           spill   tree   则 放弃 精确 查找 ， 不 回溯 提高 性能 。 但是 精度 将 难以 接受 。       提高 精度 ： 分裂 的 时候 两个 子树 可以 有 交集 ， 相交 部分 为   $ ( \ \ tao ) $ ,   overlapping   buffer .   通过 这种 方法 提高 精度       混合 搜索 ： $ ( \ \ tao ) $   的 引入 带来 新 的 问题 ， 可能 某些 节点 重叠 部分 太 多 ， 导致 左右 子树 包含 了 全部 数据 ！ 为此 ， 可以 设定 一个 阈值   $ ( \ \ rho & lt ; 1 ) $ （ 典型值 70 % ） ， 如果 任何 一个 子 节点 包含 超过 这个 比率 的 样本 ， 则 对 这个 节点 不 进行   spill 分割 ， 而 进行 常规 的 不 交叠 分割 ， 并 标记 为   nonoverlapping   节点 ， 其他 的 节点 标记 为 overlapping 节点 。 搜索 的 时候 ， 只 对   nonoverlapping   节点 回溯 ！       维度 超过 30 后 ， 速度 就 会 变得 很 慢 ！ 通过 随机 投影 到 一个 低 维空间 后 ， 再 利用 混合 搜索 ！ 随机 投影 带来 的 精度 损失 ， 可以 通过 多次 投影 找回 ！         Johnson - Lindenstrauss   定理   ： W .   Johnson   and   J .   Lindenstrauss .   Extensions   of   lipschitz   maps   into   a   hilbert   space .   Contemp .   Math . ,   26 : 189 – 206 ,   1984 .       LSH ： A .   Gionis ,   P .   Indyk ,   and   R .   Motwani .   Similarity   Search   in   High   Dimensions   via   Hashing .   In   Proc   25th   VLDB   Conference ,   1999 .           大规模 最近 邻 搜索       Muja   M ,   Lowe   D   G .   Scalable   nearest   neighbor   algorithms   for   high   dimensional   data [ J ] .   IEEE   Transactions   on   Pattern   Analysis   and   Machine   Intelligence ,   2014 ,   36 ( 11 ) :   2227 - 2240 .           三类 最近 邻 搜索 方法 ：       partitioning   trees       hashing       neighboring   graph                   partitioning   trees           kd - tree ： 低 维空间 高效 ， 但是 维度 高 了 效果 急剧下降 ！       多个   randomized   k - d   trees      ", "tags": "machine-learning", "url": "/wiki/machine-learning/ann.html"},
      
      
      
      
      
        
      
      {"title": "计算机生成随机数算法", "text": "    Table   of   Contents           均匀分布           非 均匀 随机数                 均匀分布           计算机 算法 ,   热噪声 ,   量子 随机数       计算机 算法       平方 取中法       线性 同余 ,   $ ( x _ n   =   a   x _ { n - 1 }   +   c   ( mode   M ) ) $ .   当   c 与 M 互素 ,   a - 1 被 p ( M 任何 一个 素 因子 ) 整除 ,   如果 4 是 M 的 因子 , 则 a - 1 被 4 整除 。 那么 可以 达到 满 周期   M 。       取 M = 2 ^ L ,   a = 4alpha + 1 ,   c   =   2beta + 1       当 M 充分 大 的 时候 , 从 一阶矩 和 二阶 矩看 生成 的 数列 接近 均匀分布       Kobayashi :   $ ( x _ n   =   ( 314159269   x _ { n - 1 }   +   453806245 )   \ \ pmod { 2 ^ { 31 } } ) $               反馈 位 寄存器 法                   非 均匀 随机数           逆变换 法 ,   U 服从 0 - 1 之间 的 均匀分布 ,   目标 随机变量 的 累积 分布 函数 为 F , 那么   $ ( Y   =   F ^ { - 1 } ( U ) ) $   服从 目标 分布      ", "tags": "math", "url": "/wiki/math/random-generator.html"},
      
      
      {"title": "随机微分方程", "text": "    Table   of   Contents           关于           数学 基础           概率 空间 、 随机变量 、 随机 过程                         关于       随机 微分方程 的 读书笔记 .   An   Introduction   with   Applications .   6ed .       数学 基础       概率 空间 、 随机变量 、 随机 过程           随机变量 是 联系 事件 域 与 欧式 空间 的 桥梁 。   $ $ X :   \ \ Omega   \ \ rightarrow   R ^ n   $ $       对 欧时 空间 的 积分 定义 在 集合 测度 上 ， 对 随机变量 的 积分 定义 在 概率 测度 上 。           $ $     \ \ int _ { R ^ n }   f ( x )   dx   \ \ \ \     \ \ int _ { w   \ \ in   \ \ Omega }   f ( X ( w ) )   dP ( w )   =   \ \ int _ { R ^ n }   f ( x ) d \ \ mu _ X ( x )     $ $       $ $ dx $ $ 是 几何 测度 ， $ $ d \ \ mu _ X ( x ) $ $ 是 概率 测度 。           利用 对 随机变量 的 积分 定义 ， 可以 构造 出 Lp 空间 、 巴拿赫 空间 、 希尔伯特 空间 。      ", "tags": "math", "url": "/wiki/math/sde.html"},
      
      
      
      
      
        
      
      {"title": "磁盘空间满了", "text": "    Table   of   Contents           错误 日志                 错误 日志             Java   HotSpot   (   TM   )       64   - Bit   Server   VM   warning :   Insufficient   space     for     shared   memory   file :   Try   using   the   - Djava . io . tmpdir   =     option   to     select     an   alternate   temp   location .               执行     df   - hl     检查 磁盘空间 是否 满 了 。 显然     / dev / vda1   满 了             Filesystem             Size     Used   Avail   Use %   Mounted   on   / dev / vda1                 25G       24G       15M     100   %   /   tmpfs                         16G             0         16G         0   %   / dev / shm   / dev / vdc1               493G     151G     317G       33   %   / opt      ", "tags": "other", "url": "/wiki/other/java-insufficient-memory.html"},
      
      
      
      
      
        
      
      {"title": "BB84协议", "text": "    Table   of   Contents           BB84 协议                 BB84 协议       维基百科 链接     https : / / en . wikipedia . org / wiki / BB84         指 1984 年 ，   Charles   Bennett   and   Gilles   Brassard   提出 的 量子 秘钥 分发 协议 ！  ", "tags": "phys", "url": "/wiki/phys/bb84.html"},
      
      
      
      
      
        
      
      {"title": "PMML 预测模型教程", "text": "    Table   of   Contents           什么 是 PMML           使用 PMML 发布 预测 模型           PMML 文件 分析           基本 框架 分析           模型 分析           MiningField           Segmentation           Output   & amp ;   Target                           参考                 什么 是 PMML       PMML   是 一种 基于 XML 的 标准 语言 ， 用于 表达 数据挖掘 模型 ， 可以 用来 在 不同 的 应用程序 中 交换 模型 。     一种 非常 有用 的 应用 场景 是 在 生产 环境 中 部署 用 各种 建模 工具 训练 出来 的 模型 。     目前 最新 的 标准 是 4.3     http : / / dmg . org / pmml / pmml - v4 - 3 . html   。       PMML   文件 的 结构 遵从 了 用于 构建 预测 解决方案 的 常用 步骤 ， 包括 ：           数据 词典 ， 这是 一种 数据分析 阶段 的 产品 ， 可以 识别 和 定义 哪些 输入 数据字 段 对于 解决 眼前 的 问题 是 最 有用 的 。 这 可以 包括 数值 、 顺序 和 分类 字 段 。       挖掘 架构 ， 定义 了 处理 缺少 值 和 离群 值 的 策略 。 这 非常 有用 ， 因为 通常 情况 ， 当 将 模型 应用 于 实践 时 ， 所 需 的 输入 数据字 段 可能 为空 或者 被误 呈现 。       数据 转换 ， 定义 了 将 原始 输入 数据 预处理 至 派生 字段 所 需 的 计算 。 派生 字 段 （ 有时 也 称为 特征 检测器 ） 对 输入 字 段 进行 合并 或 修改 ， 以 获取 更 多 相关 信息 。 例如 ， 为了 预测 停车 所 需 的 制动 压力 ， 一个 预测 模型 可能 将 室外 温度 和 水 的 存在 （ 是否 在 下雨 ？ ） 作为 原始数据 。 派生 字 段 可能 会 将 这 两个 字 段 结合 起来 ， 以 探测 路上 是否 结冰 。 然后 结冰 字段 被 作为 模型 的 直接 输入 来 预测 停车 所 需 的 制动 压力 。       模型 定义 ， 定义 了 用于 构建 模型 的 结构 和 参数 。 PMML   涵盖 了 多种 统计 技术 。 例如 ， 为了 呈现 一个 神经网络 ， 它 定义 了 所有 的 神经 层 和 神经元 之间 的 连接 权重 。 对于 一个 决策树 来说 ， 它 定义 了 所有 树 节点 及 简单 和 复合 谓语 。       输出 ， 定义 了 预期 模型 输出 。 对于 一个 分类 任务 来说 ， 输出 可以 包括 预测 类及 与 所有 可能 类 相关 的 概率 。       目标 ， 定义 了 应用 于 模型 输出 的 后处理 步骤 。 对于 一个 回归 任务 来说 ， 此 步骤 支持 将 输出 转变 为 人们 很 容易 就 可以 理解 的 分数 （ 预测 结果 ） 。       模型 解释 ， 定义 了 将 测试数据 传递 至 模型 时 获得 的 性能 度量 标准 （ 与 训练 数据 相对 ） 。 这些 度量 标准 包括 字 段 相关性 、 混淆 矩阵 、 增益 图及 接收者 操作 特征 （ ROC ） 曲线图 。       模型 验证 ， 定义 了 一个 包含 输入 数据 记录 和 预期 模型 输出 的 示例 集 。 这是 非常 重要 的 一个 步骤 ， 因为 在 应用程序 之间 移动 模型 时 ， 该 模型 需要 通过 匹配 测试 。 这样 就 可以 确保 ， 在 呈现 相同 的 输入 时 ， 新 系统 可以 生成 与 旧 系统 同样 的 输出 。 如果 实际 情况 是 这样的话 ， 一个 模型 将 被 认为 经过 了 验证 ， 且 随时 可 用于 实践 。           一个 通用 的 PMML 文件 结构 如下 ( 参考   http : / / dmg . org / pmml / v4 - 3 / GeneralStructure . html   ) ：               & lt ; ? xml   version = & quot ; 1.0 & quot ; ? & gt ;       & lt ; PMML       version =     & quot ; 4.3 & quot ;           xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 3 & quot ;           xmlns : xsi =     & quot ; http : / / www . w3 . org / 2001 / XMLSchema - instance & quot ;     & gt ;             & lt ; Header       copyright =     & quot ; Example . com & quot ;     / & gt ;           & lt ; DataDictionary & gt ;     ...     & lt ; / DataDictionary & gt ;           ...   a   model   ...       & lt ; / PMML & gt ;                 使用 PMML 发布 预测 模型       下面 以 目前 应用 广泛 的   XGBoost   模型 为例 ， 介绍 使用 PMML 发布 预测 模型 。       首先 ， 我们 需要 有 一个 XGBoost 模型 ， 为此 ， 可以 以 Iris 数据 集 训练 一个 简单 的 二 分类 模型 （ 只用 其中 的 两类 ） 。   然后 利用   XGBoost   训练 得到 模型 文件 。               import       xgboost       as       xgb       from       sklearn . datasets       import       load _ iris         iris       =       load _ iris     ( )       mask       =       iris     .     target       & lt ;       2       X       =       iris     .     data     [     mask     , : ]       y       =       iris     .     target     [     mask     ]         params       =       {               &# 39 ; objective &# 39 ;       :       &# 39 ; reg : logistic &# 39 ;     ,               &# 39 ; num _ round &# 39 ;       :       10     ,               &# 39 ; max _ depth &# 39 ;       :       3       }       dtrain       =       xgb     .     DMatrix     (     X     ,       label     =     y     )       evallist       =       [ (     dtrain     ,       &# 39 ; train &# 39 ;     ) ]       bst       =       xgb     .     train     (     params     ,       dtrain     ,       evals     =     evallist     )       bst     .     save _ model     (     &# 39 ; xgb . bin &# 39 ;     )                       [ 0 ]   train - rmse : 0.364576   [ 1 ]   train - rmse : 0.27088   [ 2 ]   train - rmse : 0.203626   [ 3 ]   train - rmse : 0.154579   [ 4 ]   train - rmse : 0.118482   [ 5 ]   train - rmse : 0.091745   [ 6 ]   train - rmse : 0.071832   [ 7 ]   train - rmse : 0.056919   [ 8 ]   train - rmse : 0.045683   [ 9 ]   train - rmse : 0.037156               然后 生成 一个 特征 映射 文件 ， 因为 模型 文件 中 没有 特征 名 ， 只有 特征 id 。               f       =       open     (     &# 39 ; fmap . txt &# 39 ;     ,       &# 39 ; w &# 39 ;     )       for       i     ,       fn       in       enumerate     (     iris     .     feature _ names     ) :               f     .     write     (     &# 39 ;     % d     \ \ t     % s     \ \ t     % s     \ \ n     &# 39 ;       %       (     i     ,       fn     ,       &# 39 ; q &# 39 ;     ) )       f     .     close     ( )                 利用   jpmml - xgboost   项目   https : / / github . com / jpmml / jpmml - xgboost   提供 的 工具 ， 进行 转换 。     你 也 可以 直接 下载 我 已经   编译 好 的 jar 包   。     然后 执行 下述 命令 ， 即可 得到 转换 后 的 PMML 文件   xgb . pmml . xml 。             ! java   - jar   converter - executable - 1.2 - SNAPSHOT . jar   - - model - input   xgb . bin     - - fmap - input   fmap . txt     - - pmml - output   xgb . pmml . xml               得到 PMML 文件 xgb . pmml . xml 后 ， 我们 就 可以 在 生产 环境 部署 了 。     PMML 模型 的 部署 可以 使用     https : / / github . com / jpmml / jpmml - evaluator     进行 部署 ，     可以 很 容易 应用 到 分布式 环境 ！       下面 是 一段 预测 的 代码 ：               InputStream       is       =       new       FileInputStream     (     & quot ; path - to - pmml - file & quot ;     ) ;       PMML       pmml       =       PMMLUtil     .     unmarshal     (     is     ) ;         / /   这里 的 LocatorTransformer 是 为了 将 模型 转换 为 可 序列化 的 对象 ， 如果 不 需要 在 分布式 环境 ( 如 Spark ) 使用 模型 ， 就 可以 不用 转换       LocatorTransformer       lt       =       new       LocatorTransformer     ( ) ;       lt     .     applyTo     (     pmml     ) ;         Evaluator       evaluator       =       ModelEvaluatorFactory     .     newInstance     ( ) .     newModelEvaluator     (     pmml     ) ;         / /   预测       List     & lt ;     InputField     & gt ;       fields       =       evaluator     .     getActiveFields     ( ) ;         Map     & lt ;     FieldName     ,       Double     & gt ;       input       =       new       HashMap     & lt ; & gt ; ( ) ;       for     (     InputField       field       :       fields     ) {               input     .     put     (     field     .     getName     ( ) ,       1.2     ) ;       / / 对 每 一个 特征 指定 对应 的 值       }       Map     & lt ;     FieldName     ,       ? & gt ;       results       =       evaluator     .     evaluate     (     input     ) ;       List     & lt ;     TargetField     & gt ;       output       =       evaluator     .     getTargetFields     ( ) ;       Object       value       =       results     .     get     (     output     .     get     (     0     ) .     getName     ( ) ) ;                 PMML 文件 分析       基本 框架 分析       打开   xgb . pmml . xml   文件 ， 我们 可以 看到 一个 实际 可用 的 PMML 文件 结构 。               & lt ; ? xml   version = & quot ; 1.0 & quot ;   encoding = & quot ; UTF - 8 & quot ;   standalone = & quot ; yes & quot ; ? & gt ;       & lt ; PMML       xmlns =     & quot ; http : / / www . dmg . org / PMML - 4 _ 3 & quot ;       version =     & quot ; 4.3 & quot ;     & gt ;               & lt ; Header & gt ;                       & lt ; Application       name =     & quot ; JPMML - XGBoost & quot ;       version =     & quot ; 1.2 - SNAPSHOT & quot ;     / & gt ;                       & lt ; Timestamp & gt ;   2017 - 10 - 17T03 : 41 : 51Z   & lt ; / Timestamp & gt ;               & lt ; / Header & gt ;               & lt ; DataDictionary & gt ;                       & lt ; DataField       name =     & quot ; _ target & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; sepal   width   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; petal   length   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;                       & lt ; DataField       name =     & quot ; petal   width   ( cm ) & quot ;       optype =     & quot ; continuous & quot ;       dataType =     & quot ; float & quot ;     / & gt ;               & lt ; / DataDictionary & gt ;               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                     ...             & lt ; / MiningModel & gt ;       & lt ; / PMML & gt ;                 PMML 文件 是 基于 XML 格式 的 文本文件 ， 有且 只有 一个 根 节点   PMML   。       PMML     根 节点 除了   xmlns   属性 外 ， 有且 只有 一个 属性     version   ， 它 的 值 表明 PMML 标准 的 版本 。         PMML     子 元素 有 两个 是 必须 的 ，   Header     和     DataDictionary   。         Header       头部 信息 ， 只 包含 说明 信息 ， 对 预测 逻辑 没有 影响 ， 通常 包括 ：             -   包含 的 属性                     -   copyright   版权                     -   description   描述 文本                     -   modelVersion   模型 版本             -   包含 的 子 元素                     -   Application   描述 生成 PMML 文件 的 软件 相关 信息 ， 本 例子 说明 这个 PMML 是 由     JPMML - XGBoost     软件 生成 的 ， 该软件 版本 是     1.2 - SNAPSHOT   。                     -   Timestamp   生成 的 时间 戳                     -   Annotation   可 选 ， 描述 模型 版本 更新 信息         DataDictionary     数据 字典 ， 描述 字 段 信息 ， 包括 模型 的 输入 字段 和 输出 字 段 ， 这里   _ target   是 输出 字 段 ， 其他 三个 是 输入 字 段 。 每 一个 字段 用     DataField     元素 描述   ref   。   DataField     有 三个 必须 的 属性 ：   name     字段 或 特征 名字 ，   optype     操作 类型 ，   dataType     数据类型 。     DataDictionary     只 负责 字段 的 定义 ， 对字 段 的 处理 比如 缺失 值 处理 应该 在 模型 的   MiningField     区域 定义 。         optype     是 操作 类型 ， 有 三个 可选值 ：   categorical     类别 变量 ， 只能 进行 相等 的 判断 ;     ordinal     序数 变量 还 可以 进行 顺序 比较 ;     continuous     只有 这个 操作 类型 才能 进行 算术 运算 ， 这种 类型 的 变量 应用 比较 多 。         dataType     是 数据类型 ， 大约 有 十几种 类型   [ ref ]   。         PMML     可选 的 元素 有 4 个 ， 分别 是 ：   ref   。 包括     string   ,     interger   ,     float   ,     double   ,     date     等 常见 的 数据类型 。             MiningBuildTask     可以 包含 任意 XML 值 ， 用于 表达 模型 训练 时 的 相关 信息 ， 对模型 预测 没有 影响 。         TransformationDictionary     变换 字典 ， 用于 定义 各种 变换 。         MODEL - ELEMENT     这 是 个 模型 元素 集合 ， 用来 表达 模型 的 参数 和 预测 逻辑 。 具体 使用 时 ， 可以 是 这个 集合 里面 任意 一种 元素 ， 在 这个 例子 里面 ， 用 得 就是     MiningModel     这个 元素 ， 还 可以 是   GeneralRegressionModel   等 18 个 元素 中 的 任意 一个 ， 可以 参看   链接   。         Extension     扩展 信息             MODEL - ELEMENT     大约 包括 18 个 不同 的 模型 ， 每 一个 模型 都 有 几个 相同 的 属性 。   functionName     用于 定义 该 模型 是 回归 、 分类 还是 聚类 等等 ， 是 必须 的 属性 。 PMML 里面 一共 定义 了 7 种 类型   ref   ， 常用 的 有 回归   regression   、 分类   classification   、 聚类   clustering   。 另外 两个 可选 的 属性 是 ：   modelName     和     algorithmName   ， 只 用于 提示 ， 对模型 预测 没有 实质性 影响 。       MODEL - ELEMENT     都 包含 了 这样 一些 元素 ：   MiningSchema   （ 必须 ） 、     Output   、     Targets   等 ， 这些 在 后面 的 章节 将会 详细 介绍 。       模型 分析       找到     MiningModel     区块 ， 可以 看到 这个 元素 的 主要 结构 如下 ( 非 主要 结构 已 被 省略 ) ：               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;               & lt ; MiningSchema & gt ;                       & lt ; MiningField       name =     & quot ; _ target & quot ;       usageType =     & quot ; target & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; sepal   width   ( cm ) & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; petal   width   ( cm ) & quot ;     / & gt ;                       & lt ; MiningField       name =     & quot ; petal   length   ( cm ) & quot ;     / & gt ;               & lt ; / MiningSchema & gt ;               & lt ; Segmentation       multipleModelMethod =     & quot ; modelChain & quot ;     & gt ;                       & lt ; Segment       id =     & quot ; 1 & quot ;     & gt ;                               & lt ; True / & gt ;                               & lt ; MiningModel       functionName =     & quot ; regression & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                                     ...                             & lt ; / MiningModel & gt ;                       & lt ; / Segment & gt ;                       & lt ; Segment       id =     & quot ; 2 & quot ;     & gt ;                               & lt ; True / & gt ;                               & lt ; RegressionModel       functionName =     & quot ; regression & quot ;       normalizationMethod =     & quot ; logit & quot ;       x - mathContext =     & quot ; float & quot ;     & gt ;                                     ...                             & lt ; / RegressionModel & gt ;                       & lt ; / Segment & gt ;               & lt ; / Segmentation & gt ;       & lt ; / MiningModel & gt ;                   MiningModel     实际上 是 一种 通用 的 模型 ， 通常 用于 模型 的 融合 。 它 包含 了 一个 特有 子 的 元素     Segmentation   ， 用于 融合 多个 模型 。 本文 的 例子 里面 顶层 的 模型 （ 即   MiningModel   ） 包含 了 两个 模型 。 多个 模型 的 融合 方式 是   modelChain   ， 即前 一个 模型 的 输出 作为 后 一个 模型 的 输入 。 融合 方式 由   Segmentation   的 属性   multipleModelMethod   指定 。 实际上 ， 因为 XGBoost 用 的 是 回归 树 ， 然后 将 所有 树 的 输出 结果 相加 得到 第一个 模型 的 输出 ； 第二个 模型 只是 对 第一个 模型 的 输出 做 了 一个 简单 的   logit   变换 ， 将 原始 值 转换 为 概率 值 。         MiningSchema     是 所有 模型 都 必须 有 的 子 元素 ， 包括 模型 内 的 子 模型 也 都 有 。 对于 所有 输入 这个 模型 的 数据 ， 都 必须 经过     MiningSchema   ， 这个 元素 里面 包含 了 这个 模型 用到 的 所有 字 段 ， 相比     DataDictionary   ，   MiningSchema   可以 为 每个 模型 定义 特有 的 一些 信息 ， 还 包括 缺失 值 异常 值 处理 等 特征 预处理 操作 。   ref         MiningField       每 一个 字段 由   MiningField   定义 ， 它 包含 以下 属性 ：             name     必须 ， 字段名 字         usageType     字 段 用途 ， 默认 是   active   即 用作 输入 特征 ， 值   target     表示 该字 段 是 监督 学习 模型 的 目标 ， 也 是 模型 预测 结果 。 其他 取值 参考   ref           optype     操作 类型 ， 一般 在 数据 字典 中 定义 ， 这里 可以 重载 这个 属性         outliers     异常 值 处理 方式 ：   asIs       asMissingValues       asExtremeValues   （ 极端 值 通过 属性   lowValue   和   highValue   定义 ）         missingValueReplacement     缺失 值 替换 值 ， 如果 有 这个 属性 ， 在 进入 模型 前先用 这个 值 替换         missingValueTreatment     只是 提示 替换 的 值 的 来源 ， 对 PMML 预测 没有 影响                   & lt ; MiningField       name =     & quot ; foo & quot ;       missingValueReplacement =     & quot ; 3.14 & quot ;       missingValueTreatment =     & quot ; asMean & quot ;     / & gt ;                 Segmentation       多个 模型 用     Segmentation     来 组织 ， 每 一个 模型 都 被 包括 在子 元素     Segment     中 。     Segmentation     只有 一个 属性     multipleModelMethod     用来 表明 多个 模型 的 组合 方式 ， 可以 取得 值 如下   ref               modelChain     模型 链 ， Predicates 的 值 为 TRUE 的 模型 按照 顺序 打分 。 模型 的 输出 字 段     OutputFields     里面 的 字 段 ， 可以 作为 后续 模型 的 输入 。         sum     求和 ， 将 多个 模型 的 预测值 求和 。         average     平均 ， 将 多个 模型 的 预测值 平均 。         majorityVote     投票         weightedMajorityVote   ,     weightedAverage   ,     max   ,     median             每 一个   Segment   包含 属性   id   和   weight   ，   weight   可 选 属性 ， 在 加权 融合 的 情况 下才 有用 。 每 一个   Segment   包含 的 子 元素 有       PREDICATE       和       MODEL - ELEMENT     。 这个 例子 中 的   PREDICATE   是   & lt ; True / & gt ;   ， 表明 使用 这个 模型 计算 预测值 ， 如果 为   & lt ; False / & gt ;   则 不 使用 。 模型 元素 有 两个 ， 一个 是   MiningModel   ， 另 一个 是   TreeModel   。   TreeModel   我们 在 后面 介绍 。       Output   & amp ;   Target           Output       和       Target       都 可以 用于 定义 模型 的 输出 。         Target     可以 对 输出 结果 做 简单 的 线性变换 ： $ ( f ( x )   =   10   +   3.14   x ) $               & lt ; Targets & gt ;           & lt ; Target       field =     & quot ; amount & quot ;       rescaleConstant =     & quot ; 10 & quot ;       rescaleFactor =     & quot ; 3.14 & quot ;       min =     & quot ; - 10 & quot ;       max =     & quot ; 10.5 & quot ;       castInteger =     & quot ; round & quot ;     / & gt ;       & lt ; / Targets & gt ;                   Output     则 可以 应用 更 复杂 的 变换 ，   OutputField   的   feature   属性 ， 可以 输出 很多 有用 的 信息   ref   ， 例如 预测 原始 值 ， 决策 树叶子 结点 的 ID 值 等等 。     下面 是 一个 对模型 输出 结果 做 变换 $ ( f ( x )   =   10 ^ { x   +   0.5 }   -   1 ) $ 的 例子 ， 这个 例子 来自 于用 XGBoost 对 log1p 后 的 值 做 回归 ， 因为 多个 决策树 的 结果 加和后 ， 要 乘以 0.5 ， 所以 反 变换 就是 上面 这个 表达式 了 。               & lt ; Output & gt ;               & lt ; OutputField       name =     & quot ; rawResult & quot ;       dataType =     & quot ; double & quot ;       feature =     & quot ; predictedValue & quot ;       / & gt ;               & lt ; OutputField       name =     & quot ; _ target & quot ;       dataType =     & quot ; double & quot ;       feature =     & quot ; transformedValue & quot ;       & gt ;                       & lt ; ! - -     pow ( 10 ,   s + 0.5 )   -   1   - - & gt ;                       & lt ; Apply       function =     & quot ; round & quot ;     & gt ;                               & lt ; Apply       function =     & quot ; - & quot ;     & gt ;                                       & lt ; Apply       function =     & quot ; pow & quot ;     & gt ;                                               & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   10.0   & lt ; / Constant & gt ;                                               & lt ; Apply       function =     & quot ; +& quot ;     & gt ;                                                       & lt ; FieldRef       field =     & quot ; rawResult & quot ;     & gt ; & lt ; / FieldRef & gt ;                                                       & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   0.5   & lt ; / Constant & gt ;                                               & lt ; / Apply & gt ;                                       & lt ; / Apply & gt ;                                       & lt ; Constant       dataType =     & quot ; double & quot ;     & gt ;   1.0   & lt ; / Constant & gt ;                               & lt ; / Apply & gt ;                       & lt ; / Apply & gt ;               & lt ; / OutputField & gt ;       & lt ; / Output & gt ;                 PMML 内置 了 常用 的   数学 函数   ， 函数 的 应用 非常简单 ， 直接 创建 一个   Apply   元素 ， 并 指定 属性   function   为 函数 名 即可 ， 然后 将 参数 依次 作为 子 元素 。 参数 可以 是 常数   Constant   和 其他 字 段   FieldRef   ， 甚至 一个 新 的 函数 应用 结果   Apply   。 用户 也 可以 创建 自定义 函数 ， 将 函数 的 定义 放到     TransformationDictionary     中 ， 然后 就 可以 直接 引用 了 。       参考           https : / / www . ibm . com / developerworks / cn / opensource / ind - PMML1 / index . html       PMML - 4 _ 3 标准 文档     http : / / dmg . org / pmml / pmml - v4 - 3 . html        ", "tags": "pmml", "url": "/wiki/pmml/intro.html"},
      
      
      
      
      
        
      
      {"title": "Flask", "text": "    Table   of   Contents           关于           快速 入门           简单 应用           路由                   教程           Blueprints   and   Views           Blueprint                   Templates           Static   Files           项目 安装           测试           flask - sqlalchemy                 关于       Flask     http : / / flask . pocoo . org / docs / 1.0 /         快速 入门       简单 应用               from       flask       import       Flask       app       =       Flask     (     __ name __     )         @ app . route     (     & quot ; / & quot ;     )       def       hello     ( ) :               return       & quot ; Hello   World ! & quot ;                     运行 命令     export   FLASK _ APP = hello . py   flask   run         外 网 可见     flask   run   - - host = 0.0 . 0.0         Debug 模式     export   FLASK _ ENV = development             路由           使用     route ( )     装饰 ,     @ app . route ( ' / ' )   ,     @ app . route ( ' / hello ' )         变量 规则 ,     & lt ; variable _ name & gt ;     或者 指定 转化 类型     & lt ; converter : variable _ name & gt ;                     @ app . route     (     &# 39 ; / user / & lt ; username & gt ; &# 39 ;     )       def       show _ user _ profile     (     username     ) :               #   show   the   user   profile   for   that   user               return       &# 39 ; User       % s     &# 39 ;       %       username         @ app . route     (     &# 39 ; / post / & lt ; int : post _ id & gt ; &# 39 ;     )       def       show _ post     (     post _ id     ) :               #   show   the   post   with   the   given   id ,   the   id   is   an   integer               return       &# 39 ; Post       % d     &# 39 ;       %       post _ id         @ app . route     (     &# 39 ; / path / & lt ; path : subpath & gt ; &# 39 ;     )       def       show _ subpath     (     subpath     ) :               #   show   the   subpath   after   / path /               return       &# 39 ; Subpath       % s     &# 39 ;       %       subpath                     转换 类型 包括 :         string     默认 类型 , 不 包括   /           int     整数         float           path     包括     /           uuid     UUID ?               路由   / projects /     和     / projects     的 区别 :   后者 可以 通过 前者 访问 , 但是 前者 无法 通过 后者 访问       URL   Building     url _ for ( func _ name ,   * * kwargs )   ,       test _ request _ context ( )     可以 用于 模拟 request                   from       flask       import       Flask     ,       url _ for         app       =       Flask     (     __ name __     )         @ app . route     (     &# 39 ; / &# 39 ;     )       def       index     ( ) :               return       &# 39 ; index &# 39 ;         @ app . route     (     &# 39 ; / login &# 39 ;     )       def       login     ( ) :               return       &# 39 ; login &# 39 ;         @ app . route     (     &# 39 ; / user / & lt ; username & gt ; &# 39 ;     )       def       profile     (     username     ) :               return       &# 39 ; { }     \ \ &# 39 ;     s   profile &# 39 ;     .     format     (     username     )         with       app     .     test _ request _ context     ( ) :               print     (     url _ for     (     &# 39 ; index &# 39 ;     ) )               print     (     url _ for     (     &# 39 ; login &# 39 ;     ) )               print     (     url _ for     (     &# 39 ; login &# 39 ;     ,       next     =     &# 39 ; / &# 39 ;     ) )               print     (     url _ for     (     &# 39 ; profile &# 39 ;     ,       username     =     &# 39 ; John   Doe &# 39 ;     ) )         /       /     login       /     login     ?     next     = /       /     user     /     John     %     20     Doe                     HTTP   Methods ,   默认 的 路由 只 响应 GET 请求 ,   可以 通过   methods   参数 指定 其他 请求 ,     @ app . route ( ' / login ' ,   methods = [ ' GET ' ,   ' POST ' ] )                     from       flask       import       request         @ app . route     (     &# 39 ; / login &# 39 ;     ,       methods     =     [     &# 39 ; GET &# 39 ;     ,       &# 39 ; POST &# 39 ;     ] )       def       login     ( ) :               if       request     .     method       = =       &# 39 ; POST &# 39 ;     :                       return       do _ the _ login     ( )               else     :                       return       show _ the _ login _ form     ( )                     Static   Files ,     static     目录 用于 放置 静态 文件 ,     url _ for ( ' static ' ,   filename = ' style . css ' )         模板 ,   使用 jinja2 模板 引擎 ,     render _ template ( )   , 模板 目录     templates /   ,   模板 引擎 参考                     from       flask       import       render _ template         @ app . route     (     &# 39 ; / hello / &# 39 ;     )       @ app . route     (     &# 39 ; / hello / & lt ; name & gt ; &# 39 ;     )       def       hello     (     name     =     None     ) :               return       render _ template     (     &# 39 ; hello . html &# 39 ;     ,       name     =     name     )                     在 模板 内部 ,   可以 访问 的 对象 :     request   ,     session   ,       g       和 函数   get _ flashed _ messages ( )         模板 中 的 变量 会 自动 转移 HTML 特殊字符 ,   如果 不想 转义 , 可以 使用     Markup ( )     或者     | safe     filter       访问   request   数据 :   通过 全局 对象       request       实现         method     请求 方法     ' GET ' , ' POST '           form     表单 数据 字典         args     GET 参数 字典                           from       flask       import       request       searchword       =       request     .     args     .     get     (     &# 39 ; key &# 39 ;     ,       &# 39 ; &# 39 ;     )                     文件 上传 ,   HTML 中 指定 form 编码     enctype = \" multipart / form - data \"   ,   通过     request . files     字典 访问 文件 , key 是 文件名 ,   value 是   file   对象 , 可以 通过   . save ( )   方法 保存 文件                   from       flask       import       request       from       werkzeug . utils       import       secure _ filename         @ app . route     (     &# 39 ; / upload &# 39 ;     ,       methods     =     [     &# 39 ; GET &# 39 ;     ,       &# 39 ; POST &# 39 ;     ] )       def       upload _ file     ( ) :               if       request     .     method       = =       &# 39 ; POST &# 39 ;     :                       f       =       request     .     files     [     &# 39 ; the _ file &# 39 ;     ]                       f     .     save     (     &# 39 ; / var / www / uploads / &# 39 ;       +       secure _ filename     (     f     .     filename     ) )                     Cookies ,           response . set _ cookie ( key ,   val )     设置 cookie ,     response     对象 可以 通过   make _ response   创建         request . cookies     字典 获取 cookie                           from       flask       import       request         @ app . route     (     &# 39 ; / &# 39 ;     )       def       index     ( ) :               username       =       request     .     cookies     .     get     (     &# 39 ; username &# 39 ;     )               #   use   cookies . get ( key )   instead   of   cookies [ key ]   to   not   get   a               #   KeyError   if   the   cookie   is   missing .               resp       =       make _ response     (     render _ template     (     ...     ) )               resp     .     set _ cookie     (     &# 39 ; username &# 39 ;     ,       &# 39 ; the   username &# 39 ;     )               return       resp                     重定向 与 错误 ,     redirect ( url )   ,     abort ( code )   ,   错误 的 自定义 处理 通过 装饰 器   @ app . errorhandler ( int   code )   实现                   from       flask       import       abort     ,       redirect     ,       url _ for         @ app . route     (     &# 39 ; / &# 39 ;     )       def       index     ( ) :               return       redirect     (     url _ for     (     &# 39 ; login &# 39 ;     ) )         @ app . route     (     &# 39 ; / login &# 39 ;     )       def       login     ( ) :               abort     (     401     )               this _ is _ never _ executed     ( )         @ app . errorhandler     (     404     )       def       page _ not _ found     (     error     ) :               return       render _ template     (     &# 39 ; page _ not _ found . html &# 39 ;     ) ,       404                     Responses ,         如果 返回 的 是     response     对象 , 则 直接 以 这个 对象 响应       如果 返回 的 是     string   ,   则 以 string 为 内容 , 其他 以 默认 参数 , 构造     response     对象 响应       如果 返回 的 是     tuple   ,   则 要求 格式 为     ( response ,   status ,   headers )     or     ( response ,   headers )   ,     status     是 状态 码 ,     headers     是 list 或者 dict                           @ app . errorhandler     (     404     )       def       not _ found     (     error     ) :               resp       =       make _ response     (     render _ template     (     &# 39 ; error . html &# 39 ;     ) ,       404     )               resp     .     headers     [     &# 39 ; X - Something &# 39 ;     ]       =       &# 39 ; A   value &# 39 ;               return       resp                     Sessions                   from       flask       import       Flask     ,       session     ,       redirect     ,       url _ for     ,       escape     ,       request         app       =       Flask     (     __ name __     )         #   Set   the   secret   key   to   some   random   bytes .   Keep   this   really   secret !       app     .     secret _ key       =       b     &# 39 ; _ 5 # y2L & quot ; F4Q8z     \ \ n \ \ xec     ] / &# 39 ;         @ app . route     (     &# 39 ; / &# 39 ;     )       def       index     ( ) :               if       &# 39 ; username &# 39 ;       in       session     :                       return       &# 39 ; Logged   in   as       % s     &# 39 ;       %       escape     (     session     [     &# 39 ; username &# 39 ;     ] )               return       &# 39 ; You   are   not   logged   in &# 39 ;         @ app . route     (     &# 39 ; / login &# 39 ;     ,       methods     =     [     &# 39 ; GET &# 39 ;     ,       &# 39 ; POST &# 39 ;     ] )       def       login     ( ) :               if       request     .     method       = =       &# 39 ; POST &# 39 ;     :                       session     [     &# 39 ; username &# 39 ;     ]       =       request     .     form     [     &# 39 ; username &# 39 ;     ]                       return       redirect     (     url _ for     (     &# 39 ; index &# 39 ;     ) )               return       &# 39 ; &# 39 ; &# 39 ;                       & lt ; form   method = & quot ; post & quot ; & gt ;                               & lt ; p & gt ; & lt ; input   type = text   name = username & gt ;                               & lt ; p & gt ; & lt ; input   type = submit   value = Login & gt ;                       & lt ; / form & gt ;               &# 39 ; &# 39 ; &# 39 ;         @ app . route     (     &# 39 ; / logout &# 39 ;     )       def       logout     ( ) :               #   remove   the   username   from   the   session   if   it &# 39 ; s   there               session     .     pop     (     &# 39 ; username &# 39 ;     ,       None     )               return       redirect     (     url _ for     (     &# 39 ; index &# 39 ;     ) )                 秘钥 生成               $   python   - c     &# 39 ; import   os ;   print ( os . urandom ( 16 ) ) &# 39 ;     b   &# 39 ; _ 5 # y2L & quot ; F4Q8z \ \ n \ \ xec ] / &# 39 ;                     Message   Flashing ,   不 知道 干 啥 的       日志     logging                     app     .     logger     .     debug     (     &# 39 ; A   value   for   debugging &# 39 ;     )       app     .     logger     .     warning     (     &# 39 ; A   warning   occurred   (     % d       apples ) &# 39 ;     ,       42     )       app     .     logger     .     error     (     &# 39 ; An   error   occurred &# 39 ;     )                 教程           构建 一个 简单 的 blog 系统     http : / / flask . pocoo . org / docs / 1.0 / tutorial /         通过 工厂 方法 创建 应用     create _ app   ,                     export       FLASK _ APP     =   flaskr : create _ app     export       FLASK _ ENV     =   development   flask   run                   链接 数据库         g   是 一个 特殊 的 对象 ,   每 一个 request 对应 唯一 一个 ,   用于 在 多个 函数 中 共享 , 类似 于 request 作用域 的 全局变量         current _ app     也 是 一个 特殊 全局变量 , 指向 当前 app 实例         click . command       是 基于   Click 的 命令行 接口           app . teardown _ appcontext     注册 request 结束 后 的 hook         app . cli . add _ command ( )     将 新 的 命令 注册 到 flask 的 命令行                           import       sqlite3         import       click       from       flask       import       current _ app     ,       g       from       flask . cli       import       with _ appcontext           def       get _ db     ( ) :               if       &# 39 ; db &# 39 ;       not       in       g     :                       g     .     db       =       sqlite3     .     connect     (                               current _ app     .     config     [     &# 39 ; DATABASE &# 39 ;     ] ,                               detect _ types     =     sqlite3     .     PARSE _ DECLTYPES                       )                       g     .     db     .     row _ factory       =       sqlite3     .     Row                 return       g     .     db           def       close _ db     (     e     =     None     ) :               db       =       g     .     pop     (     &# 39 ; db &# 39 ;     ,       None     )                 if       db       is       not       None     :                       db     .     close     ( )         def       init _ db     ( ) :               db       =       get _ db     ( )                 with       current _ app     .     open _ resource     (     &# 39 ; schema . sql &# 39 ;     )       as       f     :                       db     .     executescript     (     f     .     read     ( )     .     decode     (     &# 39 ; utf8 &# 39 ;     ) )           @ click . command     (     &# 39 ; init - db &# 39 ;     )       @ with _ appcontext       def       init _ db _ command     ( ) :               & quot ; & quot ; & quot ; Clear   the   existing   data   and   create   new   tables .& quot ; & quot ; & quot ;               init _ db     ( )               click     .     echo     (     &# 39 ; Initialized   the   database .&# 39 ;     )         def       init _ app     (     app     ) :               app     .     teardown _ appcontext     (     close _ db     )               app     .     cli     .     add _ command     (     init _ db _ command     )                   flaskr / __ init __. py                 def       create _ app     ( ) :               app       =       ...               #   existing   code   omitted                 import       db               db     .     init _ app     (     app     )                 return       app                     初始化 数据库     flask   init - db             Blueprints   and   Views       Blueprint         Blueprint   是 一组 相关 的 view , 类似 于 MVC 中 的 Controller 吧 。 Flaskr 有 两个 Blueprint , 一个 是 认证 相关 函数 , 一个 是 blog 。             bp   =   Blueprint ( ' auth ' ,   __ name __ ,   url _ prefix = ' / auth ' )     创建 Blueprint     auth         通过     app . register _ blueprint ( )     注册 blueprint             flaskr / auth . py                 @ bp . route     (     &# 39 ; / register &# 39 ;     ,       methods     =     (     &# 39 ; GET &# 39 ;     ,       &# 39 ; POST &# 39 ;     ) )       def       register     ( ) :               if       request     .     method       = =       &# 39 ; POST &# 39 ;     :                       username       =       request     .     form     [     &# 39 ; username &# 39 ;     ]                       password       =       request     .     form     [     &# 39 ; password &# 39 ;     ]                       db       =       get _ db     ( )                       error       =       None                         if       not       username     :                               error       =       &# 39 ; Username   is   required .&# 39 ;                       elif       not       password     :                               error       =       &# 39 ; Password   is   required .&# 39 ;                       elif       db     .     execute     (                               &# 39 ; SELECT   id   FROM   user   WHERE   username   =   ? &# 39 ;     ,       (     username     , )                       )     .     fetchone     ( )       is       not       None     :                               error       =       &# 39 ; User   { }   is   already   registered .&# 39 ;     .     format     (     username     )                         if       error       is       None     :                               db     .     execute     (                                       &# 39 ; INSERT   INTO   user   ( username ,   password )   VALUES   ( ? ,   ? ) &# 39 ;     ,                                       (     username     ,       generate _ password _ hash     (     password     ) )                               )                               db     .     commit     ( )                               return       redirect     (     url _ for     (     &# 39 ; auth . login &# 39 ;     ) )                         flash     (     error     )                 return       render _ template     (     &# 39 ; auth / register . html &# 39 ;     )         @ bp . route     (     &# 39 ; / login &# 39 ;     ,       methods     =     (     &# 39 ; GET &# 39 ;     ,       &# 39 ; POST &# 39 ;     ) )       def       login     ( ) :               if       request     .     method       = =       &# 39 ; POST &# 39 ;     :                       username       =       request     .     form     [     &# 39 ; username &# 39 ;     ]                       password       =       request     .     form     [     &# 39 ; password &# 39 ;     ]                       db       =       get _ db     ( )                       error       =       None                       user       =       db     .     execute     (                               &# 39 ; SELECT   *   FROM   user   WHERE   username   =   ? &# 39 ;     ,       (     username     , )                       )     .     fetchone     ( )                         if       user       is       None     :                               error       =       &# 39 ; Incorrect   username .&# 39 ;                       elif       not       check _ password _ hash     (     user     [     &# 39 ; password &# 39 ;     ] ,       password     ) :                               error       =       &# 39 ; Incorrect   password .&# 39 ;                         if       error       is       None     :                               session     .     clear     ( )                               session     [     &# 39 ; user _ id &# 39 ;     ]       =       user     [     &# 39 ; id &# 39 ;     ]                               return       redirect     (     url _ for     (     &# 39 ; index &# 39 ;     ) )                         flash     (     error     )                 return       render _ template     (     &# 39 ; auth / login . html &# 39 ;     )                           register     说明             @ bp . route     是 blueprint 的 路由 ,     / register     对应 的 url   path 为     / auth / register           GET   请求 返回 注册 页面 ,     POST   请求 注册 用户         generate _ password _ hash ( )     用户 创建 密码 hash ,   数据库 中 只 保存 密码 的 hash 值                     login     说明             check _ password _ hash ( )     校验 密码         session     登陆 成功 后 , 清空 session , 并 设置 userid , session 字典 的 值 都 会 通过 加密 的 cookie 存到 浏览器 中                           @ bp . before _ app _ request       def       load _ logged _ in _ user     ( ) :               user _ id       =       session     .     get     (     &# 39 ; user _ id &# 39 ;     )                 if       user _ id       is       None     :                       g     .     user       =       None               else     :                       g     .     user       =       get _ db     ( )     .     execute     (                               &# 39 ; SELECT   *   FROM   user   WHERE   id   =   ? &# 39 ;     ,       (     user _ id     , )                       )     .     fetchone     ( )                       load _ logged _ in _ user     说明       从 session 中 查找 user _ id , 如果 没有 则 将   g . user   置   None   , 否则 查询 user 保存 到   g . user                   logout     清空 session                   @ bp . route     (     &# 39 ; / logout &# 39 ;     )       def       logout     ( ) :               session     .     clear     ( )               return       redirect     (     url _ for     (     &# 39 ; index &# 39 ;     ) )                     在 其他 的 view 中 使用 用户 认证 模块 ,   通过 装饰 器 (   参考   ) 来 实现 .     @ login _ required   ,   注意   @ login _ required     装饰 了 函数     func     相当于     func   =   login _ required ( func )     。 有 了 这个 装饰 器后 , 就 可以 给 需要 认证 的 view 方便 的 添加 认证 。                   def       login _ required     (     view     ) :               @ functools . wraps     (     view     )               def       wrapped _ view     (     * *     kwargs     ) :                       if       g     .     user       is       None     :                               return       redirect     (     url _ for     (     &# 39 ; auth . login &# 39 ;     ) )                         return       view     (     * *     kwargs     )                 return       wrapped _ view                 Templates         flaskr / templates / base . html                 & lt ;     !     doctype       html     & gt ;       & lt ;     title     & gt ;     {     %       block       title       %     } {     %       endblock       %     }       -       Flaskr     & lt ; /     title     & gt ;       & lt ;     link       rel     =     & quot ; stylesheet & quot ;       href     =     & quot ; { {   url _ for ( &# 39 ; static &# 39 ; ,   filename = &# 39 ; style . css &# 39 ; )   } } & quot ;     & gt ;       & lt ;     nav     & gt ;           & lt ;     h1     & gt ;     Flaskr     & lt ; /     h1     & gt ;           & lt ;     ul     & gt ;               {     %       if       g     .     user       %     }                   & lt ;     li     & gt ; & lt ;     span     & gt ;     { {       g     .     user     [     &# 39 ; username &# 39 ;     ]       } }     & lt ; /     span     & gt ;                   & lt ;     li     & gt ; & lt ;     a       href     =     & quot ; { {   url _ for ( &# 39 ; auth . logout &# 39 ; )   } } & quot ;     & gt ;     Log       Out     & lt ; /     a     & gt ;               {     %       else       %     }                   & lt ;     li     & gt ; & lt ;     a       href     =     & quot ; { {   url _ for ( &# 39 ; auth . register &# 39 ; )   } } & quot ;     & gt ;     Register     & lt ; /     a     & gt ;                   & lt ;     li     & gt ; & lt ;     a       href     =     & quot ; { {   url _ for ( &# 39 ; auth . login &# 39 ; )   } } & quot ;     & gt ;     Log       In     & lt ; /     a     & gt ;               {     %       endif       %     }           & lt ; /     ul     & gt ;       & lt ; /     nav     & gt ;       & lt ;     section       class     =     & quot ; content & quot ;     & gt ;           & lt ;     header     & gt ;               {     %       block       header       %     } {     %       endblock       %     }           & lt ; /     header     & gt ;           {     %       for       message       in       get _ flashed _ messages     ( )       %     }               & lt ;     div       class     =     & quot ; flash & quot ;     & gt ;     { {       message       } }     & lt ; /     div     & gt ;           {     %       endfor       %     }           {     %       block       content       %     } {     %       endblock       %     }       & lt ; /     section     & gt ;                       base     模板 说明         get _ flashed _ messages ( )     获取     flash     中 的 message         block     模块 可以 被 继承 的 模板 修改         { %   block   title   % }                 每 一个   blueprint   都 在 子目录 中                     flaskr / templates / auth / register . html                 {     %       extends       &# 39 ; base . html &# 39 ;       %     }         {     %       block       header       %     }           & lt ;     h1     & gt ;     {     %       block       title       %     }     Register     {     %       endblock       %     }     & lt ; /     h1     & gt ;       {     %       endblock       %     }         {     %       block       content       %     }           & lt ;     form       method     =     & quot ; post & quot ;     & gt ;               & lt ;     label       for     =     & quot ; username & quot ;     & gt ;     Username     & lt ; /     label     & gt ;               & lt ;     input       name     =     & quot ; username & quot ;       id     =     & quot ; username & quot ;       required     & gt ;               & lt ;     label       for     =     & quot ; password & quot ;     & gt ;     Password     & lt ; /     label     & gt ;               & lt ;     input       type     =     & quot ; password & quot ;       name     =     & quot ; password & quot ;       id     =     & quot ; password & quot ;       required     & gt ;               & lt ;     input       type     =     & quot ; submit & quot ;       value     =     & quot ; Register & quot ;     & gt ;           & lt ; /     form     & gt ;       {     %       endblock       %     }                       register     模块 说明         extends     模板 继承         block     override   每 一个 子块 ,   将     { %   block   title   % }     放到     { %   block   header   % }   内部 , 可以 避免 title   模块 定义 两次                   Static   Files           flask 自动 添加     static     view   作为 静态 文件 的 目录           项目 安装             setup . py     文件 ,     packages     告诉 python 有 哪些 包 , 通过   find _ packages   自动 搜索 包 ,   include _ package _ data   告诉 python 是否 需要 打包 数据文件 ,   MANIFEST . in   文件 告诉 python 数据文件 有 哪些                   from       setuptools       import       find _ packages     ,       setup         setup     (               name     =     &# 39 ; flaskr &# 39 ;     ,               version     =     &# 39 ; 1.0 . 0 &# 39 ;     ,               packages     =     find _ packages     ( ) ,               include _ package _ data     =     True     ,               zip _ safe     =     False     ,               install _ requires     =     [                       &# 39 ; flask &# 39 ;     ,               ] ,       )                   MANIFEST . in               include   flaskr / schema . sql   graft   flaskr / static   graft   flaskr / templates   global - exclude   * . pyc                   安装     pip   install   - e   .       - e   表示 是 edit 或者 dev 模式 , 可以 让 修改 后 的 文件 立即 生效           测试           用     pytest   和   coverage   测试 你 的 代码       测试代码 放在 与 flaskr 平行 的   tests   目录 下           flask - sqlalchemy  ", "tags": "python", "url": "/wiki/python/flask.html"},
      
      
      {"title": "ggplot绘图工具", "text": "    Table   of   Contents           关于                 关于       ggplot 本是 一个 R 绘图 工具包 ， 现在 在 python 中 也 可以 用 了 。  ", "tags": "python", "url": "/wiki/python/ggplot.html"},
      
      
      {"title": "IPython", "text": "    Table   of   Contents           关于           Display 模块                 关于       使用 IPython 多年 ， 但是 一直 没 系统地 学习 过 ， 现在 系统地 学 一下 ， 提升 效率 。       Display 模块  ", "tags": "python", "url": "/wiki/python/ipython.html"},
      
      
      {"title": "MAC上安装light gbm库", "text": "    Table   of   Contents           步骤                 步骤       参考   https : / / stackoverflow . com / questions / 44937698 / lightgbm - oserror - library - not - loaded               brew   install   cmake           brew   install   gcc   - - without - multilib           git   clone   - - recursive   https : / / github . com / Microsoft / LightGBM   ;   cd   LightGBM           mkdir   build   ;   cd   build           cmake   ..   - DCMAKE _ C _ COMPILER = / usr / local / Cellar / gcc / 6.2 . 0 / bin / gcc - 6     - DCMAKE _ CXX _ COMPILER = / usr / local / Cellar / gcc / 6.2 . 0 / bin / g ++ - 6     - DOpenMP _ C _ FLAGS = - fopemmp       注意 把 gcc 的 两个 路径 改成 你 机器 上 的 路径 ， DOpenMP _ C _ FLAGS 标记 是因为 cmake 脚本 没有 自动 找到 opemmp 的 路径 ， 手动 指定         make   - j           cd   python - packages           sudo   python   setup . py   install        ", "tags": "python", "url": "/wiki/python/light-gbm-install.html"},
      
      
      {"title": "Matplotlib 中文字体问题", "text": "    Table   of   Contents           问题 背景           问题 原因           解决 方法                 问题 背景       用   matplotlib   画图 时 ， 经常 遇到 中文 ( 非 ASCII ) 字体 乱码 问题 ， 导致 这些 字符 全部 变成   □ □ ！       问题 原因       找 不到 中文字体       解决 方法       首先 下载   msyh . ttf   即 微软 雅黑 字体 文件 ， 放到   matplotlib   的 字体 目录 里面 ， 或者 系统 字体 中 。     字体 目录 通过 如下 命令 获取 ， 不同 系统 不 一样 。               import       matplotlib       print     (     matplotlib     .     matplotlib _ fname     ( ) )                       / Library / Python / 2.7 / site - packages / matplotlib / mpl - data / matplotlibrc               / Library / Python / 2.7 / site - packages / matplotlib / mpl - data /   为 配置 目录 ， 在 该 目录 下   fonts / ttf /   目录 就是 存放 字体 文件 的 目录 。       最 简单 的 方法 是 在 运行 程序 前 ， 动态 设置               import       matplotlib       matplotlib     .     rcParams     [     u     &# 39 ; font . sans - serif &# 39 ;     ]       =       [     &# 39 ; Microsoft   YaHei &# 39 ;     ]       +       matplotlib     .     rcParams     [     u     &# 39 ; font . sans - serif &# 39 ;     ]         import       numpy       as       np       import       matplotlib . pyplot       as       plt       plt     .     plot     (     np     .     random     .     randn     (     100     ) )       plt     .     title     (     u     &# 39 ; 测试 雅黑 字体 &# 39 ;     )                 也 可以 通过 设置   matplotlibrc   文件 ， 编辑 该 文件 ， 编辑 下述 两项 ， 去掉 注释 并 将 值 设为 下述 值             font . family                   :   sans - serif     font . sans - serif             :   Microsoft   YaHei               可以 通过 下述 代码 查看 系统 支持 的 字体 ， MAC 默认 中文字体 是     STHeiti                 fm       =       matplotlib     .     font _ manager     .     FontManager     ( )       for       f       in       fm     .     ttflist     :               print       f     .     name     .     decode     (     &# 39 ; utf - 8 &# 39 ;     )                 注意 ， 要 使得 配置 生效 ， 还 需要 删除 字体 缓存 ！ MAC   中 字体 缓存 在     ~ / . matplotlib / fontList . cache     ， 删除 该 文件 即可 ！  ", "tags": "python", "url": "/wiki/python/matplotlib-chinese-font.html"},
      
      
      {"title": "Numba 加速", "text": "    Table   of   Contents               Numba 是 一个 用于 python 计算 加速 的 软件包 ， 通过 增加 Annotation 注释 的 方式 ， 不 需要 修改 原始 代码 ， 使用方便 。     下面 通过 一个 例子 展示 使用 方法 。               import       numpy       as       np       from       numba       import       jit         def       sum2d     (     arr     ) :               M     ,       N       =       arr     .     shape               result       =       0.0               for       i       in       range     (     M     ) :                       for       j       in       range     (     N     ) :                               result       + =       arr     [     i     ,     j     ]               return       result         @ jit       def       sum2d _ jit     (     arr     ) :               M     ,       N       =       arr     .     shape               result       =       0.0               for       i       in       range     (     M     ) :                       for       j       in       range     (     N     ) :                               result       + =       arr     [     i     ,     j     ]               return       result         arr       =       np     .     random     .     randn     (     10000     ,     10000     )                         %     time         sum2d     (     arr     )                       CPU   times :   user   20.4   s ,   sys :   34.1   ms ,   total :   20.4   s   Wall   time :   20.4   s                       %     time       sum2d _ jit     (     arr     )                       CPU   times :   user   122   ms ,   sys :   513   µ s ,   total :   122   ms   Wall   time :   123   ms                       %     time       np     .     sum     (     arr     )                       CPU   times :   user   77   ms ,   sys :   1.12   ms ,   total :   78.1   ms   Wall   time :   77.2   ms               通过 numba 优化 的 代码 可以 比 原始 python 代码 快 将近 两个 数量级 ， 和 numpy 的 性能 接近 ！  ", "tags": "python", "url": "/wiki/python/numba.html"},
      
      
      {"title": "Pandas 中的坑", "text": "    Table   of   Contents           Series   操作           DataFrame   的 index 操作           遍历 操作           函数 式 编程 方式 ， apply 方式 （ 个人 比较 推荐 这种 方式 ）           iteration   迭代           向 量化 string 操作                   DataFrame 的 修改           DataFrame   API           category   类型 数据处理           IO 操作           read _ csv                   可视化 plot           TIPS                 Series   操作           他 像 一个 数组 ， 你 可以 像 数组 那样 索引 ， 他 也 想 一个 字典 ， 你 可以 像 字典 那样 索引         . map ( )   ， 参数 可以 是 一个 函数 ， 也 可以 是 一个 字典 （ 或 Series ） ， 此时 就是 一个 转换 表 。           DataFrame   的   index   操作       索引 操作 多而杂 ， 现 总结 如下 。     参考     Indexing   and   Selecting   Data   。             . loc   基于 label ( 如果 只有 一个 索引 则 为行 的 index ， 行 优先 ) 的 索引 。       单个 标签 ， 如   a   ， 多个 标签 列表   [ ' a ' ,   ' b ' ,   ' c ' ]   。 注意 ， 如果 提供 两个 索引 ， 即 行 索引 加列 索引 ， 在 python 中 处理 为 一个 tuple 。       slice   object   with   label ，   ' a '   :   ' f '   .   这种 形式 的 索引 叫做   slice   object   。 例如     df . loc [ ' a ' : ' d ' ]         一个 boolean 数组 .     df . loc [ df . A   & gt ;   0.5 ]         一个 返回 上述 索引 的 单 参数 （ 该 参数 是   df   本身 ） 函数 .     df . loc [ lambda   x :   x . A   & gt ;   0.5   ]                             df1       =       pd     .     DataFrame     (     np     .     random     .     randn     (     6     ,       4     ) ,                                               index     =     list     (     &# 39 ; abcdef &# 39 ;     ) ,                                               columns     =     list     (     &# 39 ; ABCD &# 39 ;     ) )                 A           B           C           D       a           0.333368             0.953575             0.189191             0.186499       b           0.344776             0.940556             0.624198             0.278640       c           0.269827             0.449311             0.679678             0.769818       d           0.910729             0.024516             0.745065             0.399805       e           0.868005             0.822731             0.908870             0.376258       f           0.141232             0.983130             0.730339             0.782900         df     .     loc     [     &# 39 ; a &# 39 ;     ]       df     .     loc     [     &# 39 ; a &# 39 ;     ,       &# 39 ; B &# 39 ;     ]       df     .     loc     [ [     &# 39 ; a &# 39 ;     ,     &# 39 ; b &# 39 ;     ] ]       df     .     loc     [ [     &# 39 ; a &# 39 ;     ,     &# 39 ; b &# 39 ;     ] ,       [     &# 39 ; B &# 39 ;     ,     &# 39 ; C &# 39 ;     ] ]                           . iloc     则 是 基于 序号 的 索引 ( 还是 行 优先 ) ， 从 0 到   length   -   1   。           一个 整数 ，   df . iloc [ 5 ]   ,     df . iloc [ 5 , 3 ]         整数 列表 ，   df . iloc [ [ 2 , 5 , 3 ] ]   ,     df . iloc [ [ 2 , 5 , 3 ] ,   [ 3 , 2 ] ]         slice   object   with   int ，     df . iloc [ 1 : 3 ]   ,     df . iloc [ 1 : 3 ,   : 2 ]         一个 boolean 数组 ，     df . iloc [ list ( df . A & gt ; 0.5 ) ]   ， 注意 这里 如果 写成   df . iloc [ df . A   & gt ;   0.5 ]   是 有 问题 的 。 不 清楚 为啥 ， 可能 与   df . A   & gt ;   0.5   是 一个   Series   有关 ， 而   Series   在 迭代 的 时候 更 像   dict   。       一个 返回 上述 索引 的 单 参数 函数 。 与   . loc   类似 。                     . ix     则 相当于 上述 两个 之 和 ， 两种 index 都 能 处理 。             [ ]   或者   __ getitem __ ( )   和 上面 的 行 优先 相反 ， 他 是 列 优先 ， 而且 不能 实现 多维 索引 。 此外 ， 还 可以 传入 slice   object （ label 和 int 都 可以 ） 和   boolean 数组 来 筛选 特定 的 行       列名 label ，   df [ ' A ' ]         列名 列表 ，   df [ [ ' A ' , ' B ' ] ]         slice   object ，   df [ : 3 ]   ，   df [ ' a ' : ' c ' ]         boolean   array ，   df [ df . A & gt ; 0.5 ]                   . head   和   . tail   函数 访问 前 （ 后 ） 几行       以 属性 方式 访问 ， 对 DataFrame 是 列名 ， 对 Series 是 键 。 例如   df . A   ,     df . A . a           . at   ,   . iat   和   . get _ value   ， 获取 单个 值 ， 注意 两者 的 区别 。   df . at [ ' a ' , ' A ' ] ,     df . get _ value ( ' a ' , ' A ' )   。 他们 等价 于   df . loc [ ' a ' , ' A ' ]   。 这个 比 采用   [ ]   速度 要 快 ， 遍历 的 时候 推荐 用 这个 。           遍历 操作       函数 式 编程 方式 ， apply 方式 （ 个人 比较 推荐 这种 方式 ）           tablewise   函数     . pipe ( )   ，   . pipe ( f ,   args   ... )   将 DataFrame 作为 函数   f   的 第一个 参数 传 过去 ， 其他 参数 也 原样 传递 。                   def       f     (     x     ,       a     ) :               return       np     .     log     (     x     )       /       np     .     log     (     a     )       df     .     pipe     (     f     ,       3     )         #   等价 于   f ( df ,   3 )                     一行 一列 方式 应用 函数     . apply ( )   ， 用 参数   axis   指定 行 ( 取值 0 ) 或列 ( 取值 1 )                   df     .     apply     (     np     .     mean     )           #   对 每 一列 求 均值       df     .     apply     (     np     .     mean     ,       axis     =     1     )         #   对 每 一行 求 均值                     elementwise     . applymap ( )   ， 求 每 一个 元素 的 ID ，   df . applymap ( id )             iteration   迭代                 for   i   in   obj     方式 ， 对 不同 数据结构 不同             Series     :   代表 值         DataFrame     :   代表 列 label ， 即 列名         Panel     :   item   label                     . iteriems ( )   ， 对 DataFrame 相当于 对列 迭代 。             Series   :   ( index ,   value )         DataFrame     :   ( column ,   Series )         Panel     :   ( item ,   DataFrame )                     df . iterrow ( )   ， 对 DataFrame 的 每 一行 进行 迭代 ， 返回 一个 Tuple     ( index ,   Series )               df . itertuples ( )   ， 也 是 一行 一行 地 迭代 ， 返回 的 是 一个 namedtuple ， 通常 比   iterrow   快 ， 因为 不 需要 做 转换                   for       idx     ,       row       in       df     .     iterrows     ( ) :               print       idx     ,       row         for       row       in       df     .     itertuples     ( ) :               print       row         for       c     ,       col       in       df     .     iteritems     ( ) :               print       c     ,       col                       . dt   ，   对 Datetime 类型 的 Series 可以 通过 这个 对象 访问   hour ,   second ,   day   等值 。         还 可以 通过   strftime   格式化 时间                   s       =       pd     .     Series     (     pd     .     date _ range     (     &# 39 ; 20130101   09 : 10 : 12 &# 39 ;     ,       periods     =     4     ) )       s     .     dt     .     hour                     迭代 方式 通常 性能 较差 ， 可以 通过 几种 方式 进行 优化 。       将 运算 编程 向 量化 操作       采用   . apply   等 函数 式 编程 方式       将 操作 的 内 循环 等 耗费 时间 的 操作 用 cython 编写                   向 量化 string 操作       通过 访问 Series 的   . str   属性 ， 例如   s . str . lower ( )   可以 将 每 一个 元素 变成 小写 。 可以 在   . str   属性 上 使用     所有 的 字符串 函数 ， 就 相当于 每 一个 元素 使用 那样 。       DataFrame 的 修改           删除 列 或行 操作   DataFrame . drop ( labels ,   axis = 0 ,   level = None ,   inplace = False ,   errors = ' raise ' )   ， 举例                   ##   删除 一列 ， 返回 删除 后 的 DataFrame ， 对原 DataFrame 没有 影响       df     .     drop     (     &# 39 ; colname &# 39 ;     ,       axis     =     1     )         ##   在 原 DataFrame 上 删除 一列       df     .     drop     (     &# 39 ; colname &# 39 ;     ,       axis     =     1     ,       inplace     =     True     )                 DataFrame   API       category   类型 数据处理           转换 为 category ,     Series . astype ( ' category ' )           Series . cat     是 category 类型 的 cat 对象 ， 可以 通过     cat . rename _ categories ( [ obj ] )   重命名 类型 ， 也 可以 直接         赋值   cat . categories   改变 他们 的 名字 。           IO 操作       read _ csv       最 常用 的 文本文件 读取 函数 。 一些 重要 的 参数 如下 ：                     -     sep     字 段 分隔符 ， 默认 为   ,   ， 有些 文件 用   \ \ t       -     header     头部 用来 做 index 的 行 序号 ， None 表示 没有     -     names     用来 指定 行 的 label ， 一个 list     -     index _ col     是否 使用 index 列 ， 默认 false     -     nrows     读取 的 行数     -     true _ values ,   false _ values     真假 值 字符串 替换     -     na _ values     NAN 值 列表 ， 默认 作为 NAN 的 值 为   ' - 1 .# IND ' ,   ' 1 .# QNAN ' ,   ' 1 .# IND ' ,   ' - 1 .# QNAN ' ,   ' # N / A   N / A ' ,   ' # N / A ' ,   ' N / A ' ,   ' NA ' ,   ' # NA ' ,   ' NULL ' ,   ' NaN ' ,   ' - NaN ' ,   ' nan ' ,   ' - nan ' ,   ' '       -     iterator     默认 false ， 指定 为 true 用来 迭代 地读 大 文件 ， 可以 用   chunksize   指定 每次 读取 的 行数 。       可视化 plot       TIPS             df . loc [ ' a ' , ' A ' ]   =   5   是 有效 的 ， 但是   df . loc [ ' a ' ] [ ' A ' ]   =   5   对   df   是 没有 影响 的 。      ", "tags": "python", "url": "/wiki/python/pandas.html"},
      
      
      {"title": "Python Subprocess", "text": "    Table   of   Contents           基本 函数                 处理 子 进程 ,   可以 替代     os . system   , 并 提供 更 丰富 的 操作 。       参考 :               https : / / docs . python . org / 2 / library / subprocess . html           https : / / segmentfault . com / a / 1190000009176351             基本 函数             subprocess . call ( args ,   * ,   stdin = None ,   stdout = None ,   stderr = None ,   shell = False )               class   subprocess . Popen ( args ,   bufsize = 0 ,   executable = None ,   stdin = None ,   stdout = None ,   stderr = None ,   preexec _ fn = None ,   close _ fds = False ,   shell = False ,   cwd = None ,   env = None ,   universal _ newlines = False ,   startupinfo = None ,   creationflags = 0 )                 执行 基本 的 shell 命令                       subprocess     .     call     (     &# 39 ; ls   - l &# 39 ;     ,       shell     =     True     )       subprocess     .     call     ( [     &# 39 ; ls &# 39 ;     ,       &# 39 ; - l &# 39 ;     ] )       subprocess     .     Popen     (     &# 39 ; ls   - l &# 39 ;     ,       shell     =     True     )       subprocess     .     Popen     ( [     &# 39 ; ls &# 39 ;     ,       &# 39 ; - l &# 39 ;     ] )                     和子 进程 交互                   #   读取 子 进程 的 stdout       p       =       subprocess     .     Popen     (     &# 39 ; ls   - l &# 39 ;     ,       shell     =     True     ,       stdout     =     subprocess     .     PIPE     )       print       p     .     stdout     .     read     ( )         #   发送数据 给子 进程 的 stdin       p       =       subprocess     .     Popen     (     &# 39 ; cat &# 39 ;     ,       shell     =     True     ,       stdout     =     subprocess     .     PIPE     ,       stdin     =     subprocess     .     PIPE     )       stdout     ,       stderr       =       p     .     communicate     (     &# 39 ; data1     \ \ n     data2     \ \ n     data3 &# 39 ;     )                     返回值                   from       subprocess       import       *         output       =       check _ output     ( [     & quot ; mycmd & quot ;     ,       & quot ; myarg & quot ;     ] )       #   output = ` mycmd   myarg `        ", "tags": "python", "url": "/wiki/python/subprocess.html"},
      
      
      {"title": "Python 多线程和多进程编程总结", "text": "    Table   of   Contents           简介           线程 与 进程           多线程 编程           线程 的 状态           线程 的 类型           python 的 GIL           创建 线程           线程 合并 （ join 方法 ）           线程 同步 与 互斥 锁           可 重入 锁           条件 变量           队列           线程 通信           后台 线程                   进程           类 Process           不加 daemon 属性           加上 daemon 属性           设置 daemon 执行 完 结束 的 方法                   Lock           Semaphore           Event           Queue           Pipe                   Pool                   资料 来源                 简介       早已 进入 多核 时代 的 计算机 ， 怎能不 用 多线程 和 多 进程 进行 加速 。     我 在 使用 python 的 过程 中 ， 用到 过 几次 多线程 和 多 进程 加速 ， 觉得     充分利用 CPU 节省时间 是 一种 很 有 “ 延长 生命 ” 的 感觉 。 现将 网络 上 看到 的 python 的     多线程 和 多 进程 编程 常用 的 知识点 汇总 在 这里 。       线程 与 进程       线程 与 进程 是 操作系统 里面 的 术语 ， 简单 来讲 ， 每 一个 应用程序 都 有 一个 自己 的 进程 。     操作系统 会为 这些 进程 分配 一些 执行 资源 ， 例如 内存空间 等 。     在 进程 中 ， 又 可以 创建 一些 线程 ， 他们 共享 这些 内存空间 ， 并 由 操作系统 调用 ，     以便 并行计算 。       32 位 系统 受限于 总线 宽度 ， 单个 进程 最 多 能够 访问 的 地址 空间     只有 4G ， 利用   物理地址 扩展 ( PAE )       技术 ， 可以 让 CPU 访问 超过 4G 内存 。 但是 在 单个 进程 还是 只能 访问 4G     空间 ， PAE 的 优势 是 可以 让 不同 进程 累计 使用 的 内存 超过 4G 。     在 个人电脑 上 ， 还是 建议 使用 64 位 系统 ， 便于 使用 大 内存     提升 程序 的 运行 性能 。       多线程 编程       线程 的 状态       创建 线程 之后 ， 线程 并 不是 始终保持 一个 状态 。 其 状态 大概 如下 ：           New   创建 。       Runnable   就绪 。 等待 调度       Running   运行 。       Blocked   阻塞 。 阻塞 可能 在   Wait   Locked   Sleeping       Dead   消亡           线程 的 类型       线程 有着 不同 的 状态 ， 也 有 不同 的 类型 。 大致 可 分为 ：           主线 程       子 线程       守护 线程 （ 后台 线程 ）       前台 线程           python 的 GIL       GIL 即 全局 解释器 锁 ， 它 使得 python 的 多线程 无法 充分利用     多核 的 优势 ， 但是 对于 I / O 操作 频繁 的 爬虫 之类 的 程序 ，     利用 多线程 带来 的 优势 还是 很 明显 的 。     如果 要 利用 多 核优势 ， 还是 用 多 进程 吧 。       创建 线程       Python 提供 两个 模块 进行 多线程 的 操作 ， 分别 是   thread   和   threading   ，     前者 是 比较 低级 的 模块 ， 用于 更 底层 的 操作 ， 一般 应用 级别 的 开发 不 常用 。       第一种 方法 是 创建   threading . Thread   的 子类 ， 重写   run   方法 。               import       time       import       threading         class       MyThread     (     threading     .     Thread     ) :               def       run     (     self     ) :                       for       i       in       range     (     5     ) :                               print       &# 39 ; thread   { } ,   @ number :   { } &# 39 ;     .     format     (     self     .     name     ,       i     )                               time     .     sleep     (     1     )         def       main     ( ) :               print       & quot ; Start   main   threading & quot ;               #   创建 三个 线程               threads       =       [     MyThread     ( )       for       i       in       range     (     3     ) ]               #   启动 三个 线程               for       t       in       threads     :                       t     .     start     ( )                 print       & quot ; End   Main   threading & quot ;           if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               main     ( )                 输入 如下 ： （ 不同 的 环境 不 一样 ）             Start   main   threading   thread   Thread - 1 ,   @ number :   0   thread   Thread - 2 ,   @ number :   0   thread   Thread - 3 ,   @ number :   0   End   Main   threading   thread   Thread - 1 ,   @ number :   1   thread   Thread - 3 ,   @ number :   1   thread   Thread - 2 ,   @ number :   1   thread   Thread - 3 ,   @ number :   2   thread   Thread - 1 ,   @ number :   2     thread   Thread - 2 ,   @ number :   2   thread   Thread - 2 ,   @ number :   3   thread   Thread - 1 ,   @ number :   3   thread   Thread - 3 ,   @ number :   3               线程 合并 （ join 方法 ）       主线 程 结束 后 ， 子 线程 还 在 运行 ，   join   方法 使得 主线 程 等到 子 线程 结束     时才 退出 。               def       main     ( ) :               print       & quot ; Start   main   threading & quot ;                 threads       =       [     MyThread     ( )       for       i       in       range     (     3     ) ]                 for       t       in       threads     :                       t     .     start     ( )                 #   一次 让 新创建 的 线程 执行   join               for       t       in       threads     :                       t     .     join     ( )                 print       & quot ; End   Main   threading & quot ;                 线程 同步 与 互斥 锁       为了 避免 线程 不 同步 造成 是 数据 不 同步 ， 可以 对 资源 进行 加锁 。     也 就是 访问 资源 的 线程 需要 获得 锁 ， 才能 访问 。       threading   模块 正好 提供 了 一个 Lock 功能               mutex       =       threading     .     Lock     ( )                 在线 程中 获取 锁               mutex     .     acquire     ( )                 使用 完后 ， 释放 锁               mutex     .     release     ( )                 可 重入 锁       为了 支持 在 同一 线程 中 多次 请求 同一 资源 ，     python 提供 了 可 重入 锁 （ RLock ） 。     RLock 内部 维护 着 一个   Lock   和 一个   counter   变量 ，       counter   记录 了 acquire 的 次数 ， 从而 使得 资源 可以 被 多次 require 。     直到 一个 线程 所有 的 acquire 都 被 release ， 其他 的 线程 才能 获得 资源 。       创建 RLock               mutex       =       threading     .     RLock     ( )                 线程 内 多次 进入 锁 和 释放 锁               class       MyThread     (     threading     .     Thread     ) :                 def       run     (     self     ) :                       if       mutex     .     acquire     (     1     ) :                               print       & quot ; thread   { }   get   mutex & quot ;     .     format     (     self     .     name     )                               time     .     sleep     (     1     )                               mutex     .     acquire     ( )                               mutex     .     release     ( )                               mutex     .     release     ( )                 条件 变量       实用 锁 可以 达到 线程 同步 ， 前面 的 互斥 锁 就是 这种 机制 。 更 复杂 的 环境 ， 需要 针对 锁 进行 一些 条件 判断 。 Python 提供 了 Condition 对象 。 它 除了 具有 acquire 和 release 方法 之外 ， 还 提供 了 wait 和 notify 方法 。 线程 首先 acquire 一个 条件 变量 锁 。 如果 条件 不足 ， 则 该 线程 wait ， 如果 满足 就 执行 线程 ， 甚至 可以 notify 其他 线程 。 其他 处于 wait 状态 的 线程 接到 通知 后 会 重新 判断 条件 。       条件 变量 可以 看成 不同 的 线程 先后 acquire 获得 锁 ， 如果 不 满足条件 ， 可以 理解 为 被 扔 到 一个 （ Lock 或 RLock ） 的 waiting 池 。 直达 其他 线程 notify 之后 再 重新 判断 条件 。 该 模式 常用 于 生成 消费者 模式 ：               queue       =       [ ]         con       =       threading     .     Condition     ( )         class       Producer     (     threading     .     Thread     ) :               def       run     (     self     ) :                       while       True     :                               if       con     .     acquire     ( ) :                                       if       len     (     queue     )       & gt ;       100     :                                               con     .     wait     ( )                                       else     :                                               elem       =       random     .     randrange     (     100     )                                               queue     .     append     (     elem     )                                               print       & quot ; Producer   a   elem   { } ,   Now   size   is   { } & quot ;     .     format     (     elem     ,       len     (     queue     ) )                                               time     .     sleep     (     random     .     random     ( ) )                                               con     .     notify     ( )                                       con     .     release     ( )         class       Consumer     (     threading     .     Thread     ) :               def       run     (     self     ) :                       while       True     :                               if       con     .     acquire     ( ) :                                       if       len     (     queue     )       & lt ;       0     :                                               con     .     wait     ( )                                       else     :                                               elem       =       queue     .     pop     ( )                                               print       & quot ; Consumer   a   elem   { } .   Now   size   is   { } & quot ;     .     format     (     elem     ,       len     (     queue     ) )                                               time     .     sleep     (     random     .     random     ( ) )                                               con     .     notify     ( )                                       con     .     release     ( )         def       main     ( ) :               for       i       in       range     (     3     ) :                       Producer     ( )     .     start     ( )                 for       i       in       range     (     2     ) :                       Consumer     ( )     .     start     ( )                 队列       带锁 的 队列   Queue   。     创建 10 个 元素 的 队列               queue       =       Queue     .     Queue     (     10     )                 队列 通过   put   加入 元素 ， 通过   get   方法 获取 元素 。       线程 通信       线程 可以 读取 共享 的 内存 ， 通过 内存 做 一些 数据处理 。     这 就是 线程 通信 的 一种 ， python 还 提供 了 更加 高级 的 线程 通信接口 。     Event 对象 可以 用来 进行 线程 通信 ， 调用 event 对象 的 wait 方法 ，     线程 则 会 阻塞 等待 ， 直到 别的 线程 set 之后 ， 才 会 被 唤醒 。               class       MyThread     (     threading     .     Thread     ) :               def       __ init __     (     self     ,       event     ) :                       super     (     MyThread     ,       self     )     .     __ init __     ( )                       self     .     event       =       event                 def       run     (     self     ) :                       print       & quot ; thread   { }   is   ready   & quot ;     .     format     (     self     .     name     )                       self     .     event     .     wait     ( )                       print       & quot ; thread   { }   run & quot ;     .     format     (     self     .     name     )         signal       =       threading     .     Event     ( )         def       main     ( ) :               start       =       time     .     time     ( )               for       i       in       range     (     3     ) :                       t       =       MyThread     (     signal     )                       t     .     start     ( )               time     .     sleep     (     3     )               print       & quot ; after   { } s & quot ;     .     format     (     time     .     time     ( )       -       start     )               signal     .     set     ( )                 后台 线程       默认 情况 下 ， 主线 程 退出 之后 ， 即使 子 线程 没有 join 。 那么 主线 程 结束 后 ，     子 线程 也 依然 会 继续执行 。 如果 希望 主线 程 退出 后 ，     其子 线程 也 退出 而 不再 执行 ， 则 需要 设置 子 线程 为 后台 线程 。 python 提供 了   setDeamon   方法 。       进程       python 中 的 多线程 其实 并 不是 真正 的 多线程 ， 如果 想要 充分 地 使用 多核 CPU 的 资源 ， 在 python 中 大部分 情况 需要 使用 多 进程 。 Python 提供 了 非常 好用 的 多 进程 包 multiprocessing ， 只 需要 定义 一个 函数 ， Python 会 完成 其他 所有 事情 。 借助 这个 包 ， 可以 轻松 完成 从单 进程 到 并发 执行 的 转换 。 multiprocessing 支持 子 进程 、 通信 和 共享 数据 、 执行 不同 形式 的 同步 ， 提供 了 Process 、 Queue 、 Pipe 、 Lock 等 组件 。       类 Process               创建 进程 的 类 ：   Process ( [ group   [ ,   target   [ ,   name   [ ,   args   [ ,   kwargs ] ] ] ] ] )   ，     target 表示 调用 对象 ， args 表示 调用 对象 的 位置 参数 元组 。     kwargs 表示 调用 对象 的 字典 。 name 为 别名 。 group 实质 上 不 使用 。               方法 ： is _ alive ( ) 、 join ( [ timeout ] ) 、 run ( ) 、 start ( ) 、 terminate ( ) 。 其中 ， Process 以 start ( ) 启动 某个 进程 。               属性 ： authkey 、 daemon （ 要 通过 start ( ) 设置 ） 、 exitcode ( 进程 在 运行 时为 None 、 如果 为 – N ， 表示 被 信号 N 结束 ） 、 name 、 pid 。 其中 daemon 是 父 进程 终止 后 自动 终止 ， 且 自己 不能 产生 新 进程 ， 必须 在 start ( ) 之前 设置 。               例 ： 创建 函数 并 将 其 作为 单个 进程               import       multiprocessing       import       time         def       worker     (     interval     ) :               n       =       5               while       n       & gt ;       0     :                       print     (     & quot ; The   time   is   { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) )                       time     .     sleep     (     interval     )                       n       - =       1         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     start     ( )               print       & quot ; p . pid : & quot ;     ,       p     .     pid               print       & quot ; p . name : & quot ;     ,       p     .     name               print       & quot ; p . is _ alive : & quot ;     ,       p     .     is _ alive     ( )                 结果             p . pid :   8736   p . name :   Process - 1   p . is _ alive :   True   The   time   is   Tue   Apr   21   20 : 55 : 12   2015   The   time   is   Tue   Apr   21   20 : 55 : 15   2015   The   time   is   Tue   Apr   21   20 : 55 : 18   2015   The   time   is   Tue   Apr   21   20 : 55 : 21   2015   The   time   is   Tue   Apr   21   20 : 55 : 24   2015               例 ： 创建 函数 并 将 其 作为 多个 进程               import       multiprocessing       import       time         def       worker _ 1     (     interval     ) :               print       & quot ; worker _ 1 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 1 & quot ;         def       worker _ 2     (     interval     ) :               print       & quot ; worker _ 2 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 2 & quot ;         def       worker _ 3     (     interval     ) :               print       & quot ; worker _ 3 & quot ;               time     .     sleep     (     interval     )               print       & quot ; end   worker _ 3 & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p1       =       multiprocessing     .     Process     (     target       =       worker _ 1     ,       args       =       (     2     , ) )               p2       =       multiprocessing     .     Process     (     target       =       worker _ 2     ,       args       =       (     3     , ) )               p3       =       multiprocessing     .     Process     (     target       =       worker _ 3     ,       args       =       (     4     , ) )                 p1     .     start     ( )               p2     .     start     ( )               p3     .     start     ( )                 print     (     & quot ; The   number   of   CPU   is : & quot ;       +       str     (     multiprocessing     .     cpu _ count     ( ) ) )               for       p       in       multiprocessing     .     active _ children     ( ) :                       print     (     & quot ; child       p . name : & quot ;       +       p     .     name       +       & quot ;     \ \ t     p . id & quot ;       +       str     (     p     .     pid     ) )               print       & quot ; END ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! & quot ;                 结果             The   number   of   CPU   is : 4   child       p . name : Process - 3         p . id7992   child       p . name : Process - 2         p . id4204   child       p . name : Process - 1         p . id6380   END ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !   worker _ 1   worker _ 3   worker _ 2   end   worker _ 1   end   worker _ 2   end   worker _ 3               例 ： 将 进程 定义 为类               import       multiprocessing       import       time         class       ClockProcess     (     multiprocessing     .     Process     ) :               def       __ init __     (     self     ,       interval     ) :                       multiprocessing     .     Process     .     __ init __     (     self     )                       self     .     interval       =       interval                 def       run     (     self     ) :                       n       =       5                       while       n       & gt ;       0     :                               print     (     & quot ; the   time   is   { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) )                               time     .     sleep     (     self     .     interval     )                               n       - =       1         if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               p       =       ClockProcess     (     3     )               p     .     start     ( )                 注 ： 进程 p 调用 start ( ) 时 ， 自动 调用 run ( )       结果             the   time   is   Tue   Apr   21   20 : 31 : 30   2015   the   time   is   Tue   Apr   21   20 : 31 : 33   2015   the   time   is   Tue   Apr   21   20 : 31 : 36   2015   the   time   is   Tue   Apr   21   20 : 31 : 39   2015   the   time   is   Tue   Apr   21   20 : 31 : 42   2015               例 ： daemon 程序 对比 结果       不加 daemon 属性               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     start     ( )               print       & quot ; end ! & quot ;                 结果             end !   work   start : Tue   Apr   21   21 : 29 : 10   2015   work   end : Tue   Apr   21   21 : 29 : 13   2015               加上 daemon 属性               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     daemon       =       True               p     .     start     ( )               print       & quot ; end ! & quot ;                 结果             end !               注 ： 因子 进程 设置 了 daemon 属性 ， 主 进程 结束 ， 它们 就 随着 结束 了 。       设置 daemon 执行 完 结束 的 方法               import       multiprocessing       import       time         def       worker     (     interval     ) :               print     (     & quot ; work   start : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;               time     .     sleep     (     interval     )               print     (     & quot ; work   end : { 0 } & quot ;     .     format     (     time     .     ctime     ( ) ) ) ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               p       =       multiprocessing     .     Process     (     target       =       worker     ,       args       =       (     3     , ) )               p     .     daemon       =       True               p     .     start     ( )               p     .     join     ( )               print       & quot ; end ! & quot ;                 结果             work   start : Tue   Apr   21   22 : 16 : 32   2015   work   end : Tue   Apr   21   22 : 16 : 35   2015   end !               Lock       当 多个 进程 需要 访问共享 资源 的 时候 ， Lock 可以 用来 避免 访问 的 冲突 。               import       multiprocessing       import       sys         def       worker _ with     (     lock     ,       f     ) :               with       lock     :                       fs       =       open     (     f     ,       &# 39 ; a +&# 39 ;     )                       n       =       10                       while       n       & gt ;       1     :                               fs     .     write     (     & quot ; Lockd   acquired   via   with     \ \ n     & quot ;     )                               n       - =       1                       fs     .     close     ( )         def       worker _ no _ with     (     lock     ,       f     ) :               lock     .     acquire     ( )               try     :                       fs       =       open     (     f     ,       &# 39 ; a +&# 39 ;     )                       n       =       10                       while       n       & gt ;       1     :                               fs     .     write     (     & quot ; Lock   acquired   directly     \ \ n     & quot ;     )                               n       - =       1                       fs     .     close     ( )               finally     :                       lock     .     release     ( )         if       __ name __       = =       & quot ; __ main __& quot ;     :               lock       =       multiprocessing     .     Lock     ( )               f       =       & quot ; file . txt & quot ;               w       =       multiprocessing     .     Process     (     target       =       worker _ with     ,       args     =     (     lock     ,       f     ) )               nw       =       multiprocessing     .     Process     (     target       =       worker _ no _ with     ,       args     =     (     lock     ,       f     ) )               w     .     start     ( )               nw     .     start     ( )               print       & quot ; end & quot ;                 结果 （ 输出 文件 ）             Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lockd   acquired   via   with   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly   Lock   acquired   directly               Semaphore       Semaphore 用来 控制 对 共享资源 的 访问 数量 ， 例如 池 的 最大 连接数 。               import       multiprocessing       import       time         def       worker     (     s     ,       i     ) :               s     .     acquire     ( )               print     (     multiprocessing     .     current _ process     ( )     .     name       +       & quot ; acquire & quot ;     ) ;               time     .     sleep     (     i     )               print     (     multiprocessing     .     current _ process     ( )     .     name       +       & quot ; release     \ \ n     & quot ;     ) ;               s     .     release     ( )         if       __ name __       = =       & quot ; __ main __& quot ;     :               s       =       multiprocessing     .     Semaphore     (     2     )               for       i       in       range     (     5     ) :                       p       =       multiprocessing     .     Process     (     target       =       worker     ,       args     =     (     s     ,       i     *     2     ) )                       p     .     start     ( )                 结果             Process - 1acquire   Process - 1release     Process - 2acquire   Process - 3acquire   Process - 2release     Process - 5acquire   Process - 3release     Process - 4acquire   Process - 5release     Process - 4release               Event       Event 用来 实现 进程 间 同步 通信 。               import       multiprocessing       import       time         def       wait _ for _ event     (     e     ) :               print     (     & quot ; wait _ for _ event :   starting & quot ;     )               e     .     wait     ( )               print     (     & quot ; wairt _ for _ event :   e . is _ set ( ) - & gt ; & quot ;       +       str     (     e     .     is _ set     ( ) ) )         def       wait _ for _ event _ timeout     (     e     ,       t     ) :               print     (     & quot ; wait _ for _ event _ timeout : starting & quot ;     )               e     .     wait     (     t     )               print     (     & quot ; wait _ for _ event _ timeout : e . is _ set - & gt ; & quot ;       +       str     (     e     .     is _ set     ( ) ) )         if       __ name __       = =       & quot ; __ main __& quot ;     :               e       =       multiprocessing     .     Event     ( )               w1       =       multiprocessing     .     Process     (     name       =       & quot ; block & quot ;     ,                               target       =       wait _ for _ event     ,                               args       =       (     e     , ) )                 w2       =       multiprocessing     .     Process     (     name       =       & quot ; non - block & quot ;     ,                               target       =       wait _ for _ event _ timeout     ,                               args       =       (     e     ,       2     ) )               w1     .     start     ( )               w2     .     start     ( )                 time     .     sleep     (     3     )                 e     .     set     ( )               print     (     & quot ; main :   event   is   set & quot ;     )                 结果               wait _ for _ event     :       starting       wait _ for _ event _ timeout     :     starting       wait _ for _ event _ timeout     :     e     .     is _ set     - & gt ;     False       main     :       event       is       set       wairt _ for _ event     :       e     .     is _ set     ( ) - & gt ;     True                 Queue       Queue 是 多 进程 安全 的 队列 ， 可以 使用 Queue 实现 多 进程 之间 的 数据 传递 。 put 方法 用以 插入 数据 到 队列 中 ， put 方法 还有 两个 可 选 参数 ： blocked 和 timeout 。 如果 blocked 为 True （ 默认值 ） ， 并且 timeout 为 正值 ， 该 方法 会 阻塞 timeout 指定 的 时间 ， 直到 该 队列 有 剩余 的 空间 。 如果 超时 ， 会 抛出 Queue . Full 异常 。 如果 blocked 为 False ， 但 该 Queue 已满 ， 会 立即 抛出 Queue . Full 异常 。       get 方法 可以 从 队列 读取 并且 删除 一个 元素 。 同样 ， get 方法 有 两个 可 选 参数 ： blocked 和 timeout 。 如果 blocked 为 True （ 默认值 ） ， 并且 timeout 为 正值 ， 那么 在 等待时间 内 没有 取 到 任何 元素 ， 会 抛出 Queue . Empty 异常 。 如果 blocked 为 False ， 有 两种 情况 存在 ， 如果 Queue 有 一个 值 可用 ， 则 立即 返回 该值 ， 否则 ， 如果 队 列为 空 ， 则 立即 抛出 Queue . Empty 异常 。 Queue 的 一段 示例 代码 ：                 import       multiprocessing         def       writer _ proc     (     q     ) :                           try     :                                         q     .     put     (     1     ,       block       =       False     )                 except     :                                         pass               def       reader _ proc     (     q     ) :                           try     :                                         print       q     .     get     (     block       =       False     )                 except     :                                         pass         if       __ name __       = =       & quot ; __ main __& quot ;     :               q       =       multiprocessing     .     Queue     ( )               writer       =       multiprocessing     .     Process     (     target     =     writer _ proc     ,       args     =     (     q     , ) )                   writer     .     start     ( )                       reader       =       multiprocessing     .     Process     (     target     =     reader _ proc     ,       args     =     (     q     , ) )                   reader     .     start     ( )                     reader     .     join     ( )                   writer     .     join     ( )                 结果             1               Pipe       Pipe 方法 返回 ( conn1 ,   conn2 ) 代表 一个 管道 的 两个 端 。 Pipe 方法 有 duplex 参数 ， 如果 duplex 参数 为 True ( 默认值 ) ， 那么 这个 管道 是 全双工 模式 ， 也就是说 conn1 和 conn2 均 可 收发 。 duplex 为 False ， conn1 只 负责 接受 消息 ， conn2 只 负责 发送 消息 。       send 和 recv 方法 分别 是 发送 和 接受 消息 的 方法 。 例如 ， 在 全双工 模式 下 ， 可以 调用 conn1 . send 发送 消息 ， conn1 . recv 接收 消息 。 如果 没有 消息 可 接收 ， recv 方法 会 一直 阻塞 。 如果 管道 已经 被 关闭 ， 那么 recv 方法 会 抛出 EOFError 。                 import       multiprocessing       import       time         def       proc1     (     pipe     ) :               while       True     :                       for       i       in       xrange     (     10000     ) :                               print       & quot ; send :       % s     & quot ;       %     (     i     )                               pipe     .     send     (     i     )                               time     .     sleep     (     1     )         def       proc2     (     pipe     ) :               while       True     :                       print       & quot ; proc2   rev : & quot ;     ,       pipe     .     recv     ( )                       time     .     sleep     (     1     )         def       proc3     (     pipe     ) :               while       True     :                       print       & quot ; PROC3   rev : & quot ;     ,       pipe     .     recv     ( )                       time     .     sleep     (     1     )         if       __ name __       = =       & quot ; __ main __& quot ;     :               pipe       =       multiprocessing     .     Pipe     ( )               p1       =       multiprocessing     .     Process     (     target     =     proc1     ,       args     =     (     pipe     [     0     ] , ) )               p2       =       multiprocessing     .     Process     (     target     =     proc2     ,       args     =     (     pipe     [     1     ] , ) )               # p3   =   multiprocessing . Process ( target = proc3 ,   args = ( pipe [ 1 ] , ) )                 p1     .     start     ( )               p2     .     start     ( )               # p3 . start ( )                 p1     .     join     ( )               p2     .     join     ( )               # p3 . join ( )                 结果                           Pool       在 利用 Python 进行 系统管理 的 时候 ， 特别 是 同时 操作 多个 文件目录 ， 或者 远程 控制 多台 主机 ， 并行操作 可以 节约 大量 的 时间 。 当 被 操作 对象 数目 不大时 ， 可以 直接 利用 multiprocessing 中 的 Process 动态 成生 多个 进程 ， 十几个 还好 ， 但 如果 是 上 百个 ， 上 千个 目标 ， 手动 的 去 限制 进程 数量 却 又 太过 繁琐 ， 此时 可以 发挥 进程 池 的 功效 。     Pool 可以 提供 指定 数量 的 进程 ， 供 用户 调用 ， 当有 新 的 请求 提交 到 pool 中时 ， 如果 池 还 没有 满 ， 那么 就 会 创建 一个 新 的 进程 用来 执行 该 请求 ； 但 如果 池中 的 进程 数 已经 达到 规定 最大值 ， 那么 该 请求 就 会 等待 ， 直到 池中 有 进程 结束 ， 才 会 创建 新 的 进程 来 它 。       例 ： 使用 进程 池               # coding :   utf - 8       import       multiprocessing       import       time         def       func     (     msg     ) :               print       & quot ; msg : & quot ;     ,       msg               time     .     sleep     (     3     )               print       & quot ; end & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               pool       =       multiprocessing     .     Pool     (     processes       =       3     )               for       i       in       xrange     (     4     ) :                       msg       =       & quot ; hello       % d     & quot ;       %     (     i     )                       pool     .     apply _ async     (     func     ,       (     msg     ,       ) )           # 维持 执行 的 进程 总数 为 processes ， 当 一个 进程 执行 完毕 后 会 添加 新 的 进程 进去                 print       & quot ; Mark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ & quot ;               pool     .     close     ( )               pool     .     join     ( )           # 调用 join 之前 ， 先 调用 close 函数 ， 否则 会 出错 。 执行 完 close 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 所有 子 进程 结束               print       & quot ; Sub - process ( es )   done .& quot ;                 一次 执行 结果               mMsg     :       hark     ~       Mark     ~       Mark     ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~     ello       0         msg     :       hello       1       msg     :       hello       2       end       msg     :       hello       3       end       end       end       Sub     -     process     (     es     )       done     .                 函数 解释 ：             apply _ async ( func [ ,   args [ ,   kwds [ ,   callback ] ] ] )   它 是非 阻塞 ， apply ( func [ ,   args [ ,   kwds ] ] ) 是 阻塞 的 （ 理解 区别 ， 看例 1 例 2 结果 区别 ）   close ( )         关闭 pool ， 使 其 不 在 接受 新 的 任务 。   terminate ( )         结束 工作 进程 ， 不 在 处理 未 完成 的 任务 。   join ( )         主 进程 阻塞 ， 等待 子 进程 的 退出 ，   join 方法 要 在 close 或 terminate 之后 使用 。               执行 说明 ： 创建 一个 进程 池 pool ， 并 设定 进程 的 数量 为 3 ， xrange ( 4 ) 会 相继 产生 四个 对象 [ 0 ,   1 ,   2 ,   4 ] ， 四个 对象 被 提交 到 pool 中 ， 因 pool 指定 进程 数为 3 ， 所以 0 、 1 、 2 会 直接 送到 进程 中 执行 ， 当 其中 一个 执行 完 事后 才 空出 一个 进程 处理 对象 3 ， 所以 会 出现 输出 “ msg :   hello   3 ” 出现 在 ” end ” 后 。 因为 为 非 阻塞 ， 主 函数 会 自己 执行 自个 的 ， 不 搭理 进程 的 执行 ， 所以 运行 完 for 循环 后 直接 输出 “ mMsg :   hark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ” ， 主程序 在 pool . join （ ） 处 等待 各个 进程 的 结束 。       例 ： 使用 进程 池 （ 阻塞 ）               # coding :   utf - 8       import       multiprocessing       import       time         def       func     (     msg     ) :               print       & quot ; msg : & quot ;     ,       msg               time     .     sleep     (     3     )               print       & quot ; end & quot ;         if       __ name __       = =       & quot ; __ main __& quot ;     :               pool       =       multiprocessing     .     Pool     (     processes       =       3     )               for       i       in       xrange     (     4     ) :                       msg       =       & quot ; hello       % d     & quot ;       %     (     i     )                       pool     .     apply     (     func     ,       (     msg     ,       ) )           # 维持 执行 的 进程 总数 为 processes ， 当 一个 进程 执行 完毕 后 会 添加 新 的 进程 进去                 print       & quot ; Mark ~   Mark ~   Mark ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ & quot ;               pool     .     close     ( )               pool     .     join     ( )           # 调用 join 之前 ， 先 调用 close 函数 ， 否则 会 出错 。 执行 完 close 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 所有 子 进程 结束               print       & quot ; Sub - process ( es )   done .& quot ;                 一次 执行 的 结果               msg     :       hello       0       end       msg     :       hello       1       end       msg     :       hello       2       end       msg     :       hello       3       end       Mark     ~       Mark     ~       Mark     ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~       Sub     -     process     (     es     )       done     .                 例 ： 使用 进程 池 ， 并 关注 结果               msg     :       hello       0       msg     :       hello       1       msg     :       hello       2       end       end       end       : : :       donehello       0       : : :       donehello       1       : : :       donehello       2       Sub     -     process     (     es     )       done     .                 例 ： 使用 多个 进程 池               # coding :   utf - 8       import       multiprocessing       import       os     ,       time     ,       random         def       Lee     ( ) :               print       & quot ;     \ \ n     Run   task   Lee -     % s     & quot ;       %     (     os     .     getpid     ( ) )       # os . getpid ( ) 获取 当前 的 进程 的 ID               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       10     )       # random . random ( ) 随机 生成 0 - 1 之间 的 小数               end       =       time     .     time     ( )               print       &# 39 ; Task   Lee ,   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Marlon     ( ) :               print       & quot ;     \ \ n     Run   task   Marlon -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       40     )               end     =     time     .     time     ( )               print       &# 39 ; Task   Marlon   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Allen     ( ) :               print       & quot ;     \ \ n     Run   task   Allen -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       30     )               end       =       time     .     time     ( )               print       &# 39 ; Task   Allen   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         def       Frank     ( ) :               print       & quot ;     \ \ n     Run   task   Frank -     % s     & quot ;       %     (     os     .     getpid     ( ) )               start       =       time     .     time     ( )               time     .     sleep     (     random     .     random     ( )       *       20     )               end       =       time     .     time     ( )               print       &# 39 ; Task   Frank   runs       % 0.2 f       seconds .&# 39 ;       %     (     end       -       start     )         if       __ name __     = =     &# 39 ; __ main __&# 39 ;     :               function _ list     =         [     Lee     ,       Marlon     ,       Allen     ,       Frank     ]                 print       & quot ; parent   process       % s     & quot ;       %     (     os     .     getpid     ( ) )                 pool     =     multiprocessing     .     Pool     (     4     )               for       func       in       function _ list     :                       pool     .     apply _ async     (     func     )               # Pool 执行 函数 ， apply 执行 函数 , 当有 一个 进程 执行 完毕 后 ， 会 添加 一个 新 的 进程 到 pool 中                 print       &# 39 ; Waiting   for   all   subprocesses   done ...&# 39 ;               pool     .     close     ( )               pool     .     join     ( )             # 调用 join 之前 ， 一定 要 先 调用 close ( )   函数 ， 否则 会 出错 ,   close ( ) 执行 后 不会 有 新 的 进程 加入 到 pool , join 函数 等待 素有 子 进程 结束               print       &# 39 ; All   subprocesses   done .&# 39 ;                 一次 执行 结果             parent   process   7704     Waiting   for   all   subprocesses   done ...   Run   task   Lee - 6948     Run   task   Marlon - 2896     Run   task   Allen - 7304     Run   task   Frank - 3052   Task   Lee ,   runs   1.59   seconds .   Task   Marlon   runs   8.48   seconds .   Task   Frank   runs   15.68   seconds .   Task   Allen   runs   18.08   seconds .   All   subprocesses   done .               资料 来源             http : / / www . cnblogs . com / kaituorensheng / p / 4445418 . html           http : / / python . jobbole . com / 85177 /        ", "tags": "python", "url": "/wiki/python/python-multiprocessing-tutorial.html"},
      
      
      {"title": "python 拾遗", "text": "    Table   of   Contents           基本 语法 相关           函数           类           装饰 器           上下文 管理器   with           迭代 器 相关 函数           各种 坑           工具 函数           logging                 基本 语法 相关             * args   ， 将 列表   args   展开 为 参数 列表 ，   * * kwargs   ， 将 字典   kwargs   展开 成 kv 形式 的 参数 列表 。           参考     https : / / stackoverflow . com / questions / 2921847 / what - does - the - star - operator - mean - in - python       和     https : / / docs . python . org / 3 / tutorial / controlflow . html # unpacking - argument - lists             整数 除法 ， 在 python2 . x 中   /   代表 整数 除法 （ 除非 左右 操作数 有 一个 是 浮点数 ） ， 而 在 python3 . x 中 ， 代表 浮点数 除法 ， python3 . x 用   / /   代表 整数 除法 。       反射 函数 ，   type ( )   ，   isinstance ( )   ，   hasattr ( )   ，   setattr ( )     以及 属性   __ class __         Mixin ： python 通过 多重 继承 实现 组合 模式 。 sklearn 中 很多 基础 类 使用 这种 模式 实现 的 。   https : / / github . com / scikit - learn / scikit - learn / blob / 51a765a / sklearn / base . py # L281             函数       参考 ：   https : / / stackoverflow . com / documentation / python / 228 / functions             任意 参数 ：   * args ,   * * kwargs                     def       func     (     *     args     ) :               #   args   will   be   a   tuple   containing   all   values   that   are   passed   in               for       i       in       args     :                       print     (     i     )                         字符串 函数 拾遗 ： 除了   upper ,   lower     之外 还有 ：   https : / / stackoverflow . com / documentation / python / 278 / string - methods               capitalize     将 第一个 变为 大写 ， 其他 的 变为 小写 ！         title     将 每个 单词 第一个 字母 变为 大写 ， 其他 的 为 小写 ！             casefold     @ since ( 3.3 )   小写 转换 的 unicode 字符 版 ！ 希腊字母 也 可以 转                 translate     按照 替换 表 进行 字母 替换             format     格式化         split ,   rsplit     当 指定     maxsplit     参数 的 时候 ， 有 区别       string   模块 常量 ， 要 使用   import   string   导入 ：   string . ascii _ letters     字母表 ,     string . ascii _ lowercase ,   string . ascii _ uppercase ,   string . digits ,   string . hexdigits ,   string . octaldigits ,   string . punctuation ,   string . whitespace ,   string . printable         unicodedata ,   一个 有趣 的 模块         str . count ( sub [ ,   start [ ,   end ] ] )     统计 出现 的 次数         str . replace ( old ,   new [ ,   count ] )     简单 替换 ，   re . sub   基于 正则 式 替换         str . isupper ( ) ,   str . islower ( )   and   str . istitle ( )           str . ljust   and   str . rjust     字符串 对齐         ' hello ' [ : : - 1 ]     字符串 反转         str . strip ( [ chars ] ) ,   str . rstrip ( [ chars ] )   and   str . lstrip ( [ chars ] )     去掉 空白 字符         \" foo \"   in   my _ str     字符串 包含 检查         str . startswith ( )   and   str . endswith ( )     字符串 首尾         encode ,   decode     字符串 和 unicode 字符串 之间 的 转换       共同 前缀 ！                           import       os       mylist       =       [     & quot ; & amp ; abcd & quot ;     ,       & quot ; & amp ; abbe & quot ;     ,       & quot ; & amp ; ab & quot ;     ]       print       (     os     .     path     .     commonprefix     (     mylist     ) )                 类       参考     https : / / stackoverflow . com / documentation / python / 419 / classes             Bound ,   unbound ,   and   static   methods           Bound   方法 就是 通过 类 的 实例 访问 的 方法 （ instancemethod ） ，   unbound   方法 就是 通过 类 本身 访问 的 动态 方法 （ Python3 . x   已 废弃 )               a       =       A     ( )       a     .     f       #   & lt ; bound   method   A . f   of   & lt ; __ main __. A   object   at   ...& gt ; & gt ;       a     .     f     (     2     )       #   4         #   Note :   the   bound   method   object   a . f   is   recreated   * every   time *   you   call   it :       a     .     f       is       a     .     f         #   False       #   As   a   performance   optimization   you   can   store   the   bound   method   in   the   object &# 39 ; s       #   __ dict __ ,   in   which   case   the   method   object   will   remain   fixed :       a     .     f       =       a     .     f       a     .     f       is       a     .     f         #   True                     @ classmethod   装饰 器 ， 第一个 参数 代表 类 ， 类 方法 ， 既 可以 通过 类 调用 ， 也 可以 通过 类 的 实例 调用       @ staticmethod   ， 只能 通过 类 调用           无 装饰 器 ，   只能 通过 实例 调用               类 的 继承 ： 从 3 . x 开始 ，   super     不再 需要 传入 参数 了 ！                       class       Square     (     Rectangle     ) :               def       __ init __     (     self     ,       s     ) :                       #   call   parent   constructor ,   w   and   h   are   both   s                       super     (     Rectangle     ,       self     )     .     __ init __     (     s     ,       s     )                         self     .     s       =       s                     抽象类 ：   abc   模块 ， 需要 制定   __ metaclass __   变量 为   ABCMeta   ， 子类 需要 注册 ！                   from       abc       import       ABCMeta         class       AbstractClass     (     object     ) :               #   the   metaclass   attribute   must   always   be   set   as   a   class   variable               __ metaclass __       =       ABCMeta               #   the   abstractmethod   decorator   registers   this   method   as   undefined             @ abstractmethod             def       virtual _ method _ subclasses _ must _ define     (     self     ) :                     #   Can   be   left   completely   blank ,   or   a   base   implementation   can   be   provided                     #   Note   that   ordinarily   a   blank   interpretation   implicitly   returns   ` None ` ,                     #   but   by   registering ,   this   behaviour   is   no   longer   enforced .         class       Subclass     :             def       virtual _ method _ subclasses _ must _ define     (     self     ) :                     return         #   registration   is   mandatory   to   truly   create   an   abstract   class       AbstractClass     .     register     (     SubClass     )                     多重 继承 ：   class   FooBar ( Foo ,   Bar )         私有 变量 ：   _ non _ public     通过 添加 一个 下划线 实现 ！ （ 哈哈 ！ ）       属性 方法 ： 可以 将 属性 设置 为 只读 ， 不 提供   setter   方法                   class       MyClass     :                 def       __ init __     (     self     ) :                     self     .     _ my _ string       =       & quot ; & quot ;                 @ property               def       my _ string     (     self     ) :                       return       self     .     _ my _ string                 @ my _ string . setter               def       my _ string     (     self     ,       new _ value     ) :                       self     .     _ my _ string       =       new _ value                 @ my _ string . deleter               def       x     (     self     ) :                       del       self     .     _ my _ string                 装饰 器       任何 传入 一个 函数 ， 返回 一个 函数 的 函数 都 可以 作为 装饰 器 ！ ( 貌似 无效 ！ py2 . 7 )               def       super _ secret _ function     (     f     ) :               return       f         @ super _ secret _ function       def       my _ function     ( ) :               print     (     & quot ; This   is   my   secret   function .& quot ;     )                 这里   @     相当于 实现     my _ function   =   super _ secret _ function ( my _ function )         装饰 器类 ：   __ call __     方法 进行 装饰 。 如果 需要 装饰 成员 方法 ， 需要 指定   __ get __   方法 ！     装饰 器 可以 使用 参数 ， 只要 在 函数 内部 定义 一个 类 ， 然后 返回 即可 。               class       Decorator     (     object     ) :               & quot ; & quot ; & quot ; Simple   decorator   class .& quot ; & quot ; & quot ;                 def       __ init __     (     self     ,       func     ) :                       self     .     func       =       func                 def       __ call __     (     self     ,       *     args     ,       * *     kwargs     ) :                       print     (     &# 39 ; Before   the   function   call .&# 39 ;     )                       res       =       self     .     func     (     *     args     ,       * *     kwargs     )                       print     (     &# 39 ; After   the   function   call .&# 39 ;     )                       return       res         @ Decorator       def       testfunc     ( ) :               print     (     &# 39 ; Inside   the   function .&# 39 ;     )         testfunc     ( )       #   Before   the   function   call .       #   Inside   the   function .       #   After   the   function   call .                 上下文 管理器     with             把 文件 当做 上下文 ， 上下文 结束 的 时候 会 自动 关闭 文件 ！       自己 的 上下文 管理器 需要 实现 两个 方法     __ enter __ ( )   and   __ exit __ ( )             迭代 器 相关 函数             enumerate ( iter )     返回   ( idx ,   value )   的 迭代 器 ， 可以 应用 在 需要 使用 index 的 时候 。         ( x   for   x   ...   )     圆括号 生成 迭代 器 的 一个 语法 糖 。           各种 坑           不要 用 可变 对象 作为 函数 默认值 。 字典 , 集合 , 列表 等等 对象 是 不 适合 作为 函数 默认值 的 .         因为 这个 默认值 是 在 函数 建立 的 时候 就 生成 了 ,   每次 调用 都 是 用 了 这个 对象 的 ” 缓存 ” .                   In       [     1     ] :       def       append _ to _ list     (     value     ,       def _ list     =     [ ] ) :             ...     :                       def _ list     .     append     (     value     )             ...     :                       return       def _ list             ...     :         In       [     2     ] :       my _ list       =       append _ to _ list     (     1     )         In       [     3     ] :       my _ list       Out     [     3     ] :       [     1     ]         In       [     4     ] :       my _ other _ list       =       append _ to _ list     (     2     )         In       [     5     ] :       my _ other _ list       Out     [     5     ] :       [     1     ,       2     ]       #   看到 了 吧 ， 其实 我们 本来 只想 生成 [ 2 ]   但是 却 把 第一次 运行 的 效果 页 带 了 进来                     生成器 不 保留 迭代 过后 的 结果                   In       [     12     ] :       gen       =       (     i       for       i       in       range     (     5     ) )         In       [     13     ] :       2       in       gen       Out     [     13     ] :       True         In       [     14     ] :       3       in       gen       Out     [     14     ] :       True         In       [     15     ] :       1       in       gen       Out     [     15     ] :       False       #   1 为什么 不 在 gen 里面 了 ?   因为 调用 1 - & gt ; 2 , 这个 时候 1 已经 不 在 迭代 器 里面 了 , 被 按 需 生成 过 了                     在 循环 中 修改 列表 项 ， 例如 在 循环 中 删除 某个 元素 导致 index 紊乱                   In       [     44     ] :       a       =       [     1     ,       2     ,       3     ,       4     ,       5     ]         In       [     45     ] :       for       i       in       a     :             ....     :               if       not       i       %       2     :             ....     :                       a     .     remove     (     i     )             ....     :         In       [     46     ] :       a       Out     [     46     ] :       [     1     ,       3     ,       5     ]       #   没有 问题         In       [     50     ] :       b       =       [     2     ,       4     ,       5     ,       6     ]         In       [     51     ] :       for       i       in       b     :             ....     :                 if       not       i       %       2     :             ....     :                         b     .     remove     (     i     )             ....     :         In       [     52     ] :       b       Out     [     52     ] :       [     4     ,       5     ]       #   本来 我 想要 的 结果 应该 是 去除 偶数 的 列表                     IndexError ，                   In       [     55     ] :       my _ list       =       [     1     ,       2     ,       3     ,       4     ,       5     ]         In       [     56     ] :       my _ list     [     5     ]       #   根本 没有 这个 元素         In       [     57     ] :       my _ list     [     5     : ]       #   这个 是 可以 的                     全局变量 和 局部变量 重名 问题                   In       [     58     ] :       def       my _ func     ( ) :             ....     :                       print     (     var     )       #   我 可以 先 调用 一个 未定义 的 变量             ....     :         In       [     59     ] :       var       =       &# 39 ; global &# 39 ;       #   后 赋值         In       [     60     ] :       my _ func     ( )       #   反正 只要 调用函数 时候 变量 被 定义 了 就 可以 了       global         In       [     61     ] :       def       my _ func     ( ) :             ....     :               var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     62     ] :       var       =       &# 39 ; global &# 39 ;         In       [     63     ] :       my _ func     ( )         In       [     64     ] :       print     (     var     )         global       #   局部变量 没有 影响 到 全局变量         In       [     65     ] :       def       my _ func     ( ) :             ....     :                       print     (     var     )       #   虽然 你 全局 设置 这个 变量 ,   但是 局部变量 有 同名 的 ,   python 以为 你 忘 了 定义 本地 变量 了             ....     :                       var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     66     ] :       var       =       &# 39 ; global &# 39 ;         In       [     67     ] :       my _ func     ( )       - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -       UnboundLocalError                                                       Traceback       (     most       recent       call       last     )       & lt ;     ipython     -     input     -     67     -     d82eda95de40     & gt ;       in       & lt ;     module     & gt ;     ( )       - - - - & gt ;       1       my _ func     ( )         & lt ;     ipython     -     input     -     65     -     0     ad11d690936     & gt ;       in       my _ func     ( )                   1       def       my _ func     ( ) :       - - - - & gt ;       2                       print     (     var     )                   3                       var       =       &# 39 ; locally   changed &# 39 ;                   4         UnboundLocalError     :       local       variable       &# 39 ; var &# 39 ;       referenced       before       assignment         In       [     68     ] :       def       my _ func     ( ) :             ....     :                       global       var       #   这个 时候 得加 全局 了             ....     :                       print     (     var     )       #   这样 就 能 正常 使用             ....     :                       var       =       &# 39 ; locally   changed &# 39 ;             ....     :         In       [     69     ] :       var       =       &# 39 ; global &# 39 ;         In       [     70     ] :         In       [     70     ] :       my _ func     ( )       global         In       [     71     ] :       print     (     var     )       locally       changed       #   但是 使用 了 global 就 改变 了 全局变量                       is     和     = =     前者 比较 是否是 同一个 对象 ， 后者 比较 值 是否 相等           工具 函数           获取 对象 成员 ，   getmembers   ,     dir   等 函数                   from       inspect       import       getmembers       for       k     ,     v       in       (     getmembers     (     object     ) ) :               print       k     ,     &# 39 ; : &# 39 ;     ,       v                 logging       python 打 日志 模块               import       logging       logging     .        ", "tags": "python", "url": "/wiki/python/python.html"},
      
      
      {"title": "Python 教程", "text": "    Table   of   Contents           关于           安装           数据结构           List                         关于       深感 需要 一门 语言 作为 个人 核心 语言 ， python   很 不错 ， 所以 打算 深入 学习 这门 语言 。     于是 ， 刷 一遍 python 官方 文档 很 有 必要 ！       官方 文档 地址         安装       WINDOW 平台 直接 安装 编译 好 的 二进制码 就行 ， 按照 正常 软件 安装 流程 安装 。       MAC   可以 先 安装   brew ,   然后     brew   install   python     一般 系统 自带   python       Linux   安装 ， 一般 自带 python 。 这里 介绍 一个 local 模式 。       首先 下载   python   对应 版本 的 源代码 ， 然后 编译 。 编译 时 指定     - - prefix = 你 要 安装 的 目录       通过 将 编译 后 的 bin 目录 加 到 环境变量   PATH   最 前面 的 方式 ， 覆盖 系统 路径 中 的 python 。     在 . bashrc 中 加入 代码     export   PATH = / path - to - local - python / bin / : $ PATH         数据结构       List           可以 用来 做 stack ， 使用     append   ,     pop     方法 ！       也 可以 用来 做 队列 ， 但是 不够 高效 ！ 可以 使用     collections . deque   ， 它 被 设计 为 在 双向   append , pop   都 很 高效 ！      ", "tags": "python", "url": "/wiki/python/python-tutorial.html"},
      
      
      {"title": "Python 日期时间相关模块总结", "text": "    Table   of   Contents           简介           time   模块           基本 用法                   datetime   模块           calendar   模块           总结                 简介       Python 标准 库 提供 了 对 日期 、 时间 、 日历 进行 操作 的 模块   time ,   datetime ,   calendar   。     其中   time   模块 基本上 是 对 Unix 时间 戳 的 操作 和 处理 ， 以及 涉及 到 操作系统     相关 的 操作 ，   datetime   模块 则 是 对 日期 和 时间 进行 的 处理 封装 ， 支持 时间     之间 的 运算 ， 在 对 日期 和 时间 的 处理 上 比   time   模块 要 方便 。       time   模块         time   模块 在 python 标准 库中 ， 被 放在 了 通用 操作系统 服务 目录 下 ，     由此可见 这个 模块 跟 操作系统 有着 很大 的 关系 。     因为 这个 原因 ， 某些 函数 是 与 操作系统 平台 有关 的 。     由于 是 基于 Unix 时间 戳 ， 导致 时间 表示 的 范围     被 限定 在 1970 - 2038 年 之间 。     这个 模块 中 的 基本 数据 结果 是   struct _ time   ， 实际上 是 一个 有 名字 的 元组 。       这个 模块 提供 的 时间 操作 函数 主要 是 时间 戳 ， 时间 字符串 和   struct _ time   三种 数据 中 的 相互 转换 。     还有 一些 操作系统 跟 时间 有关 的 系统 调用 。       基本 用法       生成 Unix 时间 戳 ， 单位 是 秒               & gt ; & gt ; & gt ;       ts       =       time     .     time     ( )       & gt ; & gt ; & gt ;       ts       1445495655.495                 时间 戳 是 一个 浮点数 ， 可以 通过 内置 函数 转换 为 其他 格式 。       时间 戳 转换 为   struct _ time                 & gt ; & gt ; & gt ;       time     .     gmtime     (     ts     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     6     ,       tm _ min     =     34     ,       tm _ sec     =     15     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     =     0     )         & gt ; & gt ; & gt ;       time     .     localtime     (     ts     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     14     ,       tm _ min     =     34     ,       tm _ sec     =     15     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     =     0     )                 时间 戳 转换 为 方便 阅读 的 字符串                     & gt ; & gt ; & gt ;       time     .     ctime     (     ts     )       &# 39 ; Thu   Oct   22   14 : 34 : 15   2015 &# 39 ;                   struct _ time   转换 为 字符串                     & gt ; & gt ; & gt ;       st       =       time     .     localtime     (     ts     )       & gt ; & gt ; & gt ;       time     .     asctime     (     st     )       &# 39 ; Thu   Oct   22   14 : 34 : 15   2015 &# 39 ;         & gt ; & gt ; & gt ;       time     .     strftime     (     &# 39 ; % Y - % m -     % d     &# 39 ;     ,       st     )       &# 39 ; 2015 - 10 - 22 &# 39 ;                   struct _ time   转换 为 时间 戳                   & gt ; & gt ; & gt ;       time     .     mktime     (     st     )       1445495655.0                 时间 字符串 转   struct _ time                         & gt ; & gt ; & gt ;       time     .     strptime     (     &# 39 ; 2015 - 10 - 22 &# 39 ;     ,     &# 39 ; % Y - % m -     % d     &# 39 ;     )       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     22     ,       tm _ hour     =     0     ,       tm _ min     =     0     ,       tm _ sec     =     0     ,       tm _ wday     =     3     ,       tm _ yday     =     295     ,       tm _ isdst     = -     1     )                 与 时间 有关 的 系统 调用 ， 如   time . sleep   。       datetime   模块       datetime 模块 提供 对 时间 和 日期 的 封装 ， 并 提供 他们 之间 的 数学 运算 。     该 模块 包含 4 个类 ， 用 得 多 的 是   datetime . timedelta   和   datetime . datetime   。                       object               timedelta               #   主要 用于 计算 时间跨度               tzinfo                     #   时 区 相关               time                         #   只 关注 时间               date                         #   只 关注 日期                       datetime         #   同时 有 时间 和 日期                 date 只 包含 年月日 3 个 属性 ， datetime 还 包含 时 、 分 、 秒 、 毫秒 。       获取 现在 时间                     & gt ; & gt ; & gt ;       datetime     .     today     ( )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       15     ,       8     ,       54     ,       88000     )       & gt ; & gt ; & gt ;       datetime     .     now     ( )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       15     ,       8     ,       41     ,       304000     )                 与 时间 戳 的 转换                       & gt ; & gt ; & gt ;       datetime     .     fromtimestamp     (     ts     )       datetime     .     datetime     (     2015     ,       10     ,       22     ,       14     ,       34     ,       15     ,       495000     )                 利用   datetime . combine ( date , time )   可以 将 date 和 time 组合 为 datetime 。     利用   datetime . strptime   和   datetime . strftime       可以 在 时间 字符串 和 datetime 对象 间 相互 转换 。                     & gt ; & gt ; & gt ;       dt       =       datetime     .     strptime     (     &# 39 ; 2015 - 10 - 12 &# 39 ;     ,     &# 39 ; % Y - % m -     % d     &# 39 ;     )       & gt ; & gt ; & gt ;       dt       datetime     .     datetime     (     2015     ,       10     ,       12     ,       0     ,       0     )       & gt ; & gt ; & gt ;       dt     .     strftime     (     &# 39 ;     % d     / % m / % Y &# 39 ;     )       &# 39 ; 12 / 10 / 2015 &# 39 ;                 对 datetime 对象 做 部分 修改                       & gt ; & gt ; & gt ;       dt     .     replace     (     year     =     2016     )       datetime     .     datetime     (     2016     ,       10     ,       12     ,       0     ,       0     )                 转换 为 timetuple 也 就是 time . struct _ time                     & gt ; & gt ; & gt ;       dt     .     timetuple       time     .     struct _ time     (     tm _ year     =     2015     ,       tm _ mon     =     10     ,       tm _ mday     =     12     ,       tm _ hour     =     0     ,       tm _ min     =     0     ,       tm _ sec     =     0     ,       tm _ wday     =     0     ,       tm _ yday     =     285     ,       tm _ isdst     = -     1     )                   datetime . timedelta   对象 和   datetime . time   对象 的 属性 是 类似 的 ， 只不过 前者 是 时间差 。       total _ seconds   方法 返回 总 的 秒数 。         timedelta   和   datetime   之间 的 数学 运算 可以 归纳 为                       timedelta       =       datetime       -       datetime       datetime       =       datetime       +       timedelta                 例如                     & gt ; & gt ; & gt ;       delta       =       timedelta     (     hours     =     1     )       & gt ; & gt ; & gt ;       delta       datetime     .     timedelta     (     0     ,       3600     )       & gt ; & gt ; & gt ;         & gt ; & gt ; & gt ;       dt       +       delta       datetime     .     datetime     (     2015     ,       10     ,       12     ,       1     ,       0     )                 calendar   模块       这个 模块 主要 提供 日历 的 一些 操作 ， 可以 很 方便 生成 一个 文本 日历 。                       & gt ; & gt ; & gt ;       print       calendar     .     month     (     2015     ,     10     )                 October       2015       Mo       Tu       We       Th       Fr       Sa       Su                           1         2         3         4         5         6         7         8         9       10       11       12       13       14       15       16       17       18       19       20       21       22       23       24       25       26       27       28       29       30       31                 calendar 包含 一个   Calendar   对象 ， 描述 了 日历 数据 的 结构 和 一些 设置 操作 。     而 格式化 任务 交给 了 两个 子类   TextCalendar   和   HTMLCalendar   。       总结       时间 模块 的 比较 ：     如果 对 时间 操作 是 在 字符串 和 时间 戳间 转换 的话 ， 用 time 模块 。     如果 需要 对 时间 进行 比较复杂 的 数学 运算 的话 ， 用 datetime 模块 。  ", "tags": "python", "url": "/wiki/python/python-date-time-module.html"},
      
      
      {"title": "requests包", "text": "    Table   of   Contents           关于           快速 入门           基本操作                         关于       如果 你 还 在 为 urllib 包而 烦恼 ， 不妨 试试 Python   Requests 包 。     这个 包 的 特点 是 ， 简单明了 ！       快速 入门       参考   官方 文档   。       基本操作           GET :     requests . get ( \" https : / / github . com / timeline . json \" )         POST :       PUT       HEAD       DELETE           OPTIONS               传递 URL 参数 ：                       payload       =       {     &# 39 ; key1 &# 39 ;     :       &# 39 ; value1 &# 39 ;     ,       &# 39 ; key2 &# 39 ;     :       &# 39 ; value2 &# 39 ;     }       r       =       requests     .     get     (     & quot ; http : / / httpbin . org / get & quot ;     ,       params     =     payload     )       print       r     .     url                     响应 内容     r . text   （ 文本 ） ,     r . content   （ 二进制 ） ， 编码   r . encoding   。 requests 会 自动 帮 你 解码     gzip   ！       JSON 响应     r . json ( )         原始 套 接字                   r       =       requests     .     get     (     url     ,       stream     =     True     )       r     .     raw       r     .     iter _ content     (     chunk _ size     )                     定制 HTTP 头     requests . get ( url ,   headers = headers )         POST   multipart - encoded                   files       =       {     &# 39 ; file &# 39 ;     :       open     (     &# 39 ; report . xls &# 39 ;     ,       &# 39 ; rb &# 39 ;     ) }       r       =       requests     .     post     (     url     ,       files     =     files     )                     Cookie     r . cookie     是 一个 字典      ", "tags": "python", "url": "/wiki/python/requests.html"},
      
      
      {"title": "seaborn 绘图工具", "text": "    Table   of   Contents           关于           配置           主题 风格           移除 上边 和 右边 的 边框           设置 局部 式样           线性 渐变 颜色 画板                   可视化 数据 集           绘制 分布图   distplot           两个 变量 分布图   jointplot           线性 回归 图           category   变量                   Grid                 关于       kaggle   上 很多 人用 这个 工具 做图 ， 图片 还 不错 ， 比   ggplot   貌似 要 更加 符合 matlab 风格 。     网站 地址     https : / / stanford . edu / ~ mwaskom / software / seaborn         配置       主题 风格       默认 风格 已经 不错 ， 如果 要 配置 风格 可以 使用 包 的   axes _ style ( )   和   set _ style ( )   命令 。     五中 默认 主题 是   darkgrid ,   whitegrid ,   dark ,   white ,   ticks   ， 默认 是   darkgrid   。     关键词 汇总 有   grid   代表 有 网格 ， 没有 的 代表 没有 网格 。               import       seaborn       as       sns       sns     .     set _ style     (     & quot ; whitegrid & quot ;     )       data       =       np     .     random     .     normal     (     size     =     (     20     ,     6     ) )       +       np     .     arange     (     6     )       /       2       sns     .     boxplot     (     data     =     data     )                 移除 上边 和 右边 的 边框       对于   white ,   ticks   主题 ， 通常 可以 移除 上边 和 右边 的 边框 ， 通过   sns . despine ( )   命令 就 可以 了 。       设置 局部 式样       使用     sns . axes _ style ( )     函数 和     with     环境 。                               with       sns     .     axes _ style     (     & quot ; darkgrid & quot ;     ) :               plt     .     subplot     (     211     )               sinplot     ( )       plt     .     subplot     (     212     )       sinplot     (     -     1     )                 重写 其他 配置 属性 ：                           sns     .     set _ style     (     & quot ; darkgrid & quot ;     ,       {     & quot ; axes . facecolor & quot ;     :       & quot ; . 9 & quot ;     } )                 所有 的 配置 属性 可以 通过   . axes _ style ( )   ， 传入 空 参数 得到 ：                         sns     .     axes _ style     ( )         {     &# 39 ; axes . axisbelow &# 39 ;     :       True     ,         &# 39 ; axes . edgecolor &# 39 ;     :       &# 39 ; . 8 &# 39 ;     ,         &# 39 ; axes . facecolor &# 39 ;     :       &# 39 ; white &# 39 ;     ,         &# 39 ; axes . grid &# 39 ;     :       True     ,         &# 39 ; axes . labelcolor &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; axes . linewidth &# 39 ;     :       1.0     ,         &# 39 ; figure . facecolor &# 39 ;     :       &# 39 ; white &# 39 ;     ,         &# 39 ; font . family &# 39 ;     :       [     u     &# 39 ; sans - serif &# 39 ;     ] ,         &# 39 ; font . sans - serif &# 39 ;     :       [     u     &# 39 ; Arial &# 39 ;     ,           u     &# 39 ; Liberation   Sans &# 39 ;     ,           u     &# 39 ; Bitstream   Vera   Sans &# 39 ;     ,           u     &# 39 ; sans - serif &# 39 ;     ] ,         &# 39 ; grid . color &# 39 ;     :       &# 39 ; . 8 &# 39 ;     ,         &# 39 ; grid . linestyle &# 39 ;     :       u     &# 39 ; - &# 39 ;     ,         &# 39 ; image . cmap &# 39 ;     :       u     &# 39 ; Greys &# 39 ;     ,         &# 39 ; legend . frameon &# 39 ;     :       False     ,         &# 39 ; legend . numpoints &# 39 ;     :       1     ,         &# 39 ; legend . scatterpoints &# 39 ;     :       1     ,         &# 39 ; lines . solid _ capstyle &# 39 ;     :       u     &# 39 ; round &# 39 ;     ,         &# 39 ; text . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; xtick . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; xtick . direction &# 39 ;     :       u     &# 39 ; out &# 39 ;     ,         &# 39 ; xtick . major . size &# 39 ;     :       0.0     ,         &# 39 ; xtick . minor . size &# 39 ;     :       0.0     ,         &# 39 ; ytick . color &# 39 ;     :       &# 39 ; . 15 &# 39 ;     ,         &# 39 ; ytick . direction &# 39 ;     :       u     &# 39 ; out &# 39 ;     ,         &# 39 ; ytick . major . size &# 39 ;     :       0.0     ,         &# 39 ; ytick . minor . size &# 39 ;     :       0.0     }         ` `     `         ###   设置 绘图 上下文       内置 四中 上下文 ：     ` paper ,   notebook ,   talk ,   poster `     ， 默认 是     ` notebook `     。       上下文 可以 通过     ` sns . set _ context ( &# 39 ; paper &# 39 ; ) `     这种 形式 进行 设置 。           ` set _ context `     函数 还 可以 指定 两个 参数 ，     ` font _ scale `     和     ` rc `     参数 ， 用来 指定 字体大小 和 其他 运行 参数 。 其他 运行 参数 ？       ` sns . set _ context ( & quot ; notebook & quot ; ,   font _ scale = 1.5 ,   rc = { & quot ; lines . linewidth & quot ; :   2.5 } ) `         函数     ` sns . set ( ) `     将 所有 配置 设置 为 默认 情况     (     传入 空 参数     )     ， 或者 更 多 其他 参数 ， 包括     ` rc `     参数 。         ##   Color   palettes       ` sns . color _ palette ( ) `     获取 和 设置     color       palette       使用     hls     循环 颜色 系统 ， 可以 指定 第一个 参数 为     ` hls `     ， 或者 使用     ` hls _ palette ( ) `     函数 。       绘制 颜色 画板 的 函数 是       ` sns . palplot ( pal ) `     。       一种 更亮 的 版本 是       husl       系统 。         ###   Color   Brewer       类似 于     color       map       ？       好多 颜色 ， 先不看 了 。 看不下去 了 。         ###   使用 命名 好 的 颜色       xkcd       颜色 命名 有     954     中 颜色 ， 可以 通过 字典       ` sns . xkcd _ rgb `       查看 。       使用 方式 也 是 通过 这个 字典 。       利用 这个 字典 ， 可以 将 颜色 名字 列表 转换 为 颜色 画板 。         ` `     `     python       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       1     ] ,       sns     .     xkcd _ rgb     [     & quot ; pale   red & quot ;     ] ,       lw     =     3     )       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       2     ] ,       sns     .     xkcd _ rgb     [     & quot ; medium   green & quot ;     ] ,       lw     =     3     )       plt     .     plot     ( [     0     ,       1     ] ,       [     0     ,       3     ] ,       sns     .     xkcd _ rgb     [     & quot ; denim   blue & quot ;     ] ,       lw     =     3     ) ;         colors       =       [     & quot ; windows   blue & quot ;     ,       & quot ; amber & quot ;     ,       & quot ; greyish & quot ;     ,       & quot ; faded   green & quot ;     ,       & quot ; dusty   purple & quot ;     ]       sns     .     palplot     (     sns     .     xkcd _ palette     (     colors     ) )                 交互式 颜色 选择 工具     http : / / www . luminoso . com / colors /         线性 渐变 颜色 画板               sns     .     palplot     (     sns     .     color _ palette     (     & quot ; Blues & quot ;     ) )       sns     .     palplot     (     sns     .     color _ palette     (     & quot ; BuGn _ r & quot ;     ) )       sns     .     palplot     (     sns     .     color _ palette     (     & quot ; GnBu _ d & quot ;     ) )                 更 多 画板 工具 后面 再 详细 地 添加 吧 。       可视化 数据 集       绘制 分布图     distplot               distplot ( )   ， 它会 绘制 histogram   直方图 ， 并且 会 拟合 一个     kernel   density   estimate   ( KDE )   。                                           x       =       np     .     random     .     normal     (     size     =     100     )       sns     .     distplot     (     x     ) ;                         Histograms   直方图 ：   将     distplot     的 参数设置 为     kde = False     就 可以 去掉 KDE ，   rugplot ( )       也 可以 通过 制定 参数     rug = True     来 实现 。 rug 图是 像 条形码 一样 的 图 ， 用 竖线 的 疏密 来 表达 数据 的 多少 。               KDE 图 ， 将   distplot     的 参数设置 为     hist = False   即可 ， 也 可以 通过     . kdeplot ( )     来 实现 。               fit ， 制定 fit 参数               两个 变量 分布图     jointplot             散点图 ，     . jointplot ( x =   ' x ' ,   y = ' y ' ,   data = data )         两 变量 直方图 ， hexbins   plot ， 添加 参数     kind = ' hex '         两 变量 KDE 图 ，   kind = \" kde \"   ， 也 可以 通过   kdeplot         高 level   KDE 图 ，     sns . kdeplot ( df . x , df . y ,   shade = True ,   n _ levels = 60 )               jointplot     返回 一个     JoinGrid                   . pairplot     观察 两 两 变量 之间 的 关系 。 可以 指定 第三个 变量 为     hue   ， 使得 用 颜色 来 区分 第三个 变量 。               线性 回归 图             regplot ( )   and   lmplot ( )   ， 线性 回归 图 ，   lmplot ( )   功能 比较 强大 。         lmplot     的     order   参数 可以 指定 回归 的 阶数 。         robust = True     可以 指定 去掉 异常 值 。         scatter _ kws = { \" s \" :   80 }     这个 参数 可以 指定 scatter   的 属性         hue     指定 第三个 变量 作为 颜色 画图         col     指定 第三个 变量 作为 列 分隔             row     指定 第三个 变量 作为 行 分隔               在 其他 作图 上下文 做 回归 图 ，   指定 作图 类型 为 ：   kind = \" reg \"                 category   变量             sns . stripplot     其中 一个 变量 是 category 类型       因为 tripplot 作图 时 ， 数据 点会 重叠 ， 所以 可以 指定 参数   jitter = True   ， 增加 一点 随机 偏移 ， 让 数据 点 不 重叠       另 一种 防止 重叠 的 方法 使用 函数     sns . swarmplot   ， 会 采用 防止 重叠 算法 ， 可以 指定 第三个 变量 为   hue           . boxplot ( )     会 将 另 一个 连续变量 的 分布 也 显示 出来         sns . violinplot     会 将 另 一个 变量 的 KDE 也 显示 出来       用     scale     指定 第三个 变量       用     hue     指定 第三个 变量 ， 同时 指定     split = Ture         统计 估计     barplot         对 category 进行 统计     sns . countplot   ， 类似 于   df . count _ values ( ) . plot           sns . pointplot     不 知道 干 啥 的           Grid           FaceGrid       PairGrid      ", "tags": "python", "url": "/wiki/python/seaborn.html"},
      
      
      {"title": "命令行模块", "text": "    Table   of   Contents           argparse 模块           安装                 argparse 模块       基本 模板               import       argparse       parser       =       argparse     .     ArgumentParser     (     description     =     &# 39 ; 描述 信息 &# 39 ;     )       parser     .     add _ argument     (     &# 39 ; integers &# 39 ;     ,       metavar     =     &# 39 ; N &# 39 ;     ,       type     =     int     ,       nargs     =     &# 39 ; +&# 39 ;     ,                                               help     =     &# 39 ; an   integer   for   the   accumulator &# 39 ;     )       args       =       parser     .     parse _ args     ( )       print     (     args     .     integers     )                     主动 传参     parser . parse _ args     默认 parse     sys . argv   , 也 可以 主动 传参       添加 参数 的 方法     ArgumentParser . add _ argument ( name   or   flags ... [ ,   action ] [ ,   nargs ] [ ,   const ] [ ,   default ] [ ,   type ] [ ,   choices ] [ ,   required ] [ ,   help ] [ ,   metavar ] [ ,   dest ] )                     #   可 选 参数       parser     .     add _ argument     (     &# 39 ; - f &# 39 ;     ,       &# 39 ; - - foo &# 39 ;     )         #   位置 参数 ,   不 需要 输入 参数 名 ,   根据 位置 来 判断       parser     .     add _ argument     (     &# 39 ; bar &# 39 ;     )         #   flag 选项 ,   保存 常数       #   - - foo       #   foo   =   42       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       action     =     &# 39 ; store _ const &# 39 ;     ,       const     =     42     )         #   多 选项       #   - - foo   1   - - foo   2       #   foo   =   [ 1 ,   2 ]       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       action     =     &# 39 ; append &# 39 ;     )         #   多值       #   - - foo   1   2       #   foo   =   [ 1 , 2 ]       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       nargs     =     2     )       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       nargs     =     &# 39 ; * &# 39 ;     )         #   默认值       #   foo   =   42       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       default     =     42     )         #   指定 类型       #   - - foo   34       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       type     =     int     )         #   文件类型       #   - - foo   file . txt   - - out   out . txt       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       type     =     file     )       #   read   file       parser     .     add _ argument     (     &# 39 ; - - out &# 39 ;     ,       type     =     argparse     .     FileType     (     &# 39 ; w &# 39 ;     ) )       #   write   file       parser     .     add _ argument     (     &# 39 ; - - cb &# 39 ;     ,       type     =     callable     )       #   自定义 类型 ,   type 是 一个 函数         #   指定 选择 范围       #   - - move   rock       #   move   =   rock       parser     .     add _ argument     (     &# 39 ; move &# 39 ;     ,       choices     =     [     &# 39 ; rock &# 39 ;     ,       &# 39 ; paper &# 39 ;     ,       &# 39 ; scissors &# 39 ;     ] )         #   必选 选项       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       required     =     True     )         #   帮助 信息       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       action     =     &# 39 ; store _ true &# 39 ;     ,                                                 help     =     &# 39 ; foo   the   bars   before   frobbling &# 39 ;     )         #   改变 目标 变量 的 名字       #   - - foo   42       #   bar   =   42       parser     .     add _ argument     (     &# 39 ; - - foo &# 39 ;     ,       dest     =     &# 39 ; bar &# 39 ;     )         #   将 选项 增加 到 已有 对象 上       #   - - foo   42       #   C . foo   =   42       class       C     (     object     ) :               pass       c       =       C     ( )       parser     .     parse _ args     (     namespace     =     c     )                 自定义 action   ref   :   输入 密码 , 将 输入 密码 隐藏   ref                 class       Password     (     argparse     .     Action     ) :               def       __ call __     (     self     ,       parser     ,       namespace     ,       values     ,       option _ string     ) :                       if       values       is       None     :                               values       =       getpass     .     getpass     ( )                       setattr     (     namespace     ,       self     .     dest     ,       values     )         parser       =       argparse     .     ArgumentParser     ( )       parser     .     add _ argument     (     &# 39 ; - p &# 39 ;     ,       action     =     Password     ,       nargs     =     &# 39 ; ? &# 39 ;     ,       dest     =     &# 39 ; password &# 39 ;     )       args       =       parser     .     parse _ args     ( )                 更 多 选项 :     https : / / docs . python . org / 2.7 / library / argparse . html         安装         pip   install   Click    ", "tags": "python", "url": "/wiki/python/click.html"},
      
      
      {"title": "日志模块", "text": "    Table   of   Contents           基本 配置                 基本 配置               logging     .     basicConfig     (               format     =     &# 39 ;     % ( asctime ) s           % ( levelname ) - 8s           % ( message ) s     &# 39 ;     ,               level     =     logging     .     INFO     ,               datefmt     =     &# 39 ; % Y - % m -     % d       % H : % M : % S &# 39 ;     )        ", "tags": "python", "url": "/wiki/python/logging.html"},
      
      
      {"title": "编译tensorflow", "text": "    Table   of   Contents           关于           环境           安装           安装 JDK8           安装 bazel                   参考资料                 关于       记录 从 源码 编译 tensorflow 的 过程       环境           CentOS   6     rpm   - - query   centos - release         GLIBC   版本 2.12 ；     strings   / lib64 / libc . so . 6   | grep   GLIBC _             安装       安装 JDK8       因为 bazel 需要 JDK8 。 你 可以 从 Oracle 官网 下载 ， 也 可以 下载 我 的 备份     https : / / pan . baidu . com / s / 1jJoMjuY         下载 后 解压 到 某个 目录 ， 例如 在 我 的 机器 上 是     ~ / jdk1 . 8.0 _ 161   ， 确保 这个 目录 下 有     / bin     这个 子目录 。       配置 环境变量             export   JAVA _ HOME = ~ / jdk1 . 8.0 _ 161   export   PATH = $ JAVA _ HOME / bin : $ PATH               执行   java   - version   命令 ， 查看 是否 配置 正确             java   version   & quot ; 1.8 . 0 _ 161 & quot ;   Java ( TM )   SE   Runtime   Environment   ( build   1.8 . 0 _ 11 - b12 )   Java   HotSpot ( TM )   64 - Bit   Server   VM   ( build   25.11 - b03 ,   mixed   mode )               安装 bazel       可以 直接 通过 软件包 安装 ， 参考 官网 文档   https : / / docs . bazel . build / versions / master / install . html         下面 介绍 从 源码 安装 步骤 ， 首先 下载 源文件   https : / / github . com / bazelbuild / bazel / releases   ， 并 解压 到   bazel /   目录 。     然后 运行 脚本   . / compile . sh         参考资料           https : / / www . tensorflow . org / install / install _ sources      ", "tags": "python", "url": "/wiki/python/compile-tf.html"},
      
      
      
      
      
        
      
      {"title": "2018年新技术review", "text": "    Table   of   Contents           关于                 关于  ", "tags": "report", "url": "/wiki/report/2018-technology-review.html"},
      
      
      
      
      
        
      
      {"title": "MS-Word使用技巧汇总", "text": "  关于       微软 的 word 文档 是 一个 很 好 的 文字处理 工具 ， 但是 有时候 也 需要 一些 技巧 。     比如 在 撰写 大 论文 的 时候 ， 高效 的 排版 技巧 是 非常 必要 的 ， 往往 能够 减少     很多 不必要 的 时间 浪费 。 本文 汇总 了 我 在 使用 word 文档 时 的 一些 技巧 ，     大多数 是从 网上 总结 别人 的 经验 得到 的 ， 也 有 一些 自己 摸索 出来 的 技巧 。     如果 你 有 其他 需求 ， 最好 放狗 搜 ， 一般 都 能 搜索 到 。       格式 篇       长 文档 格式 的 一致性       通过 对 段落 添加 式样 标签 ， 如 “ 标题 1 ” ， “ 正文 ” 等 ， 而 不是 对 每 一个 段落         手动 添加 格式 。 这样 的 好处 是 ， 当 需要 更改 所有 相同 段落 的 时候 ， 比如         需要 将 所有 正文 段落 改成 四号 字体 ， 只 需要 更改 式样 标签 的 式样 即可 。         如果 你 熟悉 HTML 标签 就 会 知道 ， 这 相当于 给 一个 内容 块 添加 了 一个 标签 ，         如果 要 修改 这个 标签 的 式样 ， 只 需要 为 这个 标签 添加 格式 即可 ， 而 不用         在 每 一个 段落 上 申明 它 的 格式 。 总之 一个 原则   一定 不要 手动 为 一个 文本         单独 添加 格式 ！                 公式 排版       公式 对齐       制表 位 与 对齐 。 如果 你 需要 排版 数学公式 ， 需要 将 公式 居中 ， 而 将 公式 的     编号 放在 行末 并 右 对齐 。 显然 用 空格 来 控制 格式 不是 一个 好 主意 。 可以 采用     制表 位 实现 ， 创建 新 式样 公式 ， 并 修改 格式 ， 在 制表 位中 添加 两个 制表 位 ，     20 居中 和 40 右 对齐 ， 然后 在 公式 前 和 编号 前 分别 添加 制表符 即可 。 效果 如下 图 。     总之 一个 原则   尽量 不要 通过 空格 来 调整 格式 ！                 公式 自动 编号       “ 插入 ” - & gt ; “ 引用 ” - & gt ; “ 题注 ” 插入 公式 编号 ；     在 需要 引用 公式 的 地方 用 “ 插入 ” - & gt ; “ 引用 ” - & gt ; “ 交叉 引用 ” 引用 公式 编号 ，     可以 选择 “ 整项 题注 ” 引用 公式 编号 行 的 所有 内容 。       页眉 自动 根据 章节 标题 自动 插入 ：       可以 采用 域 的 方式 在 文中 的 页眉 中 自动 插入 标题 的 编号 或 名称 ：     （ 1 ） 双击 页眉 位置 ， 页眉 处于 可 编辑 状态 ；       （ 2 ） 选择 插入 - 文档 部件 - 域       （ 3 ） 选择 “ 类别 - 链接 和 引用 ” 下 的 styleref ， 选择 章节 使用 的 “ 样式 名 ” （ 要 使用 样式 哦 ） ， “ 域 选项 ” 里 不选 任何 项即 插入 章节 标题 ； 若 “ 域 选项 ” 框中 选择 了 “ 插入 段落 编号 ” ， 则 插入 章节 编号 。       文档 分节       为什么 要 分节 ？           分节 可以 对 每 一节 应用 不同 的 页眉 、 页脚 和 页码 。       分节 文档 转换 为 PDF 时 ， 可以 自动 保证 每 一节 的 第一页 在 奇数 页 。           分节 编制 页码       首先 我们 需要 把 封面 页 和 正文 页 进行 分节 ， 首先 将 封面 页 和 正文 的 第一页 放在 一起 ， 中间 不要 间隔 ， 然后 在 正文 的 第一个 字 之前 点击 光标 ， 插入 分节 符 。       然后 取消 链接 到 前 一条 页眉 ， 最后 设置 页码 格式 中 起始页 码为 1 .       参考     http : / / jingyan . baidu . com / article / 4f7d57129d9c501a201927ea . html    ", "tags": "software", "url": "/wiki/software/ms-word.html"},
      
      
      
      
      
        
      
      {"title": "Spark reducebykey", "text": "    Table   of   Contents               spark   reduceByKey   是 默认 包含 了 map 端 combine 操作 的 ,   不过 如果 key 是 array 除外 , 这种 无法 使用 map 段 合并  ", "tags": "spark", "url": "/wiki/spark/spark-reducebykey.html"},
      
      
      {"title": "Spark Streaming", "text": "    Table   of   Contents           简介           StreamingContext           Input   DStreams   and   Receivers           基本 源                   DStreams   的 变换 操作           UpdateStateByKey   操作           Transform   操作           Window   操作           join   操作                         简介       Spark   Streaming   将流 数据 按照 时间 离散 化 ， 每 单位 时间 一个 batch ！ 这 是 和 其他 流 处理 系统 不同 的 地方 。     好处 是 效率 更高 ， 缺点 是 牺牲 了 一定 的 实时性 。               StreamingContext               import       org . apache . spark ._       import       org . apache . spark . streaming ._         val       conf       =       new       SparkConf     ( ) .     setAppName     (     appName     ) .     setMaster     (     master     )       val       ssc       =       new       StreamingContext     (     conf     ,       Seconds     (     1     ) )                 每 一个   Batch   是 一个 DStream 对象 ， 一个   DStream   对象 实际上 是 一些 列 的 RDD 。               Input   DStreams   and   Receivers           输入 源 ：       基本 数据源 ： 文件系统 和 套 接字 连接       高级 源 ： Kafka ,   Flume ,   Kinesis ,   etc               每 一个   DStream   都 有 至少 一个   Receiver ， 本地 模式 运行 时 ， 线程 数目 应该 多余 源 的 数目 。           基本 源           文件系统 ：   streamingContext . fileStream [ KeyClass ,   ValueClass ,   InputFormatClass ] ( dataDirectory )         RDD 队列 ：   streamingContext . queueStream ( queueOfRDDs )         Custom   Receivers           DStreams   的 变换 操作           支持 RDD 上 类似 的 变换 操作 。           UpdateStateByKey   操作       用于 维护 一个 持续 的 状态 ， 状态 可以 是 任意 数据类型 。               def       updateFunction     (     newValues     :       Seq     [     Int     ] ,       runningCount     :       Option     [     Int     ] )     :       Option     [     Int     ]       =       {               val       newCount       =       ...         / /   add   the   new   values   with   the   previous   running   count   to   get   the   new   count               Some     (     newCount     )       }       val       runningCounts       =       pairs     .     updateStateByKey     [     Int     ] (     updateFunction       _     )                 Transform   操作       直接 对 rdd 进行 操作               val       spamInfoRDD       =       ssc     .     sparkContext     .     newAPIHadoopRDD     ( ... )       / /   RDD   containing   spam   information         val       cleanedDStream       =       wordCounts     .     transform       {       rdd       = & gt ;           rdd     .     join     (     spamInfoRDD     ) .     filter     ( ... )       / /   join   data   stream   with   spam   information   to   do   data   cleaning           ...       }                 Window   操作           window   大小       sliding   大小                           / /   Reduce   last   30   seconds   of   data ,   every   10   seconds       val       windowedWordCounts       =       pairs     .     reduceByKeyAndWindow     ( (     a     :     Int     ,     b     :     Int     )       = & gt ;       (     a       +       b     ) ,       Seconds     (     30     ) ,       Seconds     (     10     ) )                 join   操作           Stream - stream   joins   将 两个   DStream   join 到 一起 ， 实际上 是 在 每个 时 间隙 ， 将 两个   DStream   的   RDD   join 到 一起 。      ", "tags": "spark", "url": "/wiki/spark/spark-streaming.html"},
      
      
      {"title": "Spark动态加载JAR包的问题", "text": "    Table   of   Contents           问题                 问题       在 开发 Spark 程序 时 ， 有时候 需要 动态 加载 jar 包到 系统 的   classpath   。     例如 ， file : / / / xxx . jar   包中 存在 一个 类 A 的 子类 AA ， A 类 在 当前 Spark     程序 中 存在 ， 但是 没有 子类 AA ，               val       jarPath       =       lines     (     1     )       val       myJar       =       Array     (     new       Path     (     jarPath     ) .     toUri     .     toURL     )         log     .     info     (     s & quot ; Load   jar       $ jarPath     & quot ;     )       val       parentLoader       =       Thread     .     currentThread     ( ) .     getContextClassLoader       val       classLoader       =       new       URLClassLoader     (     myJar     ,       parentLoader     )         val       a       =       classLoader     .     loadClass     (     & quot ; AA & quot ;     ) .     newInstance     ( ) .     asInstanceOf     [     A     ]                 如果 AA 操作 要 用到 SparkContext ， 那么 将会 报错     java . lang . ClassNotFoundException   。     可以 通过 将 jar 包 增加 到 上下文 中 ， 解决 此 问题 ！               sc     .     addJar     (     jarPath     )        ", "tags": "spark", "url": "/wiki/spark/dynamic-add-jar.html"},
      
      
      {"title": "Storm", "text": "    Table   of   Contents           安装 本地 环境           基本概念                 安装 本地 环境       这里 只 介绍 本地 环境 的 安装 ， 集群 的 安装 可以 参考 本地 环境 安装 和 网上 资料 。           安装   zookeeper ， zookeeper 在 storm 中 用于 管理 和 协调 集群 。           ` ` ` bash     brew   install   zookeeper     ` ` ` `           安装                   基本概念               拓扑   Topologies                 http : / / storm . apache . org / releases / 1.0 . 6 / Concepts . html            ", "tags": "spark", "url": "/wiki/spark/storm.html"},
      
      
      
      
      
        
      
      {"title": "Feature Column 特征变换", "text": "    Table   of   Contents           关于           Numeric   column           Bucketized   column           Categorical   identity   column           Categorical   vocabulary   column           Hashed   Column           Crossed   column           Indicator   and   embedding   columns           Passing   feature   columns   to   Estimators                 关于               Numeric   column       创建 单个 标量 特征 列               #   Defaults   to   a   tf . float32   scalar .       numeric _ feature _ column       =       tf     .     feature _ column     .     numeric _ column     (     key     =     & quot ; SepalLength & quot ;     ,       dtype     =     tf     .     float64     )                 创建 向量 特征 列               #   Represent   a   10 - element   vector   in   which   each   cell   contains   a   tf . float32 .       vector _ feature _ column       =       tf     .     feature _ column     .     numeric _ column     (     key     =     & quot ; Bowling & quot ;     ,                                                                                                                         shape     =     10     )                 Bucketized   column       分桶 的 列 , 将 数值 按照 桶 划分 成 多个 类别 特征               #   First ,   convert   the   raw   input   to   a   numeric   column .       numeric _ feature _ column       =       tf     .     feature _ column     .     numeric _ column     (     & quot ; Year & quot ;     )         #   Then ,   bucketize   the   numeric   column   on   the   years   1960 ,   1980 ,   and   2000 .       bucketized _ feature _ column       =       tf     .     feature _ column     .     bucketized _ column     (               source _ column       =       numeric _ feature _ column     ,               boundaries       =       [     1960     ,       1980     ,       2000     ] )                 Categorical   identity   column       相当于 对 整数 做   onehot               #   Create   categorical   output   for   an   integer   feature   named   & quot ; my _ feature _ b & quot ; ,       #   The   values   of   my _ feature _ b   must   be   & gt ; =   0   and   & lt ;   num _ buckets       identity _ feature _ column       =       tf     .     feature _ column     .     categorical _ column _ with _ identity     (               key     =     &# 39 ; my _ feature _ b &# 39 ;     ,               num _ buckets     =     4     )       #   Values   [ 0 ,   4 )         #   In   order   for   the   preceding   call   to   work ,   the   input _ fn ( )   must   return       #   a   dictionary   containing   &# 39 ; my _ feature _ b &# 39 ;   as   a   key .   Furthermore ,   the   values       #   assigned   to   &# 39 ; my _ feature _ b &# 39 ;   must   belong   to   the   set   [ 0 ,   4 ) .       def       input _ fn     ( ) :               ...               return       ( {       &# 39 ; my _ feature _ a &# 39 ;     : [     7     ,       9     ,       5     ,       2     ] ,       &# 39 ; my _ feature _ b &# 39 ;     : [     3     ,       1     ,       2     ,       2     ]       } ,                               [     Label _ values     ] )                 Categorical   vocabulary   column       对 字符串 做 字典 映射 , 然后   onehot ,   可以 指定 列表 , 或者 文件               #   Given   input   & quot ; feature _ name _ from _ input _ fn & quot ;   which   is   a   string ,       #   create   a   categorical   feature   by   mapping   the   input   to   one   of       #   the   elements   in   the   vocabulary   list .       vocabulary _ feature _ column       =               tf     .     feature _ column     .     categorical _ column _ with _ vocabulary _ list     (                       key     =     feature _ name _ from _ input _ fn     ,                       vocabulary _ list     =     [     & quot ; kitchenware & quot ;     ,       & quot ; electronics & quot ;     ,       & quot ; sports & quot ;     ] )         #   Given   input   & quot ; feature _ name _ from _ input _ fn & quot ;   which   is   a   string ,       #   create   a   categorical   feature   to   our   model   by   mapping   the   input   to   one   of       #   the   elements   in   the   vocabulary   file       vocabulary _ feature _ column       =               tf     .     feature _ column     .     categorical _ column _ with _ vocabulary _ file     (                       key     =     feature _ name _ from _ input _ fn     ,                       vocabulary _ file     =     & quot ; product _ class . txt & quot ;     ,                       vocabulary _ size     =     3     )                                 Hashed   Column       Hash 取模 ,   返回   category   column               #   pseudocode       feature _ id       =       hash     (     raw _ feature     )       %       hash _ bucket _ size                         hashed _ feature _ column       =               tf     .     feature _ column     .     categorical _ column _ with _ hash _ bucket     (                       key       =       & quot ; some _ feature & quot ;     ,                       hash _ bucket _ size       =       100     )       #   The   number   of   categories                 Crossed   column       特征 交叉               def       make _ dataset     (     latitude     ,       longitude     ,       labels     ) :               assert       latitude     .     shape       = =       longitude     .     shape       = =       labels     .     shape                 features       =       {     &# 39 ; latitude &# 39 ;     :       latitude     .     flatten     ( ) ,                                       &# 39 ; longitude &# 39 ;     :       longitude     .     flatten     ( ) }               labels     =     labels     .     flatten     ( )                 return       tf     .     data     .     Dataset     .     from _ tensor _ slices     ( (     features     ,       labels     ) )           #   Bucketize   the   latitude   and   longitude   using   the   ` edges `       latitude _ bucket _ fc       =       tf     .     feature _ column     .     bucketized _ column     (               tf     .     feature _ column     .     numeric _ column     (     &# 39 ; latitude &# 39 ;     ) ,               list     (     atlanta     .     latitude     .     edges     ) )         longitude _ bucket _ fc       =       tf     .     feature _ column     .     bucketized _ column     (               tf     .     feature _ column     .     numeric _ column     (     &# 39 ; longitude &# 39 ;     ) ,               list     (     atlanta     .     longitude     .     edges     ) )         #   Cross   the   bucketized   columns ,   using   5000   hash   bins .       crossed _ lat _ lon _ fc       =       tf     .     feature _ column     .     crossed _ column     (               [     latitude _ bucket _ fc     ,       longitude _ bucket _ fc     ] ,       5000     )         fc       =       [               latitude _ bucket _ fc     ,               longitude _ bucket _ fc     ,               crossed _ lat _ lon _ fc     ]         #   Build   and   train   the   Estimator .       est       =       tf     .     estimator     .     LinearRegressor     (     fc     ,       ...     )                 Indicator   and   embedding   columns       onehot 编码               tf     .     feature _ column     .     indicator _ column     (     categorical _ column     )                 embedding 列               tf     .     feature _ column     .     embedding _ column     (               categorical _ column     =     categorical _ column     ,               dimension     =     embedding _ dimensions     )                 Passing   feature   columns   to   Estimators             tf . estimator . LinearClassifier     和     tf . estimator . LinearRegressor   :   所有 的 特征 列 .         tf . estimator . DNNClassifier     和     tf . estimator . DNNRegressor   :   只 支持   dense   列 .   其他 特征 列 必须 转 成     indicator _ column   或者   embedding _ column   .         tf . estimator . DNNLinearCombinedClassifier     和     tf . estimator . DNNLinearCombinedRegressor   :       The   linear _ feature _ columns   argument   accepts   any   feature   column   type .       The   dnn _ feature _ columns   argument   only   accepts   dense   columns .              ", "tags": "tensorflow", "url": "/wiki/tensorflow/feature-column.html"},
      
      
      {"title": "Tensorflow 数据读取", "text": "    Table   of   Contents           TFRecordDataset           TFRecordDataset 的 生成                   Dataset 读取                 TensorFlow   导入 数据 的 基本 机制 有 两种 , 一种 是 利用     placeholder   , 一种 是 将 数据 读取 也 作为 一个 OP 。     如果 在 GPU 上 执行 计算 图 的 计算 , 那么 前 一种 方式 会 因为 从 CPU 拷贝 数据 到 GPU 而 堵塞 , 后 一种 方式 可以 利用 多线程 提升 性能 。       TFRecordDataset         TFRecordDataset     是 Tensorflow 中 标准 格式 ,       TFRecordDataset 的 生成       如果 利用 Spark 可以 利用     spark - tensorflow - connector   ,     可以 直接 下载 编译 好 的 jar 包 , 通过     - - jars   path - to - spark - tensorflow - connector . jar     导入 这个 包 , 然后 就 可以 直接 将   DataFrame   转换成 分布式 TFRecord 文件 。     不过 这个 包要   Spark2 . 0   以上 才 支持 。               import       org . apache . commons . io . FileUtils       import       org . apache . spark . sql .     {       DataFrame     ,       Row       }       import       org . apache . spark . sql . catalyst . expressions . GenericRow       import       org . apache . spark . sql . types ._         val       path       =       & quot ; test - output . tfrecord & quot ;       val       testRows     :       Array     [     Row     ]       =       Array     (       new       GenericRow     (     Array     [     Any     ] (     11     ,       1     ,       23L     ,       10.0 F     ,       14.0     ,       List     (     1.0     ,       2.0     ) ,       & quot ; r1 & quot ;     ) ) ,       new       GenericRow     (     Array     [     Any     ] (     21     ,       2     ,       24L     ,       12.0 F     ,       15.0     ,       List     (     2.0     ,       2.0     ) ,       & quot ; r2 & quot ;     ) ) )       val       schema       =       StructType     (     List     (     StructField     (     & quot ; id & quot ;     ,       IntegerType     ) ,                                                                   StructField     (     & quot ; IntegerCol & quot ;     ,       IntegerType     ) ,                                                                 StructField     (     & quot ; LongCol & quot ;     ,       LongType     ) ,                                                                 StructField     (     & quot ; FloatCol & quot ;     ,       FloatType     ) ,                                                                 StructField     (     & quot ; DoubleCol & quot ;     ,       DoubleType     ) ,                                                                 StructField     (     & quot ; VectorCol & quot ;     ,       ArrayType     (     DoubleType     ,       true     ) ) ,                                                                 StructField     (     & quot ; StringCol & quot ;     ,       StringType     ) ) )         val       rdd       =       spark     .     sparkContext     .     parallelize     (     testRows     )         / / Save   DataFrame   as   TFRecords       val       df     :       DataFrame       =       spark     .     createDataFrame     (     rdd     ,       schema     )       df     .     write     .     format     (     & quot ; tfrecords & quot ;     ) .     option     (     & quot ; recordType & quot ;     ,       & quot ; Example & quot ;     ) .     save     (     path     )         / / Read   TFRecords   into   DataFrame .       / / The   DataFrame   schema   is   inferred   from   the   TFRecords   if   no   custom   schema   is   provided .       val       importedDf1     :       DataFrame       =       spark     .     read     .     format     (     & quot ; tfrecords & quot ;     ) .     option     (     & quot ; recordType & quot ;     ,       & quot ; Example & quot ;     ) .     load     (     path     )       importedDf1     .     show     ( )         / / Read   TFRecords   into   DataFrame   using   custom   schema       val       importedDf2     :       DataFrame       =       spark     .     read     .     format     (     & quot ; tfrecords & quot ;     ) .     schema     (     schema     ) .     load     (     path     )       importedDf2     .     show     ( )                 提交 命令               ##   jar 包在 本地       $ SPARK _ HOME   / bin / spark - shell   - - jars   target / spark - tensorflow - connector _ 2.11 - 1.10 . 0 . jar       ##   jar 包在 HDFS 上       $ SPARK _ HOME   / bin / spark - shell   - - jars   viewfs : / / xxxx / path / to / spark / tensorflow / connector . jar               Dataset 读取               #   定义 转换 函数 , 输入 时 序列化 的       def       parse _ tfrecords _ function     (     example _ proto     ) :               features       =       {                       &# 39 ; label &# 39 ;       :       tf     .     FixedLenFeature     ( [     1     ] ,       tf     .     float32     ) ,                       &# 39 ; idx &# 39 ;       :       tf     .     VarLenFeature     (     tf     .     int64     ) ,                       &# 39 ; value &# 39 ;       :       tf     .     VarLenFeature     (     tf     .     float32     )               }                 #   tf . parse _ single _ example   将 序列化 的 数据 解码 成 张量               parsed _ features       =       tf     .     parse _ single _ example     (     example _ proto     ,       features     )               return       parsed _ features     [     & quot ; idx & quot ;     ] ,       parsed _ features     [     & quot ; value & quot ;     ] ,       parsed _ features     [     &# 39 ; label &# 39 ;     ]         dataset       =       tf     .     data     .     TFRecordDataset     (     filenames     )       #   map ,   repeat ,   prefetch ,   shuffle ,   batch   都 是 可选 的 , 可以 直接   make _ initializable _ iterator       it       =       dataset     .     map     (     parse _ tfrecords _ function     )     .     repeat     (     10     )     .     prefetch     (     10240     )     .     shuffle     (     10240     )     .     batch     (     128     )     .     make _ initializable _ iterator     ( )       idx _ op     ,       value _ op     ,       label _ op       =       it     .     get _ next     ( )         #   需要 初始化       sess     .     run     (     it     .     initializer     )            ", "tags": "tensorflow", "url": "/wiki/tensorflow/dataset.html"},
      
      
      {"title": "Tensorflow 未归档知识点", "text": "    Table   of   Contents           GFile           AUC 计算           tf . nn . embedding _ lookup _ sparse                 GFile       TensorFlow   提供 GFile   API , 用于 操作 文件系统 , 包括 本地 文件 , google 云 存储     gs : / /   , 以及 HDFS     hdfs : / /   。           File   I / O   wrappers   without   thread   locking                   text _ dir       =       &# 39 ; hdfs : / / user / xxx / data / dir &# 39 ;       file       =       tf     .     gfile     .     Open     (     text _ dir     ,       &# 39 ; w &# 39 ;     )       s       =       &# 39 ; hello   world ! &# 39 ;       file     .     write     (     s     )       file     .     close     ( )                 AUC 计算       Tensorflow   提供 AUC 计算 的 方法   tf . contrib . metrics . streaming _ auc ( predict ,   label )     或者     tf . metrics . streaming _ auc ( label ,   predict )         tf . nn . embedding _ lookup _ sparse       如果 输入 样本 有 空值 , 那么 会 出现 shape 不 匹配 错误 。 当 输入 存在 特征 idx 和 val 都 是 空 的 样本 , 那么 就 会 出现 这个 问题 。 此时 应该 补 0 !             InvalidArgumentError   ( see   above   for   traceback ) :   Incompatible   shapes :   [ 17 , 1 ]   vs .   [ 32 , 1 ]      ", "tags": "tensorflow", "url": "/wiki/tensorflow/messy.html"},
      
      
      {"title": "Tensorflow 简介", "text": "    Table   of   Contents           关于           安装           学习 资料                 关于       从 TensorFlow 问世 开始 , 就 开始 关注 TensorFlow 了 。 但是 一直 没有 系统地 学习 , 这里 记录 一下 TensorFlow 的 笔记 。       安装       现在 直接 用 pip 安装 即可 , 在 国内 可以 使用   USTC   或者   THU   的 镜像 。               #   不 使用 镜像     pip   install   tensorflow       #   使用 USTC 的 镜像     pip   install   - i   https : / / mirrors . ustc . edu . cn / pypi / web / simple     tensorflow               学习 资料           官方 教程     https : / / www . tensorflow . org / tutorials /         中文 教程     http : / / www . tensorfly . cn /         TensorFlow   开发 模板     https : / / github . com / tobegit3hub / tensorflow _ template _ application        ", "tags": "tensorflow", "url": "/wiki/tensorflow/intro.html"},
      
      
      {"title": "Tensorflow 编译吐血历史", "text": "    Table   of   Contents           关于           安装 Java8           编译 Bazel           编译 TensorFlow           运行 测试                 关于       因为 要 在 CentOS6 中 运行 TensorFlow , 但是 TensorFlow 依赖 的 Glibc 版本 都 比较 高 , 所以 只能 从 源码 编译 , 蛋疼 !       因为 编译 TensorFlow 需要 bazel , 而 bazel 官方 编译 好 的 包 也 需要 CentOS7 , 所以 bazel 也 需要 从 源码 编译 !       bazel   需要 Java8 , 所以 如果 没有 安装 Java8 的 还 需要 先 安装 Java8       安装 Java8           去 Oracle 官网 下 一个 :   https : / / www . oracle . com / technetwork / java / javase / downloads / jdk8 - downloads - 2133151 . html               或者 用 我 的 百度 云 镜像     https : / / pan . baidu . com / s / 161G0d3GYsw5dY6En4SHpjA         下载 完 解压 即可 , 然后 配置 JAVA _ HOME 环境变量 , 鉴于 你 的 系统 可能 有 多个 JAVA 版本 , 建议 你 只 在 脚本 中 或者 在 命令行 中 临时 设置 这个 变量                   export       JAVA _ HOME     =   path - to - java - directory     export       PATH     =   path - to - java - directory / bin :   $ PATH                 我 的 做法 是 将 上述 保存 到 env . sh 中 , 然后 在   compile . sh   中 加载     source   env . sh         通过     java   - version     确认 当前 使用 的 是 刚 下载 的 java8 , 而 不是 系统 的 java7 等 版本 。       编译 Bazel       参考 :   https : / / docs . bazel . build / versions / master / install - compile - source . html         设置 好 JAVA8 环境变量 之后 ,   GCC 版本 也 不能 太低 , 安装 高 版本 GCC6 . 0       执行 编译 命令     . / compile . sh   , 然后 发现错误 :             bazel - out / host / bin / third _ party / protobuf / 3.4 . 0 / js _ embed :   / usr / lib64 / libstd c++ . so . 6 :   version     `   GLIBCXX _ 3.4 . 21   &# 39 ;     not   found     (   required   by   bazel - out / host / bin / third _ party / protobuf / 3.4 . 0 / js _ embed   )                 参考 文章   https : / / www . jianshu . com / p / 6f9652a9410d   将 libstd c++ 改为 静态 链接             tools / cpp / unix _ cc _ configure . bzl : 222         & quot ; - lstd c++ & quot ;             改为                       & quot ; - l : libstd c++ . a & quot ;                 编译 TensorFlow           运行 配置   . / configure         编译 pip 包     bazel   build   - - config = opt   / / tensorflow / tools / pip _ package : build _ pip _ package             配置 的 时候 , OPENCL 关闭 , jemalloc 也 要 关闭 , 将 所有 都 关闭 , 哈哈       编译 时 增加   - lrt   参数 , 参考     https : / / www . cnblogs . com / lasclocker / p / 9578623 . html         运行 测试       将 上述 编译 好 的 三个 文件     libtensorflow _ framework . so ,   libtensorflow . jar ,   libtensorflow _ jni . so     复制到 一个 目录 中 , 然后 创建 测试代码     HelloTF . java                 import       org . tensorflow . Graph     ;       import       org . tensorflow . Session     ;       import       org . tensorflow . Tensor     ;       import       org . tensorflow . TensorFlow     ;         public       class       HelloTF       {           public       static       void       main     (     String     [ ]       args     )       throws       Exception       {               try       (     Graph       g       =       new       Graph     ( ) )       {                   final       String       value       =       & quot ; Hello   from   & quot ;       +       TensorFlow     .     version     ( ) ;                     / /   Construct   the   computation   graph   with   a   single   operation ,   a   constant                   / /   named   & quot ; MyConst & quot ;   with   a   value   & quot ; value & quot ; .                   try       (     Tensor       t       =       Tensor     .     create     (     value     .     getBytes     (     & quot ; UTF - 8 & quot ;     ) ) )       {                       / /   The   Java   API   doesn &# 39 ; t   yet   include   convenience   functions   for   adding   operations .                       g     .     opBuilder     (     & quot ; Const & quot ;     ,       & quot ; MyConst & quot ;     ) .     setAttr     (     & quot ; dtype & quot ;     ,       t     .     dataType     ( ) ) .     setAttr     (     & quot ; value & quot ;     ,       t     ) .     build     ( ) ;                   }                     / /   Execute   the   & quot ; MyConst & quot ;   operation   in   a   Session .                   try       (     Session       s       =       new       Session     (     g     ) ;                             Tensor       output       =       s     .     runner     ( ) .     fetch     (     & quot ; MyConst & quot ;     ) .     run     ( ) .     get     (     0     ) )       {                       System     .     out     .     println     (     new       String     (     output     .     bytesValue     ( ) ,       & quot ; UTF - 8 & quot ;     ) ) ;                   }               }           }       }                 编译     javac   - cp   libtensorflow . jar   HelloTF . java         执行     java   - cp   libtensorflow . jar : .         - Djava . library . path = .     HelloTF     如果 加载 so 失败 , 可以 增加 参数     - Dorg . tensorflow . NativeLibrary . DEBUG = 1    ", "tags": "tensorflow", "url": "/wiki/tensorflow/compile.html"},
      
      
      
      
      
        
      
      {"title": "atom使用技巧", "text": "    Table   of   Contents          ", "tags": "tools", "url": "/wiki/tools/atom.html"},
      
      
      {"title": "bash使用技巧", "text": "    Table   of   Contents           例子           基本 语法                 例子       提示符 修改               export       PS1     =     &# 39 ; \ \ u @ \ \ h   \ \ w   \ \ $   &# 39 ;       (   普通 示例   )       export       PS1     =     &# 39 ; \ \ [ \ \ e [ 0 ; 32m \ \ ] [ \ \ u @ \ \ h   \ \ w   \ \ $ ] \ \ [ \ \ e [ m \ \ ] &# 39 ;       (   颜色 示例   )       export       PS1     =     &# 39 ; \ \ t :   &# 39 ;       (   时间 示例   )       export       PS1     =     &# 39 ; \ \ u @ \ \ h   [ \ \ $ ( ls   |   wc   - l ) ] : \ \ $   &# 39 ;       (   显示 当前目录 行下 文件 数量   )                 基本 语法           流控制   if   ...   elif   ...   else                   if     condition     then             statements     [     elif     condition             then     statements .   ..   ]       [     else             statements     ]       fi                 其中 condition 可以 是 一个 命令 ， 也 可以 是 一个 条件 表达式 ， 其中 条件 判断 有 很多 细节 。           for   循环 。                   ##   use   list       for     i   in     1       2       3       5       6       do               echo       $ i       done         for     i   in     {     1   , 3 , 6 , 7   }       do               echo       $ i       done         ##   use   seq   command       for     i   in     $ (   seq     1       2       20     )       do               echo       $ i       done         ##   C - style       for       ( (     c     =     1     ;     c & lt ;   =     5     ;     c++   ) )       do               echo       $ i       done         ##   use   file       for     f   in   / etc / *     do               echo     f     done        ", "tags": "tools", "url": "/wiki/tools/bash.html"},
      
      
      {"title": "CentOS 上搭建PPTP VPN", "text": "    Table   of   Contents           关于           过程                 关于       记录 PPTP   VPN   搭建 流程 ， 亲测 成功 。     我 是 参考 这个 地方   http : / / www . dabu . info / centos6 - 4 - structures - pptp - vpn . html         过程           安装 ppp 和 iptables                 yum   install   - y   perl   ppp   iptables                   安装   pptpd           首先 查看 ppp 版本 ， 不同 版本 对应 的 pptpd 版本 也 不 一样             yum   list   installed   ppp               找到 对应 的 ppp 版本 :   http : / / poptop . sourceforge . net / yum / stable / packages /               ppp   2.4 . 4 — — — — — — & gt ; pptpd   1.3 . 4   ppp   2.4 . 5 — — — — — — & gt ; pptpd   1.4 . 0               我 的 电脑 是 i686 ,   2.4 . 5 版本 ， 所以 找到 对应 的 pptpd 版本 链接 为     http : / / poptop . sourceforge . net / yum / stable / packages / pptpd - 1.4 . 0 - 1 . fc12 . i686 . rpm         推荐 用   rpm   安装             rpm   - Uvh   http : / / poptop . sourceforge . net / yum / stable / packages / pptpd - 1.4 . 0 - 1 . fc12 . i686 . rpm   yum   install   pptpd               手动 安装 参考 原始 链接 。           修改 配置 pptpd                 cp   / etc / ppp / options . pptpd   / etc / ppp / options . pptpd . bak   vi   / etc / ppp / options . pptpd               添加 DNS 解析             ms - dns   8.8 . 8.8   ms - dns   8.8 . 4.4                   配置 用户 与 密码 ：                 cp   / etc / ppp / chap - secrets       / etc / ppp / chap - secrets . bak   vi   / etc / ppp / chap - secrets               加入 一行 ， 空格 隔开 ， 注意 后面 的 * 号             你 的 登陆 用户名   pptpd   你 的 登陆密码   *                   配置 pptpd   ip 转发                 cp   / etc / pptpd . conf           / etc / pptpd . conf . bak   vi   / etc / pptpd . conf               加入 下面 两行 ， 并 以 空行 结尾 这个 文件 。 下面 的 代码 不 需要 更改 其中 的 IP             localip   192.168 . 9.1   remoteip   192.168 . 9.11 - 30   / / 表示 vpn 客户端 获得 ip 的 范围                   配置 流量 转发                 vi   / etc / sysctl . conf               将   net . ipv4 . ip _ forward   =   0     改成     net . ipv4 . ip _ forward   =   1         保存 修改     / sbin / sysctl   - p             启动 pptpd 服务                 / sbin / service   pptpd   start     #   或者   service   pptpd   start                 到 此 ， 可以 测试 pptp 拨号 了 ， 应该 可以 成功 拨号 ， 用户名 和 密码 填 上面 配置 的 用户名 和 密码 ！     但是 还 没有 网络 访问 权限 。           配置 网络流量 转发                 iptables   - t   nat   - A   POSTROUTING         - s   192.168 . 9.0 / 24   - j   SNAT   - - to - source     你 的 服务器 公网 IP               保存 转发 规则 ， 并 重启 服务             / etc / init . d / iptables   save   / sbin / service   iptables   restart   service   pptpd   restart               至此 ， 就 可以 使用   VPN   科学 上网 了 ！           设置 开机 启动                 chkconfig   pptpd   on   chkconfig   iptables   on                   如果 你 嫌 安装 麻烦 ， 可以 试试   shadesocks ,   openvpn 等 易用 的 VPN 方案 。      ", "tags": "tools", "url": "/wiki/tools/vpn.html"},
      
      
      {"title": "Electron 开发跨平台应用程序", "text": "    Table   of   Contents           关于           快速 入门           基本概念                 关于           项目 地址 :     https : / / github . com / sindresorhus / awesome - electron         文档 :     https : / / electronjs . org / docs / tutorial / quick - start         你 所 见到 的 很多 APP 都 用 的 是 这个 工具 开发 的 , 比如   Atom ,   VS   Code ,   大象   etc           快速 入门               参考               https : / / electronjs . org / docs / tutorial / quick - start           https : / / electronjs . org / docs / tutorial / first - app                     安装 ,   需要 先 安装   node   ,   和     npm                 基本概念               const       electron       =       require     (     &# 39 ; electron &# 39 ;     )       const       {       app     ,       BrowserWindow       }       =       require     (     &# 39 ; electron &# 39 ;     )                   electron     包含 两个 对象     app     和     BrowserWindow   ,     app     可以 用来 管理应用程序 的 生命周期 ,     BrowserWindow     用来 创建 窗口 。               / / 创建 窗口 对象       function       createWindow       ( )       {           / /   Create   the   browser   window .           win       =       new       BrowserWindow     ( {       width     :       800     ,       height     :       600       } )             / /   and   load   the   index . html   of   the   app .           win     .     loadFile     (     &# 39 ; index . html &# 39 ;     )       }         / /   将 创建 窗口 动作 注册 到 app 完成 初始化 之后       app     .     on     (     &# 39 ; ready &# 39 ;     ,       createWindow     )        ", "tags": "tools", "url": "/wiki/tools/electron.html"},
      
      
      {"title": "Flink", "text": "    Table   of   Contents           关于           基本概念           编程 模型           分布式 运行 时 环境           DataStream   API                   快速 入门           Debugging                   scala   API           flink   vs   storm           算子           Physical   partitioning           Task   chaining   and   resource   groups                   Windows           JOIN           ProcessFunction           异步 IO 操作           Streaming   Connectors           Tabel   API           参考 链接                 关于       实时 数据处理 利器 。       基本概念       编程 模型           不同 的 抽象 级别       stateful   streaming :   通过   process   function   嵌入 到     DataStream     API   中 。 提供 event 自由 处理 能力 。         DataStream     和     DataSet     API 。 绝大多数 应用 关注 这个 级别 的 API 即可 , 关注 数据 的 变换 , 聚合 , JOIN         Table   API     扩展 关系数据库 ,   类似 于 HIVE 表         SQL     和 table   api 类似 , 但是 是 用 SQL 来 表达                               编程 和 数据流       基本 block 是     stream     和     transformations   。   transformations   输入 一个 或 多个 流 , 输出 一个 或 多个 流 , 类似 于 Spark 的 变换 算子 。       每 一个 数据流 从   source   开始 ,   到     sink     结束                               Parallel   Dataflows       stream 可以 有 两种 变换 数据 的 模式 :   one - to - one ,   redistributing       one - to - one :   从 source 到   map ( )   操作 之间 , 保持数据 分片 和 时间 顺序       redistributing :   从   map ( )   到   keyBy / window   操作 之间 。 不 保持 分片 和 时间 顺序 , 类似 于 Spark 的 shuffle                                           Windows           时间 驱动 :   每 30s       数据 驱动 :   每 300 个 样本       窗 的 类型 :       tumbling   windows ,   没有 重叠 , 滚动 窗       sliding   windows ,   有 重叠 , 滑动 窗       session   windows ,   用 没有 行为 的 时间 区间 来 截断                           Time           Event   Time ,   事件 创建 的 时间       Ingestion   time ,   事件 进入 到 flink   dataflow 的 时间       Processing   Time ,   事件 被 处理 的 时间 点                                   Time   Characteristic           是否 指定 时间 戳       使用 哪 种 时间                   Event   Time   and   Watermarks           event   time   事件 时间 ,   数据 不 一定 按照 事件 时间 顺序 过来       watermark   告诉 operator 到 接受 到 当前 watermark 的 时候 , 之后 不再 有 该 事件 之前 的 数据 了       Periodic   Watermarks     AssignerWithPeriodicWatermarks                                   source   functions   with   Timestamps   and   Watermarks           源中 直接 设置 时间 戳 和 发送 watermark       使用   timestamp   assigner   设置 时间 戳 和 水印                   Timestamp   Assigners   /   Watermark   Generators           在 数据源 后面 增加       预定 义 的 assigner             stream . assignAscendingTimestamps (   _. getCreationTime   )         固定 延迟     stream . assignTimestampsAndWatermarks ( new   BoundedOutOfOrdernessTimestampExtractor [ MyEvent ] ( Time . seconds ( 10 ) ) (   _. getCreationTime   ) )                             Stateful   Operations   有 状态 的 操作           需要 cache 多个 事件 才能 操作       通过   keyBy   实现 ,   实际上 是 维持 一个 key / value 存储       保证 后续 操作 只 跟 这个   key   有关 , 不用 考虑 其他   key   数据 , 让 flink 分配 key 的 操作 是 透明 的                               Checkpoints   for   Fault   Tolerance   容错       stream   replay   和   checkpointing       保留 操作 的 状态 , 可以 从 checkpoint 通过 回放 的 方式 , 重新 消费 数据               Batch   on   Streaming       有限 的 流         DataSet     API                   分布式 运行 时 环境           JobManagers ( maters )   :   规划 任务 、 协调 checkpoint 和 恢复 。       至少 有 一个 ,   可以 有 多个 来 备份               TaskManagers ( workers )   :   执行 具体任务 ,   缓存 状态 ,   改变 数据流       Client   :   类似 于 Spark 的 client , 用于 汇报 进展       部署 模式       standalone   cluster       YARN ,   Mesos                               task   slots   :   一个 slot 是 一个 线程                       slot   sharing                           State   Backends :             内存 中 的 一个 hash   map       rocksDB                   Savepoints :   用户 手动 触发 ,   checkpoint   是 自动 触发               DataStream   API             aggregate     函数                         快速 入门           wordcount                   object       WikipediaAnalysis       {               def       main     (     args     :       Array     [     String     ] )       {                       val       env       =       StreamExecutionEnvironment     .     getExecutionEnvironment                       val       edits       =       env     .     socketTextStream     (     & quot ; localhost & quot ;     ,       1025     )                       val       result       =       edits     .     flatMap     (     line       = & gt ;       line     .     split     (     & quot ; \ \ \ \ s +& quot ;     ) )                               .     map     ( (     _     ,       1     ) ) .     keyBy     (     0     )                               .     timeWindow     (     Time     .     seconds     (     5     ) )                               .     sum     (     1     )                       result     .     print     ( )                         env     .     execute     ( )                 }       }                 创建 流     nc   - l   1025     netcat               数据源             sourceFunction   ,     ParallelSourceFunction   ,     RichParallelSourceFunction         文件 源         readTextFile ( path )           readFile ( fileInputFormat ,   path )           readFile ( fileInputFormat ,   path ,   watchType ,   interval ,   pathFilter )                 socket         socketTextStream                 集合         fromCollection ( Seq )     iterator         fromElements ( elements :   _ * )           fromParallelCollection ( SplittableIterator )           generateSequence ( from ,   to )                 Custom         FlinkKafkaConsumer08                             流 的 变换               sinks             writeAsText           writeAsCsv           print / printToErr           writeUsingOutputFormat ( )   /   FileOutputFormat           writeToSocket           addSink     自定义 的 sink                   iterators ,     IterativeStream                 Execution   Parameters 。     https : / / ci . apache . org / projects / flink / flink - docs - release - 1.8 / dev / execution _ configuration . html               env . getConfig                     Fault   Tolerance           Controlling   Latency         env . setBufferTimeout ( timeoutMillis )     默认 是 100ms ,   即使 没有 满 也 会 将 buff 发送 出去                   Debugging           本地 执行 环节 ,   可以 直接 从 IDE 执行 , 设置 断点 。   StreamExecutionEnvironment . createLocalEnvironment         Collection   Data   Sources 。 利用     env . fromElements       env . fromCollection     从 序列 创建 流 , 测试       Iterator   Data   Sink .     DataStreamUtils . collect                     import       org . apache . flink . streaming . experimental . DataStreamUtils       import       scala . collection . JavaConverters . asScalaIteratorConverter         val       myResult     :       DataStream     [ (     String   ,     Int     ) ]       =       ...       val       myOutput     :       Iterator     [ (     String   ,     Int     ) ]       =       DataStreamUtils     .     collect     (     myResult     .     javaStream     ) .     asScala                 scala   API           增加 依赖                   & lt ; ! - -   https : / / mvnrepository . com / artifact / org . apache . flink / flink - scala   - - & gt ;       & lt ; dependency & gt ;               & lt ; groupId & gt ;   org . apache . flink   & lt ; / groupId & gt ;               & lt ; artifactId & gt ;   flink - scala _ 2.11   & lt ; / artifactId & gt ;               & lt ; version & gt ;   $ { flink . version }   & lt ; / version & gt ;               & lt ; scope & gt ;   provided   & lt ; / scope & gt ;       & lt ; / dependency & gt ;         & lt ; ! - -   https : / / mvnrepository . com / artifact / org . apache . flink / flink - streaming - scala   - - & gt ;       & lt ; dependency & gt ;               & lt ; groupId & gt ;   org . apache . flink   & lt ; / groupId & gt ;               & lt ; artifactId & gt ;   flink - streaming - scala _ 2.11   & lt ; / artifactId & gt ;               & lt ; version & gt ;   $ { flink . version }   & lt ; / version & gt ;               & lt ; scope & gt ;   provided   & lt ; / scope & gt ;       & lt ; / dependency & gt ;                     导入 基础 包                   import       org . apache . flink . api . scala ._       import       org . apache . flink . streaming . api . scala ._         / /   偏 函数 支持       import       org . apache . flink . streaming . api . scala . extensions ._                     scala   偏 函数 支持 ,   偏 函数 支持 flink 提供 了 额外 的 API ,     xxxWith   。     https : / / ci . apache . org / projects / flink / flink - docs - stable / dev / scala _ api _ extensions . html             flink   vs   storm           flink   有 状态 ,   而 storm 无 状态 , 需要 自己 管理 状态       flink   支持 窗口 ,   而 storm 不 支持 。 统计 特征 啊           算子             map           flatMap           filter           keyBy         DataStream   →   KeyedStream         reduce     KeyedStream   →   DataStream         fold     KeyedStream   →   DataStream ,     keyedStream . fold ( \" start \" ) ( ( str ,   i )   = & gt ;   {   str   +   \" - \"   +   i   } )           Aggregations   ,     min   ,     max   ,     sum     KeyedStream   →   DataStream         window   ,   KeyedStream   →   WindowedStream         windowAll   ,   DataStream   →   AllWindowedStream         Window   Apply       apply   ,   WindowedStream   →   DataStream ;   AllWindowedStream   →   DataStream               Window   Reduce   ,     reduce   ,   WindowedStream   →   DataStream         Window   Fold   ,   WindowedStream   →   DataStream         Aggregations   on   windows   ,   WindowedStream   →   DataStream         Union     DataStream *   →   DataStream         Window   Join     DataStream , DataStream   →   DataStream         Window   CoGroup   ,   DataStream , DataStream   →   DataStream             Connect   ,   两个 流 共享 状态 。 DataStream , DataStream   →   ConnectedStreams               CoMap ,   CoFlatMap   ,   ConnectedStreams   →   DataStream         Split   ,   将 一个 流 分为 多个 ,   DataStream   →   SplitStream         Select   ,   从 splitstream   中 选择 一个 。 SplitStream   →   DataStream         Iterate   ,   实现 迭代 计算 ,   DataStream   →   IterativeStream   →   DataStream         Extract   Timestamps   ,   抽取 时间 戳 ,   DataStream   →   DataStream         Project   ,   从 tuple 中 提取 部分 字 段 ,   scala 可以 用 模式匹配 , 但 要 加上 扩展     https : / / ci . apache . org / projects / flink / flink - docs - release - 1.8 / dev / scala _ api _ extensions . html             Physical   partitioning           自定义 分区 ,     dataStream . partitionCustom         随机 分区 ,     dataStream . shuffle ( )         重 平衡 , 用于 解决 不同 分区 数据 倾斜 的 问题 ,     dataStream . rebalance         在 单机 上 增加 或 缩小 分片     dataStream . rescale         广播 变量 到 所有 分片     dataStream . broadcast             Task   chaining   and   resource   groups           手动 维护 任务 链 , 构建 任务 分组 ( slot )       创建 新链 ,     someStream . filter ( ... ) . map ( ... ) . startNewChain ( ) . map         禁止 任务 链 ,     someStream . map ( ... ) . disableChaining ( )         设置 slot 分组 ,     someStream . filter ( ... ) . slotSharingGroup ( \" name \" )   ,   默认 的 分组 名是   default             Windows           窗 的 生命周期       窗口 创建 于 第一个 事件 到来 的 时候 , 结束 时间 是   窗口 结束 时间   +   指定 的 时延               每 一个 窗 都 有 一个     trigger     和 一个 窗 函数   ProcessWindowFunction ,   ReduceFunction ,   AggregateFunction   or   FoldFunction     与 之 关联         trigger     指定 什么 时候 应用 窗 函数         Evictor     可以 在 触发 之后 , 移除 窗口 中 的 一些 元素               Keyed   vs   Non - Keyed   Windows       通过     keyBy     来 划分 key , 变成 逻辑 上 的 分 key 的 流 。 ( 逻辑 上 ? ? )       所有 相同 key 的 数据 被 发送到 同一个 task ,   而 不分 key 会 使 所有 的 数据 发送到 同一个 task               Window   Assigners       定义 如何 将 元素 关联 到 window 。 在     window ( ... )     和     windowAll ( )     中 指定       预定 义   window   assigner       tumbling   windows   滚动 窗 , 时间 无 重叠 切分       固定 大小 ,   没有 重叠           TumblingEventTimeWindows . of ( Time . seconds ( 5 ) )           TumblingEventTimeWindows . of ( Time . days ( 1 ) ,   Time . hours ( - 8 ) )     第二个 参数 是 offset , 中国 位于 东 8 区 , 所以 加 8 小时 offset               sliding   windows   滑动 窗       固定 大小 ,   有 重叠         SlidingEventTimeWindows . of ( Time . seconds ( 10 ) ,   Time . seconds ( 5 ) )     大小 为 10s , 滑动 距离 为 5s 的 滑动 窗         SlidingProcessingTimeWindows . of ( Time . hours ( 12 ) ,   Time . hours ( 1 ) ,   Time . hours ( - 8 ) )                 session   windows   用 两次 事件 时间 间隔 超过 某个 阈值 进行 划分       不 重叠 ,   没有 固定 的 起止 时间 。 通过 固定 时间 间隔 ( session   gap ) 没有 收到 数据 来 划分 窗口         EventTimeSessionWindows . withGap ( Time . minutes ( 10 ) )     使用 event 时间         ProcessingTimeSessionWindows . withGap ( Time . minutes ( 10 ) )     使用 处理 时间         EventTimeSessionWindows . withDynamicGap     自定义 gap               global   windows       需要 自己 定义     trigger   ,   否则 这个 窗口 不会 结束 ,   因此 也 不会 做 任何 事情         GlobalWindows . create ( )                 自定义 :   实现     WindowAssigner     类                           Window   Functions           ReduceFunction   增量 聚合         . reduce   {   ( v1 ,   v2 )   = & gt ;   ( v1 ._ 1 ,   v1 ._ 2   +   v2 ._ 2 )   }                     AggregateFunction   增量 聚合           通用 的 reduce 函数 ,   3 个 类型 ,   输入 类型 IN , 累积 类型 ACC , 输出 类型 OUT       AggregateFunction   接口 的 函数 用途         createAccumulator     创建 ACC , 返回 ACC 的 初始值         add     将 新 的 元素 加到 ACC 中         getResult     获取 最终 结果         merge     合并 两个 中间 的 ACC                   ` ` ` scala     class   AverageAggregate   extends   AggregateFunction [ ( String ,   Long ) ,   ( Long ,   Long ) ,   Double ]   {         override   def   createAccumulator ( )   =   ( 0L ,   0L )       override   def   add ( value :   ( String ,   Long ) ,   accumulator :   ( Long ,   Long ) )   =             ( accumulator ._ 1   +   value ._ 2 ,   accumulator ._ 2   +   1L )       override   def   getResult ( accumulator :   ( Long ,   Long ) )   =   accumulator ._ 1   /   accumulator ._ 2       override   def   merge ( a :   ( Long ,   Long ) ,   b :   ( Long ,   Long ) )   =             ( a ._ 1   +   b ._ 1 ,   a ._ 2   +   b ._ 2 )     }       ` `           -   FoldFunction   高阶 函数   -   . fold ( \" \" )   {   ( acc ,   v )   = & gt ;   acc   +   v ._ 2   } `             -   ProcessWindowFunction     -   这个 函数 会先 缓存 所有 ,   尽量 用 其他 聚合 函数 , 可以 增量 聚合             -   windowfunction                       triggers           每 一个     WindowAssigner     都 有 一个 默认 的 trigger       自定义 trigger     trigger ( ... )           onElement ( )     在 每个 元素 被 加到 窗口 后 调用         onEventTime ( )     当 注册 一个 事件 时间 触发 的 时候 调用         onProcessingTime ( )     当 注册 处理 时间 的 时候 调用         onMerge     合并 状态         clear ( )     窗口 被 移除 的 时候 调用                   evictor             evictBefore     在 窗口 函数 之前 调用         evictAfter     在 窗口 函数 之后 调用         CountEvictor     保留 用户 定义 数目 的 元素 , 丢弃 其他 的         DeltaEvictor     到 最后 一个 元素 的 时间 区间 , 只 保留 低于 阈值 的 原始         TimeEvictor     移除 超过 一定 区间 的               Allowed   Lateness           JOIN           实现 流 的 JOIN 操作 ,   实现   inner - join ,   除了 key 相同 , 还 要求 在 同一个 窗口 内                   stream     .     join     (     otherStream     )               .     where     ( & lt ;     KeySelector     & gt ; )               .     equalTo     ( & lt ;     KeySelector     & gt ; )               .     window     ( & lt ;     WindowAssigner     & gt ; )               .     apply     ( & lt ;     JoinFunction     & gt ; )                     Interval   Join       JOIN   A 和 B 两个 流 , 除了 key 相同 , 还 要求 相对 时间 间隔 在 一定 范围 内         . intervalJoin ( greenStream . keyBy ( elem   = & gt ;   / *   select   key   * / ) ) . between ( Time . milliseconds ( - 2 ) ,   Time . milliseconds ( 1 ) )         指定 上限 和 下限                   ProcessFunction           low - level   流 处理 操作       events   流 的 元素       state   容错 、 一致性 、 只有 keyedstream 有       timers   事件 时间 、 处理 时间 、 只有 keyedstream 有       可以 看做     FlatMapFunction     +   可以 访问 keyed   state   和   timers       每 一个 事件 接受 到 的 时候 被 调用       通过     RuntimeContext     访问   keyed   state         stream . keyBy ( ... ) . process ( new   MyProcessFunction ( ) )                     Low - level   Joins             CoProcessFunction                     Timer   Coalescing           降低 时间 分辨率                   异步 IO 操作           访问 外部 系统 ( 例如 外部 存储 ) ,   原始 的 访问 方式 如 在     MapFunction     中 访问 将会 发送 同步 请求 , 这 导致 接口 调用 占用 大量 的 处理 时间       异步 实际上 是 并发 请求       KV 存储 一般 存在 异步 请求 客户端       需要 实现       实现   AsyncFunction   发送 请求         callback   处理 返回 的 结果       应用 异步 IO 操作 到   DataStream         Timeout     超时 时间 ,   默认 会 抛出 异常 ,   任务 重启 ,   通过 重载     AsyncFunction # timeout     方法 处理 异常         Capacity     并发 量                       结果 顺序       无序 ,   通过     AsyncDataStream . unorderedWait     调用 , 低 延时 , 低 overhead       有序 ,   通过     AsyncDataStream . orderedWait     调用                           / * *         *   An   implementation   of   the   &# 39 ; AsyncFunction &# 39 ;   that   sends   requests   and   sets   the   callback .         * /       class       AsyncDatabaseRequest       extends       AsyncFunction     [     String   ,     (     String   ,     String     ) ]       {                 / * *   The   database   specific   client   that   can   issue   concurrent   requests   with   callbacks   * /               lazy       val       client     :       DatabaseClient       =       new       DatabaseClient     (     host     ,       post     ,       credentials     )                 / * *   The   context   used   for   the   future   callbacks   * /               implicit       lazy       val       executor     :       ExecutionContext       =       ExecutionContext     .     fromExecutor     (     Executors     .     directExecutor     ( ) )                   override       def       asyncInvoke     (     str     :       String     ,       resultFuture     :       ResultFuture     [ (     String   ,     String     ) ] )     :       Unit       =       {                         / /   issue   the   asynchronous   request ,   receive   a   future   for   the   result                       val       resultFutureRequested     :       Future     [     String     ]       =       client     .     query     (     str     )                         / /   set   the   callback   to   be   executed   once   the   request   by   the   client   is   complete                       / /   the   callback   simply   forwards   the   result   to   the   result   future                       resultFutureRequested     .     onSuccess       {                               case       result     :       String       = & gt ;       resultFuture     .     complete     (     Iterable     ( (     str     ,       result     ) ) )                       }               }       }         / /   create   the   original   stream       val       stream     :       DataStream     [     String     ]       =       ...         / /   apply   the   async   I / O   transformation       val       resultStream     :       DataStream     [ (     String   ,     String     ) ]       =               AsyncDataStream     .     unorderedWait     (     stream     ,       new       AsyncDatabaseRequest     ( ) ,       1000     ,       TimeUnit     .     MILLISECONDS     ,       100     )                 Streaming   Connectors           当前 支持 的 系统       Apache   Kafka   ( source / sink )       Apache   Cassandra   ( sink )       Amazon   Kinesis   Streams   ( source / sink )       Elasticsearch   ( sink )       Hadoop   FileSystem   ( sink )       RabbitMQ   ( source / sink )       Apache   NiFi   ( source / sink )       Twitter   Streaming   API   ( source )               Connectors   in   Apache   Bahir       Apache   ActiveMQ   ( source / sink )       Apache   Flume   ( sink )       Redis   ( sink )       Akka   ( sink )       Netty   ( source )               容错       Kafka         FlinkKafkaConsumer08         构造函数       topic   name   或   topic   name 列表       序列化 方式   DeserializationSchema   /   KafkaDeserializationSchema       解析 从 kafka 中来 的 数据         DeserializationSchema           T   deserialize ( byte [ ]   message )         预定 义 的 schema         TypeInformationSerializationSchema   ,     TypeInformationKeyValueSerializationSchema     基于 flink 的     TypeInformation           JsonDeserializationSchema     和     JSONKeyValueDeserializationSchema           AvroDeserializationSchema                         属性       bootstrap . servers       zookeeper . connect       group . id                       指定 消费 的 起始 位置       myConsumer . setStartFromEarliest ( )             / /   start   from   the   earliest   record   possible       myConsumer . setStartFromLatest ( )                 / /   start   from   the   latest   record       myConsumer . setStartFromTimestamp ( ... )     / /   start   from   specified   epoch   timestamp   ( milliseconds )       myConsumer . setStartFromGroupOffsets ( )     / /   the   default   behaviour                           Kafka   Consumers   and   Fault   Tolerance           对源 进行 checkpoint         env . enableCheckpointing ( 5000 )   / /   checkpoint   every   5000   msecs                     Kafka   Consumers   Topic   and   Partition   Discovery           设置 配置 属性     flink . partition - discovery . interval - millis         创建   consumer   时用 正则表达式 匹配 ,     new   FlinkKafkaConsumer08 [ String ] ( Pattern . compile ( \" test - topic - [ 0 - 9 ] \" ) ,   new   SimpleStringSchema ,   properties )                 Kafka   Producer                   val       stream     :       DataStream     [     String     ]       =       ...         val       myProducer       =       new       FlinkKafkaProducer011     [     String     ] (                       & quot ; localhost : 9092 & quot ;     ,                       / /   broker   list                       & quot ; my - topic & quot ;     ,                                   / /   target   topic                       new       SimpleStringSchema     )           / /   serialization   schema         / /   versions   0.10 +   allow   attaching   the   records &# 39 ;   event   timestamp   when   writing   them   to   Kafka ;       / /   this   method   is   not   available   for   earlier   Kafka   versions       myProducer     .     setWriteTimestampToKafka     (     true     )         stream     .     addSink     (     myProducer     )                         & lt ; dependency & gt ;           & lt ; groupId & gt ;   org . apache . flink   & lt ; / groupId & gt ;           & lt ; artifactId & gt ;   flink - connector - kafka _ 2.11   & lt ; / artifactId & gt ;           & lt ; version & gt ;   1.8 . 0   & lt ; / version & gt ;       & lt ; / dependency & gt ;                         side   outputs           在 主要 的 结果 stream 中 filter 出 满足 一定 规则 的 stream                   测试           单元测试       集成 测试 :   端到 端 测试 flink   pipeline         flink - test - utils _ 2.11                             Tabel   API           在 stream 上 定义 table       连续 query       将 启动 的 时候 收到 的 第一条 记录 到 当前 记录 看做 一个 有限 表 , 聚合 操作 在 这个 有限 表中 执行         SELECT   user ,   count ( 1 )   as   cnt   FROM   clicks   GROUP   BY   user                 窗 聚合         SELECT   user ,   TUMBLE _ END ( cTime ,   INTERVAL   ' 1 '   HOURS )   as   endT ,   COUNT ( 1 )   as   cnt   FROM   clicks   GROUP   BY   user ,   TUMBLE _ END ( cTime ,   INTERVAL   ' 1 '   HOURS )                             参考 链接             https : / / ci . apache . org / projects / flink / flink - docs - stable /           https : / / ci . apache . org / projects / flink / flink - docs - stable / concepts / programming - model . html        ", "tags": "tools", "url": "/wiki/tools/flink.html"},
      
      
      {"title": "Geohash 原理", "text": "    Table   of   Contents          ", "tags": "tools", "url": "/wiki/tools/geohash.html"},
      
      
      {"title": "Git常用命令汇总", "text": "    Table   of   Contents           关于           提交           打 tag           分支           . gitignore 文件           强大 的 分支           创建 分支           分支 合并           删除 分支           分支 管理           远程 分支                   Hook           常见 案例 汇总                 关于       这里 汇集 了 git 常用命令 和 用法 ， 便于 速查 ， 不定期 更新 。       提交           给   git   commit   加上   - a   选项 ， Git   就 会 自动 把 所有 已经 跟踪 过 的 文件 暂存 起来 一并 提交 ， 从而 跳过   git   add   步骤 ：           打 tag       将 当前 版本 添加 tag   & lt ; tagname & gt ;               git   tag   & lt ; tagname & gt ;               列出 标签             $   git   tag   v0 . 1   v1 . 3     $   git   tag   - l     &# 39 ; v1 . 4.2 . * &# 39 ;     v1 . 4.2 . 1   v1 . 4.2 . 2   v1 . 4.2 . 3   v1 . 4.2 . 4               将 tag 推送 到 远程 服务器 ， 参见 [ 1 - 2 ] ， 推送 所有 的 tags             git   push   - - tags               只 推送 单个 tag             git   push   origin   & lt ; tagname & gt ;                     http : / / stackoverflow . com / a / 5195913 / 4349983           https : / / git - scm . com / book / zh / v1 / Git - % E5 % 9F % BA % E7 % A1 % 80 - % E6 % 89 % 93 % E6 % A0 % 87 % E7 % AD % BE             分支               ##   创建 dev 分支     git   branch   dev     ##   切换 到 dev 分支     git   checkout   dev     ##   上述 两条 命令 也 可以 合并 为 一条     git   checkout   - b   dev       ##   在 dev 分支 作 修改 后 ， 再 切换 回 master 分支     git   checkout   master     ##   还 可以 创建 多个 其他 分支     git   checkout   - b   issu35   git   checkout   - b   issu37     ##   在 master 分支 中 ， 合并 dev 分支       ##   可 与 选择 只 合并 某 一个 分支     git   merge   issu35         ##   合并 后 ， 可以 删除 issu35 分支     git   branch   - d   issu35     ##   如果 合并 分支 时 ， 发生冲突 ， 解决 冲突 后 再 合并       ##   可以 通过 命令 查看 冲突 的 地方     git   status                 . gitignore   文件       文件   . gitignore   的 格式 规范 如下 ：           所有 空行 或者 以 注释 符号     ＃     开头 的 行 都 会 被   Git   忽略 。       可以 使用 标准 的     glob     模式匹配 。       匹配 模式 最后 跟 反 斜杠 （   /   ） 说明 要 忽略 的 是 目录 。       要 忽略 指定 模式 以外 的 文件 或 目录 ， 可以 在 模式 前 加上 惊叹号 （ ! ） 取反 。           所谓 的   glob   模式 是 指   shell   所 使用 的 简化 了 的 正则表达式 。           星号 （   *   ） 匹配 零个 或 多个 任意 字符 ；       [ abc ]   匹配 任何 一个 列 在 方括号 中 的 字符 （ 这个 例子 要么 匹配 一个   a ， 要么 匹配 一个   b ， 要么 匹配 一个   c ） ；       问号 （ ? ） 只 匹配 一个 任意 字符 ； 如果 在 方括号 中 使用 短 划线 分隔 两个 字符 ， 表示 所有 在 这 两个 字符 范围 内 的 都 可以 匹配 （ 比如   [ 0 - 9 ]   表示 匹配 所有   0   到   9   的 数字 ） 。           我们 再 看 一个   . gitignore   文件 的 例子 ：             #   此 为 注释   –   将 被   Git   忽略   #   忽略 所有   . a   结尾 的 文件   * . a   #   但   lib . a   除外   ! lib . a   #   仅仅 忽略 项目 根目录 下 的   TODO   文件 ， 不 包括   subdir / TODO   / TODO   #   忽略   build /   目录 下 的 所有 文件   build /   #   会 忽略   doc / notes . txt   但 不 包括   doc / server / arch . txt   doc / * . txt   #   ignore   all   . txt   files   in   the   doc /   directory   doc / * * / * . txt               A     * * /     pattern   is   available   in   Git   since   version   1.8 . 2 .       如果 不 小心 把 文件 加 到 缓存 区 ， 可以 先 通过 如下 命令 删除 ， 然后 再 将 模式 加 到   . gitignore   文件 中 。     使用   git   rm   的   - - cached   参数 。 后面 可以 列出 文件 或者 目录 的 名字 ， 也 可以 使用   glob   模式 。             git   rm   - - cached   readme . txt   git   rm   log /   \ \ *   . log               注意 到 星号   *   之前 的 反 斜杠     \ \   ， 因为   Git   有 它 自己 的 文件 模式 扩展 匹配 方式 ， 所以 我们 不用   shell   来 帮忙 展开 （ 译注 ： 实际上 不加 反 斜杠 也 可以 运行 ， 只不过 按照   shell   扩展 的话 ， 仅仅 删除 指定 目录 下 的 文件 而 不会 递归 匹配 。 上面 的 例子 本来 就 指定 了 目录 ， 所以 效果 等同 ， 但 下面 的 例子 就会用 递归 方式 匹配 ， 所以 必须 加反 斜杠 。 ） 。 此 命令 删除 所有   log /   目录 下 扩展 名为   . log   的 文件 。 类似 的 比如 ：             $   git   rm     \ \ *   ~               会 递归 删除 当前目录 及其 子目录 中 所有   ~   结尾 的 文件 。       强大 的 分支       创建 分支       创建   dev   分支 ：             git   branch   dev               git 的 分支 实际上 是 一个 指针 ， 指向 一个 提交 的 对象 ！     git 有 一个   HEAD   指针 ， 指向 当前 分支 指针 的 指针 ！     可以 通过   git   checkout   dev   命令 将   HEAD   指针 指向   dev   分支 。       可以 合并 创建 和 切换 分支 为 一条 指令   git   checkout   - b   dev   .       由于 git 创建 分支 非常 快 ， 只是 写入 42 字节 到 文件 ！ 所以 可以 频繁 地 使用 分支 ！       分支 合并       使用   git   merge   other - branch     合并 分支 :       fast - forward 模式 ： 如果 其中 当前 分支 是 另 一个 分支 的 上游 ， 那么 git 会 简单 的 将 当前 分支 对应 的 指针 移动 到 另 一个 分支 所 指向 的 commit 对象 ！     非常 快 ！       正常 合并 ： 当前 分支 不是 另 一个 分支 的 祖先 ， 那么 git 会 将 当前 分支 快照 和 另 一个 分支 的 快照 ， 以及 两者 最近 共同 祖先 ， 进行 三方 合并 ！     最后 创建 一个 新 的 快照 ， 将 当前 分支 指针 指向 它 ！       合并 冲突 ： 手动 修复 冲突 后 ， 重新 提交 ！       分 之 合并 的 图形化 工具 ：   git   mergetool         删除 分支       分支 的 删除 ：   git   branch   - d   one - branch         分支 管理           查看 当前 所有 分支 命令 ：   git   branch         查看 所有 分支 最后 一次 提交 ：   git   branch   - v   ,   - - merged , - - no - merged   参数 可以 指定 显示 合并 分支 和 未 合并 分支 ！           远程 分支       Hook       在     . git / hooks /     目录 下 的 可 执行 脚本 ， 支持   bash ,   ruby ,   python   等 。       常见 案例 汇总           已经 在 本地 有个 仓库 了 , 怎么 关联 到 远程 仓库 地址     &# 103 ; &# 105 ; &# 116 ; &# 64 ; &# 103 ; &# 105 ; &# 116 ; &# 104 ; &# 117 ; &# 98 ; &# 46 ; &# 99 ; &# 111 ; &# 109 ; &# 58 ; &# 120 ; &# 120 ; &# 120 ; &# 120 ; &# 47 ; &# 120 ; &# 120 ; &# 46 ; &# 103 ; &# 105 ; &# 116 ;                   git   remote   add   origin     git @ github . com : xxxx / xx . git               这里   origin   代表 远程 仓库 的 名字           如何 合并 远程 分支   branch1   到 本地 分支   branch2                   $ git     fetch   origin   branch1   From   github . com : tracholar / wiki     *   branch                         master           - & gt ;   FETCH _ HEAD       $ git     merge   FETCH _ HEAD               FETCH _ HEAD   是 一个 特殊 的 临时 分支 , 这 两个 操作 可以 合并 到 一个 操作             git   pull   origin   branch1               注意 , 以上 操作 都 是 在 本地 分支   branch2   上 操作 !           远程 仓库 太 大 , 只想 获取 master 分支 最后 一次 提交 的 代码 , 怎么 操作 ?                 git   clone   git @ github . com : xxxx / xx . git     - - depth   =     1     - - branch   =   master   - - single - branch                 - - depth = 1     表示 克隆 深度 为 1 , 即 只 克隆 最后 一次 提交 的 结果 ,     - - branch = master     表示 克隆 主 分支 ,   还要 加上     - - single - branch     参数 , 才 会 只 克隆 单一 分支 !       但是 ,   在   git   1.7 . 10 之前 的 版本 不 支持     - - single - branch     选项 , 可以 采用 如下 方式 ( 参考 so 上 问题   How   do   I   clone   a   single   branch   in   Git ?   )               #   创建 一个 用于 存放 代码 的 目录     mkdir   code _ dir     cd     code _ dir       #   初始化 一个 空 的 本地 git 仓库     git   init   .       #   将 本地 仓库 关联 上 远程 仓库     git   remote   add   origin   git @ git . xx . xx       #   拉取 远程 分支   branch1   到 本地 分支   branch1       git   fetch   origin   branch1 : branch1         #   如果 你 只 需要 最后 一次 提交 的 结果 , 同样 可以 加上   - - depth = 1   这个 参数     git   fetch   origin   branch1 : branch1   - - depth   =     1                     回滚 远程 代码                   #   回滚 本地 代码 到 特定 commit     git   reset   - - hard   & lt ; commit _ id & gt ;     #   强制 推送 到 远程     git   push   - f   origin   & lt ; branch & gt ;      ", "tags": "tools", "url": "/wiki/tools/git.html"},
      
      
      {"title": "go语言", "text": "    Table   of   Contents           安装 环境           基础           基础 语法                         安装 环境       只试 过 MAC   OS 环境 ， 所以 就 说 这个 ， 不同 环境 应该 差不多 。     安装 GO 运行 环境 ， 然后 配置 两个 环境变量 ｀ GOPATH ｀ 和 ｀ GOBIN ｀       基础       基础 语法           程序 入口 包是   main         导出 名 的 首字母 必须 为 大写 ！       批量 import 方式                   import       (               & quot ; fmt & quot ;               & quot ; math & quot ;       )                     函数 签名 方式 ， 可以 将 相同 类型 的 形参 类型 申明 合并                   func       add     (     x       int     ,       y       int     )       int       {               #     TODO               return       x     +     y       }                     函数 可以 返回 多值 。                   func       swap     (     x     ,       y       string     )       (     string     ,       string     )       {               return       y     ,       x       }                     赋值 语法   a   : =   \" hello   world . \"         命名 返回值 ： Go   的 返回值 可以 被 命名 ， 并且 就 像 在 函数 体 开头 声明 的 变量 那样 使用 。         返回值 的 名称 应当 具有 一定 的 意义 ， 可以 作为 文档 使用 。         没有 参数 的   return   语句 返回 各个 返回 变量 的 当前 值 。 这种 用法 被称作 “ 裸 ” 返回 。         直接 返回 语句 仅 应当 用 在 像 下面 这样 的 短 函数 中 。 在 长 的 函数 中 它们 会 影响 代码 的 可读性 。                   func       getsum     (     sum       int     )       (     x     ,       y       int     ) {               sum       =       x     +     y               return       }                     变量 声明 关键字   var   ， 类型 放在 最后 。   =   用于 初始化 变量 ， 初始化 使用         表达式 可以 省略 类型 。                   var       c     ,       python     ,       java       bool       var       i     ,       j       int       =       1     ,       2       var       ruby     ,       Go     ,       haskell       =       true     ,       false     ,       & quot ; helloword & quot ;                     短 声明 变量 ， 在 函数 中 ，     : =     简洁 赋值 语句 在 明确 类型 的 地方 ， 可以 用于 替代   var   定义 。       基本 数据类型                   bool         string         int         int8         int16         int32         int64       uint       uint8       uint16       uint32       uint64       uintptr         byte       / /   uint8   的 别名         rune       / /   int32   的 别名                 / /   代表 一个 Unicode 码         float32       float64         complex64       complex128                     零值     变量 在 定义 时 没有 明确 的 初始化 时会 赋值 为   零值   。     零值 是 ：       数值 类型 为   0   ，       布尔 类型 为   false   ，       字符串 为   \" \"   （ 空 字符串 ） 。               表达式   T ( v )   将值   v   转换 为 类型   T   。         与   C   不同 的 是   Go   的 在 不同 类型 之间 的 项目 赋值 时 需要 显式 转换 .       GO 类型 推导 ， 变量 的 类型 由右值 推导 得出 。                   func       main     ( )       {               v       : =       & quot ; HaHa & quot ;       / /   change   me !               fmt     .     Printf     (     & quot ; v   is   of   type   % T \ \ n & quot ;     ,       v     )       }                     常量 的 定义   const   Pi   =   3.14   ， 常量 不能 使用   : =   语法 定义 。   =   表示 定义 ， 而   : =   表示 赋值 ？       Go   只有 一种 循环 结构 — —   for   循环 。 循环 初始化 语句 和 后置 语句 都 是 可选 的 。                   sum       : =       0       for       i       : =       0     ;       i       & lt ;       10     ;       i     ++       {               sum       + =       i       }                     for   是   Go   的   “ while ”                   for       sum       & lt ;       1000       {               sum       + =       sum       }                     死循环                   for       {       }                     if   条件 判断                   if       x       & lt ;       0       {               return       sqrt     (     -     x     )       +       & quot ; i & quot ;       }         if       x       & gt ;     10       {               ...       }       else       {               ...       }                     GO   的 循环 条件 和 判断 条件 都 不 需要 用 小括号 括 起来 ， 但是 括 起来 也 是 可以 的 。     亲测 通过 ， 另外 for 和 if 条件 表达式 中 定义 的 变量 的 作用域 仅 在 该 语句 块 中 有效 。               switch 分支 语句 ,   switch   的 条件 从上到下 的 执行 ， 当 匹配 成功 的 时候 停止 。 不像 C 需要 break 语句       没有 条件 的   switch         没有 条件 的   switch   同   switch   true   一样 。         这一 构造 使得 可以 用 更 清晰 的 形式 来 编写 长 的   if - then - else   链 。                           switch       {               case       t     .     Hour     ( )       & lt ;       12     :                       fmt     .     Println     (     & quot ; Good   morning ! & quot ;     )               case       t     .     Hour     ( )       & lt ;       17     :                       fmt     .     Println     (     & quot ; Good   afternoon .& quot ;     )               default     :                       fmt     .     Println     (     & quot ; Good   evening .& quot ;     )               }                     defer   语句 会 延迟 函数 的 执行 直到 上层 函数 返回 。         延迟 调用 的 参数 会 立刻 生成 ， 但是 在 上层 函数 返回 前 函数 都 不会 被 调用 。       defer 栈 ： 延迟 的 函数调用 被 压入 一个 栈中 。 当 函数 返回 时 ，   会 按照 后进先出 的 顺序调用 被 延迟 的 函数调用 。       Go   具有 指针 。   指针 保存 了 变量 的 内存地址 。         类型     * T     是 指向 类型     T     的 值 的 指针 。 其零值 是     nil     。         Go 指针 和 C 指针 的 区别 在于 Go 没有 指针 运算 。       结构 体 ， 和 C 差不多 ， 利用 关键字   type   定义 声明 。                   type       Vertex       struct       {               X       int               Y       int       }                 结构 体 也 可以 通过 指针 访问 ， 还是 用 符号   .   访问 成员           结构 体 文法 。   结构 体 文法 表示 通过 结构 体字 段 的 值 作为 列表 来 新 分配 一个 结构 体 。         使用   Name :   语法 可以 仅 列出 部分 字 段 。 （ 字段名 的 顺序 无关 。 ）         特殊 的 前缀   & amp ;   返回 一个 指向 结构 体 的 指针 。                   var       (               v1       =       Vertex     {     1     ,       2     }         / /   类型 为   Vertex               v2       =       Vertex     {     X     :       1     }         / /   Y : 0   被 省略               v3       =       Vertex     { }                 / /   X : 0   和   Y : 0               p         =       & amp ;     Vertex     {     1     ,       2     }       / /   类型 为   * Vertex       )                     数组 ， 类型   [ n ] T   是 一个 有   n   个 类型 为   T   的 值 的 数组 。                   var       a       [     10     ]     int                     slice ， 一个 slice 会 指向 一个 序列 的 值 ， 并且 包含 长度 信息 ，   len ( s )   返回 序列   s   的 长度 。       slice 的 slice ， 类似 于 二维 数组 ， 初始化 代码                   game       : =       [ ] [ ]     string     {                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,                       [ ]     string     {     & quot ; _& quot ;     ,       & quot ; _& quot ;     ,       & quot ; _& quot ;     } ,               }                     slice 切片 操作 ， 类似 于 python 代码                   s     [     lo     :     hi     ]       s     [ :     hi     ]       s     [     lo     : ]                     make 函数 构造 slice ,     make ( type ,   value   [ ,   capacity ] )   .   函数   cap ( )   获取 slice 的 容量 。                   a       : =       make     ( [ ]     int     ,       5     )                     slice 的 零值 是   nil   。           slice 添加 元素 ，   append ( slice ,   value1 ,   ... ,   valueN )   方法 ， 添加 元素 的 时候 ， slice 的 容量 会 自动 增加 。               slice 用法 ：   https : / / blog . go - zh . org / go - slices - usage - and - internals               range     迭代 ， 第一个 为 下标 ， 第二个 为值 的 拷贝 。 可以 用   _   来 忽略 下标                   var       pow       =       [ ]     int     {     1     ,       2     ,       4     ,       8     ,       16     ,       32     ,       64     ,       128     }       for       i     ,       v       : =       range       pow       {                       fmt     .     Printf     (     & quot ; 2 * * % d   =   % d \ \ n & quot ;     ,       i     ,       v     )       }       for       _     ,       v       : =       range       pow       {       }       for       idx       : =       range       pow       {       }                     map   映射 键到 值 。   类似 于 python 的 字典     map   在 使用 之前 必须 用   make   来 创建 ； 值为   nil   的   map   是 空 的 ， 并且 不能 对 其 赋值 。                   m       =       make     (     map     [     string     ]     int     )       m     [     & quot ; Bell   Labs & quot ;     ]       =       8                         map   的 操作     在   map   m   中 插入 或 修改 一个 元素 ：       m [ key ]   =   elem       获得 元素 ：       elem   =   m [ key ]       删除 元素 ：       delete ( m ,   key )       通过 双 赋值 检测 某个 键 存在 ：       elem ,   ok   =   m [ key ]       如果   key   在   m   中 ，   ok   为   true 。 否则 ，   ok   为   false ， 并且   elem   是   map   的 元素 类型 的 零值 。     同样 的 ， 当 从   map   中 读取 某个 不 存在 的 键 时 ， 结果 是   map   的 元素 类型 的 零值 。               函数 可以 作为 值 传递           函数 闭包                   / /   fibonacci   函数 会 返回 一个 返回   int   的 函数 。       func       fibonacci     ( )       func     ( )       int       {               var       a0     ,       a1       =       0     ,       1               return       func     ( )       int     {                       var       tmp       =       a0       +       a1                       a0       =       a1                       a1       =       tmp                       return       a0               }       }                     go 没有 类 ！ ！ ！ 然而 ， 仍然 可以 在 结构 体 类型 上 定义方法 。     方法 接收者   出现 在   func   关键字 和 方法 名 之间 的 参数 中 。                   func       (     v       *     Vertex     )       Abs     ( )       float64       {               return       math     .     Sqrt     (     v     .     X     *     v     .     X       +       v     .     Y     *     v     .     Y     )       }       v     .     Abs     ( )                     你 可以 对 包中 的   任意   类型定义 任意 方法 ， 而 不仅仅 是 针对 结构 体 。     但是 ， 不能 对 来自 其他 包 的 类型 或 基础 类型定义 方法 。                   type       MyFloat       float64         func       (     f       MyFloat     )       Abs     ( )       float64       {               if       f       & lt ;       0       {                       return       float64     (     -     f     )               }               return       float64     (     f     )       }         func       main     ( )       {               f       : =       MyFloat     (     -     math     .     Sqrt2     )               fmt     .     Println     (     f     .     Abs     ( ) )       }                     指针 作为 函数 接收者 ， 有 两个 原因 需要 使用 指针 接收者 。 首先 避免 在 每个 方法 调用 中 拷贝 值 （ 如果 值 类型 是 大 的 结构 体 的话 会 更 有效率 ） 。         其次 ， 方法 可以 修改 接收者 指向 的 值 。       接口 是 由 一组 方法 定义 的 集合 。 接口 区分 类型 本身 的 方法 和 类型 指针 绑定 的 方法 。                   type       Abser       interface       {               Abs     ( )       float64       }                     隐式 接口 ？ 什么 鬼 东西       Stringers     一个 普遍存在 的 接口 是   fmt   包中 定义 的   Stringer 。                   type       Stringer       interface       {               String     ( )       string       }                 Stringer   是 一个 可以 用 字符串 描述 自己 的 类型 。   fmt   包   （ 还有 许多 其他 包 ） 使用 这个 来 进行 输出 。     -   错误 接口               type       error       interface       {               Error     ( )       string       }                     Reader 接口           [ 1 ]   Go 学习 网页   https : / / tour . go - zh . org /    ", "tags": "tools", "url": "/wiki/tools/go.html"},
      
      
      {"title": "gRPC 快速入门", "text": "    Table   of   Contents           关于                 关于       gRPC   是   Google   开源 的 基于   HTTP2   +   protobuf   的 RPC 方案 ,   与 之 相对 的 是 thrift 。  ", "tags": "tools", "url": "/wiki/tools/grpc.html"},
      
      
      {"title": "Hadoop", "text": "    Table   of   Contents           关于           Hadoop   shell                 关于       作为 大 数据 工作者 ， 怎能不 懂 hadoop 。       Hadoop   shell       利用 hadoop   shell   访问 HDFS 文件系统 ， 官方 文档 见   https : / / hadoop . apache . org / docs / r1 . 0.4 / cn / hdfs _ shell . html   。     这里 记录 一些 常用命令 和 技巧 。       调用 文件系统 ( FS ) Shell 命令 应 使用     bin / hadoop   fs   & lt ; args & gt ;   的 形式 。   所有 的 的 FS   shell 命令 使用 URI 路径 作为 参数 。 URI 格式 是 scheme : / / authority / path 。 对 HDFS 文件系统 ， scheme 是 hdfs ， 对 本地 文件系统 ， scheme 是 file 。 其中 scheme 和 authority 参数 都 是 可选 的 ， 如果 未 加 指定 ， 就 会 使用 配置 中 指定 的 默认 scheme 。 一个 HDFS 文件 或 目录 比如 / parent / child 可以 表示 成 hdfs : / / namenode : namenodeport / parent / child ， 或者 更 简单 的 / parent / child （ 假设 你 配置文件 中 的 默认值 是 namenode : namenodeport ） 。 大多数 FS   Shell 命令 的 行为 和 对应 的 Unix   Shell 命令 类似 ， 不同之处 会 在 下面 介绍 各 命令 使用 详情 时 指出 。 出错 信息 会 输出 到 stderr ， 其他 信息 输出 到 stdout 。               需要 添加   -   号 ， 例如   cat   命令 调用 形式 为   hadoop   fs   - cat   some - path         与 bash 相同 的 命令 有 ，   cat   ,     chmod   ,     chown   ,     du   ( 显示 文件大小 ) ,     ls   ,     lsr   ( ls   - R ) ,     mkdir     ( mkdir   - p   创建 所有 不 存在 的 父 目录 ) ,       mv   ,     rm   ,     rmr     ( rm   - R ) ,     stat   ,     tail   ,     test   - [ ezd ]   ，       特有 的 命令         dus   ，   显示 文件大小         touchz   ， 创建 一个 0 字节 的 空 文件         text   ，   将 源文件 输出 为 文本格式 。 允许 的 格式 是 zip 和 TextRecordInputStream 。               与 本地 文件系统 交互 的 命令         copyFromLocal         将 本地 路径 拷贝到 HDFS 中         copyToLocal             将 HDFS 中 的 路径 原样 拷贝到 本地         put   ， 可以 用   -   代表 标准 输入 ， 例如   hadoop   fs   - put   -   hdfs : / / host : port / hadoop / hadoopfile           get   ，   例子     hadoop   fs   - get   / user / hadoop / file   localfile           getmerge   ,   获取 分布式 文件目录 中 所有 的 文件 并 合并 到 一个 本地 文件 中 ， 使用 方法 ：   hadoop   fs   - getmerge   & lt ; src & gt ;   & lt ; localdst & gt ;   [ addnl ]   ， addnl 是 可选 的 ， 用于 指定 在 每个 文件 结尾 添加 一个 换行符 。              ", "tags": "tools", "url": "/wiki/tools/hadoop.html"},
      
      
      {"title": "Hive", "text": "    Table   of   Contents           关于           配置           Hive   基本概念           Hive   的 定位           数据 单元                   类型 系统           复杂 类型           操作           内置 函数                   Hive   SQL           优化 排序                   HIVE   命令           文件格式           textfile 格式           Avro   格式           ORC   格式           Parquet   格式           压缩 文件格式           LZO                   UDF           JOIN           编写 自己 的 UDF           UNION           Lateral   View           子 查询           采样           虚拟 列           窗 函数 和 分析 函数           窗 函数 ( 没 搞懂 )           OVER           分析 函数           其他 细节                   Enhanced   Aggregation ,   Cube ,   Grouping   and   Rollup           EXPLAIN   命令           HIVE   权限 管理           MORE           UDF   开发           UDF                   MAC   切换 不同 的 JDK           ERROR   汇总           问题           TIPS                 关于       学习 Hive 时 的 笔记       配置       环境 要求             -   Hive   1.2   需要   java   1.7 + ，   0.14 - 1.1   可以 工作 在   java1 . 6   版本 上 。             -   Hadoop   2 . x             -   可以 运行 在   Linux   和   Windows   环境 ，   Mac   通常 用来 做 开发 的 ！       添加     $ HIVE _ HOME     环境变量 ， 并 将   $ bin   目录 添加 到   $ PATH   变量 中 。         hive . metastore . warehouse . dir     配置 指明 数据仓库 的 目录 ， 默认 是     / user / hive / warehouse     和     / tmp   （ 临时 文件目录 ）       Hive   基本概念       Hive   的 定位       Hive   用来 做 数据仓库 ， 非 实时 数据处理 。       数据 单元           Database ：   用来 做 名字 空间 ， 防止 表 名字 冲突 ， 也 可以 用于 用户 权限 管理 。       Tables ： 表 ， 就是 传统 数据库 意义 上 的 表       Partitions ： 分区 ， 一个 表 通常 由 多个 分区 组成 。 获取 某个 特定 分区 的 数据 比 全表 扫描 快 ！ 每个 分区 一个 目录 ！       Buckets   ( or   Clusters ) ： 分桶 或 分簇 ， 在 一个 分区 里面 ， 可以 按照 某些 字段 的 hash 值 进行 分桶 ， 便于 采样 。 例如 PV 表 按照 userid 分桶       clustered   by   ( userid )   into   100   buckets   。           类型 系统           Integers       TINYINT — 1   byte   integer       SMALLINT — 2   byte   integer       INT — 4   byte   integer       BIGINT — 8   byte   integer               Boolean   type       BOOLEAN — TRUE / FALSE               Floating   point   numbers       FLOAT — single   precision       DOUBLE — Double   precision               Fixed   point   numbers       DECIMAL — a   fixed   point   value   of   user   defined   scale   and   precision               String   types       STRING — sequence   of   characters   in   a   specified   character   set       VARCHAR — sequence   of   characters   in   a   specified   character   set   with   a   maximum   length       CHAR — sequence   of   characters   in   a   specified   character   set   with   a   defined   length               Date   and   time   types       TIMESTAMP —   a   specific   point   in   time ,   up   to   nanosecond   precision       DATE — a   date               Binary   types       BINARY — a   sequence   of   bytes                   隐式 类型转换 ， 只能 从 低 精度 到 高精度 。 也 允许 从   STRING   到   DOUBLE 。     显示 类型 转化 可以 用 内置 函数 实现 。       复杂 类型           Structs   结构 体       Maps   key - value       Arrays   索引 list           结构 体 类型 的 操作 ！ 怎么 用 ？ ！       操作       除了 常规 的 比较 操作 ， 还 支持 正则 式 比较 ：         A   RLIKE   B ,   A   REGEXP   B   ， 字符串 A 是否 匹配 Java 正则 式 B 。 注意 有个 坑 ， 正则 式 B 中 的   \ \   需要 转义字符 ！ ！     例子 ：   lat   rlike   ' \ \ d + \ \ . \ \ d + '   是 错误 的 ， 应该 是     lat   rlike   ' \ \ \ \ d + \ \ \ \ . \ \ \ \ d + '         内置 函数           数值 类型 函数 ：   round ,   floor ,   ceil ,   rand         字符串 函数 ：     caoncat ,   substr ,   upper ,   ucase ,   lower ,   lcase ,   trim ,   ltrim ,   rtrim ,   regexp _ replace         时间 函数 ：     from _ unixtime ,   to _ date ,   year ,   month ,   day         复杂 类型 函数 ：   size ,   get _ json _ object   ,     reflect , java _ method   可以 用来 调用 所有 java 内置 的 函数 ！ ！       其他 ：   cast         内置 聚合 函数 ：   count ,   sum ,   avg ,   min ,   max             count   会 自动 去掉 NULL 值 ， 这 在 条件 count 的 时候 很 有用 ， 例如 分别 统计 在 a & gt ; 1 的 情况 下 和 a & lt ; 0 情况 下 的 uid ， 可以 用 一个 查询 搞定 ， 不用 join               select       count     (     distinct       if     (     a     & gt ;     1     ,       uid     ,       null     ) )       as       cnt1     ,                       count     (     distinct       if     (     a     & lt ;     0     ,       uid     ,       null     ) )       as       cnt0       from       some _ table                 Hive   SQL             row _ number ( )     函数 用法 ，   partition   用来 将 数据 分区 编号 ，   order   by   描述 编号 顺序                   select       uid     ,       row _ number     ( )       over       (     partition       by       uid       order       by       uid     )       from                       datediff ( d1 ,   d2 )   ，   其中 时间 字符串 要是 这种 格式   yyyy - MM - dd   ， 如果 不是 ， 需要 先 转换             from _ unixtime ( t ,   ' yyyyMMdd ' )   ,     unix _ timestamp ( str ,   ' yyyy - MM - dd ' )   这 两个 函数 可以 实现 时间 字符串 格式 转换               JOIN ： 支持 常规 的 内 连接   inner   join ， 外 链接   outer   join 。 还 支持   left   semi   join ， 用来 从 左边 表 过滤 出 满足 join 条件 的 记录 ，     相当于   where   exists   subquery   方式 ， 也 可以 替代 in ， 效率 比   inner   join 高 。 （ 此时 要 不要 将 大表放 右边 ？ ！ 哈哈 ）               join 优化 ： 将 大表放 右边 ！           Also   it   is   best   to   put   the   largest   table   on   the   rightmost   side   of   the   join   to   get   the   best   performance .                   聚合 操作 不能   distinct   两个 不同 的 列               HAVING   @ since ( 0.7 . 0 )   可以 将 聚合 函数 放在   WHERE   中 当做 条件 使用 ，           多表 / 文件 插入 操作 ： 还是 看 代码 吧 ！                   FROM       pv _ users       INSERT       OVERWRITE       TABLE       pv _ gender _ sum               SELECT       pv _ users     .     gender     ,       count _ distinct     (     pv _ users     .     userid     )               GROUP       BY       pv _ users     .     gender         INSERT       OVERWRITE       DIRECTORY       &# 39 ; / user / data / tmp / pv _ age _ sum &# 39 ;               SELECT       pv _ users     .     age     ,       count _ distinct     (     pv _ users     .     userid     )               GROUP       BY       pv _ users     .     age     ;                     动态 分区 插入 ： @ since ( 0.6 . 0 ) ， 能够 减少 调度 时间 ， 显著 提升 性能 ！ 但是 注意 几个 问题 。                   insert       overwrite       table       pv       partition       (     dt     =     &# 39 ; 2010 - 01 - 01 &# 39 ;     ,       country     )         - - - - -   动态 决定 country 分区 ， dt 分区 值 固定       insert       overwrite       table       pv       partition       (     dt     ,       country     =     &# 39 ; US &# 39 ;     )                         - - - - -   将 所有 dt 分区 下   country = &# 39 ; US &# 39 ;   子 分区 都 覆盖 ， 一般 不要 能 用 这种 写法 ！                 如果 分区 字段 为 NULL ， 会 写入 到 默认 分区 HIVE _ DEFAULT _ PARTITION 中 。       影响 动态 分区 的 一些 配置 ：           hive . exec . max . dynamic . partitions . pernode   ( default   value   being   100 )   每个   mapper   或者   reducer   能够 创建 的 最大 分区 数目 。       hive . exec . max . dynamic . partitions   ( default   value   being   1000 )   一个 表 能够 创建 的 最大 分区 数目 。       hive . exec . max . created . files   ( default   value   being   100000 )   所有 的   mapper   和   reducer   能够 创建 的 全部 文件 数目 最大值       hive . exec . dynamic . partition . mode = strict   禁止 动态 分区   nostric   使用 动态 分区 。 只能 使用 静态 分区 。           hive . exec . dynamic . partition = true / false   彻底 禁止 动态 分区 。               写入 本地 文件 ：   INSERT   OVERWRITE   LOCAL   DIRECTORY   ' / tmp / pv _ gender _ sum '             快速 采样 ：   TABLESAMPLE ( BUCKET   x   OUT   OF   y )   ， 需要 在建 表 的 时候   CLUSTERED   BY   支持 。 例如 选出 第 3 个 bucket 。                   TABLESAMPLE     (     BUCKET       3       OUT       OF       64       ON       userid     )                     UNION   ALL ：       Array   操作 ：                   CREATE       TABLE       array _ table       (     int _ array _ column       ARRAY     & lt ;     INT     & gt ;     ) ;         SELECT       pv     .     friends     [     2     ]       FROM       page _ views       pv     ;                 相关 UDAF 函数   percentile _ approx ,   histogram _ numeric ,   collect _ set ,   collect _ list       -   Map   操作 ：           Custom   Map / Reduce   Scripts ：     MAP ,   REDUCE   （ 是   TRANSFORM   的 语法 糖 而已 ） ， 或者   TRANSFORM     函数 （ 是否 只能 实现 UDF 的 功能 ， UDAF 和 UDTF 呢 ？ ）                   FROM       (                 FROM       pv _ users                 MAP       pv _ users     .     userid     ,       pv _ users     .     date                 USING       &# 39 ; map _ script &# 39 ;                 AS       dt     ,       uid                 CLUSTER       BY       dt     )       map _ output         INSERT       OVERWRITE       TABLE       pv _ users _ reduced                 REDUCE       map _ output     .     dt     ,       map _ output     .     uid                 USING       &# 39 ; reduce _ script &# 39 ;                 AS       date     ,       count     ;                 脚本 ：               import       sys       import       datetime         for       line       in       sys     .     stdin     :           line       =       line     .     strip     ( )           userid     ,       unixtime       =       line     .     split     (     &# 39 ;     \ \ t     &# 39 ;     )           weekday       =       datetime     .     datetime     .     fromtimestamp     (     float     (     unixtime     ) )     .     isoweekday     ( )           print       &# 39 ; , &# 39 ;     .     join     ( [     userid     ,       str     (     weekday     ) ] )                           CLUSTER   BY ,   DISTRIBUTE   BY ,   SORT   BY                 分组 ：   CLUSTER   BY     相当于 先 按列   DISTRIBUTE   BY   ， 然后   SORT   BY                 优化 排序       不要 使用   order   by       https : / / stackoverflow . com / questions / 13715044 / hive - cluster - by - vs - order - by - vs - sort - by                     -     ORDER   BY     全局 排序 ， 但是 只能 使用 一个 reducer     -     DISTRIBUTE   BY     采用 Hash 算法 将 map 处理 后 的 数据 分 发给 reduce ， 它 保证 了 相同 的 key 是 在 同一个 reducer     -     SORT   BY     不是 全局 排序 ， 而是 在 数据 进入 reduce 之前 完成 排序 ， 只能 保证 每个 reducer 的 输出 是 有序 的 ， 不能 保证 全局 有序 。     -     CLUSTER   BY     相当于 先   DISTRIBUTE   然后   sort 。 也 不能 保证 全局 有序 。       HIVE   命令             set   & lt ; key & gt ; = & lt ; value & gt ;     设置 参数         add   FILE [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *   ,     add   JAR [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *   ,     add   ARCHIVE [ S ]   & lt ; filepath & gt ;   & lt ; filepath & gt ; *     添加 文件           文件格式       textfile 格式       文本格式               ROW       FORMAT       DELIMITED             FIELDS       TERMINATED       BY       &# 39 ; \ \ 001 &# 39 ;             LINES       TERMINATED       BY       &# 39 ; \ \ n &# 39 ;             COLLECTION       ITEMS       TERMINATED       BY       &# 39 ; \ \ 002 &# 39 ;             MAP       KEYS       TERMINATED       BY       &# 39 ; \ \ 003 &# 39 ;         STORED       AS       TEXTFILE     ;                 Avro   格式           要求 ： Hive   0.9 . 1 +       不同 版本 要求 的 语法 还 不同 ， 具体 参看   https : / / cwiki . apache . org / confluence / display / Hive / AvroSerDe   。     0.14   以后 的 版本 可以 在 创建 表 的 时候 这样 写 ：   STORED   AS   AVRO     即可 。           ORC   格式       Optimized   Row   Columnar   格式 ， 采用 这种 格式 可以 提升 HIVE 读写 性能 。           输出 是 单个 文件 ， 减少   NameNode   的 负载       支持 Hive 所有 格式 ， 包括 复杂 格式       文件 中 存储 了 轻量级 的 索引 ， 便于 以 行为 单位 移动 指针       压缩 ： 游程 编码 （ Int ） 字典 码 （ String ）       同时 读 同一个 文件       split   文件 ， 而 不 需要 scanning   for   markers       限制 了 读写 内存           文件 结构 ： 以 Strip 为 单位 （ 默认 250MB ） 。       cli 读取 命令     hive   - - orcfiledump         创建 表 的 时候 这样 写 ：   STORED   AS   ORC     即可 。       Parquet   格式       Hadoop   生态 中 的 一个 ！       压缩 文件格式       直接 从   gzip   等 格式 中 存取 为 text 格式 表格 。               CREATE       TABLE       raw       (     line       STRING     )             ROW       FORMAT       DELIMITED       FIELDS       TERMINATED       BY       &# 39 ; \ \ t &# 39 ;       LINES       TERMINATED       BY       &# 39 ; \ \ n &# 39 ;     ;         LOAD       DATA       LOCAL       INPATH       &# 39 ; / tmp / weblogs / 20090603 - access . log . gz &# 39 ;       INTO       TABLE       raw     ;                 LZO       略       UDF       UDF ， UDAF ， UDTF       HIVE 数据类型 与   java   数据类型 对应 关系 ：               hive           java       map             HashMap       array         ArrayList     & lt ; ?     & gt ;                 JOIN           多表   JOIN   的 时候 ， 当 JOIN 条件 都 包含 同一个 Key 的 时候 ， 会 用 同一个   Map / Reduce   处理 ， 例如                   SELECT       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key1     )                 只有 一个   Map / Reduce   任务 。 而 下面 这个 会 有 2 个   Map / Reduce   任务 ( JOIN   a , b ;   JOIN   *   , c ) 。               SELECT       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key2     )                     In   every   map / reduce   stage   of   the   join ,   the   last   table   in   the   sequence   is   streamed   through   the   reducers   where   as   the   others   are   buffered .   Therefore ,   it   helps   to   reduce   the   memory   needed   in   the   reducer   for   buffering   the   rows   for   a   particular   value   of   the   join   key   by   organizing   the   tables   such   that   the   largest   tables   appear   last   in   the   sequence .           将 大表放 后面 ， 大表会 以 streaming 的 方式 进入 reducer ， 而 其他 的 一 buffer 的 方式 存在 （ 内存 ？ ） ， 可以 减少 内存 的 需求 。     默认 让 最后 的 表以 streaming 方式 进入 reducer ， 也 可以 手动 指定 。               SELECT       / * +   STREAMTABLE ( a )   * /       a     .     val     ,       b     .     val     ,       c     .     val       FROM       a       JOIN       b       ON       (     a     .     key       =       b     .     key1     )       JOIN       c       ON       (     c     .     key       =       b     .     key1     )                 当 存在 这个 hint 的 时候 ， 会 将表 b ， c 缓存 ， 而 让 a 以流 的 方式 进入 reducer 。 不 存在 的 时候 ， 则 会 将 最后 的 表以 流 的 方式 进入 reducer 。           JOIN   逻辑 发生 在   WHERE   之前 ！ 对于   inner   join ， 条件 放在   ON   还是   WHERE   都 是 一样 的 ， 但是 如果 是 其他   JOIN ，   则 会 有 区别 。       https : / / stackoverflow . com / questions / 354070 / sql - join - where - clause - vs - on - clause             但是 在 实现 的 时候 ， 会 先用 WHERE 里面 的 条件 过滤 吧 ？ ！ 否则 性能 不是 很差 ！ ？               JOIN   是 不可 交换 的 ！               左半 连接 更 有效 ！   要求 右表 的 字 段 只 在   ON   条件 中 出现 ！ 在 map 端 过滤 掉 不会 参加 join 操作 的 数据 ， 则 可以 大大 节省 网络 IO 。                   LEFT   SEMI   JOIN   implements   the   uncorrelated   IN / EXISTS   subquery   semantics   in   an   efficient   way .               MAPJOIN ， JOIN   小表 的 时候 ， 可以 减少   reducer ， 提升 性能 ！ 但是 右 链接 和 全 连接 中 不能 用 ！                   SELECT       / * +   MAPJOIN ( b )   * /       a     .     key     ,       a     .     value       FROM       a       JOIN       b       ON       a     .     key       =       b     .     key                 上述 代码 不 需要   reducer ！           The   restriction   is   that   a     FULL / RIGHT   OUTER   JOIN     b   cannot   be   performed .               如果 在   JOIN   列 上 ， 进行 分桶 了 ， 并且 其中 一个 表 的 桶 数目 是 另 一个 的 倍数 ， 那么 就 可以 采用   MAPJOIN   优化 了 。     在   MAP   的 时候 ， 左表 的 第一个 桶 只会 去取 右表 的 第一个 桶 ， 而 不是 所有 的 数据 ！ 这个 行为 不是 默认 的 ， 需要 设置 参数 ：                   set       hive     .     optimize     .     bucketmapjoin       =       true                     如果 两个 表   JOIN   的 字 段 分桶 且 排序 的 ， 并且 分桶 数目 相同 ， 那么 可以 采用   sort - merge 。 例如 满足 上述 条件 的 两个 表 的 join 可以     只 需要   Map   阶段 ！                   SELECT       / * +   MAPJOIN ( b )   * /       a     .     key     ,       a     .     value       FROM       A       a       JOIN       B       b       ON       a     .     key       =       b     .     key                 同时 需要 设置 参数 ：               set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     BucketizedHiveInputFormat     ;       set       hive     .     optimize     .     bucketmapjoin       =       true     ;       set       hive     .     optimize     .     bucketmapjoin     .     sortedmerge       =       true     ;                     MAPJOIN   可以 用来   JOIN   小表 ， 实现 优化 ， 但是 下列 情况 是 不行 的 ！ ！ ！ 我晕 ！       Union   Followed   by   a   MapJoin       Lateral   View   Followed   by   a   MapJoin       Reduce   Sink   ( Group   By / Join / Sort   By / Cluster   By / Distribute   By )   Followed   by   MapJoin       MapJoin   Followed   by   Union       MapJoin   Followed   by   Join       MapJoin   Followed   by   MapJoin                   可以 设置     hive . auto . convert . join = true     让 hive 自动 帮 你 转为   MAPJOIN 。 从   Hive   0.11 . 0     开始 ， 默认值 就是 true 。     MAPJOIN   将 小表 放到 内存 ， 保存 为 一个   HASH   MAP 。 工作 流程 是 ：   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + JoinOptimization               Local   work :   read   records   via   standard   table   scan   ( including   filters   and   projections )   from   source   on   local   machine   build   hashtable   in   memory   write   hashtable   to   local   disk   upload   hashtable   to   dfs   add   hashtable   to   distributed   cache     Map   task   read   hashtable   from   local   disk   ( distributed   cache )   into   memory   match   records &# 39 ;   keys   against   hashtable   combine   matches   and   write   to   output               MAPJOIN   的 限制 ：           一次 只能 一个 KEY       chain   of   MAPJOINs   是 不 可以 的 ， 除非 写成 子 查询 形式 。   mapjoin ( table ,   subquery ( mapjoin ( table ,   subquery .... )             每 一次   MAPJOIN   都 需要 重新 建立   HASH   表 ， 包括 上传 和 下载               优化 链式   MAPJOIN                       select       / * +   MAPJOIN ( time _ dim ,   date _ dim )   * /       count     (     *     )       from       store _ sales       join       time _ dim       on       (     ss _ sold _ time _ sk       =       t _ time _ sk     )       join       date _ dim       on       (     ss _ sold _ date _ sk       =       d _ date _ sk     )       where       t _ hour       =       8       and       d _ year       =       2002                 通过 两个 值 设置   MAPJOIN               set       hive     .     auto     .     convert     .     join     .     noconditionaltask       =       true     ;       set       hive     .     auto     .     convert     .     join     .     noconditionaltask     .     size       =       10000000     ;                 SMB   Map   Join :   Sort - Merge - Bucket   ( SMB )   joins       表 已经 是 分桶 并且 排序 好 的 ，   JOIN   过程 通过 顺序   merge   已经 排序 好 的 表 即可 。 （ 效率 比 普通   JOIN   高 ）           However ,   if   the   tables   are   partitioned ,   there   could   be   a   slow   down   as   each   mapper   would   need   to   get   a   very   small   chunk   of   a   partition   which   has   a   single   key .                   set       hive     .     auto     .     convert     .     sortmerge     .     join     =     true     ;       set       hive     .     optimize     .     bucketmapjoin       =       true     ;       set       hive     .     optimize     .     bucketmapjoin     .     sortedmerge       =       true     ;         - - - - - -   大表 自动 转化 设置       set       hive     .     auto     .     convert     .     sortmerge     .     join     .     bigtable     .     selection     .     policy               =       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     TableSizeBasedBigTableSelectorForAutoSMJ     ;                 大表 选择 策略 会 自动 决定 哪个 表 被   streaming ， 而 不是   hash   并且   streaming 。 可 选 策略 有               org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     AvgPartitionSizeBasedBigTableSelectorForAutoSMJ       (     default     )       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     LeftmostBigTableSelectorForAutoSMJ       org     .     apache     .     hadoop     .     hive     .     ql     .     optimizer     .     TableSizeBasedBigTableSelectorForAutoSMJ                 如果 表有 不同 数量 的 keys （ SORT   列 ） ， 会 发生 异常 ！       SMB   存在 的 目的 主要 是 为了 解决 大表 与 大表间 的   Join   问题 ， 分桶 其实 就是 把 大表 化成 了 “ 小表 ” ， 然后   Map - Side   Join   解决 之 ， 这是 典型 的 分而治之 的 思想 。       这个 Blog 写 的 不错 ，   https : / / my . oschina . net / leejun2005 / blog / 178631         编写 自己 的 UDF       TRANSFORM   貌似 不能 实现   UDAF 。 可以 用 java 写 UDF 或 UDAF ， UDTF 等 。 需要 jdk1 . 7 版本 。               ADD       JAR       hdfs     :     / / /     user     /     hadoop     -     data     /     user _ upload     /     hive     -     kv     -     udaf _ 2     .     10     -     0     .     0     .     1     .     jar     ;       CREATE       TEMPORARY       FUNCTION       kv       as       &# 39 ; KV &# 39 ;     ;                 UNION           UNION   ALL ： 不去 重 融合   1.2 . 0   以前 只 支持 这个       UNION   DISTINCT ： 去 重 ， 1.2 . 0   以后 默认 （ UNION ） 是 这个 融合       UNION   常 需要 对 列名 重命名 ， 使得 UNION 的 时候 ， 列名 是 相同 的           Lateral   View       用 在   UDTF   中 。 对于 输入 的 一行 ， 输出 是 多行 。 0.12 . 0   版本 开始 ， 列名 可以 不用 写 ， 会 自动 采用 UDTF 输出 的 StructObjectInspector 对象 自动 得到 列名 。 参考   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + LateralView         一个 例子 ， adid _ list 是 一个 Array ，   explode ( )   函数 会 将 这个 list 输出 为 多行 。               SELECT       pageid     ,       adid       FROM       pageAds       LATERAL       VIEW       explode     (     adid _ list     )       adTable       AS       adid     ;         SELECT       adid     ,       count     (     1     )       FROM       pageAds       LATERAL       VIEW       explode     (     adid _ list     )       adTable       AS       adid       GROUP       BY       adid     ;         SELECT       *       FROM       src       LATERAL       VIEW       OUTER       explode     (     array     ( ) )       C       AS       a       limit       10     ;         select       uid     ,       key     ,       val       from       table       LATERAL       VIEW       explode     (     kv     )       adTable       AS       key     ,       val                 FROM   语句 里面 可以 包含 多个   Lateral   View 。 通过     OUTER     关键字 可以 让     explode     输出 为 NULL 的 时候 ，     记录 至少 存在 一行 ！ （ 没有 这个 关键字 ， 结果 中将 不会 出现 记录 ）       子 查询       子 查询 放在   FROM   里面 ， 在   0.13   版本 后 ， 可以 放在   IN   和   EXISTS   之中 ， 但是 存在 一些 限制 。           只能 放在 表达式 右边       IN / NOT   IN   子 查询 只 支持 单列       EXISTS / NOT   EXISTS   必须 有 一个 或者 多个   correlated   predicates   （ where   条件 ？ ）       对父 查询 字段 的 引用 只 支持 在   WHERE   中 。           采样               TABLESAMPLE       (     BUCKET       x       OUT       OF       y       [     ON       colname     ] )                 colname   可以 是非 分区 字 段 以外 的 字 段   或者     RAND ( )   。     表 采样 是 很 慢 的 ， 如果 建表 的 时候 采用   CLUSTERED   BY   创建 ，     那么 可以 加快 采样 速度 ， 因为 只要 简单 地 取出 对应 的 BUCKET 就 可以 了 ， 而 不用 全表 扫描 。       更 多 信息 参考 ：   https : / / cwiki . apache . org / confluence / display / Hive / LanguageManual + Sampling         block   sampling   @ since ( 0.8 )               TABLESAMPLE       (     n       PERCENT     )       TABLESAMPLE       (     ByteLengthLiteral     )           - - - -   例如   100M       TABLESAMPLE       (     n       ROWS     )         - - - - -   例子       SELECT       *       FROM       source       TABLESAMPLE     (     100     M     )       s     ;                 这个 是 在   HDFS   block   level   上 进行 的 采样 ，     所以 一些 压缩 格式 表 数据 不 支持 这个 特性 。 复现 可以 通过 设置 种子 来 实现   set   hive . sample . seednumber = & lt ; INTEGER & gt ; ;   。       虚拟 列         INPUT __ FILE __ NAME ,   BLOCK __ OFFSET __ INSIDE __ FILE     在   Mapper   里面 分别 指 输入 文件名   和   全局 文件 位置       简单 例子               select       INPUT __ FILE __ NAME     ,       key     ,       BLOCK __ OFFSET __ INSIDE __ FILE       from       src     ;       select       key     ,       count     (     INPUT __ FILE __ NAME     )       from       src       group       by       key       order       by       key     ;       select       *       from       src       where       BLOCK __ OFFSET __ INSIDE __ FILE       & gt ;       12000       order       by       key     ;                 窗 函数 和 分析 函数       @ since ( 0.11 )       窗 函数 ( 没 搞懂 )           LEAD ，   LEAD   开窗 函数 返回 位于 分区 中 当前 行 的 下方 （ 之后 ） 的 某个 给定 偏移量 位置 的 行 的 值 。 如果 超过 窗 的 末尾 了 ， 返回 NULL 。 例子     https : / / docs . aws . amazon . com / zh _ cn / redshift / latest / dg / r _ Examples _ of _ LEAD _ WF . html         LAG ， LAG   开窗 函数 返回 位于 分区 中 当前 行 的 上方 （ 之前 ） 的 某个 给定 偏移量 位置 的 行 的 值 。 如果 超过 窗 的 开头 ， 返回 NULL 。 例子   https : / / docs . aws . amazon . com / zh _ cn / redshift / latest / dg / r _ Examples _ of _ LAG _ WF . html         FIRST _ VALUE ， 返回 分区 第一个 值       LAST _ VALUE ， 返回 分区 最后 一个 值           OVER       可以 将 聚合 函数 的 返回值 应用 到 每 一列 （ 窗 函数 的 功能 ） ， 就 像 分析 函数 那样 ！ ！           标准 聚合 函数     COUNT ,   SUM ,   AVG ,   MAX ,   MIN           PARTITION   BY     一个 或 多个 分区 列 ，   ORDER   BY     一个 或 多个 字 段       可以 在   OVER   里面 指定 窗     ROWS   ( ( CURRENT   ROW )   |   ( UNBOUNDED   |   [ num ] )   PRECEDING )   AND   ( UNBOUNDED   |   [ num ] )   FOLLOWING                     SELECT       a     ,       SUM     (     b     )       OVER       (     PARTITION       BY       c     ,       d       ORDER       BY       e     ,       f     )       FROM       T     ;         SELECT       a     ,       SUM     (     b     )       OVER       (     PARTITION       BY       c       ORDER       BY       d       ROWS       BETWEEN       UNBOUNDED       PRECEDING       AND       CURRENT       ROW     )       FROM       T     ;                 分析 函数           RANK ， 返回 序数 ， 可能 存在 相同 的 序号 ， 因为 同时 排 第一 之类 的       ROW _ NUMBER ， 返回 行号       DENSE _ RANK ， 返回 序号 ， 不 存在 相同 的 序号 ， 即使 相同 ， 也 会 让 后面 一个 序号 + 1       CUME _ DIST ， 返回 累积 分布   0 - 1   之间 的 值 。       PERCENT _ RANK ， 百分比 排名   0 - 1   之间 的 值 。 计算公式   ( row _ number   -   1 )   /   ( count   -   1 )       NTILE ， 对 排名 进行 分组 ， 尽可能 保证 每组 数目 均匀 ， 返回 分组 编号 。           其他 细节             DISTINCT     关键词 使用 在 这些 函数 中要 到   2.1 . 0   版本 之后 。       在   OVER   中 使用 聚合 函数 也 要 到   2.1 . 0   版本 之后                   COUNT     (     DISTINCT       a     )       OVER       (     PARTITION       BY       c     )         SELECT       rank     ( )       OVER       (     ORDER       BY       sum     (     b     ) )       FROM       T       GROUP       BY       a     ;                 Enhanced   Aggregation ,   Cube ,   Grouping   and   Rollup           GROUPING   SETS ： 等价 于 多个   GROUP   BY   然后   UNION                   SELECT       a     ,       b     ,       SUM     (     c     )       FROM       tab1       GROUP       BY       a     ,       b       GROUPING       SETS       (       (     a     ,     b     )       ,       a     )         - - - -   等价 于       SELECT       a     ,       b     ,       SUM     (       c       )       FROM       tab1       GROUP       BY       a     ,       b       UNION       SELECT       a     ,       null     ,       SUM     (       c       )       FROM       tab1       GROUP       BY       a                         Grouping __ ID               Cubes   and   Rollups       WITH   CUBE / ROLLUP     关键字 ， 只能 用 在     GROUP   BY     之中 。       GROUP   BY   a ,   b ,   c   WITH   CUBE     会 组合 所有 的 可能     ( a ,   b ,   c ) ,   ( a ,   b ) ,   ( b ,   c ) ,   ( a ,   c ) ,   ( a ) ,   ( b ) ,   ( c ) ,   (   )   。     而     GROUP   BY   a ,   b ,   c ,   WITH   ROLLUP     等价 于       GROUP   BY   a ,   b ,   c   GROUPING   SETS   (   ( a ,   b ,   c ) ,   ( a ,   b ) ,   ( a ) ,   (   ) )   .               设置     hive . new . job . grouping . set . cardinality     的 值 ， 当 候选 分组 数目 （ 上面 分别 是 8 和 4 ） 超过 这个 值时 ， 将 开启 额外 的   Mapper   Reducer   任务 来 处理 。       EXPLAIN   命令       用来 显示   query     的 执行 计划 的 ， 例如 对于 这个 query 有 几个 stage 等 。         EXPLAIN   [ EXTENDED | DEPENDENCY | AUTHORIZATION ]   query         HIVE   权限 管理       略       MORE       HIVE   on   spark !       UDF   开发       支持   JAVA （ 或 Scala ）   写 UDF   UDAF   UDTF ！ 依赖 ：           Java   UDF ：   https : / / github . com / apache / hive / tree / master / ql / src / java / org / apache / hadoop / hive / ql / udf / generic         Scala   UDF 及 单元测试 的 例子 ：   https : / / github . com / sharethrough / hive - udfs             UDF       实现 普通 函数   ( v1 ,   ... )   - & gt ;   ( w1 ,   ... ) 。     需要 继承   org . apache . hadoop . hive . ql . exec . UDF   这个 类 ， 并 实现     evaluate     方法 。         UDFTrim   模板         MAC   切换 不同 的 JDK       参考 ：   https : / / stackoverflow . com / questions / 20974607 / can - java - 7 - and - java - 8 - co - exist - on - osx               use - java     ( )       {               export       JAVA _ HOME     =     `   / usr / libexec / java _ home   - v     1   .   $ 1     `       }                 然后 使用     use - java   7     就 可以 切换 到     jdk   1.7     了 。       ERROR   汇总           metainfo   超大 ：                     org . apache . hadoop . yarn . exceptions . YarnRuntimeException :   java . io . IOException :   Split   metadata   size   exceeded   10000000 .   Aborting   job   job _ 1469532484579 _ 647683               解决 方法 ：   set   mapreduce . jobtracker . split . metainfo . maxsize = - 1 ;         问题           HIVE   中 使用   VIEW   视图 ！           TIPS               HIVE   中 上传 本地 csv 文件 作为 表格 的 简单 方法 ： 利用 hive 建表 命令 ， 创建 一个 表格 ， 然后 将 本地 csv 文件 通过   hadoop   shell   上 传到     表 对应 的 HDF 文件夹 即可 ！ 注意 建表 的 时候 要 用 文本格式 ， 注意 分隔符 要 匹配 。               SQL   将列 重命名 不要 命名 为 已 存在 的 列 的 名字 ！ 否则 将会取 存在 的 列 的 值 ， 而 不是 你 想要 的 值 ！           group   by   和   sort   by   func ( col ) ， 可以 是 一个 函数       设置 mapreduce 阶段 的 mapper 和 reducer 数目                   #   YARN :   Hadoop   2       set     mapreduce . job . maps   =   & lt ; num & gt ;   ;       set     mapreduce . job . reduces   =   & lt ; num & gt ;   ;                     设置 每个 reducer 最大 处理 数据量     set   hive . exec . reducers . bytes . per . reducer = & lt ; number & gt ;         用     distribute   by     控制 进入 reducer 的 样本       不要 用     count ( distinct   id )     因为 只能 用 一个 reducer ！ 可以 用   sum ( 1 )   +   group   by   id   ， 多 一个 job 但是 快 很多 ， 因为   group   by   可以 用 多个 reducer 。     一个 数据 ， 约 3 亿 不同 的 id ， 第一种 用时 40 分钟 ， 其中 reducer 耗时 35 分钟 ！ 后 一种 5 分钟 ！       https : / / stackoverflow . com / questions / 19311193 / why - is - countdistinct - slower - than - group - by - in - hive             另 一种 去 重 的 方法               select       sum     (     if     (     r     =     1     ,       1     ,       0     ) )       as       distinct _ num     ,               sum     (     1     )       as       num       from       (               select                       id     ,                       row _ number     ( )       over       (     partition       by       id     )       as       r               from       tableA       )                       SET   hive . exec . parallel = true ;     让 不同 job 并发 执行 ！       skew   join   优化 ：     set   hive . optimize . skewjoin = true ;   将 一个 join 操作 变为 两个 ， 第一个 会 将 同一个 key 分散 到 不同 的 reduce       skew   groupby   优化 ：   set   hive . groupby . skewindata =   true   ;         输入 是否 融合 小 文件 ：                   set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     HiveInputFormat     ;           - - -   不 融合       set       hive     .     input     .     format     =     org     .     apache     .     hadoop     .     hive     .     ql     .     io     .     CombineHiveInputFormat     ;         - - -   融合                     动态 分区 设置                   set       hive     .     exec     .     dynamic     .     partition     .     mode     =     nonstrict     ;       set       hive     .     exec     .     dynamic     .     partition     =     true     ;       set       hive     .     exec     .     max     .     dynamic     .     partitions     = & lt ;     最大 总 的 分区 数目     & gt ;     ;       set       hive     .     exec     .     max     .     dynamic     .     partitions     .     pernode     = & lt ;     每 一个     MR     节点 创建 的 最大 分区 数目     & gt ;     ;                     bigint   和   string   比较 是否 相等 的 时候 ,   会 将 他们 都 转换 为 double 进行 比较 , 会 损失 精度 , 尤其 是 在 JOIN 的 时候 , 问题 比较 大 , 建议 把 bigint 转成 string 再 比较 !       UDAF   在     Cannot   recognize   return   type   class     terminatePartial ( )     是因为 使用 了 不能 被 ObjectInspector 识别 的 数据类型 或类 了       改变 MAP 的 内存大小 , 可以 解决 map 内存 爆掉 的 异常     set   mapreduce . map . memory . mb = 4096 ;        ", "tags": "tools", "url": "/wiki/tools/hive.html"},
      
      
      {"title": "Java 挖坑记录", "text": "    Table   of   Contents           字符串 相关           并发 相关                 字符串 相关         String . split     默认 会 去掉 收尾 的 多余 字符               String       args       =       & quot ; 0.035635684 \ \ t \ \ t13001002926 \ \ t \ \ t & quot ;     ;       System     .     out     .     println     (     args     .     split     (     & quot ; \ \ t & quot ;     ) .     length     ) ;       / / 结果 是 3         System     .     out     .     println     (     args     .     split     (     & quot ; \ \ t & quot ;     ,       -     1     ) .     length     ) ;       / / 结果 是 5                 并发 相关             synchronized     和     Lock     来 保证 原子 性 ,     synchronized     修饰 的 代码 , 能够 保证 任一 时刻 只有 一个 线程 执行 该 代码 块 。         @ volatile     可见 性 。 当 一个 共享 变量 被 volatile 修饰 时 ， 它会 保证 修改 的 值会 立即 被 更新 到 主存 ， 当有 其他 线程 需要 读取 时 ， 它会 去 内存 中 读取 新值 。       优质 博客     https : / / www . cnblogs . com / dolphin0520 / p / 3920373 . html        ", "tags": "tools", "url": "/wiki/tools/java.html"},
      
      
      {"title": "kubernetes", "text": "    Table   of   Contents           关于           基础           构建 集群                         关于       Kubernetes   构建 在   Google   15   年 生产 环境 经验 基础 之上 , 并 结合 来自 社区 的 最佳 创意 和 实践 。             https : / / queue . acm . org / detail . cfm ? id = 2898444           https : / / kubernetes . io / docs / tutorials /             基础       构建 集群  ", "tags": "tools", "url": "/wiki/tools/kubernetes.html"},
      
      
      {"title": "Linux 基础知识", "text": "    Table   of   Contents           关于           常用命令 集合           文件系统 相关           权限 相关           操作系统 相关           字符串 工具           sort   命令           watch   命令           常用 参数 ：           例子                   wc   命令           tar   打包 命令           网络 命令                   VIM           三种 模式           移动 光标           编辑 命令           插入 模式           命令 模式           外部 工具 集成           参考                         关于       作为 一个 合格 的 工程师 ， 怎能不 懂 Linux ， 所以 学习 记录 一些 知识点 。       常用命令 集合       文件系统 相关           重定向 ：   2 & gt ; & amp ; 1   表示 将 stderr 重定向 到 stdout ，   2 & gt ; / dev / null   表示 重定向 stderr 到 空洞 ， 也 就是 不 打印 出 stderr 。         后面 这个 在 运行 hadoop 和 spark 的 时候 有用 。 也 可以 将 stdout 和 stderr 重定向 到 不同 文件 ， 例如 ：                                 some - cmd     1   & gt ; stdout . txt     2   & gt ; stderr . txt               权限 相关             sudo   :             在   sudo   出现 之前 ， 使用   su   命令 提升 权限 ， 缺点 是 必须 知道 超级 用户 的 密码 。   sudo   可以 将 用户 的 名字 、 可以 使用 的 特殊 命令 、 按照 那种 用户组 执行 等 信息 保存 在     文件 中 （ 通常 是   / etc / sudoers   ）     sudo   [ - bhHpV ] [ - s   ] [ - u   & lt ; 用户 & gt ; ] [ 指令 ]     或     sudo   [ - klv ]     参数 [ 编辑 ]     　 　 - b   　 在 后台 执行 指令 。     　 　 - h   　 显示 帮助 。     　 　 - H   　 将 HOME 环境变量 设为 新 身份 的 HOME 环境变量 。     　 　 - k   　 结束 密码 的 有效期限 ， 也 就是 下次 再 执行 sudo 时 便 需要 输入 密码 。     　 　 - l   　 列出 目前 用户 可 执行 与 无法 执行 的 指令 。     　 　 - p   　 改变 询问 密码 的 提示 符号 。     　 　 - s   　 执行 指定 的 shell 。     　 　 - u   & lt ; 用户 & gt ;   　 以 指定 的 用户 作为 新 的 身份 。 若 不 加上 此参数 ， 则 预设 以 root 作为 新 的 身份 。     　 　 - v   　 延长 密码 有效期限 5 分钟 。     　 　 - V   　 显示 版本信息 。           - S       从 标准 输入 流 替代 终端 来 获取 密码           操作系统 相关       后台 可靠 运行 的 几种 方法 ：                     -   后台 运行 加 一个   & amp ;   即可 ， 例如   sleep   100   & amp ;   ， 只能 一直 保持 会话 ， 如果 shell 会话关 了 ， 这个 子 进程 也 会 关掉 。           -   nohup   方式 ：         我们 知道 ， 当 用户 注销 （ logout ） 或者 网络 断开 时 ， 终端 会 收到   HUP （ hangup ） 信号 从而 关闭 其 所有 子 进程 。 因此 ， 我们 的 解决办法 就 有 两种 途径 ： 要么 让 进程 忽略   HUP   信号 ， 要么 让 进程 运行 在 新 的 会 话 里 从而 成为 不 属于 此 终端 的 子 进程 。     只 需 在 要 处理 的 命令 前 加上   nohup   即可 ， 标准 输出 和 标准 错误 缺 省会 被 重定向 到   nohup . out   文件 中 。 一般 我们 可 在 结尾 加上 \" & amp ; \" 来 将 命令 同时 放入 后台 运行 ， 也 可用 \" & gt ; filename   2 & gt ; & amp ; 1 \" 来 更改 缺省 的 重定向 文件名 。                       -   setsid   方式 ：         nohup   无疑 能 通过 忽略   HUP   信号 来 使 我们 的 进程 避免 中途 被 中断 ， 但 如果 我们 换个 角度 思考 ， 如果 我们 的 进程 不 属于 接受   HUP   信号 的 终端 的 子 进程 ， 那么 自然 也 就 不会 受到   HUP   信号 的 影响 了 。 setsid   就 能 帮助 我们 做到 这 一点 。                               使用 的 时候 ， 也 只 需 在 要 处理 的 命令 前 加上   setsid   即可 。     -   当 我们 将 \" & amp ; \" 也 放入 “ ( ) ” 内 之后 ， 我们 就 会 发现 所 提交 的 作业 并 不 在 作业 列表 中 ， 也就是说 ， 是 无法 通过 jobs 来 查看 的 。 让 我们 来 看看 为什么 这样 就 能 躲过   HUP   信号 的 影响 吧 。                                                       subshell   示例     [   root @ pvcent107   ~   ]     #   ( ping   www . ibm . com   & amp ; )       [   root @ pvcent107   ~   ]     #   ps   - ef   | grep   www . ibm . com     root             16270               1         0       14   : 13   pts / 4           00   : 00 : 00   ping   www . ibm . com   root             16278       15362         0       14   : 13   pts / 4           00   : 00 : 00   grep   www . ibm . com     [   root @ pvcent107   ~   ]     #                 从 上例 中 可以 看出 ， 新 提交 的 进程 的 父   ID （ PPID ） 为 1 （ init   进程 的   PID ） ， 并 不是 当前 终端 的 进程   ID 。 因此 并 不 属于 当前 终端 的 子 进程 ， 从而 也 就 不会 受到 当前 终端 的   HUP   信号 的 影响 了 。     -   如果 已经 提交 了 任务 ， 但是 没加 nohup 或者 setsid ， 怎么 补救 ？ 可以 用     disown     命令 来 补救 。             -     disown   - h   jobspec     是 某个 作业 忽略 HUP 信号             -     disown   - ah     是 所有 作业 忽略 HUP 信号             -     disown   - rh     是 正在 运行 的 作业 忽略 HUP 信号     -   更 多 技巧 ， 参考 IBM 文档     1 .   IBM 文档     https : / / www . ibm . com / developerworks / cn / linux / l - cn - nohup /         字符串 工具           grep   字符 匹配                   ##   从 log . txt 中 查找 keyword 出现 的 行     grep   keyword   log . txt     ##   查询 多个 模式     grep   - E     &# 39 ; keyword | otherword &# 39 ;     log . txt   grep   - e   keyword   - e   otherword   log . txt                 grep   - E     扩展 了 默认 的 正则 式 ， 可以 使用 一下 特性 ：             [ ]   字符 选择         +   1 个 和 多个         ?   有 或 无         { }   注意 花 括号 要加 反 斜杠   \ \   转义 ， 因为 bash 中花 括号 有 特殊 含义           sort   命令       将 文件 的 每 一行 作为 一个 单位 ， 进行 排序 ， 从首 字符 开始 ， 按照 ASCII 码 进行 比较 （ 默认 情况 ） 。                         -   u   参数 ， 去 重     -   r   参数 ， 降序     -   o   参数 ， 排序 后 写入 原文件 ， 重定向 会 将 文件 内容 清空 ， 达 不到 要求 ， 可以 用 这个 参数     -   n   参数 ， 表示 不 按照 ASCII 码 排序 ， 而是 按照 数值 大小     -   t   参数   和   k   参数 ， 用来 排序 csv 格式 ， t   指明 分隔符   k   指明 排序 的 列 序号     -   f   参数 ， 忽略 大小写     -   c   参数 ， 检查 是否 排好序 ， 输出 第一个 乱序 行 信息 ， 返回 1 ； C 参数 ， 也 是 检查 ， 但 不 输出 乱序 行 信息     -   M   参数 ， 以 月份 排序 ， 会 识别 月份 （ 只 对 英文 吧 ）     -   b   参数 ， 忽略 每 一行 前面 所有 空白       watch   命令       用来 周期性地 执行 某个 程序 ， 全屏 显示 执行 结果 。 常用 来     tail   log - file   ， 是 的 ， 我 就是 要 干 这个 事情 才 搜到 这个 命令 的 。       常用 参数 ：             - n   或   - - interval     指定 间隔 秒钟 数 ， 缺省 为 2         - d   或   - - differences     高亮 变化 的 区域 ， 如果   - d = cumulative   会 将 每次 变动 积累 并 高亮         - t   或   - no - title     关闭 顶部 时间 显示           最后 接 你 要 周期 执行 的 命令 即可 。       例子               #   监控 log     watch   - n     1     - d   tail   log - file       #   显示 网络连接 变化     watch   - n     1     - d   netstat   - ant       #   每秒 高亮 显示 http 连接数 变化     watch   - n     1     - d     &# 39 ; pstree | grep   http &# 39 ;                   wc     命令       用来 统计 文件 的 文本 行 数 、 单词 数 、 字节数 等 。       参数 ： - c ,   - - bytes     打印 字节数 （ print   the   byte   counts ）       参数 ： - m ,   - - chars     打印 字符 数 （ print   the   character   counts ）       参数 ： - l ,   - - lines     打印 行 数 （ print   the   newline   counts ）       参数 ： - L ,   - - max - line - length     打印 最长 行 的 长度 （ print   the   length   of   the   longest   line ）       参数 ： - w ,   - - words     打印 单词 数 （ print   the   word   counts ）         tar     打包 命令       常用 参数     - v     显示 处理过程 。   - c     创建 ,     - f     指定 文件 ,     - r     追加 ,     - x   解压 ，     - z     gzip 压缩 。       常用 组合 有 ：             tar   cf   files . tar   files     将 files 打包 到 tar 包中         tar   rf   file . tar   file     将 文件 添加 到 tar 包中 ， 原来 已经 有个 tar 包了         tar   tf   file . tar     测试 tar 完整性         tar   xf   file . tar     解压 tar 包         tar   zcf   file . tar . gz   files     将 files 打包 并 压缩 到   file . tar . gz         tar   zxf   file . tar . gz     解压包           网络 命令             netstat     查看 网络 状态         iptables             VIM       三种 模式           插入 模式 ： 可以 输入 文本 ， 按   i   进入       编辑 模式 ： 按   ESC   进入 ， 可以 移动 和 操纵 文本       命令 模式 ： 可以 执行 冒号 命令 ， 常用 的 有   : w   [ filename ]   写入 文件 ，   : q   退出 vim 。           移动 光标       在 编辑 模式 下 操作 。           基本 移动 ：       k       上       j       下       h       左       l       右               大 范围 移动                           ctrl   +   f   前进 ( forward ) 一页       ctrl   +   b   后退 ( back ) 一页               更大 范围 移动         *   和   #     搜索 当前 光标 所在 的     单词     ， 跳转 到 下 / 上 个 地方         (   和   )     移动 到 前 / 后 一个     句子     的 开始 ， 以 英文 句号 为准 ， 对 中文 貌似 没用 ， 相当于 段落 跳转         {   和   }     在     段落     间 移动         fa     前进 到 字符 a ， a 可 换成 其他 字符         gg     文件 起始 位置         G     最后 一行 起始 位置         ngg     n 行 起始 位置               当前页 跳转         H     当前页 的 header ，   3H   当前页 第 3 行         M     当前页 的 middle         L     当前页 最后 一行   ，   3L   当期 那 也 倒数第 3 行               行内 移动         g _     行尾 ， 空字符 不算         ^     行首 ， 空字符 不算         $     行尾         0     行首         w     下 一个 word 的 开头         e     下 一个 word 的 结尾         b     前 一个 word 的 开头               搜索 匹配         / str   ， 在 编辑 模式 下 输入   /   ， 后面 跟 要 搜索 字符串 str ， 然后 回车 搜索 。         ? str   ， 反向 搜索 字符串 str         n   ， 搜索 到 匹配 后 继续 向 后 搜索         N   ， 搜索 到 匹配 后 ， 继续 向前 搜索                   编辑 命令           替换 与 删除 ： 用   d   删除 行 ， 用   x   删除 字符 ， 用   r   替换 字符 。         rc     用 字符 c 替换 当前 字符         nrc     用 c 替换 前 n 个字符         x     删除 光标 字符         nx     删除 光标 右侧 n 个字符         dw     删除 光标 右侧 的 word         ndw     删除 n 个 word         db     删除 光标 左侧 一个 word         ndb     删除 光标 左侧 n 个 word         dd     删除 一行         ndd     删除 n 行         d $     删除 字符 到 行尾         d0     删除 字符 到 行首         J     删除 本行 回车 ， 合并 到 下行               用 输入 的 文本 替换 ， 进入 插入 模式         s     输入 文本 替换 当期 字符 ， n 个版   ns           S     删除 当前 行 进入 编辑 模式 ， n 个版   nS           cw     替换 右侧 一个 字 ， n 个版   ncw           cW   ,   c $     替换 右侧 所有 字         cb     替换 左侧 一个 字 ， n 个版   ncb           cd     替换 当前 行 ， n 个版   ncd   ， 试 过 但是 无效 ？ ！         c $     替换 到 行尾 所有         c0     替换 到 行首 所有               撤销         u     undo ， 撤销 最后 一个 操作         .     redo ， 重复 最有 一个 操作 ， 或者   CTRL   +   R                 复制   y   粘贴   p           yy     复制 一行 到 缓冲区 ， n 行版   nyy           + y     复制到 系统 剪贴板 ， n 行版   + nyy   ， mac 貌似 无效         p     将 缓冲区 内容 粘贴 到 光标 后面         P     到 前面                   插入 模式           插入 命令 有 ：         i     光标 左侧 插入         I     行 开头 插入         a     右侧 插入         A     行 结尾 插入         o     下 一行 插入         O     上 一行 插入               退出 插入 模式 命令   ESC ,   ctrl   +   [   ， 将 进入 编辑 模式           命令 模式       编辑 模式 下 按   :   号 ， 进入 命令 模式 ， 输入 命令 后 回车 即可 运行 命令 。                       -   文件 命令             -     : e   path - to - file     打开 文件             -     : w     保存 当前 文件             -     : w   path - to - file     另存为             -     : q     退出 ， 强制 版   : q !               -     : wq     保存 并 退出             -     : f   filename     重命名 文件             -     : r   filename     读取 filename 到 光标 后面     -   行号 与 文件             -     : 12     移动 到 第 12 行             -     : 12w   filename     第 12 行 写入 到 文件             -     : 12 , 32w   filename     将 第 12 - 32 行 写入 到 文件 ， 这里 可以 用   $   表示 最后 一行 ， 用   .   表示 当前 行 ， 还 可以 用   .+ 5   表示 当前 行后 的 第 5 行     -   命令 模式 下 的 搜索             -     : / str /   正向 搜索   str               -     : ? str ?   反向 搜索   str               -     : / str / w   file       正向 搜索 ， 并 将 第一个 包含 字符串   str   的 行 写入   file   文件             -     : / str1 / , / str2 / w   file     正向 搜索 ， 并 将 包含 字符串   str1   的 行至 包含 字符串   str2   的 行写     -   搜索 正则表达式             ^                                 放在 字符串 前面 ， 匹配 行首 的 字 ；   $                                 放在 字符串 后面 ， 匹配 行尾 的 字 ；     \ \ & lt ;                                 匹配 一个 字 的 字头 ；     \ \ & gt ;                                 匹配 一个 字 的 字 尾 ；   .                                 匹配 任何 单个 正文 字符 ；     [   str   ]                           匹配   str   中 的 任何 单个 字符 ；     [   ^ str   ]                         匹配 任何 不 在   str   中 的 单个 字符 ；     [   a - b   ]                           匹配   a   到   b   之间 的 任一 字符 ；   *                                 匹配 前 一个 字符 的     0     次 或 多次 出现 ；     \ \                                   转义 后面 的 字符 。                       正文 替换 :   : s   命令             : % s / str1 / str2 /     用 字符串   str2   替换 行中 首次 出现 的 字符串   str1             : s / str1 / str2 / g     用 字符串   str2   替换 行中 所有 出现 的 字符串   str1               g   放在 命令 末尾 ， 表示 对 搜索 字符串 的 每次 出现 进行 替换 , 不止 匹配 每行 中 的 第一次 出现 ； 不 加   g ， 表示 只 对 搜索 字符串 的 首次 出现 进行 替换 ； g   放在 命令 开头 ， 表示 对 正文 中 所有 包含 搜索 字符串 的 行 进行 替换 操作 ;           s   表示 后面 跟着 一串 替换 的 命令 ；       %   表示 替换 范围 是 所有 行 ， 即 全文 。                   统计 出现 次数 命令 ：   : % s / str1 / & amp ; / gn             删除 正文 命令 ： 略           恢复 意外 退出 未 保存 的 文件   : recover   命令               执行 shell 命令 ：   : ! some - command   ， 执行 完后 回到 vim               分屏             : split   缩写   : sp   上下 分屏         : vsplit   缩写   : vsp     左右分 屏       分屏 后 切换   ctrl   +   w     [ hjkl ]   ， 也 可以 直接 用   Ctrl + w + w   顺序 切换       设置 分屏 尺寸         ctrl   +   w   =     所有 屏幕 等 高 或 等 宽         ctrl   +   w   -     减少 高度         ctrl   +   w   +     增加 高度               可以 在 分屏 中 用   : q   关闭 文件 即可 关闭 分屏         : only     关闭 当前 分屏 外 所有 屏幕                   标签 页             : tabnew   [ filename ]     创建 新 标签 文件名 可 选 ， 有 表示 打开 文件 ,   和   : tabe   差不多         : tabc     关闭 当前 标签 close         : tabo     关闭 当前 标签 外 的 所有 标签 only         : tabn     下 一个 标签 next         : tabp     跳转 到 前 一个 标签 preview         : tabr     和     : tabfir     到 第一个   first         : tabl     最后 一个 last         : tabdo     为 每个 标签 执行命令                   选项 设置 ， 命令   : set   option                       autoindent                 设置 该 选项 ， 则 正文 自动 缩进   ignorecase                 设置 该 选项 ， 则 忽略 规则 表达式 中 大 小写字母 的 区别   number                         设置 该 选项 ， 则 显示 正文 行号   ruler                           设置 该 选项 ， 则 在 屏幕 底部 显示 光标 所在 行 、 列 的 位置   tabstop                       设置 按   Tab   键 跳 过 的 空格 数 。 例如   : set     tabstop     =   n ， n   默认值 为     8     mk                                 将 选项 保存 在 当前目录 的   . exrc   文件 中               外部 工具 集成             vim   +   diff   =   vimdiff     使用 方法   vimdiff   f1   f2     或者     vim   - d         在 已经 打开 的 文件 中 执行命令   : diffsplit   filename2   。       : diffpatch   ，   diffsplit   默认 上下 分屏 ， 可以 用   : vert   diffsplit   实现 左右分 屏         : diffoff   关闭 diff 的 颜色         : diffu [ pdate ]     更新 diff                 xxd   二进制 编辑         vim   - b     编辑 二进制 文件         % ! xxd     在 vim 内 切换 到 二进制 编辑         % ! xxd   - r     切 回到 文本编辑 模式                   参考             http : / / www . jianshu . com / p / bcbe916f97e1        ", "tags": "tools", "url": "/wiki/tools/linux.html"},
      
      
      {"title": "Linux基本安装", "text": "    Table   of   Contents           源码 安装 gcc6           源码 安装   Python27           安装   TensorFlow           gcc 常用 选项                 源码 安装 gcc6       以   redhat   环境 安装 gcc6 为例 ：           首先 下载   gcc - 6.3 . 0 . tar . gz       解压     tar   xzf   gcc - 6.3 . 0 . tar . gz         下载 必须 的 依赖     . / contrib / download _ prerequisites     ， 会 直接 从   gnu   官网 下载 ， 国内 可能 很 慢 ，     可以 修改 这个 文件 中 的 命令 ， 使用 镜像 的 源 ， 推荐 一个 源 挺快 的     https : / / ftp . mirrorservice . org / sites / sourceware . org / pub / gcc / infrastructure /   ， 也 可以 使用 国内 的 其他 源 ， 比如   USTC ,   THU   的 镜像 。       在 gcc 目录 外键 一个 目标 文件目录     cd   ..     & amp ;     mkdir   objdir         配置 gcc     .. / gcc - 6.3 . 0 / configure   - - prefix = $ HOME / gcc - 6.3 . 0     ， 可以 将   prefix 参数 修改 为 你 想要 的 位置       多线程 make     make   - j4     修改 j 参数 调整 线程 数 ， 这个 过程 需要 较长时间 ， 建议 用   nohup   后台 编译 。       安装     make   install             源码 安装   Python27           下载   Python - 2.7 . 13 . tgz       解压     tar   xzf   Python - 2.7 . 13 . tgz     然后 进入 解压 后 目录     cd   Python - 2.7 . 13         配置     . / configure   - - prefix = $ HOME / local / usr / Python - 2.7 . 13     - - enable - shared     可以 修改   prefix   指定 安装 目录 ， 使能 共享 库 ；     如果 要 安装   TensorFlow ， 需要 使能   ucs4   支持 ， 增加 参数     - - enable - unicode = ucs4     即可 。       安装     make   - j4   & amp ; & amp ;   make   install             安装   TensorFlow       可能 存在 TensorFlow 的   glibc   和   libcxx   版本 与 系统 自带 的 版本 不 一致 ， 会 导致   import   异常 ， 我 是 参考 这个 链接 解决 的 ：       https : / / stackoverflow . com / questions / 33655731 / error - while - importing - tensorflow - in - python2 - 7 - in - ubuntu - 12 - 04 - glibc - 2 - 17 - not - f   。 可能 不 一定 适合 你 的 情况 ， 但是 原理 是 一样 的 ， 即 手动 下载   tf   支持 的   glibc   和 libcxx 版本 ， 然后 设置 环境变量     LD _ LIBRARY _ PATH     使   python   加载 正确 的 版本 。 我 的 命令 如下             mkdir   ~ / libcenv     cd     ~ / libcenv   wget   http : / / launchpadlibrarian . net / 137699828 / libc6 _ 2.17 - 0ubuntu5 _ amd64 . deb   wget   http : / / launchpadlibrarian . net / 137699829 / libc6 - dev _ 2.17 - 0ubuntu5 _ amd64 . deb     ###   论坛 中 的 那个 链接 此时 已经 失效 ， 换 一个 版本 也 可以 ， 应该 只要 版本 高于 tf 要求 的 libcxx 版本 即可 ， 没有 测试 过 ， 不负责任     wget   ftp : / / rpmfind . net / linux / centos / 7.3 . 1611 / os / x86 _ 64 / Packages / libstd c++ - 4.8 . 5 - 11 . el7 . x86 _ 64 . rpm   ar   p   libc6 _ 2.17 - 0ubuntu5 _ amd64 . deb   data . tar . gz     |     tar   zx   ar   p   libc6 - dev _ 2.17 - 0ubuntu5 _ amd64 . deb   data . tar . gz     |     tar   zx   rpm2cpio   libstd c++ - 4.8 . 5 - 11 . el7 . x86 _ 64 . rpm   |     cpio   - idmv               done ！       最后 一步 ， 我 安装 python 的 时候 忘记 指定   - - enable - unicode = ucs4   ， 导致 报错     undefined   symbol :   PyUnicodeUCS4 _ AsUTF8String   ， 重新 编译   python 即可 ！ 对此 的 解释 可以 参考     http : / / blog . csdn . net / taolinke / article / details / 50472451         运行 时要 指定 环境变量 ， 你 也 可以 通过 shell 函数 来 做         LD _ LIBRARY _ PATH = \" $ HOME / libcenv / lib / x86 _ 64 - linux - gnu / : $ HOME / libcenv / usr / lib64 / \"   $ HOME / libcenv / lib / x86 _ 64 - linux - gnu / ld - 2.17 . so   $ HOME / local / usr / Python - 2.7 . 13 / bin / python         gcc 常用 选项             - I & lt ; path & gt ;     增加   include   目录 , 可以 多次 使用         - L & lt ; path & gt ;     增加 动态 库 搜索 目录 , 可以 多次 使用 , 链接 时 的 目录         - Wl , - rpath , & lt ; path & gt ;     增加 动态 库 搜索 目录 , 可以 多次 使用 , 运行 时 的 目录           参考   http : / / www . runoob . com / w3cnote / gcc - parameter - detail . html    ", "tags": "tools", "url": "/wiki/tools/gcc.html"},
      
      
      {"title": "maven", "text": "    Table   of   Contents           关于           流程           创建 maven 项目           构建 项目           运行 项目                   核心 概念           Pom ,   project   object   Model .           Maven   插件           Maven   生命周期           Maven   依赖 管理           Maven   库                   maven 常用 参数 和 命令           参考 链接                 关于       Maven   是 一个 项目管理 和 构建 自动化 工具 。 但是 对于 我们 程序员 来说 ， 我们 最 关心 的 是 它 的 项目 构建 功能 。     所以 这里 我们 介绍 的 就是 怎样 用   maven   来 满足 我们 项目 的 日常 需要 。       流程       创建 maven 项目             mvn   archetype : generate   - DgroupId   =   com . mycompany . helloworld   - DartifactId   =   helloworld   - Dpackage   =   com . mycompany . helloworld   - Dversion   =     1   . 0 - SNAPSHOT               archetype : generate   目标 会 列出 一系列 的   archetype   让 你 选择 。   Archetype   可以 理解 成 项目 的 模型 。   Maven   为 我们 提供 了 很 多种 的 项目 模型 ， 包括 从 简单 的   Swing   到 复杂 的   Web   应用 。 我们 选择 默认 的   maven - archetype - quickstart   ， 是 编号   # 807 ， 不同 版本 这个 值会 不同 ， 不过 都 会 是 默认值 ，     所以 按 回车 就 好 了 。       创建 的 配置 属性 对应 的 意义 参考 pom 文件 说明 ， 这些 属性 是 我们 在 命令行 中 用   - D   选项 指定 的 。 该 选项 使用   - Dname = value   的 格式 。       构建 项目               cd     helloworld   mvn   package               当 你 第一次 运行   maven   的 时候 ， 它会 从 网上 的   maven   库   ( repository )   下载 需要 的 程序 ， 存放 在 你 电脑 的 本地 库   ( local   repository )   中 ， 所以 这个 时候 你 需要 有   Internet   连接 。 Maven   默认 的 本地 库是   ~ / . m2 / repository /   ， 在   Windows   下 是   % USER _ HOME % . m2 \ \ repository \ \   。       maven   在   helloworld   下面 建立 了 一个 新 的 目录   target /   ， 构建 打包 后 的   jar   文件   helloworld - 1.0 - SNAPSHOT . jar   就 存放 在 这个 目录 下 。 编译 后 的   class   文件 放在   target / classes /   目录 下面 ， 测试   class   文件 放在   target / test - classes /   目录 下面 。       运行 项目       为了 验证 我们 的 程序 能 运行 ， 执行 下面 的 命令 ：             java   - cp   target / helloworld - 1.0 - SNAPSHOT . jar   com . mycompany . helloworld . App               核心 概念       Pom ,   project   object   Model .       配置文件 ， maven 根据 该 文件 构建 项目 ， 可以 继承 。 各 节点 的 含义     -   project ： 顶级 元素     -   modelVersion ： object   model 版本 ， 强制     -   groupId ： 项目 创建 团体 或 组织 的 唯一 标识 ， 通常 是 域名 倒 写 ， 例如   org . apache . maven . plugins     -   artifactId   ： 是 项目 artifact 唯一 的 基 地址 名     -   packaging   ： artifact 打包 的 方式 ， 如 jar 、 war 、 ear 等等 。 默认 为 jar 。 这个 不仅 表示 项目 最终 产生 何种 后缀 的 文件 ，         也 表示 build 过程 使用 什么样 的 lifecycle 。     -   version   ： artifact 的 版本 ， 通常 能 看见 为 类似 0.0 . 1 - SNAPSHOT ， 其中 SNAPSHOT 表示 项目 开发 中 ， 为 开发 版本     -   name ：   表示 项目 的 展现 名 ， 在 maven 生成 的 文档 中 使用     -   url ： 表示 项目 的 地址 ， 在 maven 生成 的 文档 中 使用     -   description ：   表示 项目 的 描述 ， 在 maven 生成 的 文档 中 使用     -   dependencies ：   表示 依赖 ， 在子 节点 dependencies 中 添加 具体 依赖 的 groupId   artifactId 和 version     -   build ：   表示 build 配置     -   parent ：   表示 父 pom       在   POM   中 ， groupId ,   artifactId ,   packaging ,   version   叫作   maven   坐标 ， 它 能 唯一 的 确定 一个 项目 。 有 了   maven   坐标 ， 我们 就 可以 用 它 来 指定 我们 的 项目 所 依赖 的 其他 项目 ， 插件 ， 或者 父 项目 。 一般   maven   坐标 写成 如下 的 格式 ：   groupId : artifactId : packaging : version   。     像 我们 的 例子 就 会 写成 ：   com . mycompany . helloworld :   helloworld :   jar :   1.0 - SNAPSHOT   .       我们 的   helloworld   示例 很 简单 ， 但是 大 项目 一般 会 分成 几个 子项目 。 在 这种 情况 下 ， 每个 子项目 就 会 有 自己 的   POM   文件 ， 然后 它们 会 有 一个 共同 的 父 项目 。 这样 只要 构建 父 项目 就 能够 构建 所有 的 子项目 了 。 子项目 的   POM   会 继承 父 项目 的   POM 。 另外 ， 所有 的   POM 都 继承 了 一个   Super - POM 。 Super - POM   设置 了 一些 默认值 ， 它 遵循 了 惯例 优于 配置 的 原则 。 所以 尽管 我们 的 这个   POM   很 简单 ， 但是 这 只是 你 看得见 的 一部分 。 运行 时候 的   POM   要 复杂 的 多 。   如果 你 想 看到 运行 时候 的   POM   的 全部内容 的话 ， 可以 运行 下面 的 命令 ：   mvn   help : effective - pom   .       Maven   插件       我们 用 了     mvn   archetype : generate     命令 来 生成 一个 项目 。 那么 这里 的     archetype : generate     是 什么 意思 呢 ？ archetype   是 一个 插件 的 名字 ， generate 是 目标 ( goal ) 的 名字 。 这个 命令 的 意思 是 告诉   maven   执行   archetype   插件 的   generate   目标 。 插件 目标 通常 会 写成     pluginId : goalId         一个 目标 是 一个 工作 单元 ， 而 插件 则 是 一个 或者 多个 目标 的 集合 。 比如说 Jar 插件 ， Compiler 插件 ， Surefire 插件 等 。 从 看 名字 就 能 知道 ， Jar   插件 包含 建立 Jar 文件 的 目标 ，   Compiler   插件 包含 编译 源代码 和 单元测试 代码 的 目标 。 Surefire   插件 的话 ， 则 是 运行 单元测试 。       看到 这里 ， 估计 你 能 明白 了 ， mvn   本身 不会 做太多 的 事情 ， 它 不 知道 怎么样 编译 或者 怎么样 打包 。 它 把 构建 的 任务 交给 插件 去 做 。 插件 定义 了 常用 的 构建 逻辑 ， 能够 被 重复 利用 。 这样 做 的 好处 是 ， 一旦 插件 有 了 更新 ， 那么 所有 的   maven   用户 都 能 得到 更新 。       Maven   生命周期       我们 用 的 第二个 命令 是 ： mvn   package 。 这里 的   package   是 一个 maven 的 生命周期 阶段   ( lifecycle   phase   ) 。 生命周期 指 项目 的 构建 过程 ， 它 包含 了 一系列 的 有序 的 阶段   ( phase ) ， 而 一个 阶段 就是 构建 过程 中 的 一个 步骤 。       那么 生命周期 阶段 和 上面 说 的 插件 目标 之间 是 什么 关系 呢 ？ 插件 目标 可以 绑定 到 生命周期 阶段 上 。 一个 生命周期 阶段 可以 绑定 多个 插件 目标 。 当   maven   在 构建 过程 中 逐步 的 通过 每个 阶段 时 ， 会 执行 该 阶段 所有 的 插件 目标 。       maven   能 支持 不同 的 生命周期 ， 但是 最 常用 的 是 默认 的 Maven 生命周期   ( default   Maven   lifecycle   ) 。 如果 你 没有 对 它 进行 任何 的 插件 配置 或者 定制 的话 ， 那么 上面 的 命令   mvn   package   会 依次 执行 默认 生命周期 中 直到 包括   package   阶段 前 的 所有 阶段 的 插件 目标 ：     -   process - resources   阶段 ： resources : resources     -   compile   阶段 ： compiler : compile     -   process - classes   阶段 ： ( 默认 无 目标 )     -   process - test - resources   阶段 ： resources : testResources     -   test - compile   阶段 ： compiler : testCompile     -   test   阶段 ： surefire : test     -   prepare - package   阶段 ： ( 默认 无 目标 )     -   package   阶段 ： jar : jar       Maven   依赖 管理       之前 我们 说 过 ， maven   坐标 能够 确定 一个 项目 。 换句话说 ， 我们 可以 用 它 来 解决 依赖 关系 。 在   POM   中 ， 依赖 关系 是 在   dependencies   部分 中 定义 的 。 在 上面 的   POM   例子 中 ， 我们 用   dependencies   定义 了 对于   junit   的 依赖 ：               & lt ; dependencies & gt ;               & lt ; dependency & gt ;                   & lt ; groupId & gt ;   junit   & lt ; / groupId & gt ;                   & lt ; artifactId & gt ;   junit   & lt ; / artifactId & gt ;                   & lt ; version & gt ;   3.8 . 1   & lt ; / version & gt ;                   & lt ; scope & gt ;   test   & lt ; / scope & gt ;               & lt ; / dependency & gt ;       & lt ; / dependencies & gt ;                 那 这个 例子 很 简单 ， 但是 实际 开发 中 我们 会 有 复杂 得 多 的 依赖 关系 ， 因为 被 依赖 的   jar   文件 会 有 自己 的 依赖 关系 。 那么 我们 是不是 需要 把 那些 间接 依赖 的   jar   文件 也 都 定义 在 POM 中 呢 ？ 答案 是 不 需要 ， 因为   maven   提供 了 传递 依赖 的 特性 。       所谓 传递 依赖 是 指   maven   会 检查 被 依赖 的   jar   文件 ， 把 它 的 依赖 关系 纳入 最终 解决 的 依赖 关系 链中 。 针对 上面 的   junit   依赖 关系 ， 如果 你 看 一下   maven   的 本地 库 （ 我们 马上会 解释   maven   库 ） ~ / . m2 / repository / junit / junit / 3.8 . 1 /   ， 你 会 发现   maven   不但 下载 了   junit - 3.8 . 1 . jar ， 还 下载 了 它 的   POM   文件 。 这样   maven   就 能 检查   junit   的 依赖 关系 ， 把 它 所 需要 的 依赖 也 包括 进来 。       在   POM   的   dependencies   部分 中 ， scope   决定 了 依赖 关系 的 适用范围 。 我们 的 例子 中   junit   的   scope   是   test ， 那么 它 只会 在 执行   compiler : testCompile   and   surefire : test   目标 的 时候 才 会 被 加到   classpath   中 ， 在 执行   compiler : compile   目标 时 是 拿 不到   junit   的 。       我们 还 可以 指定   scope   为   provided ， 意思 是   JDK   或者 容器 会 提供 所 需 的 jar 文件 。 比如说 在 做 web 应用 开发 的 时候 ， 我们 在 编译 的 时候 需要   servlet   API   jar   文件 ， 但是 在 打包 的 时候 不 需要 把 这个   jar   文件 打 在   WAR   中 ， 因为 servlet 容器 或者 应用服务器 会 提供 的 。       scope   的 默认值 是   compile ， 即 任何 时候 都 会 被 包含 在   classpath   中 ， 在 打包 的 时候 也 会 被 包括 进去 。       Maven   库       当 第一次 运行   maven   命令 的 时候 ， 你 需要   Internet   连接 ， 因为 它 要 从 网上 下载 一些 文件 。 那么 它 从 哪里 下载 呢 ？ 它 是从   maven   默认 的 远程 库 ( http : / / repo1 . maven . org / maven2 )   下载 的 。 这个 远程 库有   maven   的 核心 插件 和 可供 下载 的   jar   文件 。       但是 不是 所有 的   jar   文件 都 是 可以 从 默认 的 远程 库 下载 的 ， 比如说 我们 自己 开发 的 项目 。 这个 时候 ， 有 两个 选择 ： 要么 在 公司 内部 设置 定制 库 ， 要么 手动 下载 和 安装 所 需 的 jar 文件 到 本地 库 。       本地 库是 指   maven   下载 了 插件 或者   jar   文件 后 存放 在 本地 机器 上 的 拷贝 。 在   Linux   上 ， 它 的 位置 在   ~ / . m2 / repository ， 在   Windows   XP   上 ， 在   C : \ \ Documents   and   Settings \ \ username . m2 \ \ repository   ， 在   Windows   7   上 ， 在   C : \ \ Users \ \ username . m2 \ \ repository 。 当   maven   查找 需要 的   jar   文件 时 ， 它会 先 在 本地 库中 寻找 ， 只有 在 找 不到 的 情况 下 ， 才 会 去 远程 库中 找 。       运行 下面 的 命令 能 把 我们 的   helloworld   项目 安装 到 本地 库 ：                         $ mvn     install               一旦 一个 项目 被 安装 到 了 本地 库后 ， 你 别的 项目 就 可以 通过   maven   坐标 和 这个 项目 建立 依赖 关系 。 比如 如果 我 现在 有 一个 新 项目 需要 用到   helloworld ， 那么 在 运行 了 上面 的   mvn   install   命令 后 ， 我 就 可以 如下 所示 来 建立 依赖 关系 ：                       & lt ; dependency & gt ;                   & lt ; groupId & gt ;   com . mycompany . helloworld   & lt ; / groupId & gt ;                   & lt ; artifactId & gt ;   helloworld   & lt ; / artifactId & gt ;                   & lt ; version & gt ;   1.0 - SNAPSHOT   & lt ; / version & gt ;               & lt ; / dependency & gt ;                 好 了 ， maven   的 核心 概念 就 简单 的 介绍 到 这里 。 至于 在   Eclipse   中 如何 使用   maven ， 这个 网上 很多 了 ， google   一下 就行 。       maven 常用 参数 和 命令           mavn 常用 参数                 mvn   - e   显示 详细 错误   mvn   - U   强制 更新 snapshot 类型 的 插件 或 依赖 库 （ 否则 maven 一天 只会 更新 一次 snapshot 依赖 ）   mvn   - o   运行 offline 模式 ， 不 联网 更新 依赖   mvn   - N 仅 在 当前 项目 模块 执行命令 ， 关闭 reactor   mvn   - pl   module _ name 在 指定 模块 上 执行命令   mvn   - ff   在 递归 执行命令 过程 中 ， 一旦 发生 错误 就 直接 退出   mvn   - Dxxx = yyy 指定 java 全局 属性   mvn   - Pxxx 引用 profile   xxx                   Lifecycle   中 的 命令                 mvn   test - compile   编译 测试代码   mvn   test   运行 程序 中 的 单元测试   mvn   compile   编译 项目   mvn   package   打包 ， 此时 target 目录 下会 出现 maven - quickstart - 1.0 - SNAPSHOT . jar 文件 ， 即 为 打包 后 文件   mvn   install   打包 并 安装 到 本地 仓库 ， 此时 本 机 仓库 会 新增 maven - quickstart - 1.0 - SNAPSHOT . jar 文件 。   每个 phase 都 可以 作为 goal ， 也 可以 联合 ， 如 之前 介绍 的 mvn   clean   install                   其他 命令                 mvn   archetype : generate   创建 maven 项目   mvn   package   打包 ， 上面 已经 介绍 过 了   mvn   package   - Prelease 打包 ， 并 生成 部署 用 的 包 ， 比如 deploy / * . tgz   mvn   install   打包 并 安装 到 本地 库   mvn   eclipse : eclipse   生成 eclipse 项目 文件   mvn   eclipse : clean   清除 eclipse 项目 文件   mvn   site   生成 项目 相关 信息 的 网站               参考 链接             http : / / www . oracle . com / technetwork / cn / community / java / apache - maven - getting - started - 1 - 406235 - zhs . html           http : / / www . oracle . com / technetwork / cn / community / java / apache - maven - getting - started - 2 - 405568 - zhs . html           http : / / www . trinea . cn / android / maven /           http : / / maven . apache . org / guides / index . html        ", "tags": "tools", "url": "/wiki/tools/maven.html"},
      
      
      {"title": "PowerMock在单元测试中的应用", "text": "    Table   of   Contents           关于           配置 环境           应用                 关于           参考 IBM 教程 ,     https : / / www . ibm . com / developerworks / cn / java / j - lo - powermock / index . html             配置 环境               & lt ; properties & gt ;                       & lt ; project . build . sourceEncoding & gt ;   UTF - 8   & lt ; / project . build . sourceEncoding & gt ;                       & lt ; powermock . version & gt ;   2.0 . 2   & lt ; / powermock . version & gt ;               & lt ; / properties & gt ;                 & lt ; dependencies & gt ;                       & lt ; dependency & gt ;                               & lt ; groupId & gt ;   org . powermock   & lt ; / groupId & gt ;                               & lt ; artifactId & gt ;   powermock - module - junit4   & lt ; / artifactId & gt ;                               & lt ; version & gt ;   $ { powermock . version }   & lt ; / version & gt ;                               & lt ; scope & gt ;   test   & lt ; / scope & gt ;                       & lt ; / dependency & gt ;                       & lt ; dependency & gt ;                               & lt ; groupId & gt ;   org . powermock   & lt ; / groupId & gt ;                               & lt ; artifactId & gt ;   powermock - api - mockito2   & lt ; / artifactId & gt ;                               & lt ; version & gt ;   $ { powermock . version }   & lt ; / version & gt ;                               & lt ; scope & gt ;   test   & lt ; / scope & gt ;                       & lt ; / dependency & gt ;                         & lt ; dependency & gt ;                               & lt ; groupId & gt ;   junit   & lt ; / groupId & gt ;                               & lt ; artifactId & gt ;   junit   & lt ; / artifactId & gt ;                               & lt ; version & gt ;   4.12   & lt ; / version & gt ;                               & lt ; scope & gt ;   test   & lt ; / scope & gt ;                       & lt ; / dependency & gt ;               & lt ; / dependencies & gt ;                 应用           模拟 静态方法 :   模拟 静态方法 返回 指定 的 值       模拟 构造函数 :   模拟 构造函数 返回 的 结果       模拟 私有 以及 final 方法         mock     对象 对于 未指定 处理 规则 的 调用 会 按照 方法 返回值 类型 返回 该 类型 的 默认值 ( int , long 返回 0 ,   boolean 返回 false , 对象 则 返回 null , void 则 什么 都 不 做 )         spy   对象 对于 未指定 处理 规则 的 时候 会 调用 真实 方法       初始化 静态 field ,     SuppressStaticInitializationFor             http : / / codyaray . com / 2012 / 05 / mocking - static - java - util - logger - with - easymocks - powermock - extension           https : / / github . com / powermock / powermock / wiki / Mock - Policies                ", "tags": "tools", "url": "/wiki/tools/powermock.html"},
      
      
      {"title": "protobuff 快速入门", "text": "    Table   of   Contents           定义 消息 结构           Field   Rules           基础 数据类型                   编译 生成 目标语言 代码                 定义 消息 结构       libsvm . proto   libffm 数据 ,   field 可选 ,   rowid 和 label 也 是 可选项 。               syntax       =       & quot ; proto2 & quot ;     ;         package       tutorial     ;         option       java _ package       =       & quot ; com . tracholar . protobuf & quot ;     ;       option       java _ outer _ classname       =       & quot ; Libsvm & quot ;     ;           message       LabelPoint       {               message       Point       {                       required       int32       x       =       1     ;       / /   idx                       required       float       y       =       2     ;       / /   value                       optional       int32       f       =       3     ;       / /   field               }               repeated       Point       feature       =       1     ;               optional       float       label       =       2     ;               optional       string       rowid       =       3     ;       }         message       DataSet       {               repeated       LabelPoint       lp       =       1     ;       }                 Field   Rules           required :   必须 有 的 字 段 .       optional :   可选 的 字 段 .       repeated :   可以 有 多个 的 字 段 , 对应 于   List ,   保序 .           基础 数据类型         https : / / developers . google . com / protocol - buffers / docs / proto         int32 ,   float ,   string ,   枚举 类型               enum       EnumNotAllowingAlias       {           UNKNOWN       =       0     ;           STARTED       =       1     ;       }                 编译 生成 目标语言 代码       编译 生成 Java   代码     protoc   - - java _ out = & lt ; JAVA 代码 输出 路径 & gt ;     libsvm . proto   。       编译器     protoc     可以 直接 从 官网 下载     https : / / github . com / protocolbuffers / protobuf / releases         找到     protoc - x . x . x - & lt ; OS & gt ; . zip     下载 解压 即可 。       把 编译器 输出 的 文件 拖 到 合适 的 目录 即可 , 也 可以 在     - - java _ out     参数 中 指定 输出 目录 。       以 JAVA 为例 , 需要 在 工程 的 pom . xml 文件 中 添加 依赖 才能 使用 protobuf 的 API               & lt ; dependency & gt ;               & lt ; groupId & gt ;   com . google . protobuf   & lt ; / groupId & gt ;               & lt ; artifactId & gt ;   protobuf - java   & lt ; / artifactId & gt ;               & lt ; version & gt ;   3.5 . 1   & lt ; / version & gt ;       & lt ; / dependency & gt ;                 测试 序列化 与 反 序列化 ,   序列化 的 对象 都 是 通过     Builder     来 创建 。           序列化                   public       class       TestLibsvm       {               public       static       void       main     (     String     [ ]       args     )       {                       Libsvm     .     DataSet     .     Builder       dataSet       =       Libsvm     .     DataSet     .     newBuilder     ( ) ;                       Random       r       =       new       Random     ( ) ;                       for     (     int       i     =     0     ;       i     & lt ;       1000     ;       i     ++ ) {                               Libsvm     .     LabelPoint     .     Builder       lp       =       Libsvm     .     LabelPoint     .     newBuilder     ( ) ;                               for     (     int       j     =     0     ;       j     & lt ;     5     ;       j     ++ ) {                                       Libsvm     .     LabelPoint     .     Point       pb       =       Libsvm     .     LabelPoint     .     Point                                                       .     newBuilder     ( )                                                       .     setF     (     j     )                                                       .     setX     (     r     .     nextInt     ( ) )                                                       .     setY     (     r     .     nextFloat     ( ) )                                                       .     build     ( ) ;                                       lp     .     addFeature     (     pb     ) ;                               }                               / / lp . setRowid ( r . nextLong ( )   +   & quot ; & quot ; ) ;                               dataSet     .     addLp     (     lp     ) ;                       }                       Libsvm     .     DataSet       dataset       =       dataSet     .     build     ( ) ;                       try       {                               dataset     .     writeTo     (     new       FileOutputStream     (     & quot ; train . data & quot ;     ) ) ;                       }     catch       (     IOException       e     ) {                               e     .     printStackTrace     ( ) ;                       }               }       }                     反 序列化 :   反 序列化 非常简单 , 直接 调用 消息 对象 的 静态方法     parseFrom     即可 ,                     Libsvm     .     DataSet     .     parseFrom     (     new       FileInputStream     (     & quot ; train . data & quot ;     ) )                 上述 序列化 与 反 序列化 都 是 针对 中间 结果 是 二进制 数据 , 如果 中间 数据 使用 文本格式 , 需要 用   TextFormat   对象 转换 。 以 python 为例               text _ format     .     MessageToString     (     message     [ ,       as _ one _ line     =     True     [ ,       as _ utf8     =     True     ] ] )       text _ format     .     Parse     (     pbstring     ,       message     )                 而 在 Java 和 C++ 中 , 可以 直接 调用     toString ( )     和     toDebugString ( )     这种 方法 实现 。   https : / / stackoverflow . com / questions / 33557965 / print - human - friendly - protobuf - message    ", "tags": "tools", "url": "/wiki/tools/protobuff.html"},
      
      
      {"title": "SBT - scala构建工具", "text": "    Table   of   Contents           关于           构建 项目           从 单个 文件 构建           构建 文件 build . sbt           目录 结构                   构建 定义           键                   Tasks   任务           两种 构建 模式           dependencies   依赖 管理           手动 管理           自动 管理           resolvers           configuration                   使用 pom   xml 文件 添加 依赖           路径           创建 文件 和 路径           路径 finder                           调试                 关于       sbt 是 scala 的 交互式 构建 工具 ， 类似 于 java 的 maven 。       构建 项目       从 单个 文件 构建       最 简单 的 方式 是从 只 包含 一个 scala 源文件 的 目录 构建 项目 。     例如 在 目录   helloword   创建 一个   HW . scala   文件               object       Hi       {           def       main     (     args     :       Array     [     String     ] )       =       println     (     & quot ; Hi ! & quot ;     )       }                 然后 运行   sbt   ， 和   run   ， 或者 直接 运行   sbt   run   命令 。       sbt 自动 寻找 下列 目录           Sources   in   the   base   directory       Sources   in   src / main / scala   or   src / main / java       Tests   in   src / test / scala   or   src / test / java       Data   files   in   src / main / resources   or   src / test / resources       jars   in   lib           构建 文件   build . sbt         位于 项目 根目录 ， 一个 简单 的 构建 文件 如下               lazy       val       root       =       (     project       in       file     (     & quot ; .& quot ;     ) ) .           settings     (               name       : =       & quot ; hello & quot ;     ,               version       : =       & quot ; 1.0 & quot ;     ,               scalaVersion       : =       & quot ; 2.11 . 7 & quot ;           )                 如果 是 要 打包 到 jar 文件 ， name 和 version 是 必须 的 。       添加 依赖               val       derby       =       & quot ; org . apache . derby & quot ;       %       & quot ; derby & quot ;       %       & quot ; 10.4 . 1.3 & quot ;         lazy       val       commonSettings       =       Seq     (           organization       : =       & quot ; com . example & quot ;     ,           version       : =       & quot ; 0.1 . 0 & quot ;     ,           scalaVersion       : =       & quot ; 2.11 . 7 & quot ;       )         lazy       val       root       =       (     project       in       file     (     & quot ; .& quot ;     ) ) .           settings     (     commonSettings     :       _     *     ) .           settings     (               name       : =       & quot ; hello & quot ;     ,               libraryDependencies       + =       derby           )                 依赖 库 的 写法 是 ：             groupID   %   artifactID   %   revision               如果 将 依赖 的 库 放在 lib 目录 下 ， 就 不 需要 添加 该 依赖 。       目录 结构           源码     sbt 的 源码 目录 结构 与 maven 一样 。                 src /       main /           resources /                 & lt ; files   to   include   in   main   jar   here & gt ;           scala /                 & lt ; main   Scala   sources & gt ;           java /                 & lt ; main   Java   sources & gt ;       test /           resources                 & lt ; files   to   include   in   test   jar   here & gt ;           scala /                 & lt ; test   Scala   sources & gt ;           java /                 & lt ; test   Java   sources & gt ;                       构建 定义 文件     包含 根目录 的   build . sbt   ， 其他 构建 文件 放在 project 目录 下 。               构建 输出 文件目录 是   target /   ， 在   . gitignore   中 应该 排除 该 目录               构建 定义       一个 构建 定义 是 一个 Project ， 拥有 一个 类型 为   Setting [ T ]   的 列表 ， Setting [ T ]   是 会 影响 到   sbt   保存 键值 对 的   map   的 一种 转换 ， T   是 每 一个   value   的 类型 。     参考 前面 的 构建 定义 示例 代码 。       每一项   Setting   都 定义 为 一个   Scala   表达式 。 在   settings   中 的 表达式 是 相互 独立 的 ， 而且 它们 仅仅 是 表达式 ， 不是 完整 的   Scala   语句 。 ? ? ? ? WHAT     这些 表达式 可以 用   val ， lazy   val ， def   声明 。   build . sbt   不 允许 使用 顶层 的   object   和   class 。 它们 必须 写 到   project /   目录 下 作为 完整 的   Scala   源文件 。       键       有 三种 类型 的   key ：           SettingKey [ T ] ： 一个   key   对应 一个 只 计算 一次 的   value （ 这个 值 在 加载 项目 的 时候 计算 ， 然后 一直 保存 着 ） 。       TaskKey [ T ] ： 一个   key   对应 一个 称之为   task   的   value ， 每次 都 会 重新 计算 ， 可能 存在 潜在 的 副作用 。       InputKey [ T ] ： 一个   key   对应 一个 可以 接收 命令行 参数 的   task 。           键 的 类型           内置 键 ，   build . sbt   会 隐式 包含   import   sbt . Keys ._   ， 所以 可以 通过   name   取 到   sbt . Keys . name 。       自定义 键 ， 创建 方法 ： settingKey ， taskKey   和   inputKey   创建 自定义   keys .                   lazy       val       hello       =       taskKey     [     Unit     ] (     & quot ; 一个   task   示例 & quot ;     )                 Tasks   任务       一个 简单 的 hello 任务 如下 ， 在 build . sbt 文件 中 加入 下列 代码               lazy       val       hello       =       taskKey     [     Unit     ] (     & quot ; Prints   &# 39 ; Hello   World &# 39 ; & quot ;     )         hello       : =       println     (     & quot ; hello   world ! & quot ;     )                 然后 执行     sbt   hello     就 可以 看到 结果 了 。       一个 任务 首先 需要 定义 一个   taskKey [ T ]   ， 在 这个 例子 中 返回 空 类型 ， 每 一个 任务 是 一个 scala 函数 ， 可以     返回 一个 结果 。 可以 在 其他 任务 中 通过   . value   属性 访问 另 一个 task 的 结果 。       两种 构建 模式           交互式 模式 ， 输入   sbt   命令 后面 不 跟 参数 ， 然后 进入 交互式 环境 ， 然后 运行 命令 构建 。       批处理 模式 ， 跟 参数 罗 。   sbt   clean   compile   \" testOnly   TestA   TestB \"         常见 命令 ， 更 多 参考                 clean       删除 所有 生成 的 文件   （ 在   target   目录 下 ） 。   compile   编译 源文件 （ 在   src / main / scala   和   src / main / java   目录 下 ） 。   test         编译 和 运行 所有 测试 。   console   进入 到 一个 包含 所有 编译 的 文件 和 所有 依赖 的   classpath   的   Scala   解析器 。 输入   : quit ，   Ctrl + D   （ Unix ） ， 或者   Ctrl + Z   （ Windows ）   返回 到   sbt 。   run   & lt ; 参数 & gt ; *       在 和   sbt   所处 的 同一个 虚拟机 上 执行 项目 的   main   class 。   package   将   src / main / resources   下 的 文件 和   src / main / scala   以及   src / main / java   中 编译 出来 的   class   文件 打包 成 一个   jar   文件 。   help   & lt ; 命令 & gt ;       显示 指定 的 命令 的 详细 帮助 信息 。 如果 没有 指定 命令 ， 会 显示 所有 命令 的 简介 。   reload     重新 加载 构建 定义 （ build . sbt ，   project / * . scala ，   project / * . sbt   这些 文件 中 定义 的 内容 ) 。 在 修改 了 构建 定义 文件 之后 需要 重新 加载 。               dependencies   依赖 管理       手动 管理       手动 将库 的 jar 包 复制到 lib 目录 下 就 可以 了 。 如果 要 更改 默认 路径 ， 需要 修改   unmanagedBase   ，     例如 修改 到   custom _ lib /   目录 可以 用 下述 命令               unmanagedBase       : =       baseDirectory     .     value       /       & quot ; custom _ lib & quot ;                 更 多 的 控制 可以 通过 重载 unmanagedJars 这个 task ， 默认 的 实现 是               unmanagedJars       in       Compile       : =       (     baseDirectory     .     value       * *       & quot ; * . jar & quot ;     ) .     classpath                 如果 要 添加 多个 路径 到 默认 路径 ， 可以 这样 写               unmanagedJars       in       Compile       ++ =       {               val       base       =       baseDirectory     .     value               val       baseDirectories       =       (     base       /       & quot ; libA & quot ;     )       +++       (     base       /       & quot ; b & quot ;       /       & quot ; lib & quot ;     )       +++       (     base       /       & quot ; libC & quot ;     )               val       customJars       =       (     baseDirectories       * *       & quot ; * . jar & quot ;     )       +++       (     base       /       & quot ; d & quot ;       /       & quot ; my . jar & quot ;     )               customJars     .     classpath       }                 这里 对 路径 的 语法 ， 参考 后面 的 路径       自动 管理       sbt 支持 三种 自动 管理 方式 ， 都 是 通过 Apache   ivy 来 实现 的 。           Declarations   in   your   project   definition       Maven   POM   files   ( dependency   definitions   only :   no   repositories )       Ivy   configuration   and   settings   files           可以 通过 下述 语句 声明 依赖 ， 其中 configuration 是 可选 的 。     多个 依赖 可以 通过   Seq   将 每 一个 依赖 作为 一个 元素 进行 添加 ， 注意 链接 操作 符号 的 区别 ，     libraryDependencies 是 一个   Seq   ?               libraryDependencies       + =       groupID       %       artifactID       %       revision       %       configuration       libraryDependencies       ++ =       Seq     (           groupID       % %       artifactID       %       revision     ,           groupID       % %       otherID       %       otherRevision       )                     If   you   are   using   a   dependency   that   was   built   with   sbt ,   double   the   first   %   to   be   % %       sbt   uses   the   standard   Maven2   repository   by   default .           revision 除了 可以 使用 常规 的 完整 版本号 外 ， 还 可以 使用   \" latest . integration \" ,   \" 2.9 .+ \" ,   or   \" [ 1.0 , ) \" 这种 形式 。       resolvers       可以 通过 设置 resolvers 来 添加 依赖 库 获取 的 位置 ， 格式 是       resolvers   + =   name   at   location   ， location 可以 是 合法 的 URI ， 例如               resolvers       + =       & quot ; Sonatype   OSS   Snapshots & quot ;       at       & quot ; https : / / oss . sonatype . org / content / repositories / snapshots & quot ;       resolvers       + =       & quot ; Local   Maven   Repository & quot ;       at       & quot ; file : / / & quot ;     +     Path     .     userHome     .     absolutePath     +     & quot ; / . m2 / repository & quot ;       externalResolvers       : =       Resolver     .     withDefaultResolvers     (     resolvers     .     value     ,       mavenCentral       =       false     )                 configuration           指定 URL                   libraryDependencies       + =       & quot ; slinky & quot ;       %       & quot ; slinky & quot ;       %       & quot ; 2.1 & quot ;       from       & quot ; https : / / slinky2 . googlecode . com / svn / artifacts / 2.1 / slinky . jar & quot ;       libraryDependencies       + =       & quot ; org . apache . felix & quot ;       %       & quot ; org . apache . felix . framework & quot ;       %       & quot ; 1.8 . 0 & quot ;       intransitive     ( )       libraryDependencies       + =       & quot ; org . testng & quot ;       %       & quot ; testng & quot ;       %       & quot ; 5.7 & quot ;       classifier       & quot ; jdk15 & quot ;       libraryDependencies       + =           & quot ; org . lwjgl . lwjgl & quot ;       %       & quot ; lwjgl - platform & quot ;       %       lwjglVersion       classifier       & quot ; natives - windows & quot ;       classifier       & quot ; natives - linux & quot ;       classifier       & quot ; natives - osx & quot ;       libraryDependencies       + =               & quot ; log4j & quot ;       %       & quot ; log4j & quot ;       %       & quot ; 1.2 . 15 & quot ;       exclude     (     & quot ; javax . jms & quot ;     ,       & quot ; jms & quot ;     )       libraryDependencies       + =                   & quot ; org . apache . felix & quot ;       %       & quot ; org . apache . felix . framework & quot ;       %       & quot ; 1.8 . 0 & quot ;       withSources     ( )       withJavadoc     ( )                 -       使用 pom   xml 文件 添加 依赖             externalPom ( )   externalPom ( Def . setting ( baseDirectory . value   /   & quot ; custom - name . xml & quot ; ) )               路径       创建 文件 和 路径       sbt   0.10 +   使用   java . io . File     文件类型 。     创建 文件 方法               val       source     :       File       =       file     (     & quot ; / home / user / code / A . scala & quot ;     )       def       readme     (     base     :       File     )     :       File       =       base       /       & quot ; README & quot ;                 sbt   添加 了   /   方法 ， 对应 于 两 参数 构造函数 。         baseDirectory     task   返回 bese 目录 绝对路径 。       路径 finder       一个 路径 finder 返回 一个   Seq [ File ]   。 例如               def       scalaSources     (     base     :       File     )     :       Seq     [     File     ]       =       {           val       finder     :       PathFinder       =       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * . scala & quot ;           finder     .     get       }                 The     * *     method   accepts   a     java . io . FileFilter     ， 筛选 目录 及 子目录 下 所有 文件 。     如果 只 访问 该 目录 可以 使用     *     函数 。     惰性 求值 使得 需要 调用   . get   才能 计算结果 。       name   filter   使用     *   表示 0 个 或 多个 字符 。 用   | |   表示 多个 filter 的 或 ， 用   - -   表示 排除 。               val       base       =       baseDirectory     .     value       (     base       /       & quot ; src & quot ;     )       *       & quot ; * Test * . scala & quot ;       (     base       /       & quot ; src & quot ;     )       * *       (     & quot ; * . scala & quot ;       | |       & quot ; * . java & quot ;     )       (     base     /     & quot ; src & quot ;     /     & quot ; main & quot ;     /     & quot ; resources & quot ;     )       *       (     & quot ; * . png & quot ;       - -       & quot ; logo . png & quot ;     )                 组合 多个 finder     +++   ,   排除 结果 可以 用     - - -   。     finder 有 一个 filter 方法 ， 用于 进一步 筛选               (     base       /       & quot ; lib & quot ;       +++       base       /       & quot ; target & quot ;     )       *       & quot ; * . jar & quot ;       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * . scala & quot ;     )       - - -       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; . svn & quot ;       * *       & quot ; * . scala & quot ;     )       (       (     base       /       & quot ; src & quot ;     )       * *       & quot ; * & quot ;     )       filter       {       _     .     isDirectory       }       base       filter       ClasspathUtilities     .     isArchive         ` `     `           -   to   string   转换               -   toString               -   absString   ，               -   getPaths   ，   返回 Seq [ String ]         ##   插件       插件 用来 扩展 构建 定义 ， 可能 是 一个 新 的 task 。         添加 插件 申明 ， 在 项目 根目录 下 的   `     /     project `   目录 添加   `     .     sbt `   文件 ， 然后 在 其中 添加 语句         `     ` `     scala       addSbtPlugin     (     & quot ; com . typesafe . sbt & quot ;       %       & quot ; sbt - site & quot ;       %       & quot ; 0.7 . 0 & quot ;     )                     assembly   打包 插件 ， 以及 shade 例子 ：                   assemblyOption       in       assembly       : =       (     assemblyOption       in       assembly     ) .     value     .     copy     (     includeScala       =       false     )         val       jarStartWith       =       Seq     (     & quot ; pmml - & quot ;     ,     & quot ; guava - & quot ;     ,     & quot ; jpmml - & quot ;     ,       & quot ; json4s - & quot ;     )       assemblyExcludedJars       in       assembly       : =       {               val       cp       =       (     fullClasspath       in       assembly     ) .     value                 val       filtered       =       cp       filterNot       {     f       = & gt ;                       jarStartWith     .     map     (     s       = & gt ;       f     .     data     .     getName     .     startsWith     (     s     ) ) .     foldLeft     (     false     ) ( (     a     ,     b     )       = & gt ;       a       | |       b     )               }               cp     .     foreach     {     c       = & gt ;                       val       tag       =       if     (     filtered     .     contains     (     c     ) )       & quot ;     Excluded & quot ;       else       & quot ; +   Included & quot ;                       println     (     s & quot ;     $ tag       :       $ {     c     .     data     .     getName     }     & quot ;     )               }               filtered       }         val       shadedRootPackage       =       & quot ; com . tracholar & quot ;       assemblyShadeRules       in       assembly       : =       Seq     (               ShadeRule     .     rename     (     & quot ; com . google . common . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll     ,               ShadeRule     .     rename     (     & quot ; org . jpmml . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll     ,               ShadeRule     .     rename     (     & quot ; org . dmg . pmml . * * & quot ;       - & gt ;       s & quot ;     $ shadedRootPackage     . @ 0 & quot ;     ) .     inAll       )         assemblyMergeStrategy       in       assembly       : =       {               case       PathList     (     & quot ; org & quot ;     ,     & quot ; dmg & quot ;     ,       & quot ; pmml & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; org & quot ;     ,     & quot ; jpmml & quot ;     ,         xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; com & quot ;     ,     & quot ; google & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       PathList     (     & quot ; com & quot ;     ,     & quot ; tracholar & quot ;     ,       & quot ; spark & quot ;     ,       xs       @       _     * )       = & gt ;       MergeStrategy     .     first               case       x       = & gt ;       (     assemblyMergeStrategy       in       assembly     ) .     value     (     x     )       }       logLevel       in       assembly       : =       Level     .     Debug                 调试       传入     - jvm - debug   & lt ; port & gt ;     Turn   on   JVM   debugging ,   open   at   the   given   port .   参数 即可 远程 调试 ， 例如     sbt   - jvm - debug   5005   run    ", "tags": "tools", "url": "/wiki/tools/sbt.html"},
      
      
      {"title": "scala语言", "text": "    Table   of   Contents           关于           基础 语法           基本 数据结构           函数 组合 子   Functional   Combinators           函数 组合           偏 函数                   类型 ， 静态 类型           隐式 转换           隐式 函数           隐式 类   2.10 .+                   构建 工具   SBT           字符串   -   核心 数据结构           字符串 插值   String   Interpolation ， Scala   2.10 .+                   集合   -   核心 数据结构           List           Set           Seq           Map                   层次结构           常用 子类           一些 描述 特性 的 特质           可变   vs   不可 变           可变 集合           与 Java 转换                   使用 specs 进行 测试           并发 编程 *           线程 安全 的 三种 工具 。                   Java 跨平台 交互 ： 在 Java 中 使用 Scala           扩展 规格                   泛型 编程           Test           FlatSpec           FunSuit                         关于       学习 scala 后 ， 发现 scala 就是 灵活 版 的 java ， 他 通过 引入 函数 式 编程 的 一些 概念 来 达到 这个 目的 ，     并且 由于 基于 JVM ， 能够 复用 所有 的 java 库 ！ ！ 如果 你 嫌 java 臃肿 ， 不妨 试试 scala 。           相关 链接 ：       scala   API     http : / / www . scala - lang . org /         Twitter 教程     https : / / twitter . github . io / scala _ school / zh _ cn / index . html                     基础 语法           不 变量   val   ， 变量   var         基础 类型 ：       Int               流程 控制 ， 直接 看 例子                   for     (     i       & lt ; -       0       to       100     ) {               println     (     i     )       }                       def   创建 函数 ， 类型 标签                   def       addOne     (     m     :       Int     )     :       Int       =       m       +       1                     匿名 函数     ( x : Int )   = & gt ;   x + 1         函数 的 部分 应用                   def       adder     (     m     :     Int     ,       n     :     Int     )       =       m       +       n       val       add2       =       adder     (     2     ,       _ :     Int     )                     柯 理化 函数                   def       muliply     (     m     :       Int     )       (     n     :       Int     )       =       m       *       n                     可变 长 参数                   def       cap     (     args       String     * )       =       {               args     .     map       {                       arg       = & gt ;       arg     .     capitalize               }       }                     类     class         构造函数 不是 特殊 的 方法 ， 他们 是 除了 类 的 方法 定义 之外 的 代码 。                   class       Calculator     (     brand     :       String     )       {           / * *             *   A   constructor .             * /           val       color     :       String       =       if       (     brand       = =       & quot ; TI & quot ;     )       {               & quot ; blue & quot ;           }       else       if       (     brand       = =       & quot ; HP & quot ;     )       {               & quot ; black & quot ;           }       else       {               & quot ; white & quot ;           }             / /   An   instance   method .           def       add     (     m     :       Int     ,       n     :       Int     )     :       Int       =       m       +       n       }                     Scala 是 高度 面向 表达式 的 ： 大多数 东西 都 是 表达式 而 非 指令 。       继承                   class       ScientificCalculator     (     brand     :       String     )       extends       Calculator     (     brand     )       {           def       log     (     m     :       Double     ,       base     :       Double     )       =       math     .     log     (     m     )       /       math     .     log     (     base     )       }                     抽象类                   abstract       class       Shape       {                       def       getArea     ( )     :     Int             / /   subclass   should   define   this       }                         Traits   特质     很 像 接口 ， 通过   with   关键字 ， 一个 类 可以 扩展 多个 特质 。       ` ` ` scala     trait   Car   {         val   brand :   String     }       trait   Shiny   {         val   shineRefraction :   Int     }     class   BMW   extends   Car   with   Shiny   {         val   brand   =   \" BMW \"         val   shineRefraction   =   12     }     ` ` `     -   泛型 ， 方法 和 trait 都 可以 引入 类型 参数                       trait       Cache     [     K   ,     V     ]       {           def       get     (     key     :       K     )     :       V           def       put     (     key     :       K     ,       value     :       V     )           def       delete     (     key     :       K     )       }       def       remove     [     K     ] (     key     :       K     )                 如何 实现 像 java 那样 的 父类 占位 符 。           apply 方法                   class       Bar       {               def       apply     ( )       =       0       }       val       bar       =       new       Bar       bar     ( )       / /   res :   Int   =   0                     单例 对象 ， 工厂 模式                   object       Timer       {           var       count       =       0             def       currentCount     ( )     :       Long       =       {               count       + =       1               count           }       }       Timer     .     currentCount     ( )                     函数 即 对象 .     函数 是 一些 特质 的 集合 。 具体来说 ， 具有 一个 参数 的 函数 是 Function1 特质 的 一个 实例 。 这个 特征 定义 了   apply ( )   语法 糖 ， 让 你 调用 一个 对象 时 就 像 你 在 调用 一个 函数 。                   object       addOne       extends       Function1     [     Int   ,     Int     ]       {               def       apply     (     m     :       Int     )     :       Int       =       m       +       1       }       class       AddOne       extends       (     Int       = & gt ;       Int     )       {           def       apply     (     m     :       Int     )     :       Int       =       m       +       1       }                     包 ， 和 Java 的 一样       模式匹配     匹配 值                   val       times       =       1         times       match       {               case       1       = & gt ;       & quot ; one & quot ;               case       2       = & gt ;       & quot ; two & quot ;               case       _       = & gt ;       & quot ; some   others & quot ;       }         / /   守卫 匹配       times       match       {               case       i       if       i       = =       1       = & gt ;       & quot ; one & quot ;               case       i       if       i       = =       2       = & gt ;       & quot ; two & quot ;               case       _       = & gt ;       & quot ; some   others & quot ;       }                 匹配 类型               def       bigger     (     o     :       Any     )     :       Any       =       {           o       match       {               case       i     :       Int       if       i       & lt ;       0       = & gt ;       i       -       1               case       i     :       Int       = & gt ;       i       +       1               case       d     :       Double       if       d       & lt ;       0     .     0       = & gt ;       d       -       0.1               case       d     :       Double       = & gt ;       d       +       0.1               case       text     :       String       = & gt ;       text       +       & quot ; s & quot ;           }       }                 匹配 类 成员               def       calcType     (     calc     :       Calculator     )       =       calc       match       {           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 20B & quot ;       = & gt ;       & quot ; financial & quot ;           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 48G & quot ;       = & gt ;       & quot ; scientific & quot ;           case       _       if       calc     .     brand       = =       & quot ; hp & quot ;       & amp ; & amp ;       calc     .     model       = =       & quot ; 30B & quot ;       = & gt ;       & quot ; business & quot ;           case       _       = & gt ;       & quot ; unknown & quot ;       }                     样本 类   case   class                   case       class       Calculator     (     brand     :       String     ,       model     :       String     )                 case   classes   are   designed   to   be   used   with   pattern   matching .   Let ’ s   simplify   our   calculator   classifier   example   from   earlier .     样本 类 就是 被 设计 用 在 模式匹配 中 的 。 让 我们 简化 之前 的 计算器 分类器 的 例子 。               def       calcType     (     calc     :       Calculator     )       =       calc       match       {           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 20B & quot ;     )       = & gt ;       & quot ; financial & quot ;           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 48G & quot ;     )       = & gt ;       & quot ; scientific & quot ;           case       Calculator     (     & quot ; hp & quot ;     ,       & quot ; 30B & quot ;     )       = & gt ;       & quot ; business & quot ;           case       Calculator     (     ourBrand     ,       ourModel     )       = & gt ;       & quot ; Calculator :   % s   % s   is   of   unknown   type & quot ;     .     format     (     ourBrand     ,       ourModel     )       }                 我们 也 可以 将 匹配 的 值 重新命名 。                   case       c     @ Calculator     (     _     ,       _     )       = & gt ;       & quot ; Calculator :   % s   of   unknown   type & quot ;     .     format     (     c     )                     异常 ，   try   ...   catch   ...   finally       private [ spark ]   private   作用域 为 包含 spark 类 的 地方 才 可见           基本 数据结构           String   实际上 就是 java . lang . String       List   列表                   val       numbers       =       List     (     1     ,     2     ,     3     ,     4     )                     Set   集                   scala     & gt ;       Set     (     1     ,     2     ,     1     )       res0     :       scala . collection . immutable . Set     [     Int     ]       =       Set     (     1     ,       2     )                     Tuple   元组                   val       hostPort       =       (     & quot ; localhost & quot ;     ,       80     )       hostPort     .     _ 1         / /   localhost       hostPort     .     _ 2         / /   80                 与 样本 类 不同 ， 元组 不能 通过 名称 获取 字 段 ， 而是 使用 位置 下标 来 读取 对象 ； 而且 这个 下标 基于 1 ， 而 不是 基于 0 。     在 创建 两个 元素 的 元组 时 ， 可以 使用 特殊 语法 ：   1   - & gt ;   2   ， 见 映射           unpack ：   val   ( v1 ,   v2 )   =   ( 1 , 2 )   ,     函数参数 unpack 例子           _ *                     def       hello     (       names     :       String *     )       {           println     (       & quot ; Hello   & quot ;       +       names     .     mkString     (     & quot ;   and   & quot ;       )       )       }         scala     & gt ;       val       names       =       List     (     & quot ; john & quot ;     ,       & quot ; paul & quot ;     ,       & quot ; george & quot ;     ,       & quot ; ringo & quot ;     )       names     :       List     [     String     ]       =       List     (     john     ,       paul     ,       george     ,       ringo     )       scala     & gt ;       hello     (       names     :       _     *       )       Hello       john       and       paul       and       george       and       ringo                     Map   映射 ， 类似 于 python 的 字典 ， c 的 hash _ map                   Map     (     1       - & gt ;       2     )         / /   值 映射       Map     (     & quot ; foo & quot ;       - & gt ;       & quot ; bar & quot ;     )         / /   字符串 映射       Map     (     1       - & gt ;       Map     (     & quot ; foo & quot ;       - & gt ;       & quot ; bar & quot ;     ) )         / /   映射 到 映射       Map     (     & quot ; timesTwo & quot ;       - & gt ;       {     timesTwo     (     _     ) } )           / /   映射 到 函数                   Map   中要 获取 键 对应 的 值 ， 需要 使用   Map . get   方法 。     -   选项   Option     Option   是 一个 表示 有 可能 包含 值 的 容器 。     Option   本身 是 泛型 的 ， 有 两个 子类     Some [ T ]     或     None   。     在 模式匹配 中 会 用到 。               val       result       =       res1       match       {               case       Some     (     n     )       = & gt ;       n     *     2               case       None       = & gt ;       0       }                 Option 基本 的 接口 是 这样 的 ：               trait       Option     [     T     ]       {           def       isDefined     :       Boolean           def       get     :       T           def       getOrElse     (     t     :       T     )     :       T       }                 Option 本身 是 泛型 的 ， 并且 有 两个 子类 ：   Some [ T ]   或   None 。       Map . get   使用   Option   作为 其 返回值 ， 表示 这个 方法 也许 不会     返回 你 请求 的 值 。     类似 于 Haskell 的   Maybe   ？       函数 组合 子   Functional   Combinators           map   组合 子     例子 ：   List ( 1 , 2 , 3 , 4 )   map   { i : Int   = & gt ;   i * i }   ，     或者 这样 调用                   numbers     .     map     ( (     i     :     Int     )       = & gt ;       i       *       2     )                     foreach ，   很 像 map ， 但是 没有 返回值 。 仅 用于 有 副作用 的 函数 ？                   numbers     .     foreach     ( (     i     :     Int     )       = & gt ;       i       *       2     )                     filter ， 一处 任何 传入 函数 计算结果 为   false   的 元素 。       zip ， 将 两个 列表 的 内容 聚合 到 一个 对偶 列表 中 。                   List     (     1     ,     2     ,     3     ) .     zip     (     List     (     & quot ; a & quot ;     ,     & quot ; b & quot ;     ,     & quot ; c & quot ;     ) )         / / [ ( 1 , a ) , ( 2 , b ) , ( 3 , c ) ]                       partition   ,   使用 给定 的 谓词 函数 （ 返回 true 和 false 的 函数 ） 分割 列表 ， 返回 tuple         find   ， 返回 集合 中 第一个 匹配 谓词 函数 的 元素         drop     和     dropWile   ，   drop   删除 前 i 个 元素 ，   dropWhile   将 删除         元素 直到 不 满足条件 为止 。       foldLeft ，   左 折叠 。 需要 传入 一个 初始值 和 一个二元 函数       foldRight ， 右 折叠       flatten ， 展平 。       flatMap ， 等价 于   flatten   .   map           函数 组合             compose   组合 其它 函数 形成 新 的 函数   f ( g ( x ) )   。                   val       fg       =       f       _       compose       g       _                   println   是 啥 ？ 为 甚 不能 组合 。     -     andThen   ， 与   compose   很 像 ， 只是 执行 顺序 相反 ， 先 执行 第一个 。       偏 函数       不是 部分 应用 函数 ， 篇 函数 是 指 只能 接受 该 类型 的 某些 特定 的 值 。       isDefinedAt   用来 确定 该 函数 能否 接受 一个 给定 的 参数 。               val       one     :       PartialFunction     [     Int   ,     String     ]       =       {       case       1       = & gt ;       & quot ; one & quot ;       }       one     .     isDefinedAt     (     1     )           / /   true       one     .     isDefinedAt     (     2     )           / /   false                 PartialFunctions 可以 使用 orElse 组成 新 的 函数 ， 得到 的 PartialFunction 反映 了 是否 对 给定 参数 进行 了 定义 。               scala     & gt ;       val       two     :       PartialFunction     [     Int   ,     String     ]       =       {       case       2       = & gt ;       & quot ; two & quot ;       }       two     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       three     :       PartialFunction     [     Int   ,     String     ]       =       {       case       3       = & gt ;       & quot ; three & quot ;       }       three     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       wildcard     :       PartialFunction     [     Int   ,     String     ]       =       {       case       _       = & gt ;       & quot ; something   else & quot ;       }       wildcard     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       val       partial       =       one       orElse       two       orElse       three       orElse       wildcard       partial     :       PartialFunction     [     Int   ,   String     ]       =       & lt ;     function1     & gt ;         scala     & gt ;       partial     (     5     )       res24     :       String       =       something       else         scala     & gt ;       partial     (     3     )       res25     :       String       =       three         scala     & gt ;       partial     (     2     )       res26     :       String       =       two         scala     & gt ;       partial     (     1     )       res27     :       String       =       one         scala     & gt ;       partial     (     0     )       res28     :       String       =       something       else                 模式匹配 其实 是 一个 偏 函数 ！ 偏 函数 是 函数 的 子类 ， 所以 所有 在 使用 函数 的 地方 都 可以 使用 偏 函数 ， 即 模式匹配 ！       类型 ， 静态 类型       随着 类型 系统 表达能力 的 提高 ， 我们 可以 生产 更 可靠 的 代码 。     所有 的 类型信息 会 在 编译 时 被 删去 ， 因为 它 已 不再 需要 。 这 就是 所谓 的 擦除 。           参数 化 多态 ， 秩 1 多态性 rank - one 。 下面 是 一个 错误 的 例子 ， 将会报 编译 错误 。                   def       foo     [     A   ,     B     ] (     f     :       A - & gt ; List     [     A     ] ,       b     :       B     )       =       f     (     b     )       def       foo     [     A     ] (     f     :       A - & gt ; List     [     A     ] ,       b     :       Int     )       =       f     (     i     )                     类型 推断     Hindley   Milner 算法 。   Scala 编译器 为 我们 做 类型 推断 ，     使得 可以 不 明确 指定 返回 类型 。                   def       id     [     T     ] (     x       :       T     )       =       x       val       x       =       id     (     & quot ; hey & quot ;     )                     变性   Variance ， 如果 T ' 是 T 的 子类 ， 那么 Container [ T ' ] 和 Container [ T ] 的 关系 呢 ？       协变 ，   C [ T ' ] 也 是 C [ T ] 的 子类 ，   [ + T ]       逆变 ，   C [ T ' ] 是 C [ T ] 的 父类 ，   [ - T ]       不变 ，   没有 关系 ，   [ T ]                   逆变 的 例子 ， 函数 特质 。 参数 用 父类 ， 调用 用 子类 ， 表明 以 父类 为 类型 参数 的 函数     是 以 子类 为 类型 参数 的 函数 的 子类 。 有点 绕 ， 理解 一下 。           边界 ， 指定 泛型 的 大 类型 ？   T   & lt ; :   SomeType     指定 T 是 SomeType 的 子类 。                   scala     & gt ;       def       cacophony     [     T     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )       & lt ;     console     & gt ; :     7     :       error :       value       sound       is       not       a       member       of       type       parameter       T                     def       cacophony     [     T     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )                                                                                                                       ^         scala     & gt ;       def       biophony     [     T       & lt ; :       Animal     ] (     things     :       Seq     [     T     ] )       =       things       map       (     _     .     sound     )       biophony     :       [     T       & lt ; :       Animal     ]     (     things :       Seq     [     T     ] )     Seq     [     java . lang . String     ]         scala     & gt ;       biophony     (     Seq     (     new       Chicken     ,       new       Bird     ) )       res5     :       Seq     [     java . lang . String     ]       =       List     (     cluck     ,       call     )                   T   : & gt ;   SomeType     指定 T 是 SomeType 的 超类 。     List   同样   定义 了   : : [ B   & gt ; :   T ] ( x :   B )     来 返回 一个 List [ B ] ， 例如 下面 这个 例子 中 ，     flock 是 Bird 类型 ， Bird 是 Animal 的 子类 。   : :   操作 后 返回 的 是 超类 Animal 的 列表 。               scala     & gt ;       new       Animal       : :       flock       res59     :       List     [     Animal     ]       =       List     (     Animal     @     11     f8d3a8     ,       Bird     @     7     e1ec70e     ,       Bird     @     169     ea8d2     )                     量化   Quantification 。     有时候 ， 不 关心 类型 变量 时 ， 可以 用 通配符 取而代之 ， 注意 区分 变量 和 类型 变量 。     个人 理解 ： 下面 这个 例子 与 类型 无关 ， 只 与 List 的 接口 有关 ， 所以 不 影响 类型 推导 系统 。           可以 为 通配符 指定 边界 。               def       count     [     A     ] (     l     :       List     [     A     ] )       =       l     .     size       def       count     (     l     :       List     [     _     ] )       =       l     .     size         def       hashcodes     (     l     :       Seq     [     _       & lt ; :       AnyRef     ] )       =       l       map       (     _     .     hashCode     )                     View   bounds （ type   classes ） ，     & lt ; %   .     在 隐式 函数 可以 帮助 满足 类型 推断 时 ， 它们 允许 按 需 的 函数 应用 。                   class       Container     [     A       & lt ; %       Int     ]       {       def       addIt     (     x     :       A     )       =       123       +       x       }                         更 多 类型 限制 ， 我 已经 晕 了 ， 不要 问 我 ， 自己 看 教程 ！               关于 类型 ， 还有 一些 内容 ， 看 教程     https : / / twitter . github . io / scala _ school / zh _ cn / advanced - types . html                 断言 assert 和 require ， 通常 用 require 做 参数 检查 ， 而用 assert 做 测试 相关 的 。                       def       assert     (     assertion     :       Boolean     )       {           if       ( !     assertion     )               throw       new       java     .     lang     .     AssertionError     (     & quot ; assertion   failed & quot ;     )       }         def       assume     (     assumption     :       Boolean     )       {           if       ( !     assumption     )               throw       new       java     .     lang     .     AssertionError     (     & quot ; assumption   failed & quot ;     )       }         def       require     (     requirement     :       Boolean     )       {           if       ( !     requirement     )               throw       new       IllegalArgumentException     (     & quot ; requirement   failed & quot ;     )       }                 隐式 转换       隐式 函数               implicit       def       intToString     (     x     :     Int     )       :       x . toString                 隐式 类   2.10 .+       隐式 类 的 主 方法 可以 用于 隐式 类型转换 。               object       Helpers       {           implicit       class       IntWithTimes     (     x     :       Int     )       {               def       times     [     A     ] (     f     :       = & gt ;       A     )     :       Unit       =       {                   def       loop     (     current     :       Int     )     :       Unit       =                       if     (     current       & gt ;       0     )       {                           f                           loop     (     current       -       1     )                       }                   loop     (     x     )               }           }       }         import       Helpers ._       5       times       println     (     & quot ; HI & quot ;     )         HI       HI       HI       HI       HI                 可以 利用   Scala . math   库   Numeric   对 数字 类型 变量 进行 限制       构建 工具   SBT           安装 命令   brew   install   sbt         项目 布局       项目   –   项目 定义 文件       project / build / . scala   –   主 项目 定义 文件       project / build . properties   –   项目 、 sbt 和 Scala 版本 定义       src / main   –   你 的 应用 程序代码 出现 在 这里 ， 在 子目录 表明 代码 的 语言 （ 如 src / main / scala ,   src / main / java ）       src / main / resources   –   你 想要 添加 到 jar 包中 的 静态 文件 （ 如 日志 配置 ）       src / test   –   就 像 src / main ， 不过 是 对 测试       lib _ managed   –   你 的 项目 依赖 的 jar 文件 。 由 sbt   update 时 填充       target   –   生成物 的 目标 路径 （ 如 自动 生成 的 thrift 代码 ， 类 文件 ， jar 包 ）                           字符串   -   核心 数据结构       scala 的 字符串 很多 是 直接 借助于 java 的 String 类 ， 但是 还有 一些 scala 的 特性 需要 说明 一下 。       字符串 插值   String   Interpolation ， Scala   2.10 .+       scala 提供 三种 字符串 插值 方法 ， s ,   f   and   raw 。     -   s 支持 局部变量 和 表达式 。     -   f 表明 对 变量 进行 格式化 ， 类似 于 printf 的 功能 。 类型 安全 ， 如果 不 匹配 ， 将会 报错 。   % s   是 通用 的 ？ ！     -   raw 字符串 就是 不会 对 转义字符 转义 。 相当于 python 里面 的 r               / /   插入 局部变量       val       name       =       & quot ; James & quot ;       println     (     s & quot ; Hello ,       $ name     & quot ;     )         / /   Hello ,   James       / /   插入 表达式       println     (     s & quot ; 1   +   1   =       $ {     1       +       1     }     & quot ;     )         val       height       =       1.9 d       val       name       =       & quot ; James & quot ;       / /   f   使用 类似 于 printf 格式 字符       println     (     f & quot ;     $ name     % s   is       $ height     % 2.2 f   meters   tall & quot ;     )         / /   James   is   1.90   meters   tall         println     (     raw & quot ; a \ \ nb & quot ;     )       / /   Output :   a \ \ nb                 集合   -   核心 数据结构       List           创建 集合                   List     (     1     ,     2     ,     3     ,     4     )       1       : :       2       : :       3       : :       Nil         val       L       =       1       to       1000       toList                   : :   是 将 前面 的 数据 prepend 到 后面 的 列表 中 ， 等同于   + :   。               连接 集合 ，     L1   ++   L2   ，   可以 连接 元素 类型 不同 的 集合 ， 最终 生成 的 集合 类型 是 这 两个         集合 元素 类型 的 超集 。 和   : : :   一样                 z   / :   L     相当于     foldLeft   z   L   ,     ( 0   / :   L ) ( ( a , b ) = & gt ; a + b )   求和             z   : \ \   L     右 折叠             : +     append 操作 ，     L   : +   6                 索引 操作 ：   . apply ( n : Int )     取 下标 n 的 元素 ， 可以 通过   ( )   进行 访问 ， 如   L ( 0 )             内置 的 数学 函数 ，   . max   ,     . min   ,     . sum   ,     . product         内置 的 基本 属性 ，   . length   ,     . size   ,     . head   ,     . last           filter   ,     flatMap   ,     map   ,     withFilter   ,     zip   ,     zipWithIndex   （ 相当于 python 的 enumerater ） 。         其中   flatMap   =   flatten   .   map   ， 因此 穿 进去 的 函数 需要 返回 一个   GenTraversableOnce   ， 比如 返回 一个 列表 。           Set             +     增加 一个 元素 ， 返回 新 的 集合         -     减少 一个 元素         & amp ;     交集     |     并集 （   ++   ）     & amp ; ~     差集   (   - -   )       与 list 一样 的 折叠 、 map 、 reduce 等 集合 相关 操作       索引   apply ( e : A )   ，   ( e : A )   一样           Seq       貌似 与 list 没 啥 区别 ， 需要 再 仔细 看看 。               scala     & gt ;       Seq     (     1     ,       1     ,       2     )       res3     :       Seq     [     Int     ]       =       List     (     1     ,       1     ,       2     )                 请 注意 返回 的 是 一个 列表 。 因为 Seq 是 一个 特质 ； 而 列表 是 序列 的 很 好 实现 。       \" . mkString ( seq ) \"   方法 可以 实现 python 的   join   方法 的 功能 。       Map           创建 MAP                 Map ( &# 39 ; a &# 39 ;   - & gt ;   1 ,   &# 39 ; b &# 39 ;   - & gt ;   2 )               层次结构               traverable ,     foreach     实现 遍历           基本操作         def   head   :   A     返回 第一个 元素         def   tail   :   Traversable [ A ]     除去 第一个 元素 剩下 的 集合                   函数 组合 子             def   map   [ B ]   ( f :   ( A )   = & gt ;   B )   :   CC [ B ]     返回 每个 元素 都 被   f   转化 的 集合         def   foreach [ U ] ( f :   Elem   = & gt ;   U ) :   Unit     在 集合 中 的 每个 元素 上 执行   f   。         def   find   ( p :   ( A )   = & gt ;   Boolean )   :   Option [ A ]     返回 匹配 谓词 函数 的 第一个 元素         def   filter   ( p :   ( A )   = & gt ;   Boolean )   :   Traversable [ A ]     返回 所有 匹配 谓词 函数 的 元素 集合                   划分 ：             def   partition   ( p :   ( A )   = & gt ;   Boolean )   :   ( Traversable [ A ] ,   Traversable [ A ] )     按照 谓词 函数 把 一个 集合 分割 成 两 部分         def   groupBy   [ K ]   ( f :   ( A )   = & gt ;   K )   :   Map [ K ,   Traversable [ A ] ]     按照 Key 函数 将 一个 集合 分为 多个 .     S . groupBy ( x = & gt ; x % 3 )                 转换 ：         def   toArray   :   Array [ A ]           def   toArray   [ B   & gt ; :   A ]   ( implicit   arg0 :   ClassManifest [ B ] )   :   Array [ B ]           def   toBuffer   [ B   & gt ; :   A ]   :   Buffer [ B ]           def   toIndexedSeq   [ B   & gt ; :   A ]   :   IndexedSeq [ B ]           def   toIterable   :   Iterable [ A ]           def   toIterator   :   Iterator [ A ]           def   toList   :   List [ A ]           def   toMap   [ T ,   U ]   ( implicit   ev :   & lt ; : & lt ; [ A ,   ( T ,   U ) ] )   :   Map [ T ,   U ]     例如 转换 命令行 参数 ，   List ( \" A = 3 \" , \" B = 5 \" ) . map ( l   = & gt ;   l . split ( \" = \" ) ) . toMap           def   toSeq   :   Seq [ A ]           def   toSet   [ B   & gt ; :   A ]   :   Set [ B ]           def   toStream   :   Stream [ A ]           def   toString   ( )   :   String           def   toTraversable   :   Traversable [ A ]                 iterable ,     iterator ( )     返回 一个 迭代 器 ， 通常 不会 用 ， 一般 会 用 函数 组合 子 和   for                             def       hasNext     ( )     :       Boolean       def       next     ( )     :       A                     Seq   序列 ， 有 顺序 的 对象 序列       Set   没有 重复 的 对象 集合                   def       contains     (     key     :       A     )     :       Boolean       def       + (     elem     :       A     )     :       Set     [     A     ]       def       - (     elem     :       A     )     :       Set     [     A     ]                     Map   键值 对           常用 子类           HashSet :   implements   immutable   sets   using   a       hash   trie           HashMap :   implements   immutable   maps   using   a   hash   trie       TreeMap   是 SortedMap 子类       Vector   快速 随机 访问             继承     Seq   ,     IndexedSeq   ,     Iterable   ,     Traversable         Range   等 间隔 的 Int 有序 序列 。             继承     traverable   ,     Iterable   ,     Seq   .                   val       r0       =       0       until       10       val       r1       =       0       until       10       by       2       new       Range     (     start     :       Int     ,       end     :       Int     ,       step     :       Int     )                 一些 描述 特性 的 特质             IndexedSeq     快速 随机 访问 元素 和 一个 快速 的 长度 操作         LinearSeq     通过 head 快速访问 第一个 元素 ， 也 有 一个 快速 的 tail 操作 。           可变   vs   不可 变       不可 变       优点       在 多线程 中 不会 改变     缺点       一点 也 不能 改变     Scala 允许 我们 是 务实 的 ， 它 鼓励 不变性 ， 但 不 惩罚 我们 需要 的 可变性 。 这 和 var   vs .   val 非常 相似 。 我们 总是 先 从 val 开始 并 在 必要 时 回退 为 var 。       我们 赞成 使用 不可 改变 的 版本 的 集合 ， 但 如果 性能 使然 ， 也 可以 切换 到 可变 的 。 使用 不可 变 集合 意味着 你 在 多线程 不会 意外 地 改变 事物 。       可变 集合           ListBuffer 和 ArrayBuffer       LinkedList   and   DoubleLinkedList       PriorityQueue       Stack   和   ArrayStack       StringBuilder   有趣 的 是 ， StringBuilder 的 是 一个 集合           与 Java 转换       您 可以 通过 JavaConverters   package 轻松 地 在 Java 和 Scala 的 集合 类型 之间 转换 。 它用 asScala   装饰 常用 的 Java 集合 以和用 asJava   方法 装饰 Scala 集合 。                     import       scala . collection . JavaConverters ._             val       sl       =       new       scala     .     collection     .     mutable     .     ListBuffer     [     Int     ]             val       jl       :       java . util . List     [     Int     ]       =       sl     .     asJava             val       sl2       :       scala . collection . mutable . Buffer     [     Int     ]       =       jl     .     asScala             assert     (     sl       eq       sl2     )                 双向 转换 ：               scala     .     collection     .     Iterable       & lt ; = & gt ;       java     .     lang     .     Iterable       scala     .     collection     .     Iterable       & lt ; = & gt ;       java     .     util     .     Collection       scala     .     collection     .     Iterator       & lt ; = & gt ;       java     .     util     . {       Iterator     ,       Enumeration       }       scala     .     collection     .     mutable     .     Buffer       & lt ; = & gt ;       java     .     util     .     List       scala     .     collection     .     mutable     .     Set       & lt ; = & gt ;       java     .     util     .     Set       scala     .     collection     .     mutable     .     Map       & lt ; = & gt ;       java     .     util     . {       Map     ,       Dictionary       }       scala     .     collection     .     mutable     .     ConcurrentMap       & lt ; = & gt ;       java     .     util     .     concurrent     .     ConcurrentMap                 此外 ， 也 提供 了 以下 单向 转换               scala     .     collection     .     Seq       = & gt ;       java     .     util     .     List       scala     .     collection     .     mutable     .     Seq       = & gt ;       java     .     util     .     List       scala     .     collection     .     Set       = & gt ;       java     .     util     .     Set       scala     .     collection     .     Map       = & gt ;       java     .     util     .     Map                 使用 specs 进行 测试       貌似 现在 包为 specs2 ， 导入 单元测试 规范   org . specs2 . mutable . Specification                 org     .     specs2     .     mutable     .     _         object       ArithmeticSpec       extends       Specification       {           & quot ; Arithmetic & quot ;       should       {               & quot ; add   two   numbers & quot ;       in       {                   1       +       1       mustEqual       2               }               & quot ; add   three   numbers & quot ;       in       {                   1       +       1       +       1       mustEqual       3               }           }       }                 并发 编程 *           Runnable / Callable 定义 如下 ， 区别 在于 Runnable 没有 返回值 ， 而 Callable 有 。                   trait       Runnable       {           def       run     ( )     :       Unit       }         trait       Callable     [     V     ]       {           def       call     ( )     :       V       }                     线程 ， Scala 并发 是 建立 在 Java 并发 模型 基础 上 的 。 在 Sun   JVM 上 ， 对 IO 密集 的 任务 ， 我们 可以 在 一台 机器运行 成千上万 个 线程 。         一个 线程 需要 一个   Runnable   。 你 必须 调用 线程 的     start     方法 来 运行 Runnable 。                   val       hello       =       new       Thread     (     new       Runnable       {           def       run     ( )       {               println     (     & quot ; hello   world & quot ;     )           }       } )         hello     .     start     ( )                 创建 一个 自己 的 线程 步骤 ， 首先 创建 一个 实现 Runnable 接口 的 类 ， 然后 将 该类 的 实例 作为 参数 传给   new   Thread ( )   即可 ，     然后 再 调用 创建 的 线程 的   . start ( )   方法 就 可以 运行 了 。       也 可以 利用 java 的 线程 的 执行 服务 构建 一个 线程 池 。   java . util . concurrent . { Executors ,   ExecutorService }                 val       pool     :       ExecutorService       =       Executors     .     newFixedThreadPool     (     poolSize     )       pool     .     execute     (     new       MyRunableClass     ( ) )                     Futures .   Future   代表 异步 计算 。 你 可以 把 你 的 计算 包装 在 Future 中 ， 当 你 需要 计算结果 的 时候 ， 你 只 需 调用 一个 阻塞 的   get ( )   方法 就 可以 了 。 一个   Executor   返回 一个   Future   。           线程 安全 的 三种 工具 。           mutex   互斥 锁 。       volatile       AtomicReference                   / /   synchronized       class       Person     (     var       name     :       String     )       {           def       set     (     changedName     :       String     )       {               this     .     synchronized       {                   name       =       changedName               }           }       }         / /   volatile       class       Person     (     @ volatile       var       name     :       String     )       {           def       set     (     changedName     :       String     )       {               name       =       changedName           }       }         / /   AtomicReference       import       java . util . concurrent . atomic . AtomicReference         class       Person     (     val       name     :       AtomicReference     [     String     ] )       {           def       set     (     changedName     :       String     )       {               name     .     set     (     changedName     )           }       }                 略       Java 跨平台 交互 ： 在 Java 中 使用 Scala       扩展 规格       泛型 编程           使用   ClassTag   ， 用 类型 作为 参数     Spark   rdd . objectFile   源码             从 classTag 创建对象 的 方法 还 没 搞清楚 ， 参看   代码   ！ ：               import       scala . reflect .     {     classTag     ,       ClassTag     }         def       objectFile     [     T :       ClassTag     ] (                   path     :       String     ,                   minPartitions     :       Int       =       defaultMinPartitions     )     :       RDD     [     T     ]       =       withScope       {               assertNotStopped     ( )               sequenceFile     (     path     ,       classOf     [     NullWritable     ] ,       classOf     [     BytesWritable     ] ,       minPartitions     )                   .     flatMap     (     x       = & gt ;       Utils     .     deserialize     [     Array     [     T     ] ] (     x     .     _ 2     .     getBytes     ,       Utils     .     getContextOrSparkClassLoader     ) )           }           private       implicit       def       arrayToArrayWritable     [     T       & lt ; %       Writable :       ClassTag     ] (     arr     :       Traversable     [     T     ] )           :       ArrayWritable       =       {           def       anyToWritable     [     U       & lt ; %       Writable     ] (     u     :       U     )     :       Writable       =       u             new       ArrayWritable     (     classTag     [     T     ] .     runtimeClass     .     asInstanceOf     [     Class     [     Writable     ] ] ,                   arr     .     map     (     x       = & gt ;       anyToWritable     (     x     ) ) .     toArray     )       }                       . asInstanceOf [ T ]     进行 类型转换 。           Test       包     http : / / www . scalatest . org /         build . sbt   引入 测试 包               libraryDependencies       + =       & quot ; org . scalatest & quot ;       %       & quot ; scalatest _ 2.10 & quot ;       %       & quot ; 2.2 . 6 & quot ;       %       & quot ; test & quot ;                 FlatSpec       \" X   should   Y , \"   \" A   must   B , \"           assert       assertResult       assertThrows           Achieving   success       FunSuit               import       org . scalatest . FunSuite         class       SetSuite       extends       FunSuite       {             test     (     & quot ; An   empty   Set   should   have   size   0 & quot ;     )       {               assert     (     Set     .     empty     .     size       = =       0     )           }             test     (     & quot ; Invoking   head   on   an   empty   Set   should   produce   NoSuchElementException & quot ;     )       {               assertThrows     [     NoSuchElementException     ]       {                   Set     .     empty     .     head               }           }       }                 现在 用 的 是 3.0 ，   intellij   好像 支持 得 不好 。 最好 还是 用   2.2 . 6 吧 ！  ", "tags": "tools", "url": "/wiki/tools/scala.html"},
      
      
      {"title": "scala语言底层实现所设计的数据结构", "text": "    Table   of   Contents           关于           Hash   Tries           hashtable                 关于       闲得无聊 ， 学习 一下 。       Hash   Tries       scala   里面 不可 变 的 的 HashMap   和   HashSet   使用   HashTries 实现       hashtable       可变 的 HashMap 和 HashSet 使用 hashtable 实现 。  ", "tags": "tools", "url": "/wiki/tools/scala-data-implement.html"},
      
      
      {"title": "Spark", "text": "    Table   of   Contents           安装           启动 主机 和 worker           Spark   shell                   SparkContext           RDD           RDD   操作           RDD 持久 化           理解 闭包           KV 值 操作           通用 的 变换           Action                   共享 变量           提交 spark 任务           Spark   Streaming           Spark   SQLContext ，           DataFrame                   MLlib           spark . ml   包           基础 类           特征提取           特征 变换           特征选择           分类   org . apache . spark . ml . classification           回归   org . apache . spark . ml . regression           聚类   org . apache . spark . ml . clustering           协同 过滤           DataFrame                   spark . mLlib           基本 数据结构           模型 评估                   TIPS           使用 log4j                         安装       从 Spark 官网 下载安装 包 ， 然后 解压 即可 。 非常简单       启动 主机 和 worker       进入 spark 目录 ， 然后 运行 脚本             . / sbin / start - master . sh               即可 。 进程 会 在 后台 运行 ， 你 可以 通过     http : / / localhost : 8080     进行 监控 。       启动 worker 的 脚本 是             . / bin / spark - class   org . apache . spark . deploy . worker . Worker   spark : / / IsP : PORT               其中 IP 和 PORT 可以 在 监控 页面 看到 。       关闭 worker 很 简单 ， 直接 关闭 worker 运行 的 shell 或者 ctr   +   c 中断 即可 。     关闭 主机 需要 运行 脚本             . / sbin / stop - master . sh               Spark   shell       启动 scala 版 的 shell 命令 为   . / bin / spark - shell   ， python 版 的 命令 为   . / bin / pyspark         SparkContext       sc 是 spark 的 入口 ， 通过   SparkConf   来 创建 它 。               val       sparkConf       =       new       SparkConf     ( ) .     setAppName     (     & quot ; FromPostgreSql & quot ;     )           .     setMaster     (     & quot ; local [ 4 ] & quot ;     )           .     set     (     & quot ; spark . executor . memory & quot ;     ,       & quot ; 2g & quot ;     )       val       sc       =       new       SparkCsontext     (     sparkConf     )                 对 了 ， 目前 spark 只 支持 的 scala 版本 是 2.10 . x ， 所以 用 2.11 . x 版本 可能 会 出错 。       使用   sc . stop ( )   方法 停止 SparkContext 。 貌似 不 执行 stop ， 本地 用   sbt   run   运行 时会 出现 错误信息 ，     但是 提交 jar 方式 运行 没 问题 。     参考   https : / / stackoverflow . com / questions / 28362341 / error - utils - uncaught - exception - in - thread - sparklistenerbus   .           issue       使用   sbt   run   方式 运行 任务 ， 如果 涉及 到   saveAsTextFile   操作 时 ， 会 出错 ， 原因 未知 。                   RDD           RDD ， 全 称为 Resilient   Distributed   Datasets ， 是 一个 容错 的 、 并行 的 数据结构 ， 可以 让 用户 显式 地 将 数据 存储 到 磁盘 和 内存 中 ， 并 能 控制数据 的 分区 。       in - memory   cache .     cache ( )         RDD   常用 操作         count ( )           foreach   ,     map   ,     flatMap   ,     filter   ,               并行 化 容器 ， 可以 通过   SparkContext . parallelize     方法 创建 分布式 便于 并行计算 的 数据结构 。 也 可以 用来 将 scala 的 容器 转换 为 RDD 结构 的 tips                   val       data       =       Array     (     1     ,     2     ,     4     ,     5     ,     6     ,     7     )       val       distData       =       sc     .     parallelize     (     data     )                     从 外部 数据库 创建 ， 支持 本地 文件系统 ， HDFS ， Cassandra ，   HBase ，   Amazon   S3 ，   等 。         支持 的 文件格式 包括 文本文件 ，   SequenceFiles ， 其他 Hadoop 输入 格式 。         其中 文本格式 可以 通过   SparkContext . textFile ( URI   [ ,   partition _ number ] )   方法 创建 RDD 。       支持 本地 文件 和 网络 文件 的 URI ， \" / home / user / path - to - file \" ,   \" hdfs : / / path - to - file \"       支持 文件夹 ， 压缩文件 ， 通配符 等 方式 。 例如 \" / path - to - file / * . gz \" ,   \" / path - to - file / directory \"       指定 分区 数目 ， 每 一个 分区 是 64MB ， 默认 创建 一个 分区 。       也 可以 通过     SparkContext . wholeTextFiles     读取 一个 目录 下 的 所有 文本文件 ， 返回 的 是   ( filename ,   content ) ，         而   textFile     则 返回 所有 的 行       其他 Hadoop 输入 格式 可以 使用     SparkContext . hadoopRDD     方法 。       其他 基于     org . apache . hadoop . mapreduce     API   的 输入 格式 可以 通过       SparkContext . newAPIHadoopRDD     方法 创建         RDD . saveAsObjectFile     和     SparkContext . objectFile     支持 保存 RDD 为 简单 的 序列化 java 对象 。                   RDD   操作           支持 两种 操作   map ，   reduce       变换 ： 从 一个 已经 存在 的 数据 创建 新 的 数据 ， 如     map   ,     reduce   ,     reduceByKey   。 所有 的 变换 操作 都 是 惰性 求值 ， 而且 不 保存         中间 结果 。 如果 重新 计算 ， 中间 结果 也 会 重新 计算 。 如果 需要 保存 中间 结果 可以 通过   RDD . persist ( )   方法 指明 保存 该 RDD 。       传递函数 给 spark ， 不同 的 语言 不同       scala 中 可以 通过 以下 几种 方式       匿名 函数       单例 模式 对象 的 一个 静态方法       一个 类 的 实例 对象 的 一个 成员 方法 ， 这种 情况 需要 传递 整个 对象 过去 。 同样 ， 如果 函数 应用 了 外部 的 对象 的 一个 域 ， 那么 也 需要 传递 整个 对象 。         为了 避免 这个 问题 ， 可以 创建 该域 的 一个 本地 拷贝 。                                   class       MyClass       {           val       field       =       & quot ; Hello & quot ;           def       doStuff     (     rdd     :       RDD     [     String     ] )     :       RDD     [     String     ]       =       {       rdd     .     map     (     x       = & gt ;       field       +       x     )       }       }         / /   修改 后 的 doStuff   函数       def       doStuff     (     rdd     :       RDD     [     String     ] )     :       RDD     [     String     ]       =       {           val       field _       =       this     .     field           rdd     .     map     (     x       = & gt ;       field _       +       x     )       }                       -   java ,     ` org . apache . spark . api . java . function `   对象 ， 或者 java   8   的 lambda 表达式   -   python ，   lambda 表达式 ， 本地 函数 ， 模块 的 顶级 函数 ， 对象 的 方法                   重新 分区 ，   repartition   会 重新分配 所有 数据 ， 如果 是 降低 分区 数目 ， 可以 用   coalesce   ， 它会 避免 移动 所有 数据 ，         而 只是 移动 丢弃 的 分区 的 数据 ， 参考   stackoverflow 的 讨论   。           RDD 持久 化       持久 化 的 两个 方法     . cache ( )   和   . persist ( StorageLevel . SOME _ LEVEL )   ， 存储 级别 有 ：           MEMORY _ ONLY   ：   默认 级别 ， 以   deserialized   Java   objects   保存 在 内存 （ JVM ） ， 内存 放不下 的 部分 每次 也 是 重新 计算       MEMORY _ AND _ DISK   ：   保存 在 内存 ， 放不下 的 放在 磁盘       MEMORY _ ONLY _ SER   ：   序列化 后 再 保存 在 内存 ， 放不下 重新 计算       MEMORY _ AND _ DISK _ SER   ： 与 上 一个 术语 差异 在于 放不下 的 放 磁盘       DISK _ ONLY   ：   只放 磁盘       MEMORY _ ONLY _ 2 ,   MEMORY _ AND _ DISK _ 2 ,   etc .   ：   多 保存 一个 备份       OFF _ HEAP   ( experimental )   ：   Store   RDD   in   serialized   format   in   Tachyon           在 python 中 都 是 用 pickle 序列化 ， 只有 这 一种 。     手动 移除 cache 的 方法 是     RDD . unpersist ( )   ， 如果 不 手动 移除 ， Spark   也 会 自动 处理 cache 的 。       理解 闭包           在 RDD 的 foreach 中 ， 对外部 变量 的 引用 实际上 是 复制 了 该 对象 到 executor 中 ， 然后 引用 executor 中 的 那个 对 像 ， 所以 不会 改变 本想 引用 的 那个 对象 。         可以 使用   Accumulator   来 实现 改变 主 对象 。       输出 RDD 到 stdout ， 同样 存在 一个 问题 ， 在 foreach 和 map 中 的 prinln 是 输出 到 executor 的 stdout 。 可以 通过   RDD . collect ( ) . foreach ( println )   方法 实现 ，         如果 该 只是 打印 一部分 ， 可以 通过   RDD . take ( 100 ) . foreach ( println )     来 实现 。           KV 值 操作           由于 KV 类型 可以 是 很多 不同 类型 ， 通用 的 操作 不 多 ， 最 常用 的 是     shuffle     操作 ， 例如   grouping   和   aggregating   by   key 。       在 spark 中 通过 创建 Tuple2 对象 实现 K - V ， 例如 在 下述 代码 中                   val       lines       =       sc     .     textFile     (     & quot ; data . txt & quot ;     )       val       pairs       =       lines     .     map     (     s       = & gt ;       (     s     ,       1     ) )       val       counts       =       pairs     .     reduceByKey     ( (     a     ,       b     )       = & gt ;       a       +       b     )                 注意 ， 在 使用 自定义 的 对象 作为 key 的 时候 ， 需要 确保   . equals ( )   方法 与   hashCode ( )   方法 兼容 。       通用 的 变换           map ( func )       filter ( func )       flatMap ( func ) ,   相当于 先 做 map ， 然后 做 flat 操作       mapPartitions ( func ) ， map 到 每 一个 分区       mapPartitionsWithIndex ( func ) ，   带有 index 的 版本       sample ，   采样       union ， 并集       intersection ， 交集       distinc ,   去 重       groupByKey ， 输入 ( K , V ) ， 输出 ( K ,   Iter   )       reduceByKey ( func ) ， 输入 ( K , V )       aggregateByKey       sortByKey         join ( otherDataset   [ ,   numTasks ] )   ,     ( K , V ) ,   ( K , W )   - & gt ;   ( K ,   ( V , W ) )           cogroup       cartesian   笛卡尔 积 ？       pipe       coalesce       repartition     略           Action           reduce       collect       count       first       take ( n )       takeSample       takeOrdered       saveAsTextFile ( path )       saveAsSequenceFile ( path ) ,   java   and   scala       countByKey ， 对 每 一个 key 单独 计数       foreach ( func )           共享 变量           broadcast 变量 ， 不同 的 executor 共享                   val       broadcastVar       =       sc     .     broadcast     (     Array     (     1     ,       2     ,       3     ) )       broadcastVar     .     value                 优点 在于 ， 不同于 简单 复制 ， 可以 采用 P2P 协议 来 提升 在 多个 节点 之间 复制 的 性能 ！ 对于 很大 的 共享 对象 ， 性能 提升 很 明显 ！       https : / / stackoverflow . com / questions / 26884871 / advantage - of - broadcast - variables             Accumulator ,                   val       accum       =       sc     .     accumulator     (     0     ,       & quot ; My   Accumulator & quot ;     )       sc     .     parallelize     (     Array     (     1     ,       2     ,       3     ,       4     ) ) .     foreach     (     x       = & gt ;       accum       + =       x     )       accum     .     value                 一般 需要 实现 自己 的 AccumulatorParam 子类 ，               object       VectorAccumulatorParam       extends       AccumulatorParam     [     Vector     ]       {           def       zero     (     initialValue     :       Vector     )     :       Vector       =       {               Vector     .     zeros     (     initialValue     .     size     )           }           def       addInPlace     (     v1     :       Vector     ,       v2     :       Vector     )     :       Vector       =       {               v1       + =       v2           }       }         / /   Then ,   create   an   Accumulator   of   this   type :       val       vecAccum       =       sc     .     accumulator     (     new       Vector     ( ... ) ) (     VectorAccumulatorParam     )                 提交 spark 任务       使用   bin / spark - submit   脚本 提交 ， 语法             . / bin / spark - submit     \ \         - - class   & lt ; main - class & gt ;     \ \         - - master   & lt ; master - url & gt ;     \ \         - - deploy - mode   & lt ; deploy - mode & gt ;     \ \         - - conf   & lt ; key & gt ;   =   & lt ; value & gt ;     \ \         ...     #   other   options         & lt ; application - jar & gt ;     \ \           [   application - arguments   ]                     For   Python   applications ,   simply   pass   a   . py   file   in   the   place   of       instead   of   a   JAR ,   and   add   Python   . zip ,   . egg   or   . py   files   to   the   search   path   with   - - py - files .           Spark   Streaming       简单 地说 ， 就是 用来 从 其他 地方 拉 数据 的 。     输入 数据流   = & gt ;   Spark   streaming   = & gt ;   batches   of   input   data   = & gt ;   Spark   engine   = & gt ;   batches   of   processed   data       Spark   SQLContext ，           从 SparkContext 创建                   org     .     apache     .     spark     .     sql     .     SQLContext       val       sc     :       SparkContext       / /   An   existing   SparkContext .       val       sqlContext       =       new       org     .     apache     .     spark     .     sql     .     SQLContext     (     sc     )                     使用   . sql   函数 进行 SQL 查询 ， Spark   SQL 支持 的 语法                   SELECT       [     DISTINCT     ]       [     column       names     ]     |     [     wildcard     ]       FROM       [     kesypace       name     . ]     table       name       [     JOIN       clause       table       name       ON       join       condition     ]       [     WHERE       condition     ]       [     GROUP       BY       column       name     ]       [     HAVING       conditions     ]       [     ORDER       BY       column       names       [     ASC       |       DSC     ] ]                 如果 使用 join 进行 查询 ， 则 支持 的 语法 为 ：               SELECT       statement       FROM       statement       [     JOIN       |       INNER       JOIN       |       LEFT       JOIN       |       LEFT       SEMI       JOIN       |       LEFT       OUTER       JOIN       |       RIGHT       JOIN       |       RIGHT       OUTER       JOIN       |       FULL       JOIN       |       FULL       OUTER       JOIN     ]       ON       join       condition                 -       DataFrame       Spark   DataFrame 的 设计 灵感 正是 基于 R 与 Pandas 。     我们 通过 外部 Json 文件创建 一个 DataFrame ：               val       dataFrame       =       sqlContext     .     load     (     & quot ; / example / data . json & quot ;     ,       & quot ; json & quot ;     )       dataFrame     .     show     ( )                     With   a   SQLContext ,   applications   can   create   DataFrames   from   an   existing   RDD ,   from   a   Hive   table ,   or   from   data   sources .                   / /   Create   the   DataFrame       val       df       =       sqlContext     .     read     .     json     (     & quot ; examples / src / main / resources / people . json & quot ;     )         / /   Show   the   content   of   the   DataFrame       df     .     show     ( )       / /   age     name       / /   null   Michael       / /   30       Andy       / /   19       Justin         / /   Print   the   schema   in   a   tree   format       df     .     printSchema     ( )       / /   root       / /   | - -   age :   long   ( nullable   =   true )       / /   | - -   name :   string   ( nullable   =   true )         / /   Select   only   the   & quot ; name & quot ;   column       df     .     select     (     & quot ; name & quot ;     ) .     show     ( )       / /   name       / /   Michael       / /   Andy       / /   Justin         / /   Select   everybody ,   but   increment   the   age   by   1       df     .     select     (     df     (     & quot ; name & quot ;     ) ,       df     (     & quot ; age & quot ;     )       +       1     ) .     show     ( )       / /   name         ( age   +   1 )       / /   Michael   null       / /   Andy         31       / /   Justin     20         / /   Select   people   older   than   21       df     .     filter     (     df     (     & quot ; age & quot ;     )       & gt ;       21     ) .     show     ( )       / /   age   name       / /   30     Andy         / /   Count   people   by   age       df     .     groupBy     (     & quot ; age & quot ;     ) .     count     ( ) .     show     ( )       / /   age     count       / /   null   1       / /   19       1       / /   30       1                     直接 在 文件 上 运行 SQL ！                   val       df       =       sqlContext     .     sql     (     & quot ; SELECT   *   FROM   parquet . ` examples / src / main / resources / users . parquet ` & quot ;     )                     注册 UDF                   sqlContext     .     udf     .     register     (     & quot ; strLen & quot ;     ,       (     s     :       String     )       = & gt ;       s     .     length     ( ) )                 MLlib           不同 的 包 的 特点 ， 推荐   spark . ml           spark . mllib     contains   the   original   API   built   on   top   of   RDDs .   在 2.0 版本 不 在 支持 新 特性 了 ， 不再 维护 。         spark . ml     provides   higher - level   API   built   on   top   of     DataFrames     for   constructing   ML   pipelines .                   spark . ml   包       基础 类           基于 DataFrame ， 借助于 抽象 ， 将 模型 抽象 为 三个 基本 类 ， estimators （ 实现 fit 方法 ） ,   transformers （ 实现 transform 方法 ） ,   pipelines       一个 正常 的 模型 应该 同时 实现     fit     和     transform     两个 方法         transform     将 生成 一个 新 的 DataFrame ， 包含 了 预测 的 结果         fit     的 DataFrame 需要 包含 两列   featuresCol   和   labelCol   默认 名字 为   label             transform     之前 的 DataFrame 需要 包含 一列   featuresCol ， 默认 名字 为 features ， 输出 三列 （ 依赖于 参数 ） ， 三列 有 默认 名字 ， 都 可以 通过 setter 函数 进行 设置 。           predictedCol   预测 的 标签 ， 默认 名字 为     prediction         rawPredictedCol   预测 的 裸 数据 ？ 向量 ？ 逻辑 回归 是   wx   貌似 ， 默认 名字 为     rawPrediction         probabilityCol   预测 的 概率 ， 默认 名字 为     probability                     模型 参数 封装 类     Param   ， 他 的 一个 常用 子类 是     ParamMap   ， 实现 了 Map 接口 ， 可以 通过     get ,   put   进行 操作 。     在 2.0 版本 开始 ， Spark 对 Estimators 和 Transformers 提供 统一 的 参数 API 。                       val       paramMap       =       ParamMap     (     lr     .     maxIter       - & gt ;       20     )           .     put     (     lr     .     maxIter     ,       30     )       / /   Specify   1   Param .     This   overwrites   the   original   maxIter .           .     put     (     lr     .     regParam       - & gt ;       0.1     ,       lr     .     threshold       - & gt ;       0.55     )       / /   Specify   multiple   Params .                         paramMap       =       {     lr     .     maxIter     :       20     }       paramMap     [     lr     .     maxIter     ]       =       30       #   Specify   1   Param ,   overwriting   the   original   maxIter .       paramMap     .     update     ( {     lr     .     regParam     :       0.1     ,       lr     .     threshold     :       0.55     } )       #   Specify   multiple   Params .                       pipeline     将 不同 模型 （ transform ） 堆叠 起来 ， 类似 于 sklearn 里面 的 pipeline 。     pipeline 保存 了 一个 Array [ PipelineStage ] ， 可以 通过   . setStage ( Array [ _   & lt ; :   PipelineStage ] )   函数 进行 设置 。     pipeline 实现 了 estimator 的 fit 接口 和 transformer 的 transform 接口 。       模型 持久 化     save ,   load           PipelineStage   抽象类 ， 啥 也 没干 ？ ？ ？ ？ ？ ？ ？ ？ ！ ！ ！ ！ ！   transformer   还是 它 的 子类 ！ ！             UnaryTransformer     单列 转换 对象 ， 是 transformer 的 子 抽象类 ， 也 实现 了 pipelinestage 接口 。         有 两个 变量   inputCol   和   outputCol   代表 输入输出 列 的 名字 。         有 几个 常用 的 实例 ， 例如 Tokenizer ， HashingTF 等 。               模型 的 保存 和 加载 ， 利用 类 的 静态方法   . load   加载 ( MLReader 的 实现 ) ， 而用 实例 的   . save   方法 （ MLWriter 的 实现 ） 保存 模型 到 文件 。               模型 评估     Evaluator   ( 实现   evaluate ( dataFrame )   方法 ) ，     RegressionEvaluator   回归 ，     BinaryClassificationEvaluator   二元 分类 ，       MulticlassClassificationEvaluator     多元 分类 。             BinaryClassificationEvaluator     除了   evaluate   方法 之外 ， 还有 几个 重要 的 属性 和 属性 setter 。 标签 列名   labelCol   ， 度量 名称     metricName     默认 为 areaUnderROC ， 即 AUC 。   rawPredictionCol     预测 结果 列名 。 以及 相应 的 setter 和 getter 。         MulticlassClassificationEvaluator   ， 三个 属性     labelCol   ，   metricName     （ supports   \" f1 \"   ( default ) ,   \" precision \" ,   \" recall \" ,   \" weightedPrecision \" ,   \" weightedRecall \" ） ，   predictionCol           RegressionEvaluator   ， 三个 属性       labelCol   ，   metricName     （ \" rmse \"   ( default ) :   root   mean   squared   error ，   \" mse \" :   mean   squared   error ，   \" r2 \" :   R2   metric ，   \" mae \" :   mean   absolute   error ） ，   predictionCol                     交叉 验证 选择 模型 超 参数 。 交叉 验证     CrossValidator     类 ， 有 4 个 基本 方法             . setEstimator           . setEvaluator           . setEstimatorParamMaps ( paramGrid )     参数 网络         . setNumFolds ( k )     k - fold 交叉 验证 的 参数 k     同是 他 也 是 一个 estimator ， 调用 它 的   fit   方法 训练 模型 ， 返回 训练 好 的 模型 CrossValidatorModel 或 模型 序列 。     他 也 是 一个 transformer ， 调用   transform   方法 直接 执行 多个 transform 。                   训练 集 和 测试 集 的 分割     TrainValidationSplit   与 交叉 验证 类 类似 ， 取代   . setNumFolds   的 是 函数   . setTrainRatio ( ratio )   。               参数 网格 可以 通过     ParamGridBuilder   对象 创建 ， 他 有 三个 方法 ，   addGrid ( param ,   values : Array )   添加 一个 参数 网格 ，       baseOn ( paramPair )   设置 指定 参数 为 固定值 ，   build ( )   方法 返回 一个   Array [ ParamMap ]   数组               特征提取           TF - IDF ( HashingTF   and   IDF ) ， 传统 的 词 统计 是 通过 维护 一个 查找 的 词典 （ hash 表 或者 查找 树 实现 ） ，     HashTF 则 是 直接 通过 对 特征 计算 hash 函数 映射 到 低维 索引 。 还 可以 通过 第二个 hash 函数 确定 是否 存在 冲突 。     有 什么 优势 ？ ？ ？ 使用 的 类   HashingTF ,   IDF ,   Tokenizer         Word2Vec ， 低维词 向量 学习 ， 对应 的 类 ：   Word2Vec         CountVectorizer ， 直接 统计 ：   CountVectorizer             特征 变换           Tokenizer ： 将 文本 转换 为 一个 一个 的 词 。 例如 中文 分词 就算 一个 ， 对于 英文 可以 简单 的 用 空白 字符 分割 就行 。 可用 的 类 有 ：         Tokenizer     常规 Tokenizer         RegexTokenizer     正则 式 Tokenizer               StopWordsRemover ： 停止 词 的 移除 。   StopWordsRemover   ， 可以 通过   setCaseSensitive   设置 大小写 敏感 ，     和   setStopWords ( value :   Array [ String ] )   设置 停止 词 词典 。       n - gram ： 将 输入 的 一串 词 转换 为 n - gram 。   NGram         Binarizer ： 通过 阈值 将 数值 特征 变成 二值 特征 。 类   Binarizer   ， 主要 方法 ：   setThreshold         PCA ： PCA   降维 。   PCA   ， 方法   setK         PolynomialExpansion ： 将 特征 展开 为 多项式 特征 ， 实现 特征 交叉 。   x1 , x2 - & gt ; x1 ^ 2 , x2 ^ 2 , x1x2   。   PolynomialExpansion   方法 :   setDegree         DCT ： 离散 余弦 变换 。   DCT           StringIndexer     将 字符串 类型 的 变量 （ 或者 label ） 转换 为 索引 序号 ， 序号 会 按照 频率 排序 ， 不是 字典 序 。 对于 不 在 词典 的 string ， 默认 抛出 异常 ， 也 可以 通过   setHandleInvalid ( \" skip \" )   直接 丢弃 。         IndexToString     和   StringIndexer   配合 使用 可以 让 字符串 类型 的 变量 的 处理 变得 透明 ， 这个 是 将 index 变成 原来 的 字符串         OneHotEncoder   ： 将 单个 数字 转换 为 0 - 1 编码 的 向量 。   1 - & gt ; ( 0 , 1 , 0 , 0 )   。 常用 在 类别 特征 的 变换 。         VectorIndexer   ： 将 输入 向量 中 的 类别 特征 自动编码 为 index 。 比较 高端 ， 需要 学习 一下 ！   setMaxCategories ( 4 )   表示 特征 的 值 数目 超过 4 个 就 认为 是 连续 特征 ， 否则 认为 需要 编码 。         Normalizer   ： 归一化 。 需要 指定 p - norm 的 值   setP   。 按照 p 范数 归一化 ， 默认 为 2 。 可以 用 在 输出 概率 或 score 时 归一化 ？         StandardScaler   ： 标准化 特征 到 方差 为 1 ， 也 可以 将 均值 设为 0 .   方法 ：   setWithStd ( bool ) ,   setWithMean ( bool )           MinMaxScaler   ： 归一化 到 0 - 1 之间 。 也 可以 指定 min 和 max         MaxAbsScaler   ： @ since ( 2.0 . 0 ) ， 除以 最大值 的 绝对值 ， 从而 将 特征 归一化 到 [ - 1 , 1 ]         Bucketizer   ： 分桶 。 方法   setSplits ( splits )   来 设置 分割 点 ， 分割 点 需要 严格 递增 。         ElementwiseProduct   ： 对 输入 向量 乘以 一个 权值 。 方法   setScalingVec   设置 权值 。         SQLTransformer   ： 让 你 用 SQL 语句 进行 变换 特征 。 例子 ：                   val       sqlTrans       =       new       SQLTransformer     ( ) .     setStatement     (           & quot ; SELECT   * ,   ( v1   +   v2 )   AS   v3 ,   ( v1   *   v2 )   AS   v4   FROM   __ THIS __& quot ;     )                       VectorAssembler   ： 将 多个 特征 合并 到 一个 向量 ， 也 可以 合并 向量 。 通过   setInputCols ( Array [ String ]   设置 要 合并 的 列 。         QuantileDiscretizer   ： 首先 对 特征 采样 ， 然后 根据 采样 值 将 特征 按照 等量 分桶 （ 等频 离散 化 ） 。 基于 采样 ， 所以 每次 不同 。 方法   setNumBuckets ( i : Int )   设置 桶 的 个数 。           特征选择             VectorSlicer   ： 通过 slice 选择 特征 ， 人工 指定 indices 。 方法   setIndices   设置 选择 的 indices 。 字符串 indices 通过   setNames   方法 设置 索引 。         RFormula   ： 通过 R 模型 公式 选择 特征 ， 例如   clicked   ~   country   +   hour   ， 输出 列是 默认 是 公式 的 响应 变量 名字 。 它会 对 字符串 one - hot 编码 ， 对 数值 列 转换 为 double 类型 。 需要 人工 指定 哪些 特征 。         ChiSqSelector   ： 通过 卡方 独立性 检验 来 选择 特征 。 方法   setNumTopFeatures   指定 要 选择 的 卡方值 前 多少 个 。                   val       labelIndexer       =       new       StringIndexer     ( )               .     setInputCol     (     & quot ; label & quot ;     )               .     setOutputCol     (     & quot ; indexedLabel & quot ;     )               .     fit     (     training     )       val       rf       =       new       RandomForestClassifier     ( )               .     setPredictionCol     (     & quot ; indexedPrediction & quot ;     )               .     setLabelCol     (     & quot ; indexedLabel & quot ;     )       setRFParam     (     rf     ,       param     )       val       labelConverter       =       new       IndexToString     ( )               .     setInputCol     (     & quot ; indexedPrediction & quot ;     )               .     setOutputCol     (     & quot ; prediction & quot ;     )               .     setLabels     (     labelIndexer     .     labels     )       val       pipeline       =       new       Pipeline     ( )               .     setStages     (     Array     (     labelIndexer     ,       rf     ,       labelConverter     ) )                 分类     org . apache . spark . ml . classification             逻辑 回归 ：   LogisticRegression   ， 参数   maxIter ,   regParam ,   elasticNetParam     分别 是 最大 迭代 次数 、 正则 项 系数 、 elastic 网 的 参数 。       决策树 ：   DecisionTreeClassifier         随机 森林 ：   RandomForestClassifier         GBDT ：   GBTClassifier         多层 感知器 （ 全 连接 神经网络 ） ：   MultilayerPerceptronClassifier   ，   setLayers   指定 每层 的 节点 数目 。   有没有 预 训练 ？ 需要 研究 一下 ！ ！ ！         One - vs - All ：   OneVsRest   ， 将 二 分类 变成 多 分类 模型 ， 采用 One - vs - all 策略 。 方法   setClassifier   设置 二 分类器 。       朴素 贝叶斯 ：   NaiveBayes   ，           回归     org . apache . spark . ml . regression             线性 回归 ：   LinearRegression         广义 线性 回归 ：   GeneralizedLinearRegression     @ since ( 2.0 . 0 )       决策树 回归 ：   DecisionTreeRegressor         随机 森林 回归 ：   RandomForestRegressor         GBDT 回归 ：   GBTRegressor         Survival   regression ：   AFTSurvivalRegression   ， 什么 东西 ？       Isotonic   回归 ：   IsotonicRegression   ， 什么 东西 ？           聚类     org . apache . spark . ml . clustering               KMeans   ， K - means 聚类 ， 通过   setK   设置 类 的 数目 。   K - means ++ 的 分布式 实现   。         LDA   ： Latent   Dirichlet   allocation         BisectingKMeans   ： Bisecting   k - means   聚类 ， @ since ( 2.0 . 0 )   不 懂 ？         GaussianMixture   ： GMM   模型 。 @ since ( 2.0 . 0 )           协同 过滤             ALS   ： ALS 算法 ， 2.0 才 加到 ml 包 里面 ， 之前 在 mllib 包 。           DataFrame       DataFrame 相当于   RDD [ Row ] ， 而 Row 相当于 一个 可以 包含 各种 不同 数据 的 Seq 。     DataFrame 通过 collect 函数 之后 就是 Array [ Row ]       通过 工厂 方法   SQLContext . createDataFrame   创建 DataFrame ， 可以 从 一下 几个 数据源 创建           从   List ( label ,   FeatureVector )   序列 创建       从     JavaRDD   创建       从     RDD     创建       从     List [ Row ]     创建       从     RDD [ Row ]     创建                   val       training       =       sqlContext     .     createDataFrame     (     Seq     (           (     1.0     ,       Vectors     .     dense     (     0.0     ,       1.1     ,       0.1     ) ) ,           (     0.0     ,       Vectors     .     dense     (     2.0     ,       1.0     ,       -     1.0     ) ) ,           (     0.0     ,       Vectors     .     dense     (     2.0     ,       1.3     ,       1.0     ) ) ,           (     1.0     ,       Vectors     .     dense     (     0.0     ,       1.2     ,       -     0.5     ) )       ) ) .     toDF     (     & quot ; label & quot ;     ,       & quot ; features & quot ;     )                 spark 的 DataFrame 每 一列 可以 存储 向量 ！ 甚至 图像 ！ 任意 值 都行 ！ ！               SQL 操作           select ( col1 ,   col2 ,   ... )   选取 部分 列       sample   采样       sort   排序       unionAll   融合 其他 表       orderBy       limit       join   内 连接       groupyBy       filter ( sql 表达式 )                   lazy   val   rdd   对象 ， 可以 通过 RDD 接口 操作               df . sqlContext   可以 访问 创建 该 DataFrame   的 SQLContext 对象 ， rdd . sparkContext   可以 访问 创建 RDD 的 SparkContext 对象 。               保存 到 磁盘                       df     .     rdd     .     map       {       转换 操作       }       .     saveAsTextFile     (     filepath     )                 spark . mLlib           LogisticRegressionWithLBFGS       LogisticRegressionModel ，   要   model . clearThreshold     predict 才 会 输出 概率 ， 否则 输出 的 是 判决 后 的 值           基本 数据结构           Vector ,   可以 通过 工厂 对象   Vectors   创建 ， 普通 向量   Vectors . dense   ， 稀疏 向量   Vectors . sparse   ， 通过   . toArray   方法 转换 为   Array [ Double ]         LabeledPoint ,   二元 组     ( label : Double ,   features :   Vector )         Matrix ，   可以 通过 工厂 对象   Matrices   创建 ， 普通 矩阵     Matrices . dense   ， 稀疏 矩阵   Matrices . sparse         RowMatrix ， 前面 的 向量 和 矩阵 都 是 存在 单机 中 ， 这种 和 下面 的 矩阵 是 分布式 存储 的 。       IndexedRowMatrix ， indexedrow 是 ( long ,   vector ) 的 包装 使得 index 是 有 意义 的       CoordinateMatrix ，       BlockMatrix                   val       rows     :       RDD     [     Vector     ]       =       ...       / /   an   RDD   of   local   vectors       / /   Create   a   RowMatrix   from   an   RDD [ Vector ] .       val       mat     :       RowMatrix       =       new       RowMatrix     (     rows     )           val       rows     :       RDD     [     IndexedRow     ]       =       ...       / /   an   RDD   of   indexed   rows       / /   Create   an   IndexedRowMatrix   from   an   RDD [ IndexedRow ] .       val       mat     :       IndexedRowMatrix       =       new       IndexedRowMatrix     (     rows     )                 模型 评估       包名   org . apache . spark . mllib . evaluation             两 分类   BinaryClassificationMetrics       多 分类   MulticlassMetrics           TIPS       使用 log4j               package       org . apache . log4j     ;             public       class       Logger       {                 / /   Creation   & amp ;   retrieval   methods :               public       static       Logger       getRootLogger     ( ) ;               public       static       Logger       getLogger     (     String       name     ) ;                 / /   printing   methods :               public       void       trace     (     Object       message     ) ;               public       void       debug     (     Object       message     ) ;               public       void       info     (     Object       message     ) ;               public       void       warn     (     Object       message     ) ;               public       void       error     (     Object       message     ) ;               public       void       fatal     (     Object       message     ) ;                 / /   generic   printing   method :               public       void       log     (     Level       l     ,       Object       message     ) ;       }         / /   例子       import       org . apache . log4j . Logger       val       log       =       Logger     .     getLogger     (     getClass     .     getName     )       log     .     info     (     & quot ; info & quot ;     )                         如果 对 RDD 操作 里面 有 随机 的 因素 在 里面 ， 那么 每次 操作 会 不 一样 ！ ！               Spark   in   Action   [ BOOK ]     https : / / zhangyi . gitbooks . io / spark - in - action             Spark   Programming   Guide     https : / / spark . apache . org / docs / latest / programming - guide . html        ", "tags": "tools", "url": "/wiki/tools/spark.html"},
      
      
      {"title": "Spring 快速入门", "text": "    Table   of   Contents           概念           Bean   可 重用 对象           SpringBoot   简单   Web   APP           定时 执行 任务           文件 上传           页面 模板           Bean   自动 配置           表单 认证                 概念           Bean :   Java 可 重用 对象 ,   可以 复用 ,   单例 模式       IOC （ 控制 反转 ， Inverse   Of   Control ） ,   DI （ 依赖 注入 ， Dependency   Injection ） ， 两者 是 一样 的           Bean   可 重用 对象       下述 BasicBean 的 属性 value 可以 通过   getter   和   setter   获取 和 设置 。               public       class       BasicBean       {               public       String       getValue     ( )       {                       return       value     ;               }                 public       void       setValue     (     String       value     )       {                       this     .     value       =       value     ;               }                 private       String       value       =       & quot ; & quot ;     ;       }                 如果 我们 要 创建 一个     BasicBean     对象 , 并 设置   value   的 值 为     Hello   Test   Bean   ,   可以 按照 Java 的 基本 语法 实现 如下               BasicBean       bean       =       new       BasicBean     ( ) ;       bean     .     setValue     (     & quot ; Hello   Test   Bean & quot ;     ) ;                 但是 , 如果 我们 要 在 很多 不同 的 函数 中 使用 同一个     BasicBean     对象 , 我们 就 需要 实现 单例 模式 来 做 。       Spring 里面 提供 一种 更 方便 的 方式 ,   通过 XML 配置文件 和   BeanFactory   实现 对象 可 重用 。               & lt ; ? xml   version   =   & quot ; 1.0 & quot ;   encoding   =   & quot ; UTF - 8 & quot ; ? & gt ;         & lt ; beans       xmlns   =       & quot ; http : / / www . springframework . org / schema / beans & quot ;                     xmlns : xsi   =       & quot ; http : / / www . w3 . org / 2001 / XMLSchema - instance & quot ;                     xsi : schemaLocation   =       & quot ; http : / / www . springframework . org / schema / beans             http : / / www . springframework . org / schema / beans / spring - beans - 3.0 . xsd & quot ;     & gt ;                 & lt ; bean       id   =       & quot ; test _ bean & quot ;       class =     & quot ; com . tracholar . web . demo . BasicBean & quot ;     & gt ;                       & lt ; property       name =     & quot ; value & quot ;       value =     & quot ; Hello   Test   Bean & quot ;       / & gt ;               & lt ; / bean & gt ;       & lt ; / beans & gt ;                   id     是 唯一 标识 ,     class     对 应该 对象 的 类 ,     property     对应 的 是 属性 。 有 了 这个 配置文件 后 ,   重复使用 这个 对象 就 很 方便 了 。               BeanFactory       factory       =       new       ClassPathXmlApplicationContext     (     & quot ; Beans . xml & quot ;     ) ;       BasicBean       bean       =       (     BasicBean     )       factory     .     getBean     (     & quot ; test _ bean & quot ;     ) ;       System     .     out     .     println     (     bean     .     getValue     ( ) ) ;       / /   Hello   Test   Bean                 当然 , Bean 也 可以 配置 成 多例 模式 , 这样 每次 得到 的 都 是 一个 新 的 对象 。       利用     @ Autowired     注解 , 可以 避免 属性 的     get     和     set     方法               public       class       BasicBean       {               @ Autowired               private       String       value       =       & quot ; & quot ;     ;       }                 SpringBoot   简单   Web   APP       HTTP 的 Get ,   POST   等 请求 最终 是 由   Controller   处理 ,   首先 实现 一个 这样 的 处理 类               @ RestController       public       class       GreetingController       {                 private       static       final       String       template       =       & quot ; Hello ,   % s ! & quot ;     ;                 @ RequestMapping     (     & quot ; / greeting & quot ;     )               public       BasicBean       greeting     (     @ RequestParam     (     value     =     & quot ; name & quot ;     ,       defaultValue     =     & quot ; World & quot ;     )       String       name     )       {                       BasicBean       bean       =       new       BasicBean     ( ) ;                       bean     .     setValue     (     String     .     format     (     template     ,       name     ) ) ;                         return       bean     ;               }         }                 Annotation     @ RestController     表明 这个 类 用于 处理 HTTP 请求 ,     @ RequestMapping ( \" / greeting \" )   表明 处理 的 路径 为     / greeting   ,       RequestParam   可以 帮 你 配置 一些 参数 的 属性 , 比如 默认值 。 SpringBoot   会 自动 将 返回 的 对象 转换成 JSON , 所以 请求 返回 的 结果 将 是 一个 JSON 。         @ RequestMapping     会 将 GET 请求 和 POST 请求 都 发到 这个 方法 , 如果 要 分别 处理 , 可以 用     @ GetMapping     和     @ PostMapping     分别 指定 。       @ RequestMapping     也 可以 加 到 整个 类 ,   比如 对 整个 类 加上 注解     @ RequestMapping ( \" / home \" )   ,   那么 这个 类 的 方法 都 是 在 子 路径 下 ,     greeting   对应 的 路径 就是     / home / greeting   。       有 了   Controller   之后 , 我们 启动 一个 简单 的 Server , 就 可以 测试 了 。               @ SpringBootApplication       public       class       TestWebDemo       {               public       static       void       main     (     String     [ ]       args     ) {                       SpringApplication     .     run     (     TestWebDemo     .     class     ,       args     ) ;               }                   @ Bean               public       EmbeddedServletContainerFactory       servletContainer     ( )       {                       TomcatEmbeddedServletContainerFactory       factory       =                                       new       TomcatEmbeddedServletContainerFactory     ( ) ;                       return       factory     ;               }               / *   上述 方法 等同于 在 配置文件 中 的 设置               & lt ; bean   id = & quot ; xxx & quot ;   class   =   & quot ; org . springframework . boot . context . embedded . tomcat . TomcatEmbeddedServletContainerFactory & quot ;   / & gt ;                 * /       }                   @ SpringBootApplication     Annotation   相当于 以下 3 个 Annotation 共同 的 效果 :             @ Configuration     将 这个 类 作为 Bean 的 定义 类 ,   Bean 的 定义 不但 可以 通过 配置文件 , 也 可以 通过 配置 类中用   @ Bean   标记 的 方法 ,   返回 的 结果 就是 一个 Bean 。         @ EnableAutoConfiguration         作用 从 classpath 中 搜索 所有 META - INF / spring . factories 配置文件 然后 ， 将 其中             org . springframework . boot . autoconfigure . EnableAutoConfiguration   key 对应 的 配置 项 加载 到 spring 容器 。         @ EnableWebMvc           @ ComponentScan     在 同一个 包中 扫描 其他 组件 ,   例如 Controller 。           Spring 会 自动 查找 所有 的 Controller ,   所以 不用 做 额外 的 配置 。 启动 运行 后 ,   从 浏览器 打开     localhost : 8080   , 可以 看到 返回 的 JSON               {               & quot ; value & quot ;     :       & quot ; Hello ,   World ! & quot ;       }                 定时 执行 任务       参考 :     https : / / spring . io / guides / gs / scheduling - tasks /         首先 创建 一个 定时 执行 模块 ,   用     @ Scheduled     注解 标注 定 是 执行 的 方法 ,     fixedRate     参数 指定 间隔 毫秒 数 。               @ Component       public       class       ScheduledTasks       {               private       static       final       Logger       log       =       LoggerFactory     .     getLogger     (     ScheduledTasks     .     class     ) ;                 private       static       final       SimpleDateFormat       dateFormat       =       new       SimpleDateFormat     (     & quot ; HH : mm : ss & quot ;     ) ;                 @ Scheduled     (     fixedRate       =       1000     )               public       void       reportTime     ( ) {                       log     .     info     (     & quot ; Now :   { } & quot ;     ,       dateFormat     .     format     (     new       Date     ( ) ) ) ;               }       }                 但是 spring 默认 不 起用 Scheduled 模块 , 需要 在     TestWebDemo     上 再 加个   @ EnableScheduling   注解 。 执行 后 结果 如下               2018   - 12 - 27     15   : 04 : 04.568     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 04     2018   - 12 - 27     15   : 04 : 05.567     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 05     2018   - 12 - 27     15   : 04 : 06.567     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 06     2018   - 12 - 27     15   : 04 : 07.567     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 07     2018   - 12 - 27     15   : 04 : 08.568     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 08     2018   - 12 - 27     15   : 04 : 09.568     INFO     76968     - - -     [   pool - 1 - thread - 1   ]     com . tracholar . web . demo . ScheduledTasks         :   Now :     15   : 04 : 09               文件 上传       参考 :     https : / / spring . io / guides / gs / uploading - files /                 @ Controller       public       class       FileUploader       {               Logger       logger       =       LoggerFactory     .     getLogger     (     this     .     getClass     ( ) ) ;                 @ PostMapping     (     & quot ; / upload & quot ;     )               public       String       uploadHandler     (     @ RequestParam     (     & quot ; file & quot ;     )       MultipartFile       file     ,                                                                       RedirectAttributes       redirectAttributes     )       throws       IOException     {                       logger     .     info     (     file     .     getOriginalFilename     ( ) ) ;                       File       f       =       new       File     (     & quot ; / tmp / www _ upload / & quot ;       +       file     .     getOriginalFilename     ( ) ) ;                       logger     .     info     (     & quot ; Move   to   & quot ;       +       f     .     getAbsolutePath     ( ) ) ;                       file     .     transferTo     (     f     ) ;                       redirectAttributes     .     addFlashAttribute     (     & quot ; message & quot ;     ,       & quot ; You   successfully   upload   & quot ;       +       file     .     getOriginalFilename     ( ) ) ;                       return       & quot ; redirect : / & quot ;     ;               }       }                   MultipartFile     对象 用于 表示 文件 ,     @ Controller     注解 和     @ RestController     都 表示 这个 类 是 一个 控制器 ,       但是   @ Controller   返回 的 是 一个     ModelAndView   , 如果 方法 返回 的 是 string , 表示 是 页面 名字 ,   参考 下面 的     页面 模板   。       @ RestController     等于 加 了     @ ResponseBody     注解 的     @ Controller   ,   也 就是 方法 返回 的 是 内容 本身 , 不会 加载 某个 页面 。       页面 模板       Spring   Boot   的 页面 模板 和 资源 文件 放在     resources     下面 ,             resources / templates     存放 前端 html 页面         resources / static     存放 静态 文件 ,     这个 目录 对应 于 HTTP 请求 时 的 根目录   。 例如 :     resources / static / style . css     可以 通过 URL     / style . css   直接 引用 !           如果 没有 这 两个 文件夹 , 可以 自己 创建 。     在 Controller 中 返回 一个 页面 非常简单 ,   返回 页面 的 文件名 ( 不 包含 后缀 ) 即可 ,   也 可以 返回     ModelAndView     对象 。     但是 , 要 使得 引擎 生效 , 需要 增加   spring - boot - starter - thymeleaf   的 依赖 。   thymeleaf     引擎 语法 参考     https : / / www . thymeleaf . org / doc / tutorials / 2.1 / thymeleafspring . html                 & lt ; dependency & gt ;               & lt ; groupId & gt ;   org . springframework . boot   & lt ; / groupId & gt ;               & lt ; artifactId & gt ;   spring - boot - starter - thymeleaf   & lt ; / artifactId & gt ;               & lt ; version & gt ;   1.5 . 15 . RELEASE   & lt ; / version & gt ;       & lt ; / dependency & gt ;                         @ Controller       public       class       FileUploader       {               Logger       logger       =       LoggerFactory     .     getLogger     (     this     .     getClass     ( ) ) ;                 @ GetMapping     (     & quot ; / upload & quot ;     )               public       ModelAndView       showUploadPage     ( )       {                       return       new       ModelAndView     (     & quot ; upload _ file & quot ;     ) ;               }               @ PostMapping     (     & quot ; / upload & quot ;     )               public       String       uploadHandler     (     @ RequestParam     (     & quot ; file & quot ;     )       MultipartFile       file     ,                                                                       RedirectAttributes       redirectAttributes     )       throws       IOException     {                       logger     .     info     (     file     .     getOriginalFilename     ( ) ) ;                       File       f       =       new       File     (     & quot ; / tmp / www _ upload / & quot ;       +       file     .     getOriginalFilename     ( ) ) ;                       logger     .     info     (     & quot ; Move   to   & quot ;       +       f     .     getAbsolutePath     ( ) ) ;                       file     .     transferTo     (     f     ) ;                       redirectAttributes     .     addFlashAttribute     (     & quot ; message & quot ;     ,       & quot ; You   successfully   upload   & quot ;       +       file     .     getOriginalFilename     ( ) ) ;                       return       & quot ; redirect : / & quot ;     ;               }       }                 Bean   自动 配置       配置文件 :     resources / application . properties     配置文件 的 语法 参考     http : / / blog . didispace . com / springbootproperties /               my . conf . files = www . baidu . com , www . google . com                       @ Component       @ ConfigurationProperties     (     prefix       =       & quot ; my . conf & quot ;     )       public       class       MyConfig       {               public       List     & lt ;     String     & gt ;       getFiles     ( )       {                       return       files     ;               }                 public       void       setFiles     (     List     & lt ;     String     & gt ;       files     )       {                       this     .     files       =       files     ;               }                 private       List     & lt ;     String     & gt ;       files       =       new       ArrayList     & lt ; & gt ; ( ) ;         }         @ RestController       @ RequestMapping     (     & quot ; / conf & quot ;     )       public       class       MyConfigController       {               @ Autowired               private       MyConfig       myConfig     ;                 @ RequestMapping     (     & quot ; / & quot ;     )               public       Object       getConf     ( ) {                       return       myConfig     .     getFiles     ( ) ;               }       }                 注解   ConfigurationProperties   表明 对 这个 类 属性 进行 配置 , 配置文件 中 key 的 前缀 是   my . conf   ,       在 使用 这个 类时 , 加上     @ Autowired     注解 ,   spring 就 会 从 注册 组件 中 寻找 对应 的 类 , 并 创建对象 , 这个 过程 叫 自动 装配 ,   对象 统一 由 spring 来 管理 。     参考   spring   依赖 注入     https : / / www . jianshu . com / p / 3942cce05f71   。           首先 想 说 说 IoC （ Inversion   of   Control ， 控制 倒转 ） 。 这是 spring 的 核心 ， 贯穿 始终 。     所谓 IoC ， 对于 spring 框架 来说 ， 就是 由 spring 来 负责 控制 对象 的 生命周期 和 对象 间 的 关系 。     这是 什么 意思 呢 ， 举个 简单 的 例子 ， 我们 是 如何 找 女朋友 的 ？ 常见 的 情况 是 ， 我们 到处 去 看 哪里 有长 得 漂亮 身材 又 好 的 mm ，     然后 打听 她们 的 兴趣爱好 、 qq 号 、 电话 号 、 ip 号 、 iq 号 … … … ， 想 办法 认识 她们 ， 投其所好 送 其 所要 ， 然后 嘿嘿 … … 这个 过程 是 复杂 深奥 的 ，     我们 必须 自己 设计 和 面对 每个 环节 。 传统 的 程序开发 也 是 如此 ， 在 一个 对象 中 ， 如果 要 使用 另外 的 对象 ， 就 必须 得到 它 （ 自己 new 一个 ， 或者 从 JNDI 中 查询 一个 ） ，     使用 完 之后 还要 将 对象 销毁 （ 比如 Connection 等 ） ， 对象 始终 会 和 其他 的 接口 或类 藕合 起来       那么 IoC 是 如何 做 的 呢 ？ 有点像 通过 婚介 找 女朋友 ， 在 我 和 女朋友 之间 引入 了 一个 第三者 ： 婚姻 介绍所 。     婚介 管理 了 很多 男男女女 的 资料 ， 我 可以 向 婚介 提出 一个 列表 ， 告诉 它 我 想 找个 什么样 的 女朋友 ， 比如 长得 像 李嘉欣 ， 身材 像 林熙雷 ， 唱歌 像 周杰伦 ，     速度 像 卡洛斯 ， 技术 像 齐达内 之类 的 ， 然后 婚介 就 会 按照 我们 的 要求 ， 提供 一个 mm ， 我们 只 需要 去 和 她 谈恋爱 、 结婚 就行了 。     简单明了 ， 如果 婚介 给 我们 的 人选 不 符合要求 ， 我们 就 会 抛出 异常 。 整个 过程 不再 由 我 自己 控制 ， 而是 有 婚介 这样 一个 类似 容器 的 机构 来 控制 。     Spring 所 倡导 的 开发方式 就是 如此 ， 所有 的 类 都 会 在 spring 容器 中 登记 ， 告诉 spring 你 是 个 什么 东西 ， 你 需要 什么 东西 ，     然后 spring 会 在 系统 运行 到 适当 的 时候 ， 把 你 要 的 东西 主动 给 你 ， 同时 也 把 你 交给 其他 需要 你 的 东西 。 所有 的 类 的 创建 、 销毁 都 由   spring 来 控制 ，     也就是说 控制 对象 生存 周期 的 不再 是 引用 它 的 对象 ， 而是 spring 。 对于 某个 具体 的 对象 而言 ， 以前 是 它 控制 其他 对象 ， 现在 是 所有 对象 都 被 spring 控制 ，     所以 这 叫 控制 反转 。 如果 你 还 不 明白 的话 ， 我 决定 放弃 。           表单 认证       创建 可 认证 表单 数据 对象 ,   用     javax . validation . constraints . *     中 的 Annotation 标注 约束条件 。               public       class       PersonData       {               @ NotNull               @ Size     (     min       =       5     ,       max       =       30     )               private       String       name     ;                 public       Integer       getAge     ( )       {                       return       age     ;               }                 public       void       setAge     (     Integer       age     )       {                       this     .     age       =       age     ;               }                 public       String       getName     ( )       {                       return       name     ;               }                 public       void       setName     (     String       name     )       {                       this     .     name       =       name     ;               }                 @ NotNull               @ Min     (     10     )               @ Max     (     100     )               private       Integer       age     ;       }                 利用     @ Valid     注解 对 请求 参数 标注 ,   用   BindingResult   获取 约束条件 判断 结果 。               @ Controller       @ RequestMapping     (     & quot ; / person & quot ;     )       public       class       PersonController       {               @ GetMapping     (     & quot ; / & quot ;     )               public       String       form     ( ) {                       return       & quot ; person & quot ;     ;               }               @ PostMapping     (     & quot ; / & quot ;     )               @ ResponseBody               public       Object       checkPersonInfo     (     @ Valid       PersonData       person     ,       BindingResult       bindingResult     ) {                       if       (     bindingResult     .     hasErrors     ( ) )       {                               return       bindingResult     .     getAllErrors     ( ) ;                       }                         return       person     ;               }       }                 问题 :   为什么 Controller 的 方法 可以 自动 指定 这么 多 不同 类型 的 参数 ?  ", "tags": "tools", "url": "/wiki/tools/spring.html"},
      
      
      {"title": "ssh使用技巧汇总", "text": "    Table   of   Contents           关于           常用命令                 关于       记录 ssh 使用 技巧       常用命令           生成 公钥                 ssh - keygen      ", "tags": "tools", "url": "/wiki/tools/ssh.html"},
      
      
      {"title": "Thrift 快速入门", "text": "    Table   of   Contents           关于           安装           Helloword           python 实现                         关于       Thrift 是 一种 接口 描述语言 和 二进制 通讯 协议 ， 它 被 用来 定义 和 创建 跨 语言 的 服务 。     它 被 当作 一个 远程 过程 调用 框架 来 使用 ， 是 由 Facebook 为 “ 大规模 跨 语言 服务 开发 ” 而 开发 的 。           远 过程 调用 ( RPC ) ,   可以 通过 像 调用 本地 函数 一样 , 调用 远程 服务 。           安装       MAC     brew   install   thrift         thrift 采用 thrift 文件   +   编译器 的 方式 ,   最后 生成 目标语言 的 代码 , 这些 代码 直接 嵌入 到 你 的 项目 中 就 可以 使用 了 。       Helloword       参考 :     https : / / thrift . apache . org /       https : / / www . ibm . com / developerworks / cn / java / j - lo - apachethrift / index . html         helloword . thrift               service           HelloWorldServer           {                       string           call     (     1     :           string           name     )           }                     编译     thrift   - - gen   & lt ; language & gt ;   & lt ; Thrift   filename & gt ;         以 JAVA 为例 ,   执行 编译 命令     thrift   - - gen   java   helloword . thrift     可以 得到 一个     HelloWorldServer . java     文件 ,   将 它 放到 适当 的 目录 即可 。     在 JAVA 工程 的 pom . xml 文件 中 , 增加   libthrift   依赖 。               & lt ; dependency & gt ;           & lt ; groupId & gt ;   org . apache . thrift   & lt ; / groupId & gt ;           & lt ; artifactId & gt ;   libthrift   & lt ; / artifactId & gt ;           & lt ; version & gt ;   0.11 . 0   & lt ; / version & gt ;       & lt ; / dependency & gt ;                 服务端 实现     HelloWorldServerImpl . java     实现 接口               public       class       HelloWorldServerImpl       implements       HelloWorldServer     .     Iface       {               @ Override               public       String       call     (     String       name     )       throws       TException     {                       return       & quot ; Hello ,   & quot ;       +       name     ;               }       }                 启动 一个 简单 的 服务器               public       class       HelloWorldServerThread       {               public       static       final       int       port       =       9120     ;               public       static       void       main     (     String     [ ]       args     )       throws       Exception     {                       ServerSocket       socket       =       new       ServerSocket     (     port     ) ;                       TServerSocket       tsocket       =       new       TServerSocket     (     socket     ) ;                       HelloWorldServer     .     Processor       processor       =       new       HelloWorldServer     .     Processor     (     new       HelloWorldServerImpl     ( ) ) ;                       TServer       server       =       new       TSimpleServer     (     new       TServer     .     Args     (     tsocket     ) .     processor     (     processor     ) ) ;                       System     .     out     .     println     (     & quot ; Starting   the   simple   server ...& quot ;     ) ;                       server     .     serve     ( ) ;               }       }                 启动 一个 简单 的 客户端               public       class       HelloWorldClient       {               public       static       void       main     (     String     [ ]       args     )       throws       Exception     {                       TTransport       transport       =       new       TSocket     (     & quot ; localhost & quot ;     ,         HelloWorldServerThread     .     port     ) ;                       transport     .     open     ( ) ;                         TProtocol       protocol       =       new       TBinaryProtocol     (     transport     ) ;                       HelloWorldServer     .     Client       client       =       new       HelloWorldServer     .     Client     (     protocol     ) ;                       System     .     out     .     println     (     client     .     call     (     & quot ; world ! & quot ;     ) ) ;               }       }                 输出 结果 :     Hello ,   world !         python 实现       需要 安装     pip   install   thrift                 from       helloword       import       HelloWorldServer         from       thrift       import       Thrift       from       thrift . transport       import       TSocket       from       thrift . transport       import       TTransport       from       thrift . protocol       import       TBinaryProtocol       from       thrift . server       import       TServer         import       logging         logging     .     basicConfig     (     level     =     logging     .     INFO     ,                                               format     =     &# 39 ;     % ( asctime ) s           % ( filename ) s     [ line :     % ( lineno ) d     ]       % ( levelname ) s           % ( message ) s     &# 39 ;     ,                                               datefmt     =     &# 39 ; % Y - % m -     % d       % H : % M : % S &# 39 ;     ,                                               )         class       HelloWorldHandler     ( ) :               def       __ init __     (     self     ) :                       self     .     log       =       { }                 def       call     (     self     ,       name     ) :                       logging     .     info     (     & quot ; name   =   & quot ;       +       str     (     name     ) )                       return       & quot ; Hello ,   & quot ;       +       str     (     name     )         if       __ name __       = =       &# 39 ; __ main __&# 39 ;     :               handler       =       HelloWorldHandler     ( )               processor       =       HelloWorldServer     .     Processor     (     handler     )               transport       =       TSocket     .     TServerSocket     (     host     =     &# 39 ; 0.0 . 0.0 &# 39 ;     ,       port     =     9120     )               tfactory       =       TTransport     .     TBufferedTransportFactory     ( )               pfactory       =       TBinaryProtocol     .     TBinaryProtocolFactory     ( )                 ##   多线程 server               server       =       TServer     .     TThreadedServer     (     processor     ,       transport     ,       tfactory     ,       pfactory     )                 logging     .     info     (     &# 39 ; server   info   :   &# 39 ;       +       str     (     server     ) )               server     .     serve     ( )        ", "tags": "tools", "url": "/wiki/tools/thrift.html"},
      
      
      {"title": "YARN学习", "text": "    Table   of   Contents           关于           开发 指南           参考                 关于       YARN   是 HADOOP2 代 ， 相比 第一代 产品 ， 可以 开发 除了   MAPREDUCE   外 更 复杂 的 分布式 程序 ，     XGBoost   on   yarn   就是 一个 例子 ！       开发 指南               YARN   用   ResourceManager   来 分配资源 ， NodeManager   来 管理 计算 节点 ！     ApplicationMaster   用于 跟踪 任务 ， 管理 每 一个 在 YARN 上 运行 的 程序 实例 ，     通过 调用   ResourceManager   和   NodeManager   进行 管理 。       要 使用 一个   YARN   集群 ， 首先 需要 来自 包含 一个 应用程序 的 客户 的 请求 。 ResourceManager   协商 一个 容器 的 必要 资源 ， 启动 一个   ApplicationMaster   来 表示 已 提交 的 应用程序 。 通过 使用 一个 资源 请求 协议 ， ApplicationMaster   协商 每个 节点 上供 应用程序 使用 的 资源 容器 。 执行 应用程序 时 ， ApplicationMaster   监视 容器 直到 完成 。 当 应用程序 完成 时 ， ApplicationMaster   从   ResourceManager   注销 其 容器 ， 执行 周期 就 完成 了 。       具体 而言 ， 就是 要 创建 一个   ApplicationMaster 类 ， 例如   XGBoost 的 例子   。       创建 一个     application   submission   client   ，     提交 应用 给   YARN   的   ResourceManager ( RM ) 。     这个 步骤 通过 创建 一个   YarnClient   实例 实现 ！       Following   are   the   important   interfaces :           Client & lt ; - - & gt ; ResourceManager           By   using   YarnClient   objects .           ApplicationMaster & lt ; - - & gt ; ResourceManager           By   using   AMRMClientAsync   objects ,   handling   events   asynchronously   by   AMRMClientAsync . CallbackHandler           ApplicationMaster & lt ; - - & gt ; NodeManager           Launch   containers .   Communicate   with   NodeManagers   by   using   NMClientAsync   objects ,   handling   container   events   by   NMClientAsync . CallbackHandler       一个 简单 的 例子 ：               / /   Create   yarnClient       YarnClient       yarnClient       =       YarnClient     .     createYarnClient     ( ) ;       yarnClient     .     init     (     conf     ) ;       yarnClient     .     start     ( ) ;         YarnClientApplication       app       =       yarnClient     .     createApplication     ( ) ;       GetNewApplicationResponse       appResponse       =       app     .     getNewApplicationResponse     ( ) ;         / * *       -   * * ApplicationSubmissionContext * * :   defines   all   the   information   needed   by   the   RM   to   launch   the   AM .   A   client   needs   to   set   the   following   into   the   context :               -   Application   info :   id ,   name               -   Queue ,   priority   info :   Queue   to   which   the   application   will   be   submitted ,   the   priority   to   be   assigned   for   the   application .               -   User :   The   user   submitting   the   application               -   ContainerLaunchContext :   The   information   defining   the   container   in   which   the   AM   will   be   launched   and   run .   The   ContainerLaunchContext ,   as   mentioned   previously ,   defines   all   the   required   information   needed   to   run   the   application   such   as   the   local   * Resources   ( binaries ,   jars ,   files   etc . ) ,   Environment   settings   ( CLASSPATH   etc . ) ,   the   Command   to   be   executed   and   security   T * okens   ( RECT ) .       * /       ApplicationSubmissionContext       appContext       =       app     .     getApplicationSubmissionContext     ( ) ;       ApplicationId       appId       =       appContext     .     getApplicationId     ( ) ;         appContext     .     setKeepContainersAcrossApplicationAttempts     (     keepContainers     ) ;       appContext     .     setApplicationName     (     appName     ) ;                 ApplicationMaster   通过   AMRMClientAsync . CallbackHandler   处理 资源 相关 事件 ， 通常 在   onContainersAllocated   事件 启动 任务 。       本地 文件 可以 通过   Client   上 传到   HDFS ， 然后   ContainerLaunchContext . setLocalResources   创建 为 本地 资源 ， 进行 调用 ！       参考           IBM 博客     https : / / www . ibm . com / developerworks / cn / data / library / bd - hadoopyarn / index . html           Hadoop :   Writing   YARN   Applications        ", "tags": "tools", "url": "/wiki/tools/yarn.html"},
      
      
      {"title": "开发JNI native程序", "text": "    Table   of   Contents           关于           实战           加载 动态 库 的 两个 方法           JNI   数据类型                 关于       JAVA 可以 通过 JNI 即   Java   Native   Interface   来 调用 原生 库 , 下面 以 一个 例子 展示 使用 方法 。       实战           创建 Java 类   Hello . java                     package       com . tracholar . jni . demo     ;         public       class       Hello       {               / /   声明 native 方法 ,   遵循 Java 类型 和 声明 方法               public       native       static       void       say _ hi     ( ) ;                 static       {                       / /   加载 动态 库 ,   动态 库 后面 会 生成 ,   库 名字 和 文件名 之间 对应 关系 是                       / /   库名 :       hello                       / /   文件名 ( windows ) :   libhello . dll                       / /                 mac           :   libhello . dylib                       / /                 linux       :   libhello . so                         System     .     loadLibrary     (     & quot ; hello & quot ;     ) ;               }                 public       static       void       main     (     String     [ ]       args     ) {                       say _ hi     ( ) ;               }       }                     生成 头文件                   javac   - d   .   src / main / java / com / tracholar / jni / demo / Hello . java   javah   com . tracholar . jni . demo . Hello               将 生成 一个 头文件     com _ tracholar _ jni _ demo _ Hello . h                 / *   DO   NOT   EDIT   THIS   FILE   -   it   is   machine   generated   * /       # include       & lt ; jni . h & gt ;           / *   Header   for   class   com _ tracholar _ jni _ demo _ Hello   * /         # ifndef   _ Included _ com _ tracholar _ jni _ demo _ Hello       # define   _ Included _ com _ tracholar _ jni _ demo _ Hello       # ifdef   __ cplusplus       extern       & quot ; C & quot ;       {       # endif       / *         *   Class :           com _ tracholar _ jni _ demo _ Hello         *   Method :         say _ hi         *   Signature :   ( ) V         * /       JNIEXPORT       void       JNICALL       Java _ com _ tracholar _ jni _ demo _ Hello _ say _ 1hi           (     JNIEnv       *     ,       jclass     ) ;         # ifdef   __ cplusplus       }       # endif       # endif                     编写 C / C++ 代码 , 实现 这个 函数                   # include     & lt ; stdio . h & gt ;           # include       & quot ; com _ tracholar _ jni _ demo _ Hello . h & quot ;               JNIEXPORT       void       JNICALL       Java _ com _ sankuai _ itc _ dmspa _ tool _ demo _ Hello _ say _ 1hi           (     JNIEnv       *       env     ,       jclass       c     ) {               / / jcharArray   arr   =   ( * env ) - & gt ; NewCharArray ( env ,   14 ) ;               / / ( * env ) - & gt ; SetCharArrayRegion ( env ,   & amp ; arr ,   0 ,   14 ,   & quot ; hello   world ! \ \ n & quot ; ) ;               / / return   arr ;               printf     (     & quot ; hello   world !     \ \ n     & quot ;     ) ;           }                     编译 C 代码 ,   编译 时 需要 包含   jni . h   所在 的 目录 , 在   MAC 系统 下 ,   是     / Applications / Xcode . app / Contents / Developer / Platforms / MacOSX . platform / Developer / SDKs / MacOSX10 . 14 . sdk / System / Library / Frameworks / JavaVM . framework / Versions / A / Headers /   ,           在 其他 系统 一般 在 JDK 所在 的 某个 子目录 , 可以 自行 搜索 。                   #   只 编译 生成 目标 代码 ,   便于 后续 生成 动态链接库     gcc     - c   - fPIC   hello . c     - I / Applications / Xcode . app / Contents / Developer / Platforms / MacOSX . platform / Developer / SDKs / MacOSX10 . 14 . sdk / System / Library / Frameworks / JavaVM . framework / Versions / A / Headers /   - o   hello . o       #   生成 动态 库       #   ( linux )         #   gcc   hello . o   - shared   - o   libhello . so       #   ( MAC )     gcc   hello . o   - dynamiclib   - o   libhello . dylib                   执行 Java 类 , 指定     java . library . path     参数 为 动态链接库 所在 目录             java     - Djava . library . path = .     com . tracholar . jni . demo . Hello             hello   world !           加载 动态 库 的 两个 方法           第一个 方法 是 用     System . loadLibrary ( String   libname )     libname 会 通过 一个 native 方法   public   static   native   String   mapLibraryName ( String   libname ) ;             转换成 文件名 , 在 MAC 系统 下   hello   会 转换成     libhello . dylib         第二个 方法 是 用     System . load ( String   filename )     加载 , 注意 这里 的 filename 必须 是 绝对路径 !   MAC 下 也 可以 直接 加载 so 文件 。                   public       class       Hello       {               public       native       static       void       say _ hi     ( ) ;                 public       static       void       main     (     String     [ ]       args     ) {                       try       {                               String       filename       =       new       File     (     & quot ; .& quot ;     ) .     getCanonicalPath     ( )       +       File     .     separator       +       & quot ; hello . so & quot ;     ;                               System     .     load     (     filename     ) ;                               say _ hi     ( ) ;                       }     catch       (     Exception       e     ) {                               e     .     printStackTrace     ( ) ;                       }               }       }                 JNI   数据类型  ", "tags": "tools", "url": "/wiki/tools/jni.html"},
      
      
      {"title": "科学上网", "text": "    Table   of   Contents               proxifier   注册码             L6Z8A - XY2J4 - BTZ3P - ZZ7DF - A2Q9C （ Portable   Edition ）   5EZ8G - C3WL5 - B56YG - SCXM9 - 6QZAP （ Standard   Edition ）   P427L - 9Y552 - 5433E - 8DSR3 - 58Z68 （ MAC ）      ", "tags": "tools", "url": "/wiki/tools/gfw.html"},
      
      
      
      
      
        
      
        
        
      
      {"title": "第00讲: 关于《机器学习教程》", "text": "    Table   of   Contents           关于 教程 的 使用 和 面向对象           关于 作业           支持 与 赞助                 机器 学习 现在 大火 ， 得益于 天时地利人和 ， 我 也 算是 半路出家 的 人 ， 周围 也 有 一些 朋友 想 学习 机器 学习 。 所以 有感于 此 ， 打算 写 一个 教程 ， 将 自己 学习 的 路线 和 知识 分享 给 大家 ， 一方面 利 人 ， 另一方面 也 是 督促 自己 学习 和 深入 思考 。 费恩曼 貌似 说 过 这样 一句 话 ， 只有 当 你 用 自己 的 语言 将 一个 知识 描述 出来 ， 才能 说 你 真正 理解 了 它 。 费恩曼 思维 活跃 ， 写得 物理 书 也 是 我 最 喜欢 的 物理 教材 了 。 当时 看到 费恩曼 先生 的 《 费恩曼 物理学 讲义 》 才 知道 教材 还 能 写 得 这么 生动 ， 而且 看 他 写 得 物理 教材 ， 更 侧重 物理 图像 的 理解 ， 而 不是 像 国内 很多 教材 那样 ， 强调 各种 数学 推导 。       关于 教程 的 使用 和 面向对象       国家 已经 把 人工智能 提升 到 国家 战略 了 ， 而且 从 目前 的 趋势 来看 ， 未来 很多 工作 大有 可能 会 被 机器 取代 ， 因此 使用 机器 学习 解决 实际 问题 的 人 需求量 将会 越来越 大 。 甚至 可以 说 ， 未来 懂 机器 学习 就 像 现在 懂 编程 一样 ， 会 成为 技术人员 的 标配 。 因此 ， 本 教程 的 主要 面向对象 将会 降低 到 懂 一些 编程 的 所有 技术人员 ， 如果 你 已 是 机器 学习 的 老 司机 ， 在 学习 新 的 算法 的 时候 ， 使用 本 教程 也 会会 是 一个 不错 的 选择 。       关于 作业       从 我 个人 学习 的 经历 和 对 周围 人 自学 机器 学习 的 观察 来看 ， 没有 一定 的 实践 ， 只是 看 视频 、 看书 、 看 博客 ， 很难 有 较大 的 收获 。 我 学习 过 Andrew   Ng 的 公开课 ， 也 看过 斯坦福 的 CS224d 以及 CS231n 等 有名 的 课程 ， 深感 国外 课程设计 地 多么 精细 ， 尤其 是 对 它们 的 作业 设计 ， 十分 佩服 。 相比 国内 的 教程 ， 很少 有 设计 作业 的 内容 。 但是 ， 作业 过多 也 是 一件 麻烦事 ， 因为 不 知道 重点 ， 想 通过 作业 来 加深 理解 ， 看到 十几道 题 和 从零开始 的 代码 ， 也 是 很 头疼 ， 可能 并 不是 每个 人 都 能花 那么 多 精力 去 折腾 。 所以 ， 我 尽量 吧 作业控制 在 最少 的 范围 内 。 如果 你 真的 想 学习 ， 每一 讲 的 作业 是 非常 有 必要 要 做 得 ， 每一 讲 的 作业 不 多 ， 通常 是 1 - 2 个 思考题 或者 一个 编程 实践 题 ， 编程 的 启动 代码 我 都 会 提供 ， 你 只 需要 完成 关键 代码 即可 。       评论 区 可以 用来 向 我 提问 ， 或者 和 学习 的 小伙伴 们 互相 交流 ， 但 不 建议 直接 贴 作业 的 答案 。       支持 与 赞助       最后 ， 如果 您 觉得 这个 教程 有用 ， 不妨 推荐 给 同学 或 朋友 。 如果 这个 教程 对 您 帮助 很大 ， 不妨 赞助 我 一杯 咖啡 ， 支持 我 继续 提供 更好 的 教程 。  ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro.html"},
      
      
      {"title": "第01.0讲：一个例子入门机器学习", "text": "    Table   of   Contents           关于           机器 学习 的 本质           从 0 开始 机器 学习           任务 与 数据           建模           简单 规则 模型           决策树 模型                   预测           总结                   机器 学习 的 应用 举例           点击率 预估           大 数据 风控           人脸识别           出租车 派单                   复现 代码           数据 加载           鸢尾花 数据分析           决策树 建模                   思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 入门 机器 学习 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 机器 学习 ？       机器 学习 能 做 什么 ？       机器 学习 在 代码 上 具体 如何 实现 ？           学习 本 讲 ， 希望 你           年满 18 岁 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           机器 学习 的 本质       现在 人们 通常 将 机器 学习 和 人工智能 联系 在 一起 ， 实际上 ， 人工智能 涉及 的 领域 更加 宽泛 ， 机器 学习 只是 其中 一种 手段 。 人工智能 的 起源 可以 追溯到 上 世纪 50 年代 ， 1956 年 举办 的 达茂思 会议 (   Dartmouth   Conference   ) ， 在 这次 会议 上 ， 信息论 之父 Shannon 和 IBM 科学家 Nathan   Rochester 等 人 ， 一起 探讨 了 一个 议题 ： 精确 地 描述 学习 过程 和 智能 的 特征 并用 机器 进行 模拟 。 说人话 ！ 就是 用 机器 模拟出 人类 的 智能 ！       人工智能 发展 的 初期 ， 研究者 致力于 将 人类 的 知识 表达 为 一些 逻辑 规则 ， 然后 利用 搜索 进行 逻辑推理 ， 进而 实现 智能 ， 到 后来 演变 到 利用 知识库 构造 专家系统 ， 实现 所谓 的 智能 。 这 期间 ， 比较 有名 的 成就 有 IBM 的 国际象棋 程序 深蓝 打败 国际象棋 冠军 。 这一 阶段 的 人工智能 实现 ， 更 像 人类 的 演绎推理 ， 利用 少量 的 规则 ， 加上 知识库 ， 进行 推演 ， 从而 得出结论 。 但是 ， 规则 的 归纳 需要 人类 专家 干预 ， 限制 了 这种 模式 的 发展 。 2000 年 以后 ， 随着 互联网 和 摩尔定律 的 发展 ， 产生 了 大量 的 数据 和 计算资源 ， 使得 人们 可以 利用 机器 从 数据 中 自动 归纳 出 规则 ， 也 就是 数据 驱动 的 智能 。 这 其中 的 工具 就是 机器 学习 ！       所以 ，   机器 学习 就是 利用 一种 程序 从 数据 中 自动 归纳 出 有 价值 的 知识 的 一种 方法   。       所谓 演绎推理 ( Deductive   Reasoning ) ， 就 是从 一般性 的 前提 出发 ， 通过 推导 即 “ 演绎 ” ， 得出 具体 陈述 或 个别 结论 的 过程 。 演绎推理 的 逻辑 形式 对于 理性 的 重要 意义 在于 ， 它 对 人 的 思维 保持 严密性 、 一贯性 有着 不可 替代 的 校正 作用 。 我们 熟知 的 很多 数学 证明 方法 ， 例如 通过 简单 的 几条 公理 ， 推导 出 整个 欧式 几何 大厦 的 推理 过程 ， 就是 典型 的 演绎推理 。   下面 是 演绎推理 里面 一个 典型 的 三段论 推理 的 例子 ：               知识分子 都 是 应该 受到 尊重 的 ，       人民 教师 都 是 知识分子       所以 ， 人民 教师 都 是 应该 受到 尊重 的 。               演绎推理 的 核心思想 就 是从 一般 到 特殊 ， 将 一些 已经 为 真的 通用性 结论 应用 到 具体 的 问题 当中 ， 得到 具体 的 情况 下 的 结论 。 这种 推理 方式 保证 了 推理 的 严密性 ！ 在 上述 例子 中 ， 前 两条 就是 一般性 结论 ， 知识分子 是 比 人民 教师 更大 的 概念 ， 第三条 的 结论 就是 将 第一条 结论 应用 到 人民 教师 这个 具体 的 个体 上 得到 的 更 具体 的 结论 ！ 有趣 的 是 ， 柯南道 尔 的 著名 小说 中 《 福尔摩斯 》 中 的 大 侦探 福尔摩斯 也 十分 推崇 “ 演绎法 ” ！ 为此 ， 老美 还 专门 拍 了 一部 剧 《 福尔摩斯 ： 基本 演绎法 》 ！   只不过 ， 福尔摩斯 所 声称 的 一般性 结论 和 推理 方式 非常 人能 理解 ！       而 归纳法 是 根据 一类 事物 的 部分 对象 具有 某种 性质 的 有限 观察 ， 推出 这 类 事物 的 所有 对象 都 具有 这种 性质 的 推理 ， 叫做 归纳推理 （ 简称 归纳 ） 。 归纳 是从 特殊 到 一般 的 过程 ， 它 属于 合情 推理 。 通常 归纳法 难以 保证 结论 是 可靠 的 ， 例如 ， 下面 就是 经典 的 归纳法 的 例子 ：               欧洲 看到 过 的 天鹅 都 是 白色 的 。       所以 所有 的 天鹅 都 是 白色 的 ！                 黑天鹅 事件   ： 17 世纪 之前 ， 欧洲 看到 过 的 天鹅 都 是 白色 的 ， 所以 当时 欧洲人 归纳 出 一个 结论 ： 天鹅 都 是 白色 的 ！   直到 后来 ， 欧洲人 发现 了 澳洲 ， 看到 了 当地 的 黑天鹅 ， 人们 才 认识 到 这个 结论 是 错误 的 ！       从 有限 的 经验 归纳 出来 的 结论 当然 不见得 是 可靠 的 ， 但是 数学 上 也 有 完全 归纳法 ， 可以 保证 结论 是 可靠 的 的 例子 ， 我们 以前 学过 的 数学 归纳法 。 从 逻辑推理 的 角度 来看 ， 我们 现在 所用 的 机器 学习 就是 先 观察 到 一些 数据 ， 然后 从 这 有限 的 数据 中 归纳 出 一些 有用 的 规律 的 过程 ！   因此 ，   机器 学习 本质 上 就是 在 做 归纳推理 ， 并且 是 不 完全 的 归纳法   ！ 我们 前面 说 到 ， 这种 不 完全 归纳法 无法 保证 结论 的 正确性 ， 所以 如果 机器 学习 模型 预测 错 了 ， 请 不要 怪 他 ， 因为 它 是 在 做 不 完全 归纳 ， 肯定 会 犯错 的 ！ 但是 这 并 不 意味着 就 没有 用 ， 事实上 我们 人类 很多 经验 都 是 通过 不 完全 归纳法 归纳 出来 的 ， 甚至 可以 说 几乎 所有 实际 的 经验 都 来源于 不 完全 归纳 ， 完全 归纳法 只有 在 数学 上 才 存在 。 只要 归纳 的 结论 大多数 情况 下 是 对 的 ， 那么 他 就是 有用 的 ！       接下来 ， 我们 就 用 一个 实际 的 例子 来 解释 机器 学习 是 如何 从 数据 中学 到 有用 知识 的 。       从 0 开始 机器 学习       接下来 ， 我们 将 利用 一个 简单 的 分类 任务 ， 给 读者 展示 机器 学习 如何 从 数据 中学 到 有用 知识 的 。       任务 与 数据       本 任务 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               建模       我们 的 目标 是 ， 建立 一个 模型 ， 输入 鸢尾花 的 4 个 属性 变量 ， 能够 对 鸢尾花 的 种类 进行 判别 。 这样 一旦 模型 建立 好 了 之后 ， 对 新 看到 的 鸢尾花 ， 只要 测量 了 这 4 个 属性 ， 就 可以 利用 模型 对 它 的 类别 进行 预测 了 。       数学 地 角度 来说 ， 我们 要 确定 一个 函数   $ f :   R ^ 4   \ \ rightarrow   \ \ { 0 , 1 , 2 \ \ } $ ， 输入 是 一个 4 维 向量   $ \ \ vec { x }   =   ( x _ 1 ,   x _ 2 ,   x _ 3 ,   x _ 4 ) $ ， 每 一维 代表 一个 属性 变量 的 值 ， 输出 一个 分类 变量   $ y   \ \ in   \ \ { 0 , 1 , 2 \ \ } $ ， 代表 该 样本 属于 哪个 类别 。 所谓 的 建模 过程 ， 就是 利用 我们 已经 观测 到 的 数据 集 ， 去 确定 这个 函数   $ f $   的 具体 形式 。 这里 每 一个 属性 我们 都 称作 一个 特征 ， 输出 分类 变量 我们 称做 目标 （ 或 建模 目标 ） ， 这里 的 函数   $ f $   就是 我们 通常 所说 的 模型 。       简单 规则 模型       在 建立 复杂 模型 之前 ， 我们 先 来 建立 一种 简单 规则 模型 。 所谓 的 简单 规则 ， 就是 对 一个 属性 ， 通过 规则 判定 ， 确定 该 样本 属于 哪 一个 类 。 比如 ， 我们 可以 进行 数据分析 ， 将 收集 的 数据 绘制 到 以 花瓣 长度 为 横坐标 、 花瓣 宽度 为 纵坐标 的 坐标 图上               可以 看到 ， 这 三种 花 在 花瓣 长度 和 宽度 上 都 有 明显 差异 ， 我们 可以 继续 分析 ， 统计 出 每 一种 花 的 萼片 长度 、 萼片 宽度 、 花瓣 长度 、 花瓣 宽度 的 平均值 。               通过 上述 分析 ， 可以 看到 三种 花 的 花瓣 长度 ( petal   length ， 对应 绿色 的 柱子 ) 平均值 差异 比较 大 ， setosa 的 平均 花瓣 长度 在 1.5 cm 左右 ， versicolor 的 平均 花瓣 长度 在 4.2 cm 左右 ， 而   virginica 的 平均 花瓣 长度 在 5.6 cm 左右 。 因此 ， 一种 简单 规则 模型 可以 归纳 为       $ $     target   =     \ \ begin { cases }     0 ,   \ \ text { petal   length }   \ \ lt   2.8   \ \ \ \     1 ,   \ \ text { petal   length }   \ \ in   [ 2.8 ,   4.9 )   \ \ \ \     2 ,   \ \ text { petal   length }   \ \ ge   4.9     \ \ end { cases }     $ $       这里 分割 点 的 值取 的 是 两种 花 平均 中 的 平均数 。 这 实际上 就是 一个 分段 函数 ， 输入 时 花瓣 长度 ， 输出 就是 花 的 类别 ， 因此 这 就是 一个 模型 。       好 了 ， 到 目前为止 ， 你 已经 学会 了 数据挖掘 过程 中 的 最 简单 情景 了 。 通过 数据分析 ， 归纳 出 规则 ， 然后 将 规则 编码 成 一个 函数 ， 从而 得到 一个 预测 模型 ， 可以 用来 做 预测 。       很快 ， 我们 会 发现 ， 这种 方法 需要 人工 进行 数据分析 ， 总结 出 规则 ， 那么 能不能够 让 程序 自动 地 找到 这些 规则 ， 甚至 发现 更 复杂 的 规则 呢 ？ 答案 是 肯定 的 ， 决策树 就是 这样 一种 模型 ， 自动 地 发现 这些 规则 ， 甚至 复杂 的 组合 规则 。       决策树 模型       下图 是 上面 我们 人工 挖掘 出来 的 规则 模型 对应 的 决策树 模型 ， 可以 看到 它 由 很多 节点 构成 ， 包括 中间 节点 ( 椭圆形 ) 和 叶子 节点 ( 方形 ) 。 中间 节点 是 一个 规则 ， 每个 规则 是 一个 逻辑 判断 ， 例如 最 上面 的 中间 节点 （ 也 叫做 根 节点 ） 的 规则 是 花瓣 长度 小于 2.8 厘米 。 如果 规则 满足 ， 则 进入 左边 的 叶子 节点 ， 否则 就 沿着 右子 树 继续 判断 ， 我们 把 这个 过程 称作 分裂 。 叶子 节点 对应 模型 的 输出 结果 ， 最 左边 的 叶子 节点 输出 的 是 setosa ， 表明 满足 该 叶子 节点 规则 就 预测 为 setosa 。 每 一个 叶子 节点 都 对应 一条 从根 节点 出发 的 路径 ， 路径 上 长度 称作 叶子 节点 的 深度 （ 也 可以 认为 是 规则 的 数目 ） ， 叶子 节点 的 最大 深度 称作 树 的 深度 ， 图中 这棵树 的 深度 为 2 。 一个 样本 过来 ， 沿着 根 节点 开始 ， 不断 地 按照 规则 往 下 移动 ， 直到 到达 叶子 节点 。 叶子 节点 对应 的 输出 结果 就是 模型 对 这个 样本 的 预测 结果 。 这些 规则 可以 用 一棵树 状 的 图 描述 ， 因此 叫做 决策树 模型 。               决策树 节点 上 的 规则 可以 从 数据 中 通过 算法 自动 地 发现 ， 或者说 可以 通过 算法 自动 地 生成 一棵 决策树 ， 这个 过程 称作 模型 的 训练 。 决策树 如何 发现 这些 规则 我们 暂时 不要 去 深究 ， 作为 一个 入门 课程 ， 我们 重点 是 了解 模型 能干 啥 。 这里 ， 我们 利用   scikit - learn   软件包 里面 的 决策树 模型 工具 ， 建立 模型 。 模型 训练 好 了 之后 ， 我们 可以 将 决策树 画 出来 ， 进行 观察 。               算法 自动 学习 出来 的 决策树 是 一颗 二叉树 ， 根 节点 对应 规则 是 petal   length   ( cm )   & lt ; =   2.45 。 满足 这 条 规则 的 样本 ， 就 到 了 左子 树 。 左子 树 是 一个 叶子 节点 ， 图中 的 values = [ 50 ,   0 ,   0 ] 表示 整个 数据 集 的 150 个 样本 中 满足 这 条 规则 的 样本 只有 50 个 ， 且 全部 为 setosa 这个 类别 ， 因此 叶子 节点 预测 输出 的 类别 是   setosa 。 这 和 前面 我们 通过 数据分析 得出 的 规则   petal   length   & lt ;   2.8   则 为 setosa ， 非常 接近 。     < ! - -   ####   线性 模型     通过 前面 的 数据分析 ， 可以 看出 花瓣 长度 （ petal   length ） 越长 ， 越有 可能 是 virginica ； 花瓣 宽度 （ petal   width ） 越长 ， 越有 可能 是 virginica 。 那么 如何 综合 考虑 这 两个 因素 呢 ？ 答案 是 把 它们 加 起来 ， 得到 一个 分数 ， 这个 分数 越大 ， 那么 表明 越有 可能 是 virginica ， 这个 分数 越小 ， 那么 表明 越有 可能 是 setosa ！ 如果 要 区分 versicolor ， 只有 这 两个 因素 是 不够 得 ， 还要 把 其他 因素 也 加 起来 ！ 显然 ， 不同 的 因素 的 重要性 是 不同 的 ， 那么 我们 可以 对 每种 因素 赋予 不同 的 权重 。 这 就是 线性 模型 ， 对 每 一类 ， 将 所有 因素 加权 相加 ， 计算 一个 分数 ， 分数 越高 表明 样本 属于 这 一类 的 可能性 越大 。 因为 这个 得分 的 计算 是 一个 线性 函数 ， 所以 叫做 线性 模型 。 用 数学公式 表述 如下     $ $   score _ i   =   b _ i   +   \ \ vec { w }   _   i ^ T   \ \ vec { x }   $ $     为 每个 类别 计算 一个 得分 后 ， 选择 出 得分 最大 的 类 作为 预测 的 类 。 模型 的 权重 $ \ \ vec { w }   _   i $ 称作 模型 参数 ， 通过 优化 算法 优化 得到 。 优化 模型 参数 的 过程 就 叫做 模型 训练 ！ 模型 训练 的 过程 不是 这次 的 重点 ， 我们 推 到 以后 再 讲 。     线性 模型 （ 确切 地 说 是 线性 多 分类 模型 ） 里面 最 典型 的 是 多 分类 逻辑 回归 ， 它 将 每个 类 的 分数 做 归一化 ， 这个 归一化 后 的 分数 被 解释 为 样本 归属于 该类 别的 概率     $ $   P ( i | \ \ vec { x } )   =   \ \ frac { e ^ { score _ i } } { \ \ sum _ i   e ^ { score _ i } }   $ $     下面 利用 ` scikit - learn ` 多 分类 逻辑 回归 工具   ` LogisticRegression `   进行 建模 。          wzxhzdk : 0            ! [ svg ] ( / wiki / static / images / iris - 4 . svg )       从 模型 参数 来看 ， sepal   length   和   sepal   width   数值 大 的 类别 setosa 的 得分 更高 ， petal   length   数值 较大 的   versicolor 的 得分 更高 ， petal   length   和   petal   width   数值 大 的   virginica   得分 更高 。   - - >       预测       一旦 模型 建立 好 了 之后 ， 我们 就 可以 利用 模型 进行 预测 了 ， 所谓 的 预测 ， 是 指 对于 一个 新 的 样本 ， 比如 我 在 某个 路边 看到 了 一朵 鸢尾花 ， 不 知道 到底 是 哪 一类 ， 就 可以 利用 这个 模型 进行 预测 。 首先 ， 我们 需要 测量 模型 预测 所 需要 的 4 个 数据 （ 特征 ） ， 花萼 的 长度 和 宽度 ， 花瓣 的 长度 和 宽度 ， 然后 输入 的 模型 中 去 。       对 预测 前面 简单 规则 模型 ， 只 需要 花瓣 的 长度 数据 即可 预测 。 对于 决策树 模型 ， 实际上 只 需要 花瓣 的 长度 和 宽度 数据 也 可 预测 ， 如果 我们 将 决策树 深度 变得 更深 ， 那么 就 可能 要 用到 所有 数据 。 首先 ， 决策树 从根 节点 开始 搜索 ， 根 节点 对应 一条 规则   petal   length   ( cm )   & lt ; =   2.45 ， 如果 满足 这 条 规则 ， 就 到 左子 树 ， 预测 输出 为 setosa 。 如果 不 满足 ， 那么 就 到 右子 树 ， 右子 树根 节点 还是 一个 规则   petal   width   ( cm )   & lt ; =   1.75 。 我们 重复 这个 过程 ， 直到 找到 该 样本 满足 规则 的 叶子 节点 ， 叶子 节点 对应 的 输出 值 就是 模型 预测 结果 。       总结       这里 我们 以 鸢尾花 分类 任务 为例 ， 构建 了 一个 决策树 模型 进行 预测 。 总结 起来 ， 所谓 的 建模 过程 ， 就是 利用 已有 的 标注 数据 （ 已知 目标 变量 的 值 的 数据 ） ， 自动 学习 到 一个 函数   $ f : R ^ n   \ \ rightarrow   Y $ ， 根据 观察 到 的 特征向量 ， 计算 得到 目标 变量 的 值 。 这个 任务 就是 一个 3 分类 的 函数 。 虽然 这个 任务 简单 ， 但是 和 更 复杂 的 任务 一样 都 具有 以下 3 个 基本 步骤 ：           收集 （ 标注 ） 数据       建立 模型       预测           不同 的 业务 可能 收集 到 的 数据 不同 ， 收集 到 的 原始数据 需要 加工 成 模型 能用 的 数据 （ 即 特征 ） 。 不同 的 任务 建模 目标 也 不 一样 ， 比如 预测 性别 ， 那么 目标 变量 是 男 和 女 ； 预测 年龄 ， 那么 目标 变量 是 个 0 - 100 之间 的 连续 值 ； 预测 股价 涨跌 ， 那么 目标 变量 就是 涨 和 跌 。 根据 建模 目标 可以 将 问题 分为 两大类 ， 一类 是 和 这个 鸢尾花 分类 问题 类似 ， 目标 变量 取值 有 有限 的 ， 称作 分类 问题 ； 另一类 和 预测 房价 一样 目标 变量 取值 是 无限 的 ， 称作 回归 。       相同 数据 和 目标 的 情况 下 ， 也 可以 选择 不同 的 模型 ， 决策树 是 一个 久经沙场 的 模型 ， 它 的 两个 变体   随机 森林   和   梯度 提升 树   应用 非常 广泛 。 近年来 的 深度 神经网络 ， 可以 利用 非常 原始 的 数据 进行 建模 ， 减少 了 人工 特征 的 工作量 （ 毕竟 去 想 很多 可能 有用 的 变量 是 一件 不少 的 体力活 ） 。 但是 本质 上 ， 他们 都 在 干同 一件 事情 ， 从 数据 中 发现 规律 ， 这个 规律 可以 表达 为 一个 函数 ， 因此 也 叫 函数 拟合 ！       机器 学习 的 应用 举例       具体来讲 ， 机器 学习 可以 用来 做 很多 事情 ， 目前 已经 有 成功 案例 的 就 有 很多 。       点击率 预估       点击率 预估 是 很 早就 开始 应用 机器 学习 的 场景 之一 ， 也 是 机器 学习 在 工业界 应用 最为 广泛 的 场景 。 可以 说 ， 过去 5 年 ， 60 % 的 算法 工程师 都 是 在 做 各种 点击率 预估 。 点击率 预估 在 广告 、 搜索 、 推荐 三个 场景 中 应用 最为 广泛 。       在 互联网 广告 中 ， 如何 决定 给 用户 展示 哪 种 广告 一直 是 互联网 广告商 十分 在意 的 事情 。 例如 ， 当 你 在 百度 中 搜索 “ 手机 ” 时 ， 排 在 搜索 前面 的 几条 都 是 广告 ， 但是 广告位 是 有限 的 ， 现在 假定 只有 一个 广告位 ， 而 手机 广告 总共 有 1000 个 ， 那么 应该 展示 哪个 广告 呢 ？ 一种 简单 的 策略 就是 看 每个 广告 过去 的 点击率 ， 点击率 = 点击 人数 / 展示 人数 ， 点击率 高 的 广告 出现 的 概率 高 。 这个 简单 的 方法 没有 个性化 ， 那么 很 容易 通过 引入 个性化 来 提高 广告 的 总体 点击率 。 例如 ， 对于 收入水平 低 的 用户 ， 更 应该 展示 低价 的 手机 ， 而 收入水平 高 的 用户 ， 则 更 应该 展示 高价 的 手机 。 这 表明 ， 如果 我们 能够 知道 用户 对 每 一个 广告 的 个性化 点击率 而 不是 总体 的 点击率 ， 那么 我们 可以 让 展示 的 广告 更 有效率 ， 例如 为 广告 运营商 带来 更 多 的 广告费 。 但是 ， 个性化 点击率 很难 通过 历史 行为 统计 出来 ， 因为 给 同一个 用户 展示 同一个 广告 的 次数 不 可能 很多 ， 而且 广告 是 如此 之多 ， 以至于 每 一个 用户 看过 的 广告 占 总 的 广告 比例 非常低 。 例如 ， 百度 声称 在 使用 百度 推广 的 企业 有 62 万家 ， 某个 用户 看过 的 广告 都 只是 沧海一粟 。       在 搜索 排序 中 ， 同样 会 有 上述 问题 ， 你 在 京东 上 搜索 手机 时 ， 也 会 根据 你 的 点击率 给 你 排个序 。 与 广告 不同 的 是 ， 电子商务 排序 中 还 会 考虑 你 的 下单 率 ， 因为 在 电子商务 中 ， 下单 才能 带来 实实在在 的 收入 。 在 推荐 系统 中 ， 推荐 位置 同样 是 有限 的 ， 推荐 哪些 商品 和 不 推荐 哪些 商品 都 是 需要 用户 的 个性化 估计 点击率 和 下单 率 。 所以 最终 会 根据 点击率 和 下单 率 加权 取得 分 最高 的 商品 展示 在 推荐 位置 上 。       点击率 预估 和 下单 率 预估 的 方法 没有 太 大 差异 ， 这里 以 广告 点击率 预估 为例 进行 说明 。 广告 运营商 根据 用户 和 广告 的 历史 交互 数据 ， 可以 知道 用户 是否 点击 了 某个 广告 。 然后 可以 找出 用户 的 一些 信息 ， 例如 性别 、 年龄 等 人口学 属性 ， 还有 广告 的 标签 和 分类 等 信息 。 利用 机器 学习 的 算法 ， 可以 建立 一个 数学模型 ， 该 模型 可以 根据 这些 信息 计算 出 用户 的 个性化 点击率 。 如下 图 所示 ， 是 一个 点击率 预估 的 决策树 模型 ， 实际 场景 中 的 决策树 比 这个 要 复杂 得 多 ， 但是 基本原理 并 没有 太 大 差异 。 决策树 的 根 节点 是 一个 跟 用户 性别 有关 的 规则 ， 假设 我们 从 历史数据 中 收集 了 10000 个 样本 ， 这些 样本 每 一条 都 是 一次 广告 的 展示 。 性别 为 男则 进入 左子 树 ， 否则 则 进入 右子 树 。 假设 收集 的 样本 中有 4000 个 样本 对应 的 用户 性别 为 男性 ， 那么 左子 树有 4000 个 样本 ， 右子 树有 6000 个 样本 。 左子 树 的 根 节点 是 一条 与 广告 有关 的 规则 ， “ 广告 类别 = 汽车 ” ， 如果 广告 类别 是 汽车 则 进入 最 左边 的 叶子 节点 ， 这样 的 样本 有 1000 个 ， 其中 25 个 用户 点击 了 广告 ， 剩下 的 975 个 样本 用户 没有 点击 广告 。 因此 ， 这个 叶子 节点 输出 的 点击率 为 2.5 % 。 这个 结果 是从 训练 的 样本 中 计算 得到 的 ， 预测 时 ， 如果 用户 和 广告 满足 这个 规则 “ 用户 性别 = 男   & amp ; & amp ;   广告 类别 = 汽车 ” ， 那么 模型 就 预测 点击率 为 2.5 % 。 这 就是 最 简单 的 点击率 预估 模型 ， 可以 看到 ， 这 跟 很多 数据 分析师 干得 事情 是 十分相似 的 ， 最大 的 区别 在于 这些 规则 是 通过 算法 自动 生成 的 ， 可以 设想 ， 当要 考虑 的 因素 成千上万 时 ， 分析师 是 多么 的 抓狂 ， 但是 机器 不会 ， 机器 可以 生成 大量 的 这种 规则 ， 让 预测 更加 精准 ， 在 这件 事情 上 ， 机器 比 人 更加 擅长 。               大 数据 风控       在 传统 银行 中 ， 对 个人 和 企业 的 贷款 需要 进行 风险 控制 ， 为此 往往 需要 花费 大量 人力 进行 背景 调查 。 由于 这种 调查 成本 很 高 ， 所以 银行 很难 解决 小额贷款 的 风控 问题 ， 因为 成本 太高 ， 买卖 不划算 ， 银行 要么 要 你 提供 不动产 抵押 才能 贷款 ， 要么 直接 不 给 你 贷 。 在 互联网 时代 ， 人们 大量 的 信息 都 被 数字信息 记录 了 下来 ， 通过 一些 技术手段 可以 将 传统 金融 风控 所 需要 的 信息提取 出来 ， 利用 机器 学习 的 方法 ， 预测 用户 的 还款 意愿 和 还款 能力 ， 这样 就 可以 低成本 地 完成 大量 用户 的 贷款 申请 审核 工作 ， 使得 个人 小额贷款 成为 一个 赚钱 的 买卖 。       人脸识别       现在 的 数码相机 和 智能机 上 的 相机 都 具备 了 人脸识别 的 能力 ， 所谓 人脸识别 就是 可以 检测 出 图片 中 哪些 部分 是 人脸 ， 哪些地方 不是 。 一般 的 相机 在 拍照 时 ， 都 是 实时 地 识别 人脸 ， 一旦 识别 出来 后 ， 会 用 一个 正 方向 框住 人脸 。               这件 事情 看起来 十分 玄乎 ， 但是 一旦 你 了解 了 机器 学习 工作 原理 之后 ， 这 背后 的 基本原理 就 变得 简单 了 。 现在 几乎 涉及 到 图像 、 视频 等 检测 的 场景 ， 背后 最 主要 的 算法 都 是 机器 学习 。 人脸识别 的 大致 原理 是 ， 如果 有 一个 模型 （ 函数 ） ， 输入 一张 图片 ， 它会 告诉 你 这张 图片 是否 为 人脸 ， 那么 人脸 检测 就 简单 了 。 为了 从 一张 大图 中 找到 哪些 区域 是 人脸 ， 可以 先 把 这 张大 图 按照 不同 尺寸 不同 分辨率 截取 很多 小 的 图片 块 ， 对 每个 块 应用 上述 模型 ， 预测 这个 小 块 是否是 人脸 ， 然后 把 所有 是 人脸 的 块 都 标记 出来 ， 最后 把 这些 块 融合 成 一个 就 可以 了 ( 参考 上 图 ) 。       那 怎么 得到 这样 一个 模型 呢 ？ 答案 是 机器 学习 ！ 可以 先 收集 各种各样 的 图片 ， 然后 人工 标记 哪些 是 人脸 ， 哪些 不是 ， 得到 一个 标注 的 人脸识别 数据 集 。 图片 在 计算机 中是 用 数字 表示 的 ， 我们 知道 每 一张 图片 都 是 有 很多 像素点 构成 ， 对于 黑白 度 图像 ， 每 一个 像素 对应 一个 数值 ， 这个 数值 越大 ， 表明 这个 像素点 越亮 ， 这个 数值 越小 ， 表明 这个 像素点 越暗 ， 这些 敏感 交替 的 像素 一起 构成 了 整个 图片 ( 参考 上 图 ) 。 这里 每个 像素 对应 的 数值 都 可以 看做 特征 ， 虽然 我们 无法 像 分类 鸢尾花 那样 ， 认为 某个 像素 高 就 更 有 可能 是 人脸 ， 但是 和 分类 鸢尾花 一样 ， 都 是 要 找到 一个 函数 对 输入 的 这些 值 进行 计算 ， 输出 一个 结果 。 只不过 ， 计算 人脸 的 这个 函数 要 比 鸢尾花 的 复杂 得 多得多 ， 但 本质 上 都 是 在 找到 这个 函数 ， 而且 实现 的 方法 惊人 地 相似 ！ 只要 把 这些 特征 全部 扔 到 机器 学习 模型 中 ， 让 机器 自动 地 从 这些 标记 的 数据 中学 到 一个 可以 识别 人脸 的 函数 就 可以 了 。 当然 实际 的 系统 比 这个 要 复杂 得 多 ， 但是 基本原理 差不太多 。 这 在 另外 一个 方面 也 体现 了 机器 学习 相比 人工 规则 的 优越性 ， 人工 规则 实在 是 难以 找到 这样 一个 复杂 的 函数 来 识别 是否 为 人脸 。       计算机 视觉 用 计算机 来 实现 人 的 视觉 的 研究 领域 ， 里面 很多 问题 都 和 上述 人脸识别 的 问题 类似 ， 最终 都 归结为 拟合 一个 函数 的 问题 ， 这时 机器 学习 就 派上用场 了 。       出租车 派单       在 互联网 出租车 出来 以前 ， 打车 靠 的 是 运气 ， 当 你 碰到 一个 空车 并且 司机 成功 看到 你 的 时候 ， 这笔 交易 就算 成功 了 。 很多 时候 ， 这种 匹配 都 很 困难 ， 尤其 是 在 偏僻 的 地方 和 高峰期 。 当 用户 和 司机 都 通过 手机 APP 接入 互联网 之后 ， 这种 匹配 效率 就 大大提高 了 。 设想 这样 一个 问题 ， 在 一个 区域 有 5 个 乘客 和 7 个 司机 ， 每个 司机 只能 派 一单 ， 但是 可以 把 同一个 订单 派 给 多个 司机 ， 最先 抢到 的 司机 就算 接单 了 。 由于 司机 可能 因为 距离 太远 、 目的地 太偏 、 甚至 心情 不好 等 原因 拒绝 接单 ， 所以 这 5 单 即使 都 派出 去 也 不 简单 都 能 在 这 一次 派 单中 成交 ， 没 能 成交 的 订单 就 需要 等待 下 一次 派单 。 派单 的 一个 简单 的 目标 是 让 这 5 单 尽可能 地 成交 ， 如果 定义 接单 率 是 成交 的 单数 / 总 单数 ， 那么 目标 就是 让 总体 的 接单 率 最大化 。 那么 问题 来 了 ， 怎么 把 这 5 单派 给 司机 使得 总体 接单 率 最高 呢 ？ 最 简单 的 方法 是 就近 原则 ， 派 给 最近 的 司机 。 但 这种 策略 不见得 是 最优 的 ， 并 不是 所有 司机 都 愿意 接单 ， 有些 司机 不 愿意 接近 距离 的 单 ， 有些 司机 不 愿意 接 目的地 很 偏 的 单 。 这种 只 依靠 单一 因素 的 策略 匹配 成功率 不会 很 高 ， 那么 如何 把 所有 能够 考虑 的 因素 都 考虑 进来 设计 派单 策略 呢 ？ 答案 是 用 机器 学习 的 方法 预测 司机 的 接单 率 ！ 收集 历史 上 派 单 的 记录 ， 记录 里面 会 有 司机 是否 接单 ， 然后 将 需要 考虑 的 因素 计算出来 ， 作为 模型 的 特征 x ， 模型 预测 的 目标 是 不 接单 ( y = 0 ) 还是 接单 ( y = 1 ) 。 然后 就 可以 利用 算法 自动 归纳 出 一个 函数 ， 输入 是 这些 特征 ， 输出 是 接单 还是 不 接单 。 也 可以 像 点击率 预估 模型 那样 ， 输出 接单 的 概率 ， 这个 概率 就 可以 用来 设计 更好 的 派 单 策略 ！ 这个 思路 就是 滴滴 在 2017 年 数据挖掘 大会 ( KDD ) 上 提出 的 派 单 模型 的 一部分 。       类似 的 问题 还有 外卖 骑手 的 接单 问题 ， 这 类 问题 已经 大量 使用 机器 学习 的 方法 进行 优化 了 。 机器 学习 算法 正在 影响 我们 生活 的 方方面面 ， 从 打车 到 点 外卖 ， 几乎 无处不在 。       复现 代码       数据 加载               from       sklearn . datasets       import       load _ iris       import       pandas       as       pd       import       numpy       as       np       import       matplotlib . pyplot       as       plt       import       seaborn       as       sn         plt     .     style     .     use     (     &# 39 ; seaborn - talk &# 39 ;     )         iris       =       load _ iris     ( )       df       =       pd     .     DataFrame     (     iris     .     data     ,       columns     =     iris     .     feature _ names     )       df     [     &# 39 ; target &# 39 ;     ]       =       iris     .     target         #   随机 选取 几条 数据       idx       =       range     (     df     .     shape     [     0     ] )       np     .     random     .     shuffle     (     idx     )         print     (     iris     .     target _ names     )       print     (     df     .     iloc     [     idx     ]     .     head     (     10     ) )                 鸢尾花 数据分析               ax       =       plt     .     gca     ( )       df     .     groupby     (     by     =     &# 39 ; target &# 39 ;     )     .     plot     (     kind     =     &# 39 ; line &# 39 ;     ,       x     =     &# 39 ; petal   length   ( cm ) &# 39 ;     ,       y     =     &# 39 ; petal   width   ( cm ) &# 39 ;     ,       style     =     &# 39 ; .&# 39 ;     ,       ax     =     ax     )       ax     .     set _ xlim     ( [     0     ,     8     ] )       ax     .     set _ ylim     ( [     0     ,     3     ] )       plt     .     legend     (     iris     .     target _ names     ,       loc     =     &# 39 ; best &# 39 ;     )       plt     .     xlabel     (     &# 39 ; petal   length   ( cm ) &# 39 ;     )       plt     .     ylabel     (     &# 39 ; petal   width   ( cm ) &# 39 ;     )         df     .     groupby     (     by     =     &# 39 ; target &# 39 ;     )     .     mean     ( )     .     plot     (     kind     =     &# 39 ; bar &# 39 ;     )       plt     .     xticks     ( [     0     ,     1     ,     2     ] ,       iris     .     target _ names     ) ;       plt     .     xlabel     (     &# 39 ; &# 39 ;     )       plt     .     ylabel     (     u     &# 39 ; 平均值 &# 39 ;     )                 决策树 建模       决策树 模型 分为 回归 和 分类 ， 如果 目标 变量 是 类别 变量 ， 只能 取 有限 的 几个 值 ， 这样 的 问题 称为 分类 ， 我们 这个 任务 就是 分类 问题 。 而 如果 目标 变量 是 可取 连续 值 变量 ， 例如 预测 房价 ， 那么 这样 的 问题 就是 回归 。 这里 我们 只用 分类 就 好 了 ， 对应 的 类 是     DecisionTreeClassifier   ， 为了 便于 观察 ， 我们 限定 树 的 深度 为 2 。 为了 让 决策树 模型 能够 从 数据 中学 会 规则 ， 我们 需要 调用 模型 的     fit     方法 ， 并 将 数据 （ 包括 特征   iris . data   和 目标   iris . target   ） 传给 它 。       模型 从 数据 中 自动 学会 这些 规则 的 过程 ， 我们 称之为   训练   或者   拟合   。 因此 ，   fit   方法 实际上 就是 在 做   模型 训练   ！               from       sklearn . tree       import       DecisionTreeClassifier     ,       export _ graphviz       import       graphviz         clf       =       DecisionTreeClassifier     (     max _ depth     =     2     )       clf     .     fit     (     iris     .     data     ,       iris     .     target     )         dot _ data       =       export _ graphviz     (     clf     ,       feature _ names     =     iris     .     feature _ names     ,                                                             class _ names       =       iris     .     target _ names     ,                                                             out _ file     =     None     ,       filled     =     True     )         graphviz     .     Source     (     dot _ data     )                 思考 与 实践       一个 编程 小 练习 ， 探索 决策树 的 深度 与 预测 的 准确率 的 关系 ， 并 解释一下 观察 到 的 现象 的 原因 。       在 编程 之前 ， 请 先 配置 好 环境 ：           安装   Python     https : / / www . python . org / downloads / release / python - 2713 /         安装 Python 包       scikit - learn     http : / / scikit - learn . org / stable / install . html         pandas   数据处理 包       matplotlib   绘图 包       seaborn     可视化 包 ( 本 作业 暂时 不用 )       graphviz   可视化 包 ( 本 作业 暂时 不用 )                   可以 用 python 包 管理器   pip   安装 相关 包 ，   pip   默认 会 使用 国外 的 软件 源 ， 在 国内 下载 较慢 ， 建议 使用 国内 的 镜像 ：           USTC ：   https : / / lug . ustc . edu . cn / wiki / mirrors / help / pypi         THU ：   https : / / mirrors . tuna . tsinghua . edu . cn / help / pypi /                     from       sklearn . datasets       import       load _ iris       import       pandas       as       pd       import       numpy       as       np       import       matplotlib . pyplot       as       plt       from       sklearn . tree       import       DecisionTreeClassifier         plt     .     style     .     use     (     &# 39 ; seaborn - talk &# 39 ;     )         iris       =       load _ iris     ( )       df       =       pd     .     DataFrame     (     iris     .     data     ,       columns     =     iris     .     feature _ names     )       df     [     &# 39 ; target &# 39 ;     ]       =       iris     .     target         depths       =       range     (     2     ,     10     )       errors       =       [ ]         & quot ; & quot ; & quot ; 下面 是 你 的 代码 ， 请 完成 功能 ：             1 .   用 深度 为 2 - 10 的 不同 决策树 分别 对 数据 进行 建模 ， 计算 每 一颗 决策树 的 预测 准确率 。 准确率 是 预测 正确 的 样本 数目   /   总 样本 数目 。             2 .   画出 深度 - 误差 的 折线图               Hint :                   1 .   sklearn 每 一个 模型 都 会 有 一个 ` predict ` 方法 ， 可以 用来 预测 结果 。                   2 .   matplotlib   的 画图 函数   ` plt . plot `   是 有用 的 画图 工具 。       & quot ; & quot ; & quot ;        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-ml-example.html"},
      
      
      {"title": "第01.1讲：python快速入门", "text": "    Table   of   Contents           关于           安装           WINDOWS 系统           MAC           Linux                   编码 工具           Hello   word           小 任务                   数据类型 与 变量           列表 、 集合 和 字典           列表 推导 与 字典 推导                   函数 和 类           科学计算 库 ： numpy           绘 图库 ： matplotlib           思考 与 实践                 关于       在 本 讲 中 ， 你 讲 快速 了解 python 的 基本 编程 语法 和 工具 。 本 教程 是 为了 给 机器 学习 算法 学习 提供 支持 的 ， 除了 讲解 python 语法 外 ， 还会 涉及 做 算法 相关 的 工具 。       安装       WINDOWS 系统       对于 windows 系统 ， 可以 从   官网 下载   ， 安装 过程 可以 参考   这个 教程         MAC       MAC 自带 有 python ， 如果 没有 可以 通过 下述 两种 方式 安装           和 window 系统 一样 ， 通过 官网 下载安装       通过 brew 安装     brew   install   python             Linux       linux 一般 自带 python ， 如果 没有 可以 通过 下述 两种 方式 安装           通过 官网 下载安装       通过 包 管理器 安装 ， Ubuntu     sudo   apt - get   install   python   ,   CentOS     yum   install   python             编码 工具       用 常用 的 带 语法 高亮 的 代码 编辑器 就行 ， windows 推荐     notepad ++   ， mac 推荐   atom 。       如果 喜欢 IDE 可以 用 VS 或者 IDEA 。 IDE 的 好处 是 有 代码 提示 ， 写 起来 方便 。       Python 还有 一个 神奇 ， 叫做 IPython 、 Jupyter ， 愿意 折腾 的 可以 试试 。       Hello   word       和 所有 编程语言 一样 ， 首先 来 实现 一个   Hello   word 程序 ， 将 下面 的 代码 保存 到   hello . py   文件 中               print     (     & quot ; Hello   word ! & quot ;     )                 然后 打开 终端 ， 进入   hello . py   所在 目录 ， 执行命令   python   hello . py   就 可以 看到 输出 结果 了 ！ 没错 ， 你 没有 看 错 ， 整个 文件 只有 这 一行 ！             Hello   word !               你 也 可以 使用 python 交互式 命令行 ， 在 学习 python 的 语法 时 很 方便 。 使用 方法 是 打开 终端 ( WINDOW   下 是 CMD 程序 ) ， 然后 输入 python 并 回车 即可 进入 交互式 环境             Python   2.7 . 10   ( default ,   Oct   23   2015 ,   19 : 19 : 21 )   [ GCC   4.2 . 1   Compatible   Apple   LLVM   7.0 . 0   ( clang - 700.0 . 59.5 ) ]   on   darwin   Type   & quot ; help & quot ; ,   & quot ; copyright & quot ; ,   & quot ; credits & quot ;   or   & quot ; license & quot ;   for   more   information .   & gt ; & gt ; & gt ; |               然后 输入 Hello   Word 程序 并 回车 即可 看到 效果               & gt ; & gt ; & gt ;       print     (     & quot ; Hello   word ! & quot ;     )       Hello       word     !                 小 任务       将 问候语 改为 问候 你 自己 的 名字 ， 例如   Hello   Jack !       数据类型 与 变量       和 很多 编程语言 一样 ， 如果 你 要 存储 数据 ， 都 需要 一些 基础 的 数据类型 。 python 有 很多 基础 数据类型 ： 整型 、 浮点数 、 字符串 、 布尔值 ( True ,   False ) 、 空型 ( None ) 。 例如 你 要 表达 一个 公司 有 多少 人 ， 那么 需要 一个 整数 ； 如果 你 要 表达 公司 的 盈利 ， 那么 需要 一个 浮点数 ； 如果 你 要 表达 一个 用户 是否 为 管理员 ， 那么 你 需要 一个 布尔 类型 ； 最后 如果 你 要 表达 什么 都 没有 ， 你 需要 空型 。               #   整数       v       =       123         #   浮点数       pi       =       3.14159         #   字符串 ， 可以 用 单引号 和 双引号 ， 两者 是 等价 的       s       =       &# 39 ; Hello   word &# 39 ;       s       =       & quot ; hello   word & quot ;         #   字符串 支持 格式化 ,   python2 . 7 版本 用 百分号 ， python3 用 format 函数       s       =       &# 39 ; v   =       % d     &# 39 ;       %       v       s       =       &# 39 ; v   =   { 0 } &# 39 ;     .     format     (     v     )         #   布尔值       v       =       True       if       v     :               print     (     &# 39 ; True &# 39 ;     )         #   空型       v       =       None                 列表 、 集合 和 字典       python 原生 支持 3 种 常用 的 集合 类型 ： 列表 、 集合 和 字典 。 列表 就是 很多 元素 的 集合 ， 集合 和 列表 一样 ， 只是 集合 要求 元素 互不 相同 ， 字典 是 key - value 结构 。               #   一个 列表       arr       =       [     1     ,     3     ,     5     ,     7     ,     9     ,       3     ,       5     ]         #   列表 遍历       for       i       in       arr     :               print     (     i     )       #   1   3   5   7   9   3   5         #   集合       s       =       set     (     arr     )       for       i       in       s     :               print     (     i     )       #   1   3   5   7   9         #   字典       d       =       {     &# 39 ; A &# 39 ;       :       1     ,       &# 39 ; B &# 39 ;       :       2     ,       &# 39 ; C &# 39 ;       :       4     }       for       k       in       d     :               print     (     k     ,       d     [     k     ] )                 列表 推导 与 字典 推导       python 对 集合 的 遍历 支持 更 方便 的 语法 ， 叫做 列表 推导 和 字典 推导 。               #   列表 推导       arr _ new       =       [     i     *     2       for       i       in       arr     ]       #   将 arr 中 每个 元素 乘以 2         #   字典 推导       d _ new       =       {     k       :       v     *     2       for       k     ,     v       in       d     .     items     ( ) }                 函数 和 类       python 通过 关键字   def   定义 函数 ， 通过 关键字 class 定义 类 ， self 代表 类 自己 ， 类似 于 其他 编程语言 中 的 this 。               #   乘以 2 的 函数       def       times _ two     (     x     ) :               return       x       *       2         #   定义 一个 类       class       TimesTwo     (     object     ) :               def       __ init __     (     self     ) :                       pass               def       times _ two     (     self     ,       x     ) :                       return       x     *     2                 科学计算 库 ： numpy       利用 numpy 可以 方便 地 进行 向量 和 矩阵 操作 。               import       numpy       as       np       x       =       np     .     array     ( [     1.0     ,     2.0     ,     3.0     ,     5.0     ] )       y       =       np     .     array     ( [     2.0     ,     3     ,     4     ,     5     ] )       print     (     np     .     dot     (     x     ,       y     ) )       #   计算 向量 x 和 y 的 内积                 绘 图库 ： matplotlib       利用 matplotlib 会 图库 可以 方便 地画 出 各种 图形 。               import       matplotlib . pyplot       as       plt       x       =       np     .     array     ( [     1.0     ,     2.0     ,     3.0     ,     5.0     ] )       y       =       np     .     array     ( [     2.0     ,     3     ,     4     ,     5     ] )         plt     .     plot     (     x     ,       y     )       plt     .     show     ( )                 思考 与 实践       对比 for 循环 方式 对 数组 迭代 和 用 numpy 的 速度 。               import       numpy       as       np       import       time         def       for _ loop     (     arr     ) :               & quot ; & quot ; & quot ; 实现 for 循环 将 数组 arr 里面 所有 元素 平方                 arr :   np . array               & quot ; & quot ; & quot ;               pass         def       np _ loop     (     arr     ) :               & quot ; & quot ; & quot ; 实现 numpy 将 arr 所有 元素 平方               arr :   np . array               & quot ; & quot ; & quot ;               pass             arr       =       np     .     random     .     rand     (     100000000     )       start       =       time     .     clock     ( )       for _ loop     (     arr     )       print     (     &# 39 ; time   for   function   for _ loop :   { : . 3f } &# 39 ;     .     format     (     time     .     clock     ( )       -       start     ) )         start       =       time     .     clock     ( )       np _ loop     (     arr     )       print     (     &# 39 ; time   for   function   np _ loop :   { : . 3f } &# 39 ;     .     format     (     time     .     clock     ( )       -       start     ) )        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-python.html"},
      
      
      {"title": "第02.0讲：决策树模型", "text": "    Table   of   Contents           关于           回顾 决策树           决策树 生成 算法           信息熵           数据量 解释           平均 信息量 解释                   信息 增益 准则                   决策树 的 几何 解释           决策树 的 可 解释性           思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 深入 理解 决策树 模型 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 决策树 模型 ？       构建 决策树 模型 的 算法 是 怎么 实现 的 ？           学习 本 讲 ， 希望 你           至少 有 高中数学 水平 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           本 讲 所用 的 数据 集 还是 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               回顾 决策树       在 上 一 讲 中 ， 我们 简单 了解 了 一下 决策树 的 基本概念 。 如下 图 所示 ， 是 我们 上 一 讲 通过 数据分析 ， 设计 出来 的 简单 规则 模型 对应 的 决策树 。 决策树 首先 是 一颗 树 ， 树由 很多 节点 构成 。 这些 节点 分为 两类 ， 中间 节点 （ 椭圆形 ） 和 叶子 节点 （ 方形 ） 。 中间 节点 代表 一条 规则 ， 叶子 节点 代表 模型 的 决策 输出 。 有 多少 个 叶子 结点 ， 就 代表 有 多少 条 规则 。 这个 决策树 实际上 代表 3 条 规则 ， 每条 规则 可以 用 一个   IF - THEN   条件 语句 表示 ：                   IF   花瓣 长度 ( petal   length )   & lt ;   2.8 ,   THEN   sentosa       IF   花瓣 长度 ( petal   length )   & gt ; =   2.8   AND   花瓣 长度 ( petal   length )   & lt ;   4.9 ,   THEN   versicolor       IF   花瓣 长度 ( petal   length )   & gt ; =   4.9 ,   THEN   virginica           这 三个 规则 ， 共同 定义 了 一个 分段 函数 ！       $ $     y   =     \ \ begin { cases }     0 ,   \ \ text { petal   length }   \ \ lt   2.8   \ \ \ \     1 ,   \ \ text { petal   length }   \ \ in   [ 2.8 ,   4.9 )   \ \ \ \     2 ,   \ \ text { petal   length }   \ \ ge   4.9     \ \ end { cases }     $ $       对于 决策树 ， 我们 先 定义 几个 基本概念 ， 便于 后面 表述 ：           子 节点 ： 和 节点 相连 的 后继 节点 ， 比如 节点 ( petal   length   & lt ;   4.9 ) 是 节点 ( petal   length   & lt ;   2.8 ) 的 子 节点       父 节点 ： 当前 节点 是子 节点 的 父 节点 。 比如 节点 ( petal   length   & lt ;   2.8 ) 是 节点 ( petal   length   & lt ;   4.9 ) 的 父 节点       边 ： 相连 相邻 两个 节点 中间 的 部分 叫边 ， 这条 边 有 方向 ， 从父 节点 指向 子 节点 。       中间 节点 ： 有子 节点 的 节点 ， 它 不 直接 输出 预测 结果 的 节点 ， 对应 一个 规则 。 图中 椭圆形 的 节点 都 是 中间 节点 。       叶子 节点 ： 没有 子 节点 的 节点 ， 直接 输出 预测 结果 的 节点 ， 对应 一个 复杂 的 规则 ， 通常 是 多个 规则 的 组合 。 图中 方形 的 节点 都 是 叶子 节点 。       根 节点 ： 没有 父 节点 的 中间 节点 。 节点 ( petal   length   & lt ;   2.8 ) 就是 上述 决策树 的 根 节点 。       路径 ： 从根 节点 出发 ， 沿着 子 节点 移动 ， 到达 叶子 节点 时所 经历 的 所有 节点 序列 就是 一条 路径 ， 图中 黄色 区域 表示 出 一条 路径 。 路径 上边 的 数量 叫做 路径 长度 ， 实际上 也 等于 中间 节点 的 数目 。       深度 ： 最大 的 路径 长度 ， 比如 上述 决策树 深度 为 2 .       规则 ： 一个 可以 判定 真假 的 表达式 就 叫 规则 ， 比如 花瓣 长度 ( petal   length )   & lt ;   2.8       组合 规则 ： 多个 规则 通过 且 ( AND ) 和 或 ( OR ) 连接起来 的 语句 叫做 组合 规则 ， 比如   花瓣 长度 ( petal   length )   & gt ; =   2.8   AND   花瓣 长度 ( petal   length )   & lt ;   4.9           决策树 生成 算法       在 前面 一讲 ， 我们 通过 数据 可视化 分析 ， 找到 了 针对 花瓣 长度 ( petal   length ) 的 3 个 规则 。 这些 规则 的 过程 能不能够 自动化 的 从 已经 标注 好 的 数据 中 找到 呢 ？ 如果 可以 的话 ， 那么 决策树 就 可以 自动化 地 生成 了 ， 不用 再 去 做 数据分析 了 。 答案 是 肯定 的 ， 这 就是 决策树 生成 算法 。       假设 我们 对 花瓣 长度 来 设计 一个 规则 ， 一个 规则 可以 看做 将 它 的 取值 划分 成 两个 区间 ， 我们 期望 这种 划分 能够 将 数据 划分 成 两个 更加 容易 区分 类别 的 集合 ， 这里 的 关键 是 找到 划分 的 分裂 点 的 值 。 划分 方法 很多 ， 最 简单 的 方法 是 ， 选择 每 一个 可能 分裂 点 进行 划分 ， 然后 找出 里面 最好 的 分裂 点 。               如图所示 ， 是从 所有 可能 的 分裂 中 随机 取 的 两个 不同 的 分裂 方式 ， 第一种 采用 的 分裂 点 是 2 ， 在 标注 类别 的 训练样本 中 ， 三个 类别 的 样本 数目 都 是 50 个 ， 通过 分裂 后 ， 落到 左边 子 节点 的 样本 数目 分别 是 [ 50 ,   0 ,   0 ] ， 即 只有 第 1 类 的 样本 ， 看起来 区分 性 还 不错 ， 把 第一类 完美 地 识别 出来 了 ； 落到 右边 子 节点 的 样本 数目 分别 是 [ 0 ,   50 ,   50 ] ， 第 2 类 和 第 3 类 暂时 还 无法 区分 ， 不过 不用 担心 ， 我们 可以 沿着 右子 节点 继续 分裂 下去 就 可能 把 这 两类 也 分开 来 。 第二种 分裂 方式 采用 的 分裂 点 是 4 ， 相比 于 第一种 分裂 方式 ， 它 把 11 个 第 2 类 样本 也 放到 左边 了 ， 看起来 区分 性 没 第一种 好 。 那么 问题 来 了 ， 怎么 衡量 一种 分裂 方式 比 另外 一种 好 呢 ？       信息熵       信息熵 可以 用来 度量 一个 概率分布 $ \ \ { p _ i ,   i = 1 , 2 , ... \ \ } $ 的 不 确定 度 ， 熵 的 定义 是       $ $     H ( \ \ { p _ i \ \ } )   =   - \ \ sum _ i   p _ i   \ \ log   p _ i     $ $       为什么 熵 可以 度量 不 确定 度 呢 ？ 我们 来 看看 最 简单 的 一个 例子 ， 假设 我们 抛 一枚 硬币 ， 如果 硬币 是 均匀 的 ， 那么 正面 和 反面 出现 的 概率 都 是 0.5 ， 我们 计算 一下 熵 $ H   =   -   0.5 \ \ log   0.5   -   0.5 \ \ log0 . 5 = \ \ log   2   =   1 $ （ 这里 为 方便 记 ， 对数 的 底取 为 2 ） 。 如果 这个 硬币 不 那么 均匀 ， 假设 正面 朝上 的 概率 为 0.9 ， 反面 朝上 的 概率 为 0.1 ， 我们 再 来 计算 一下 熵   $ H   =     -   0.9 \ \ log   0.9   -   0.1 \ \ log0 . 1 = 0.427 $ 。 熵 变小 了 ！ 直观 来看 是 不确定性 减小 了 ！ 因为 抛 一枚 均匀 的 硬币 ， 确实 很难 猜测 它 是 正面 还是 反面 ， 但是 如果 非常 不 均匀 的 硬币 ， 正面 朝上 的 概率 是 0.9 ， 那么 我们 有 很大 的 把握 猜测 它 是 正面 ！ 如果 我们 把 熵 H 随着 正面 朝上 概率 p 画 一个 函数 图像 ， 可以 看到 它 在 0.5 处取 最大值 ， 直观 理解 是 均匀 的 硬币 最难 猜 ， 熵 最大 ； 相反 在 p = 0 和 1 处取 最小值 ， 直观 理解 是 只有 一面 的 硬币 最好 猜 ， 熵 最小 ！               数据量 解释       信息熵 的 另外 一个 解释 是 ， 要 描述 一个 随机 事情 所 需要 的 最少 数据量 （ 比特 数 ） 。 比特 是 计算机 表达 数据量 的 一个 单位 ， 计算机 中 都 是 用 0 和 1 表示 数据 ， 1 个 这样 的 0 / 1 单元 就是 1 比特 。 所以 这句 话 也 可以 这样 理解 ， 如果 我要 用 计算机 存储 这样 一个 随机 事件 的 结果 ， 最少 要用 的 数据量 。 对于 一个 均匀 硬币 ， 我要 记录 结果 是 正面 还是 反面 ， 只要 用 0 表示 反面 ， 1 表示 正面 ， 所以 需要 1 个 比特 （ 恰好 等于 熵 ） 就 可以 记录 结果 了 。 但是 如果 一枚 非常 不 均匀 的 硬币 ， 正面 朝上 的 概率 为 1 ， 那么 我们 根本 不 需要 记录 就 可以 知道 它 的 结果 肯定 是 正面 ， 也 就是 需要 0 个 比特 ， 对应 的 熵 为 0 ！ 如果 正面 朝上 的 概率 是 0.9 呢 ？ 貌似 也 需要 1 个 比特 才能 记录 这个 事件 的 结果 ， 如果 只 记录 1 个 这样 的 事件 ， 确实 如此 。 如果 我们 要 记录 很多 个 这样 的 事件 时 ， 那么 我们 可以 利用 数据 压缩工具 对 结果 进行 压缩 ， 就 好像 我们 压缩 自己 电脑 上 一个 普通 文件 那样 。 这种 压缩 有个 下界 ， 信息论 之父 Shanon 告诉 我们 ， 这个 下界 就是 熵 ， 平均 一个 事件 至少 需要 H ( p ) 个 比特 的 数据 来 记录 ！       一个 简单 的 压缩 例子 ： 设想 正面 朝上 的 概率 是 0.9999 ， 记录 10000 个 这样 的 抛 硬币 事件 ， 因此 记录 的 数据 中 大约 只有 1 个 为 0 ， 其他 全部 是 1 。 如果 我们 直接 记录 原始数据 ， 就 需要 10000 个 比特 。 如果 我们 只 记录 这个 唯一 的 0 出现 的 位置 ， 所 需要 的 数据量 不会 超过 32 比特 （ 计算机 中 表示 一个 整数 所 需要 的 比特 数 ） 。 记录 数据量 被 压缩 了 近 300 倍 ！       平均 信息量 解释       定义 ： 如果 一个 事件 发生 的 概率 为 p ， 那么 这个 事件 的 信息量 为   $   -   \ \ log   p $ 。       例如 ， 概率 为 1 的 事件 信息量 为 0 ， 因为 它 肯定 会 发生 ， 所以 这件 事情 发生 了 没有 带来 任何 信息 。 如果 我 告诉 你 明天 太阳 会 从 东方 升起 ， 你 肯定 会 认为 这是 一句 废话 ， 没有 任何 信息量 。 从 概率 的 角度 来 解释 ， 因为 太阳 从 东方 升起 的 概率 几乎 就是 1 ， 那么 这句 话 带来 的 信息量 是   $   -   \ \ log   1   =   0 $ 比特 ！ 相反 ， 如果 一件 事情 发生 的 概率 非常 小 ， 几乎 等于 0 ， 如果 它 发生 了 ， 带来 的 信息量 就 非常 大 ， 因为 $   -   \ \ log   0   = \ \ infty   $ ！ 如果 我 告诉 你 一个 人 把 狗 咬 了 ， 那么 你 肯定 会 认为 背后 有 非常 多 事情 没 暴露 出来 ， 这个 消息 的 信息量 非常 大 ， 这 就是 为什么 狗 咬 人 不是 新闻 ， 人 咬 狗 才 是 新闻 ， 这 可以 用 信息量 解释 为 新闻 的 信息量 越大 ， 大家 越 关注 。       一个 概率分布 $ \ \ {   p _ 1 ,   p _ 2 ,   ... ,   p _ n \ \ } $ 对应 了 n 个 事件 ， 比如 投 一个 六面体 骰子 ， 对应 6 个 事件 ， 每个 事件 的 概率 是 1 / 6 ， 根据 信息量 的 定义 ， 每个 事件 的 信息量 就是   $   \ \ log   6 $ ， 这 6 个 事件 的 平均 信息量 等于       $ $     - \ \ sum _ i   p _ i   \ \ log   p _ i   =   - \ \ sum   \ \ frac { 1 } { 6 }   \ \ log   \ \ frac { 1 } { 6 }   =   \ \ log   6     $ $       这 正是 这个 概率分布 的 熵 ！     < ! - -   ####   信息 增量 解释   信息熵 的 减少 量 可以 解释 为 一个 事件 的 结果 带来 的 信息量 。 比如 ， 一枚 均匀 的 硬币 朝上 这个 结果 的 信息量 是 多少 呢 ？ 在 我们 不 知道 这个 结果 时 ， 朝上 和 朝 下 的 概率 都 是 0.5 ， 不 确定 度 H = 1 ； 在 我们 知道 朝上 这个 结果 后 ， 不 确定 度 变成 H = 0 了 ！ 所以 ， 这里 不 确定 度 ( 熵 ) 减少 量 是 1 比特 ， 实际上 就是 因为 “ 一枚 均匀 的 硬币 朝上 ” 这个 结果 带来 了 1 比特 的 信息量 。 信息量 就是 不确定性 ( 熵 ) 的 减少 量 ！   - - >       信息 增益 准则       利用 信息熵 ， 我们 可以 度量 每 一种 分裂 方法 的 好坏 。 因为 熵 的 意义 是 不 确定 度 ， 那么 我们 计算 分裂 前后 这种 不 确定 度 的 减少 量 ， 不 确定 度 的 减少 越 多 ， 信息量 越大 ， 那么 分裂 后越 容易 区分 每 一类 ， 所以 分裂 方式 就 越 好 。 基于 这个 思想 ， 我们 定义 分裂 带来 的 信息 增益       $ $     \ \ Delta   I   =   \ \ text { 分裂 前 的 不 确定 度 }   -   \ \ text { 分列 后 的 不 确定 度 }   \ \ \ \             =   H ( P )   -   [ r _ 1   H ( C _ 1 )   +   r _ 2   H ( C _ 2 ) ]     $ $       P 是 分裂 前 数据 集中 每 一类 的 概率分布 ， 在 上述 的 第一种 分裂 中 ， 分裂 前 三个 类别 的 样本 都 是 50 ， 所以 每 一类 都 是 1 / 3 的 概率 ， 所以 $ H ( P )   =   \ \ log   3   =   1.098 $ 。 C1 是 分裂 后分 到 左子 树 的 数据 中 每 一类 的 概率分布 ， 在 分裂 规则 petal   length & lt ; 2 下 ， 分到 左子 树 的 样本 有 50 个 ， 且 全部 是 第一类 ， 所以 三个 类别 的 概率分布 是 $ \ \ { 1 ,   0 ,   0   \ \ } $ ， $ H ( C _ 1 )   =   1 \ \ log1   +   0 \ \ log   0   +   0 \ \ log0   =   0 ( 0   \ \ log   0   =   0 ) $ 。 $ r _ 1 $ 是 左子 树 的 样本 占 分裂 前 的 比例 ， $ r _ 1   =   50 / 150 = 1 / 3 $ 。 同样 ， 分到 右子 树 的 样本 分别 是 [ 0 ,   50 ,   50 ] ， 所以 $ H ( C _ 2 )   =   2 / 3 \ \ log   2   =   0.462 ,   r _ 2 = 2 / 3 $ 。 因为 分裂 后 有 两个 样本 集合 ， 所以 需要 把 这 两个 集合 的 不 确定 度 平均 一下 ， 平均 的 方式 是 按照 样本数 目的 加权 平均 。 根据 信息 增益 公式 ， 可以 得到   $ \ \ Delta   I   =   1.098   -   [ 1 / 3   *   \ \ log   0   +   0.462 * 2 / 3 ]   =   0.79 $ 。       类似 的 ， 我们 可以 计算 第二种 分裂 方式 的 信息 增益 ， 按照 上述 逻辑 计算结果 为 $ \ \ Delta   I   =   0.500 $ 。 这 表明 第二种 分裂 方式 的 信息 增益 没有 第一种 好 ， 直观 解释 就是 第二种 分裂 方式 带来 的 信息量 没有 第一种 多 ， 这 与 我们 直观 感受 一致 。       信息量 解释 ： 不 确定 度 的 减少 就是 信息量 ， 比如 在 抛 一枚 均匀 的 硬币 时 ， 知道 结果 之前 ， 不 确定 度为 1 比特 ， 知道 结果 后 ， 不 确定 度为 0 ， 不 确定 度 减少 量 是 1 比特 ， 也 就是 这个 结果 带来 的 信息量 。 信息 增益 是 分裂 前后 不 确定 度 的 减少 量 ， 所以 实际上 就是 分裂 带来 的 信息量 。       利用 信息 增益 准则 ， 我们 可以 设计 出 最佳 分裂 点 算法 ， 对于 某个 特征 ， 在 所有 可能 的 分裂 点 计算 分裂 的 信息 增益 ， 信息 增益 最大 的 分裂 点 就是 最佳 的 分裂 点 ！ 可能 的 分裂 点 可以 将 该 特征 所有 取值 按照 顺序排列 $ \ \ { z _ 1 ,   z _ 2 ,   .. ,   z _ k ,   ... , z _ n \ \ } $ ， 任何 两个 值 的 平均值 $   ( z   _   i   +   z   _   { i + 1 } ) / 2 $ 都 可以 作为 候选 分裂 点 。       最后 一个 问题 ， 这么 多个 特征 我们 应该 先 选择 那个 特征 分裂 呢 ？ 只有 先 选择 好 分裂 那个 特征 ， 才能 利用 上面 的 步骤 选出 最佳 分裂 点 。 方法 也 很 简单 ， 我们 对 每 一个 特征 $ x _ i $ 都 计算 它 的 最佳 分裂 点 $ t _ i $ 和 最大 信息 增益 $ \ \ Delta   I _ i $ ， 然后 将 信息 增益 最大 的 特征 $ i   ^   *   $ 作为 当前 最佳 分裂 特征 即可 。 因此 ， 决策树 生成 算法 中 每 一次 进行 分裂 的 过程 可以 归纳 为 如下 步骤 ：           计算 分裂 前 的 信息熵 H ( P )       对 每 一个 特征 的 每 一个 可能 的 分裂 点 ， 计算 分裂 后 的 信息 增益 ， 找到 最大 增益 的 特征 $ x _ i $ 和 分裂 点 $ t _ i $       得到 分裂 规则   $ x _ i   & lt ;   t _ i $   和 最大 信息 增益           利用 上述 分裂 算法 ， 可以 很 容易 得到 决策树 生成 算法 ， 从 训练 集合 开始 ， 运行 上述 分裂 算法 找到 一个 最佳 分裂 规则 ， 将 集合 分裂 成 两个 集合 。 如下 图 所示 ， 一 开始 训练 集中 3 类 数据 都 是 50 个 样本 ， 分裂 算法 找到 的 当前 最佳 分裂 规则 是 ( petal   length & lt ; 2.45 ) 。 这个 规则 将 训练 集 分为 两个 规则 B 和 C ， 集合 B 中有 50 个 样本 ， 全是 第一类 ， 集合 C 中有 100 个 样本 ， 其中 50 个 是 第 2 类 ， 50 个 是 第 3 类 。 接着 ， 对 集合 B 和 C 继续 分别 运行 分裂 算法 ， 相当于 将 B 看做 新 的 训练 集合 ， 生成 左子 树 ； 将 C 看做 新 的 训练 集合 ， 生成 右子 树 。 将 这个 步骤 不断 迭代 下去 ， 直到 达到 某个 准则 为止 。 例如 在 第一次 分裂 后 ， 分 到 左边 的 B 集合 里面 只有 一类 ， 已经 没有 不确定性 了 ， 没有 必要 继续 分裂 ， 这 就是 第一个 停止 准则 ： 如果 集合 只有 一类 样本 ， 那么 就 停止 分裂 。 而 分到 右边 的 C 集合 没有 满足 停止 准则 ， 所以 把 C 集合 看做 一个 新 的 训练 集 ， 运行 分裂 算法 找到 最佳 分裂 规则 为 ( petal   width & lt ; 1.75 ) ， C 集合 继续 分裂 成 两个 子集 ， 这 两个 子集 满足 第 2 个 停止 准则 ： 如果 决策树 的 深度 达到 某个 阈值 ， 那么 就 停止 分裂 。 这里 我们 要求 深度 不能 超过 2 ， 所以 这 两个 子集 就 停止 分裂 了 。 除了 这 两个 准则 外 ， 还有 一些 准则 ， 比如 每个 叶子 节点 的 样本 数目 不能 少于 10 个 ， 因为 叶子 节点 上 样本 数目 太 少 ， 那么 满足 这 条 组合 规则 的 样本 太少 了 ， 这 说明 这 条 规则 没有 代表性 。 叶子 节点 上 类别 最多 的 类别 ， 作为 在 输入 满足 这个 叶子 节点 对应 规则 时 ， 模型 的 输出 结果 。 例如 ， 最 左边 的 叶子 节点 全是 第一类 ， 因此 这个 叶子 节点 预测 的 结果 是 第一类 ， 也就是说 如果 输入 满足 规则 ( petal   length & lt ; 2.45 ) ， 那么 模型 的 预测 结果 就是 第一类 。 如果 所有 的 分裂 都 达到 停止 准则 了 ， 那么 一棵 决策树 就 生成 好 了 ， 然后 就 可以 用 这个 决策树 模型 对 未知 数据 进行 预测 啦 ！       决策树 生成 算法 步骤 总结 如下 ：           初始化 A = 训练 集       如果 集合 A 的 数据量 小于 一个 阈值 或者 只有 一类 样本 ， 停止 分裂 。 否则 进入 第 3 步 。       对 集合 A ， 依次 选取 第 i 个 特征 进行 分裂 ， 利用 分裂 算法 计算 该 特征 的 信息 增益 和 分裂 规则 ， 选取 信息 增益 最大 的 特征 作为 当前 分裂 特征 ， 对应 的 分裂 规则 作为 当前 的 分裂 规则 。       利用 第 3 步 生成 的 分裂 规则 ， 将 集合 A 划分 为 两个 子集 B 和 C 。 对 这 两个 子集 继续 应用 生成 算法 生成 左子 树 和 右子 树 。                   生成 算法 的 每 一个 分裂 步骤 可以 看做 对 所有 的 可能 的 分裂 规则 进行 穷举 ， 在 鸢尾花 识别 这个 任务 中 ， 特征 有 4 个 ， 假设 在 一 开始 分裂 的 时候 ， 每个 特征 有 100 个 可能 的 分裂 点 ， 那么 就 有 400 个 可能 的 分裂 方式 ， 生成 算法 就是 穷举 这 400 个 分裂 规则 ， 利用 信息 增益 准则 来 评估 这些 规则 ， 找出 最佳 规则 作为 决策树 的 规则 ！ 这个 方法 看起来 很 笨 ， 不像 我们 可以 观察 一下 分布图 就 知道 最佳 分裂 点 了 ， 但是 这是 目前 的 计算机 最 有效 的 处理 方式 了 。       决策树 的 几何 解释       在 前面 ， 我们 一直 把 决策树 看做 很多 规则 构成 的 分段 函数 。 接下来 ， 我 将 介绍 决策树 的 一个 直观 的 几何 解释 。 如果 我们 用 决策树 分类 的 特征 做 为 坐标轴 ， 例如 我们 以 花瓣 长度 ( petal   length ) 为 横轴 ， 花瓣 宽度 ( petal   width ) 为 纵轴 。 我们 将 数据 绘制 于 这样 一个二维 平面 中 ， 如下 图 所示 ， 每 一个 类别 的 数据 都 用 不同 的 颜色 表示 出来 。 上述 的 决策树 的 第一个 规则 是 ( petal   length & lt ; 2.45 ) ， 这 相当于 用图 中 垂直于 横轴 的 绿色 虚线 将 这个 平面 分割 为 左右两半 ， 左边 对应 于 ( petal   length & lt ; 2.45 ) ， 而 右边 对应 于 ( petal   length & gt ; 2.45 ) ； 第二个 基本 规则 是 ( petal   width & lt ; 1.75 ) ， 因为 这个 规则 是 在 ( petal   length & gt ; 2.45 ) 条件 下 应用 的 ， 所以 相当于 用图 中 平行 于 横轴 的 红色 虚线 将 前面 分 隔开 的 右半 空间 继续 分割 为 两 部分 ， 上边 对应 于 组合 规则 ( petal   length & gt ; 2.45   & amp ; & amp ;   petal   width & gt ; 1.75 ) ， 下边 对应 于 组合 规则 ( petal   length & gt ; 2.45   & amp ; & amp ;   petal   width & lt ; 1.75 ) 。 经过 这 两个 划分 ， 分别 形成 3 个 区域 ， 每个 区域 对应 一个 组合 规则 ， 同时 也 对应 决策树 的 一个 叶子 节点 ！ 从 几何 上 ， 可以 直观 地 看出 ， 这个 决策树 确实 将 这三类 数据 区分 开 了 ！               因此 ， 从 几何 上 来看 ， 决策树 可以 看做 对 特征 构成 的 空间 进行 划分 ， 划分 的 边界对应 于 特征 的 分裂 点 ， 划分 后 形成 的 每 一个 区域 与 决策树 的 叶子 节点 一一对应 ！       决策树 的 可 解释性       前面 我们 说 到 ， 机器 学习 就是 在 寻找 一个 可以 描述 数据 规律 的 函数 ， 复杂 的 函数 基本上 是 个 黑盒子 ， 很难 被 人 理解 。 决策树 是 其中 为数不多 可 解释性 高 的 模型 ， 因为 它 就是 很多 规则 嘛 ！ 当 一个 决策树 模型 训练 好 了 ， 我们 可以 将 决策树 模型 画 出来 。 在 预测 时 ， 我们 根据 样本 的 特征 可以 知道 这个 样本 是 被 决策树 的 哪条 规则 命中 ， 从而 得到 预测 结果 的 。 例如 ， 如果 一个 样本 满足 规则 ( petal   length & lt ; 2.45 ) ， 那么 模型 的 预测 结果 是 山 鸢尾 。 如果 有人 问 你 模型 怎么 判断 的 ， 那么 你 可以 很 肯定 的 告诉 他 ， 模型 是 根据 这 朵花 的 花瓣 长度 小于 2.45 厘米 来 判断 的 。 这个 好用 的 性质 并 不是 所有 模型 都 具备 ， 例如 后面 要 讲 到 的 神经网络 ， 可 解释性 就 很 低 。       思考 与 实践           证明 对于 任何 一种 分裂 方式 ， 信息 增益 非负 。       实现 上述 基于 信息 增益 的 决策树 生成 算法 ， 启动 代码 已 提供                   from       sklearn . datasets       import       load _ iris       import       numpy       as       np       from       scipy       import       stats         def       H     (     arr     ) :               & quot ; & quot ; & quot ; 计算 熵 的 函数 & quot ; & quot ; & quot ;               n       =       sum     (     arr     )               arr       =       [     1.0     *     i     /     n       for       i       in       arr     ]               return       sum     ( [     -       p       *       np     .     log     (     p     )       for       p       in       arr       if       p       & gt ;     0     ] )         def       InformactionInc     (     x     ,       y     ,       split     ) :               & quot ; & quot ; & quot ; 计算 利用 split 分裂 特征 x 时 ， 信息 增益               x   :   np . array   特征 数组 ,   size =   ( 样本数 ,   )               y   :   np . array   类别 ，   size = ( 样本数 ,   )               split   :   double   候选 分裂 点 的 值                 return   :   信息 增益               & quot ; & quot ; & quot ;               delta       =       0                 #   Your   Code                 #   end                 return       delta         def       FindSpliter     (     X     ,       y     ) :               & quot ; & quot ; & quot ; 寻找 最佳 分裂 特征 和 分裂 点 算法               X   :   np . array   特征 ， size   =   ( 样本数 ,   特征 数 )               y   :   np . array   类别 ， size   =   ( 样本数 ,   )               return   ( 分裂 特征 索引   i ,   分裂 点 的 值   t )               & quot ; & quot ; & quot ;               n _ samples       ,       n _ features       =       X     .     shape                 max _ infor _ inc       =       0               best _ feature       =       0               best _ split _ value       =       None                 #   Your   Code                 #   end                 return       best _ feature     ,       best _ split _ value           def       TreeGenerate     (     X     ,       y     ,       depth     =     0     ,       max _ depth     =     3     ) :               & quot ; & quot ; & quot ; 决策树 生成 算法 ， 返回 一棵树               & quot ; & quot ; & quot ;               #   满足 不 分裂 准则               if       X     .     shape     [     0     ]       & lt ;       10       or       np     .     unique     (     y     )     .     shape     [     0     ]       = =       1       or       depth       & gt ; =       max _ depth     :                       node       =       {     &# 39 ; prediction &# 39 ;       :       stats     .     mode     (     y     ) [     0     ] [     0     ] }                       return       node                 #   继续 分裂               best _ feature     ,       best _ split _ value       =       FindSpliter     (     X     ,       y     )               mask       =       X     [ : ,       best _ feature     ]       & lt ;       best _ split _ value               node       =       {                       &# 39 ; left &# 39 ;       :       TreeGenerate     (     X     [     mask     ,       : ] ,       y     [     mask     ] ,       depth     +     1     ) ,                       &# 39 ; right &# 39 ;       :       TreeGenerate     (     X     [     ~     mask     ,       : ] ,       y     [     ~     mask     ] ,       depth     +     1     )               }               return       node         def       TreePredict     (     X     ,       T     ) :               & quot ; & quot ; & quot ; 实现 决策树 预测 算法 & quot ; & quot ; & quot ;               pass           iris       =       load _ iris     ( )       T       =       TreeGenerate     (     iris     .     data     ,       iris     .     target     )       print     (     T     )         print     (       &# 39 ; ACC :   { } &# 39 ;     .     format     (     np     .     mean     (     TreePredict     (     X     ,       T     )       = =       iris     .     target     ) )       )        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/intro-dt.html"},
      
      
      {"title": "第03.0讲：机器学习建模实战", "text": "    Table   of   Contents           关于           分类 准确率           交叉 验证           过 拟合 与 决策树 减枝           过 拟合 的 方差 偏差 解释                   特征 工程           类别 特征           缺失 值                   决策树 的 其他 分裂 准则           信息 增益 率           基尼系数                   复现 代码           过 拟合 现象 代码                   思考 与 实践                 关于       本 讲 内容 将 通过 一个 例子 ， 深入 理解 机器 学习 建模 的 整个 过程 。 在 这 一 讲 中 ， 你 将 学习 到 ：           什么 是 过 拟合 与 泛化 ？ 为什么 要 划分 训练 集 、 测试 集 、 验证 集 。       什么 是 交叉 验证 ？       如何 评估 一个 分类 模型 的 好坏 ？       方差 - 偏差 分解       决策树 的 减枝 与 泛化           学习 本 讲 ， 希望 你           至少 有 高中数学 水平 。       了解 决策树 ， 如果 还 不 了解 ， 可以 参看   决策树 模型         如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           本 讲 所用 的 数据 集 还是 采用 鸢尾花 ( iris ) 数据 集 ， 你 可以 从 UCI 网站 上 下载   https : / / archive . ics . uci . edu / ml / datasets / Iris   。 如果 已经 安装 了   scikit - learn ， 那么 可以 利用 提供 的 dataset 接口 直接 调用 。 鸢尾花 数据 集是 著名 的 统计学家   Fisher   提供 的 。 下面 我们 采样 少量 的 数据 看一看 。                         sepal   length   ( cm )       sepal   width   ( cm )       petal   length   ( cm )       petal   width   ( cm )       target                       91       6.1       3.0       4.6       1.4       1               77       6.7       3.0       5.0       1.7       1               99       5.7       2.8       4.1       1.3       1               65       6.7       3.1       4.4       1.4       1               14       5.8       4.0       1.2       0.2       0               108       6.7       2.5       5.8       1.8       2               142       5.8       2.7       5.1       1.9       2               127       6.1       3.0       4.9       1.8       2               24       4.8       3.4       1.9       0.2       0               2       4.7       3.2       1.3       0.2       0                   该 数据 集 的 每 一条 记录 代表 一个 样本 ， 每 一个 样本 有 4 个 属性 变量 ：           sepal   length   ( cm )   萼片 长度       sepal   width   ( cm )   萼片 宽度       petal   length   ( cm )   花瓣 长度       petal   width   ( cm )   花瓣 宽度           每 一个 样本 有 1 个 目标 变量 target ， target 有 3 个 取值 ， 每 一种 取值 的 意义 如下 ：           0 ：   setosa   山 鸢尾       1 ：   versicolor   变色 鸢尾       2 ：   virginica   维吉尼亚 鸢尾           这个 数据 集 一共 有 150 个 样本 ， 这 三种 花都 有 50 个 样本 。 上面 显示 出来 的 只是 随机 选取 的 一部分 数据 。 每 一种 鸢尾花 的 图片 如下 ， 从左到右 分别 是   setosa , versicolor , virginica               分类 准确率       在 上 一 讲 中 ， 我们 说 到 决策树 生成 算法 ， 每 一步 都 是 穷举 所有 可能 的 分裂 规则 ， 利用 信息 增益 准则 找到 最佳 规则 ， 将 最佳 规则 加入 决策树 中 ， 持续 这个 步骤 知道 满足 停止 准则 。 在 前面 ， 我们 介绍 了 三个 停止 准则 ， 分别 是 ： 如果 集合 中 只有 一类 样本 ， 就 停止 分裂 ； 如果 集合 中 样本 个数 少于 某个 阈值 ， 就 停止 分裂 ； 如果 树 的 深度 达到 某个 阈值 ， 就 停止 分裂 。 显然 ， 如果 停止 准则 不同 ， 那么 我们 得到 的 决策树 也 不同 ， 这么 多 决策树 如何 评估 哪 一个 好 ， 哪 一个 不好 呢 ？       以 上述 鸢尾花 分类 任务 为例 ， 分类 得 好 和 坏 可以 通过 分类 转确 率来 衡量 ， 分类 准确率 = 分类 正确 的 样本 数目 / 总 样本 数目 。 如下 图 所示 ， 分别 是 3 棵 决策树 ， 深度 分别 为 0 、 1 、 2 ， 叶子 节点 中 的 3 个 数字 分别 代表 训练样本 中 3 类花 的 样本 数目 。 第 一棵 决策树 只有 一个 叶子 节点 ， 也 就是 对 所有 样本 都 预测 同一个 值 （ 训练样本 中 最 多 的 那 一类 ， 在 这个 例子 中 3 类 样本 一样 更 多 ， 任意 一个 预测 类别 都 一样 ） ， 假设 预测 为 第 0 类 ， 那么 准确率 只有 1 / 3 。 第二 棵 决策树 深度 为 1 ， 根据 花瓣 长度 是否 小于 2.45 将 特征 空间 划分 为 两 部分 ， 每 一部分 对应 一个 叶子 节点 ， 左边 的 叶子 节点 预测 为 第 0 类 ， 而 右边 的 叶子 节点 预测 为 第 1 类 （ 由于 第 1 类 样本 和 第 2 类 样本 一样 多 ， 所以 预测 为 第 1 类 和 第 2 类 效果 是 一样 的 ） 。 因此 ， 分到 左子 树 的 50 个 样本 都 会 预测 为 第 0 类 ， 都 预测 准确 ， 而 分到 右子 树 的 100 个 样本 只有 一半 预测 准确 ， 总 的 预测 准确 样本数 是 100 ， 准确率 为 2 / 3 。 随着 决策树 深度 的 加深 ， 训练样本 预测 的 准确率 会 越来越 高 ， 知道 每个 叶子 节点 上 都 只有 一类 样本 ， 那么 训练样本 的 预测 准确率 将 达到 100 % ！ 如果 以 训练 集上 的 准确率 为 评估 指标 ， 那么 显然 越深 的 决策树 预测 效果 越 好 。 而且 可以 想象 ， 对 任何 一个 训练 集 ， 总 可以 不断 地 将 决策树 加深 ， 直到 每个 叶子 节点 上 都 只有 一类 样本 。 那么 ， 训练 集上 的 准确率 越高 是否 就 代表 模型 的 预测 能力 越好 呢 ？               交叉 验证       我们 前面 讲过 ， 机器 学习 就 是从 数据 中 自动 发现 一些 有 意义 的 规律 。 这些 规律 是 有 意义 的 ， 意味着 可以 用来 指导 实践 ， 用来 对 没有 见到 过 的 数据 进行 预测 。 以 鸢尾花 为例 ， 我们 希望 通过 机器 学习 从 已经 观测 到 的 那 150 个 样本 ， 找到 一些 关于 4 个 属性 与 类别 的 规律 ， 这样 我们 就 可以 通过 这 4 个 属性 对 没有 见 过 的 鸢尾花 预测 它 的 类别 了 。 我们 用 X 表示 用来 预测 的 属性 ， 也 就是 特征 ， 用 Y 表示 预测 的 目标 类别 ， 也 就是 标签 ， 那么 在 所有 可能 的 数据 中 ， 可以 用 一个 联合 概率分布 P ( X ,   Y ) 来 表示 X 和 Y 的 概率 关系 ， 它 的 意义 是 在 所有 的 数据 中 特征 等于 X 且 类别 为 Y 的 概率 。 这些 所有 可能 的 数据 构成 的 集合 我们 称作 总体 ， P ( X ,   Y ) 就是 总体 的 概率分布 ， 已经 观测 到 的 数据   D = { ( x , y ) |   ( x , y ) ~ P ( X ,   Y ) }   只是 总体 的 一个 采样 结果 ， 它们 只是 总体 很少 的 一部分 并且 通常 假设 这些 样本 是 独立 选取 的 。 你 可以 理解 为 1 个 样本 是否 选取 与 其他 样本 没有 关系 ， 因此 样本 间 统计 独立 ， 又 因为 他们 都 来自 同一个 总体 ， 所以 他们 是 独立 同 分布 ( i . i . d ) 的 ！ 机器 学习 的 目标 就 是从 这 有限 的 观测 数据 D 中 学习 到 关于 总体 的 规律 。               因此 ， 我们 当然 不 希望 模型 学到 的 只是 训练 集上 看到 的 有限 的 规律 ， 甚至 只有 少数 样本 表现 出来 的 假 规律 。 这 也 是因为 机器 学习 采用 的 是 不 完全 归纳法 ， 所以 存在 被 经验 误导 的 可能性 ， 我们 希望 模型 学到 这种 错误 规律 的 风险 尽可能 小 ， 这样 模型 才 是 有 价值 的 ， 所以 我们 应该 在 全量 的 总体 上来 评估 模型 的 效果 才 靠 谱 。 以 鸢尾花 任务 为例 ， 确切 来说 就是 要求 模型 在 未知 的 来自 同一个 总体 的 样本 上 ， 分类 准确率 尽可能 高 。 如果 我们 用 函数 h ( x ) 表示 训练 好 的 模型 ， 对于 样本 的 特征 为 x ， 模型 的 预测 结果 $ \ \ hat { y }   =   h ( x ) $ 。 用 示性 函数 $ I ( h ( x )   =   y ) $ 表示 模型 是否 预测 正确 ， 预测 正确 结果 为 1 ， 预测 错误 结果 为 0 . 那么 模型 在 训练 集上 的 准确率 为       $ $     \ \ hat { P }   _   c   =   E _ { ( x ,   y )   \ \ sim   D }   I ( h ( x )   =   y )     $ $       上述 准确率 并 不是 一个 评估 模型 效果 好坏 的 指标 ， 而 应该 用 模型 在 总体 上 的 准确率       $ $     P   _   c   =   E _ { ( x ,   y )   \ \ sim   P ( x ,   y ) }   I ( h ( x )   =   y )     $ $       这 两个 准确率 的 唯一 区别 就是 求 期望 是 在 观测 数据 上 还是 在 总体 上 计算 的 结果 。 在 总体 上 的 期望值 才 有 意义 ， 才能 表达 模型 在 未知 数据 上 的 预测 效果 。 但是 ， 这个 期望值 我们 无法 求 ， 因为 总体 对 我们 来说 是 未知 的 ， 那 怎么办 呢 ？ 一个 简单 的 方法 是 ， 将 观测 数据 划分 成 两 部分 ， 一部分 用来 训练 模型 ， 而用 另外 一部分 计算 准确率 作为 模型 在 总体 上 准确率 的 估计值 。 这样 划分 的 两个 集合 我们 叫做 训练 集 和 测试 集 ， 在 训练 集上 训练 模型 ， 而 在 测试 集上 评估 模型 的 效果 。       有 了 测试 集后 ， 就 可以 用 测试 集上 的 预测 效果 作为 模型 在 总体 上 的 预测 效果 的 一个 较 好 的 估计 。 前面 说 过 ， 在 训练 集上 ， 深度 越深 的 决策树 准确率 约 高 ， 但是 在 测试 集上 的 效果 就 不见得 。 那么 我们 能 不能 通过 测试 集上 的 效果 来 选择 最佳 的 模型 呢 ？ 答案 是 不能 的 ！ 模型 的 建立 过程 中 不能 涉及 到 任何 测试 集上 的 信息 ， 否则 测试 集上 的 评估 结果 就 不能 很 好 地 反应 出 模型 真实 的 性能 。 为了 从 很多 模型 中 选择 一个 最好 的 ， 我们 还 需要 将 训练 集 继续 划分 成 训练 集 和 验证 集 ！ 用 训练 集 训练 多个 模型 ， 用 验证 集 选出 最好 的 一个 模型 ， 测试 集 只能 用于 评估 ！ 但是 在 实际 建模 任务 中 ， 我们 会 用 真实 的 未 观测 数据 来 评估 模型 ， 比如 训练 好 一个 推荐 的 模型 ， 上线 之后 运行 一段时间 来 验证 实际效果 ， 因此 也 有 只 将 训练 集 划分 成 训练 集 和 验证 集 两个 集合 的 做法 ， 而 用线 上 效果 来 评估 模型 。 而 在 学术论文 当中 ， 为了 与 同行 比较 效果 ， 那么 就 需要 划分 成 3 个 集合 ， 用 验证 集 选择 模型 ， 而用 测试 集 报告 本 论文 方法 的 效果 与 同行 进行 比较 。 曾经 有 学者 在 选择 模型 用到 了 测试 集 的 数据 ， 最后 被 发现 了 ， 这是 严重 的 学术 不端 行为 ！       将 训练 集 划分 成 训练 集 和 验证 集 选择 模型 的 方法 也 叫做 交叉 验证 ( Cross   Validate ) ， 交叉 验证 还有 一些 其他 方法 ， 这里 再 介绍 两种 ： k 折叠 和 留 一法 。 k 折叠 是 为了 解决 训练 集 数目 少 的 问题 ， 如果 训练 集 数目 很大 ， 上述 简单 的 划分 就 可以 了 ， 但是 如果 训练 集较 小 ， 某 一次 划分 带来 的 统计 波动 很大 ， 使得 这种 验证 方法 不 稳定 。 为了 解决 这个 问题 ， 可以 将 训练 集 随机 划分 成 k 个 相等 的 集合 ， 用 其中 k - 1 个 集合 训练 模型 ， 而用 剩下 一个 计算 评估 指标 ， 但是 这个 评估 指标 并 不是 最终 选择 模型 的 指标 。 选择 评估 集合 可以 有 k 种 不同 的 选择 方式 （ k 个 集合 任何 一个 都 可以 作为 评估 集合 ） ， 因此 可以 用 相同 的 超 参数 （ 我们 将 决策树 深度 、 每个 叶子 节点 最少 样本 数目 等等 这种 在 训练 模型 之前 就 需要 确定 的 参数 叫做 模型 的 超 参数 ） 训练 k 次 ， 可 得到 k 个 准确率 ， 然后 用 这 k 个 准确率 的 平均值 作为 最终 的 评估 指标 来 选择   不同 的 超 参数   。               留一法 可以 看做 k 等于 训练样本 数目 的 特殊 情况 ， 也 就是 每次 只 留下 一个样 本来 评估 。 在 k 折叠 交叉 验证 中 ， k 越大 ， 那么 评估 集上 估计 的 统计 误差 就 越 小 ， 因为 评估 指标 是 k 个 评估 结果 的 平均值 ！ 但是 计算 消耗 的 资源 就 越 多 ， 留一法 是 评估 指标 最 接近 总体 上 的 评估 指标 的 ， 但是 计算 消耗 的 资源 也 最 多 ， 一般 应 根据 训练 集合 的 大小 来 选择 合适 的 k 值 ， 一般 k 取 3 到 10 是 比较 合理 的 。       以 鸢尾花 任务 为例 ， 为了 选择 最佳 的 决策树 深度 这个 超 参数 ， 我们 可以 利用 5 折叠 交叉 验证 。 对 每 一个 深度 ， 利用 5 折叠 交叉 验证 计算 出 k 折叠 平均 准确率 ， 平均 准确率 最高 的 那个 深度 值 就是 最佳 的 ！ 然后 我们 可以 将 深度 设置 为 这个 最佳值 ， 在 全量 训练样本 中 重新 训练 决策树 模型 ， 作为 最终 的 预测 模型 ！       过 拟合 与 决策树 减枝       将 数据 集 划分 成 训练 集合 测试 集 ， 可以 评估 训练 集上 训练 的 模型 的 好坏 。 训练 集上 效果 越好 并 不 代表 在 测试 集上 的 效果 越 好 ， 当然 也 不 代表 在 总体 上 的 效果 越 好 。 下图 是 在 鸢尾花 任务 中 ， 将 数据 按照 8 : 2 的 比例 随机 划分 为 训练 集 和 测试 集 ， 限制 决策树 的 最大 深度 为 不同 值时 ， 训练 集 ( train ) 和 测试 集 ( test ) 上 的 误差 变化 。 随着 深度 的 增加 ， 训练 集上 的 误差 逐渐 降低 至 0 ， 而 测试 集上 的 误差 先变 低后 变高 并 发送 波动 （ 实际效果 跟 划分 结果 有关 ） 。 而 一般 的 数据 集中 ， 测试 集上 的 效果 会 随着 训练 集 误差 降低 反而 增加 ！ 这种 训练 集 和 测试 集 效果 不 一致 的 现象 ， 我们 说 模型 发生 了 过 拟合 现象 。 模型 过度 拟合 了 训练 集 ， 但是 在 未 见 过 的 测试 集上 的 效果 随 训练 集 拟合 精度 提高 反而 下降 了 ！               过 拟合 是 机器 学习 建模 中 经常 遇到 的 问题 ， 过 拟合 现象 可以 通过 交叉 验证 发现 ， 同时 通过 限制 模型 复杂度 等 措施 在 一定 程度 上 降低 过 拟合 的 程度 。 例如 可以 限制 决策树 深度 、 每个 叶子 节点 上 的 样本 数目 等 措施 ， 减少 模型 过 拟合 风险 。 此外 还 可以 通过 对 决策树 减枝 的 方法 解决 过 拟合 问题 。 事实上 ， 通过 限制 叶子 节点 上 最少 样本 数目 就是 在 做 减枝 ， 防止 决策树 过度生长 ， 这种 在 生成 决策树 过程 中 减少 决策树 的 分支 的 方法 叫做 预减 枝 。 也 可以 先 让 决策树 充分 生长 ， 然后 测试 决策树 每个 分支 上 的 最后 一次 分裂 是否 有 足够 大 的 信息 增益 ， 如果 没有 就 合并 这 两个 叶子 节点 ， 这种 方法 叫 后 减枝 。 这 两种 减枝 的 方法 都 是 通过 限制 模型 复杂度 减少 决策树 过 拟合 的 风险 。       关于 模型 选择 有个 奥卡姆 剃刀 原理 ， 这个 原理 说 如果 两个 模型 都 能 解释 数据 ， 那么 应该 选择 最 简单 的 那 一个 。 用过 拟合 来 解释 就是说 更 复杂 的 那个 模型 有 更 高 的 过 拟合 风险 ， 因此 如果 模型 效果 没有 明显 提升 的话 ， 不 应该 选择 更 复杂 的 模型 。       过 拟合 的 方差 偏差 解释       根据 偏差 - 方差 分解 ， 模型 的 预测 误差 可以 分解 为 偏差 和 方差 。 对 给定 的 待 预测 样本 $ x $ ， 估计 出来 的 预测 函数 $ \ \ hat { f } $ 会 随着 训练 集 的 不同 而 改变 ， 是 一个 随机变量 。 假设 实际 的 关系 是 $ y   =   f ( x )   +   \ \ epsilon $ ， $ \ \ epsilon $ 是 噪声 随机变量 ， 所以 $ y $ 也 是 一个 随机变量 ， 但是 $ f ( x ) $ 是 常数 。 那么 模型 预测值 $ \ \ hat { f } $ 和 实际 值 $ y $ 之间 的 期望 误差 可以 分解 为       $ $     \ \ begin { align }     E [ ( y   -   \ \ hat { f } ( x ) ) ^ 2 ]   & amp ; =   E [ y ^ 2   +   \ \ hat { f } ^ 2   -   2y \ \ hat { f } ]   \ \ \ \     & amp ; = Var [ y ] +   Var [ \ \ hat { f } ]   + [ Ey ] ^ 2   + [ E \ \ hat { f } ] ^ 2   -   2   f   E \ \ hat { f }   \ \ \ \     & amp ; =   Var [ y ]   +   Var [ \ \ hat { f } ]   +   ( f   -   E \ \ hat { f } ) ^ 2   \ \ \ \     & amp ; =   \ \ sigma ^ 2   +   Var [ \ \ hat { f } ]   +   Bias [ \ \ hat { f } ] ^ 2     \ \ end { align }     $ $       误差 可以 分为 三项 ， 第一项 是 该 问题 由于 信息 缺失 等 问题 带来 的 固有 误差 ， 无法 消除 ； 第二项 是 $ \ \ hat { f } $ 因为 训练 集 选取 的 不同 所 带来 的 统计 涨落 误差 ， 称为 方差 ； 第三项 是 把 所有 可能 的 训练 集都 训练 一遍 ， 得到 的 函数 预测值 平均 消除 统计 涨落 后 还 无法 消除 的 偏差 ！       模型 的 过 拟合 可以 看做 方差 很大 ， 对 特定 的 训练 集合 误差 很小 而 对 其他 训练 集合 误差 很大 ， 所以 对 某个 固定 的 观测 样本 ， 模型 预测 的 结果 的 波动 受 训练 集 的 选择 影响 很大 ， 也 就是 模型 的 方差 很大 。               特征 工程       类别 特征       在 实际 问题 中 常常 会 碰到 类别 特征 ， 例如 性别 分为 男和女 ， 职业 有 学生 、 白领 、 蓝领 、 无业 等 ， 文章 的 分类 有 社会 、 科技 、 经济 等等 。 这 一类 的 特征 和 鸢尾花 任务 的 特征 不同 的 是 ， 这 类 特征 的 取值 是 有限 个 并且 没有 明确 的 数值 关系 。 对于 类别 特征 ， 我们 通过 one - hot 编码 的 方式 将 它们 对应 为 数值 向量 。 one - hot 编码 的 方法 是 ， 如果 这个 特征 有 n 个 不同 的 取值 ， 那么 就 用 一个 n 维 向量 来 表示 这 一个 特征 ， 每 一维 对应 一个 取值 ， 如果 这个 特征 取 第 k 个值 ， 那么 就 将 这个 特征 第 k 维置 1 ， 而 其他 位置 都 置 0 。 以 上述 的 职业 特征 为例 ， 假设 职业 取值 有 5 个 ， 分别 是 学生 、 白领 、 蓝领 、 无业 、 未知 。 那么 我们 需要 用 一个 5 维 的 向量 来 编码 职业 这个 特征 ， 对于 某个 样本 职业 特征 为 白领 ， 那么 这个 5 维 的 向量 为 [ 0 ,   1 ,   0 ,   0 ,   0 ] ， 除了 第 2 位为 1 其他 全为 0 。 通过 这种 编码 之后 ， 类别 特征 也 可以 和 连续 值 特征 一样 进行 相同 的 处理 了 ， 这个 方法 在 以后 介绍 的 模型 中 会 大量 用到 。       不过 ， 决策树 还有 一个 更 方便 的 处理 类别 特征 的 方法 ， 直接 分裂 为 多个 子树 而 不是 两个 ！ 这样一来 ， 决策树 的 中间 节点 可能 存在 多个 分支 ， 每个 分支 对 应该 特征 的 一个 取值 ， 那么 也 就是 一个 等于 规则 ， 如下 图 所示 ， 最 左边 的 分支 对应 “ 职业 = 学生 ” 这 条 规则 。               决策树 处理 类别 特征 的 另外 一个 方法 是 子集 划分 ， 这个 方法 在 微软 的 LightGBM 中 被 用到 。 一个 类别 特征 的 n 个 不同 的 取值 构成 的 集合 ， 划分 成 两个 不同 取值 不同 的 子集 的 方法 有 $ 2 ^ n $ 个 ， 因此 遍历 每 一种 划分 要求 的 计算 复杂度 很 高 。 这种 问题 的 难点 来自 于 n 个 取值 的 无序 性 ， 试想 如果 n 个 取值 是 有序 的 ， 那么 这 就 与 之前 见到 的 连续 值 特征 一样 的 处理 就 好 了 。 基于 这个 思考 ， 可以 考虑 根据 特征 与 目标 变量 的 相关性 为 这些 值 赋予 一定 的 序 关系 。 例如 ， 在 建模 用户 的 收入水平 问题 中 ， 可以 根据 高 收入 占 比 排序 ， 将 职业 的 5 个 取值 排序 ， 假设 从 低 到 高 分别 是 ： 无业 、 未知 、 学生 、 蓝领 、 白领 。 我们 只 将 这 5 个值 划分 成 两个 子集 A 和 B ， 其中 A 中 的 职业 中 高 收入 占 比 都 比 B 要 低 ， 因此 这种 划分 方式 只有 4 种 ， 一般 情况 下 只有 n - 1 种 划分 方式 。 相比 于 遍历 无需 集合 所有 子集 大大减少 了 计算 量 。               缺失 值       在 实际 问题 中 第二类 常 遇到 问题 是 特征 缺失 值 ， 一般 用 NULL 表示 一个 样本 的 某个 特征值 缺失 ， 这种 缺失 可能 是 多种 原因 造成 的 ， 例如 确实 未知 、 录入 的 时候 忘记 填 了 、 脏 数据 等等 。 对于 缺失 值 的 处理 ， 一般 可以 根据 先验 知识 填充 对应 的 值 。 例如 在 年龄特征 缺失 时 ， 可以 用 这个 任务 下 的 用户 的 平均值 、 中位数 等 全局 统计 指标 填充 ； 在 性别 缺失 的 时候 ， 可以 用 数据 中 的 众数 填充 ， 例如 在 唯品 会上 的 建模 ， 性别 大多数 女性 ， 所以 对 性别 缺失 值用 女性 填充 。 对于 类别 特征 ， 还 可以 将 缺失 值 当做 一个 新 的 特殊 类别 处理 ， 例如 性别 取值 为 男 和 女 ， 缺失 值 可以 作为 第 3 个 取值 对待 。       对于 决策树 模型 ， 缺失 值 还有 一些 其他 处理 方式 。 在 C4 . 5 算法 中 ， 缺失 值 的 样本 会 同时 进入 到 分裂 后 的 各 分支 中 ， 为了 确保 缺失 值 样本 与非 缺失 值 样本 贡献 相同 ， 保证 公平 ， 同时 进入 各 分支 的 缺失 值 样本 会 被 加权 ， 权重 归一到 1 ， 保证 所有 的 缺失 值 贡献 之 和 为 1 ， 与非 缺失 值 的 贡献 相同 。 为此 ， 可以 在 初始化 时为 每 一个 样本 赋予 一个 权重 $ w _ i = 1 $ ， 在 以后 的 每 一次 分裂 中 ， 如果 只分 到 一个 分支 的 样本 ， 权重 不变 ； 而 分到 多个 分支 的 样本 在 每个 分支 的 权重 会 被 再次 加权 ， 变为 $ r _ i   *   w _ i $ ， $ r _ i $ 是非 缺失 样本 在 该 分支 的 占 比 。 以 职业 为例 ， 学生 、 白领 、 蓝领 、 无业 ， 未知 就是 缺失 值 ， 那么 用多叉树 进行 分裂 就 会 得到 4 个 分支 ， 假设 职业 不 缺失 的 样本 有 100 个 ， 在 每个 分支 分别 为 30 、 30 、 30 、 10 ， 而 职业 缺失 的 样本 有 10 个 ， 这 10 个 样本 会 同时 进入 这 4 个 分支 ， 且 同一个 样本 在 每个 分支 的 权重 会 从 原来 的 $ w _ i $ 更新 为 $ r _ i   *   w _ i $ ， 4 个 分支 的 比例 系数 $ r _ i $ 分别 为 0.3 、 0.3 、 0.3 、 0.1 ！ 而 在 计算 信息 增益 的 时候 ， 只 在 非 缺失 值 样本 上 计算 ， 并且 计算 的 时候 概率 不再 是 简单 的 数目 之 比 ， 而且 要 考虑 权重 ， 最后 将 结果 再 乘 上 非 缺失 样本 的 比例 。       决策树 另外 一种 处理 确实 值 的 方式 是 只 将 缺失 值 放到 一个 分支 ， 这个 方法 在 著名 的 XGBoost 中 被 采用 。 多个 分支 应该 选择 哪个 分支 呢 ？ 很 简单 ， 每个 分支 都 试一下 ， 选择 信息 增益 ( 或者 其他 准则 ) 最大 的 那 一种 方法 即可 ！       决策树 的 其他 分裂 准则       信息 增益 率       信息 增益 准则 在 二叉树 ( 即 每个 中间 节点 都 只有 两个 子 节点 的 树 ) 中是 一个 很 好 的 判断 分裂 好坏 的 准则 ， 但是 在 多叉树 中 就 不见得 了 。 在 多叉树 中 ， 取值 很多 的 类别 特征 会 天然 地有 很 高 的 信息 增益 ， 这 使得 这 类 特征 被 选择 进行 分裂 的 概率 比 连续 特征 和 只有 两个 取值 的 类别 要 高 很多 。 为了 解决 这个 问题 ， 信息 增益 率 准则 被 提出 来 了 。 信息 增益 率 是 在 信息 增益 的 基础 上 除以 属性 取值 的 \" 固有 值 \" ， 属性 a 的 固有 值 定义 为       $ $     IV ( a )   =   -   \ \ sum _ v   \ \ frac { | D _ v | } { D }   \ \ log   _   2   \ \ frac { | D _ v | } { D }     $ $       上式 中 D 和 $ D _ v $ 分别 是 待 分裂 样本 总数 和 其中 属性 a = v 的 样本 数目 ， 也 就是 根据 样本 中 属性 a 的 取值 形成 一个 概率分布 $ P ( v )   =   \ \ frac { | D _ v | } { D } $ ， 这个 分布 的 熵 就是 属性 a 的 固有 值 。 因此 ， 取值 越多 的 属性 IV 通常 越大 ， 以此 在 一定 程度 上 纠正 信息 增益 对 取值 多 的 属性 的 选择 偏好 。 利用 IV ， 信息 增益 率 可以 写为       $ $     Gain \ \ _   ratio ( D ,   a )   =   \ \ frac { Gain ( D ,   a ) } { IV ( a ) }     $ $       其中 $ Gain ( D ,   a ) $ 就是 前面 所说 的 对 集合 D 按照 属性 a 划分 的 信息 增益 $ \ \ Delta   I $ 。 与 信息 增益 相反 ， 信息 增益 率会 天然 的 偏好 取值 数目 少 的 属性 ， 在 C4 . 5 算法 中 ， 使用 了 一种 启发式 方法 ， 先 找出 信息 增益 高于 平均水平 的 属性 ， 然后 从中 选出 信息 增益 率 最高 的 ， 相当于 在 信息 增益 和 信息 增益 率 两者 中取 了 一个 折中 的 方案 。       基尼系数       决策树 的 基尼系数 与 经济学 上 度量 贫富差距 的 基尼系数 并 不是 一 回事 ， 这里 的 基尼系数 是 说 对 一个 集合 D ， D 中 的 样本 属于 不同 类别 ， 随机 取 两个 样本 他们 的 类别 不同 的 概率 。 这个 概率 越低 ， 说明 集合 的 纯度 越高 ， 这个 概率 等于 1 说明 集合 中 只有 一个 类别 。 假设 集合 D 中 每 一类 的 比例 为 $ p _ k $ ， 那么 根据 这种 概率 定义 可 得 基尼系数 为       $ $     \ \ begin { align }     Gini ( D )   & amp ; =   \ \ sum _ k   p _ k   \ \ sum _ { k '   \ \ ne   k }   p _ { k ' }   \ \ \ \     & amp ; = \ \ sum _ k   p _ k ( 1 - p _ k )   \ \ \ \     & amp ; = 1   -   \ \ sum _ k   p _ k ^ 2     \ \ end { align }     $ $       按照 某个 分裂 规则 分裂 后 ， 每个 集合 都 可以 计算 一个 基尼系数 $ Gini ( D _ v ) $ ， 我们 用 平均 基尼系数 $ \ \ sum _ v   r _ v   Gini ( D _ v ) $ 来 衡量 按照 属性 a 的 某个 分裂 规则 分裂 后 的 总体 不 纯度 ， 其中 $ r _ v $ 是 每个 集合 的 样本 占 总体 的 比例 。 这个 平均 基尼系数 越低 ， 说明 分裂 规则 越 好 ！ 因此 ， 在 决策树 分裂 算法 中 ， 用 分裂 后 最小 平均 基尼系数 选择 最佳 属性 和 分裂 点 即可 ， 这 就是 CART （ 分类 回归 树 ） 中 用到 的 分裂 准则 ， 在 著名 的 GBDT 和 XGBoost 中 也 经常 用到 。       复现 代码       过 拟合 现象 代码               import       numpy       as       np       import       pandas       as       pd       import       matplotlib . pyplot       as       plt         from       sklearn . tree       import       DecisionTreeClassifier       from       sklearn . datasets       import       load _ iris         iris       =       load _ iris     ( )         mask       =       np     .     random     .     randn     (     iris     .     data     .     shape     [     0     ] )       & lt ;       0.8       #   20 %   for   test       train _ x     ,       train _ y       =       iris     .     data     [     mask     ] ,       iris     .     target     [     mask     ]       test _ x     ,       test _ y       =       iris     .     data     [     ~     mask     ] ,       iris     .     target     [     ~     mask     ]         train _ err       =       [ ]       test _ err       =       [ ]       depths       =       range     (     1     ,       10     )       for       n       in       depths     :               clf       =       DecisionTreeClassifier     (     max _ depth     =     n     )               clf     .     fit     (     train _ x     ,       train _ y     )               train _ err     .     append     (     1       -       clf     .     score     (     train _ x     ,       train _ y     ) )               test _ err     .     append     (     1       -       clf     .     score     (     test _ x     ,       test _ y     ) )         plt     .     plot     (     depths     ,       train _ err     ,       &# 39 ; . - &# 39 ;     )       plt     .     plot     (     depths     ,       test _ err     ,       &# 39 ; . - &# 39 ;     )         plt     .     legend     ( [     &# 39 ; train   error &# 39 ;     ,       &# 39 ; test   error &# 39 ;     ] )       plt     .     xlabel     (     &# 39 ; depth &# 39 ;     )       plt     .     ylabel     (     &# 39 ; error &# 39 ;     )         plt     .     show     ( )                 思考 与 实践           举例说明 在 最终 的 决策树 中 ， 缺失 值 样本 在 所有 叶子 节点 中 的 权重 之 和 等于 1 ！       实现 k 折叠 交叉 验证 ， 选择 最佳 决策树 模型 ， 最佳 模型 参数 包括 决策树 深度 、 叶子 节点 上 最少 样本 数目 、 分裂 准则 这 三个 。                   import       numpy       as       np       import       pandas       as       pd       import       matplotlib . pyplot       as       plt         from       sklearn . tree       import       DecisionTreeClassifier       from       sklearn . datasets       import       load _ iris       from       sklearn . cross _ validation       import       KFold           iris       =       load _ iris     ( )         mask       =       np     .     random     .     randn     (     iris     .     data     .     shape     [     0     ] )       & lt ;       0.8       #   20 %   for   test       train _ x     ,       train _ y       =       iris     .     data     [     mask     ] ,       iris     .     target     [     mask     ]       test _ x     ,       test _ y       =       iris     .     data     [     ~     mask     ] ,       iris     .     target     [     ~     mask     ]         & quot ; & quot ; & quot ; 实现 K - Fold 算法       可以 利用 sklearn 的   sklearn . model _ selection . KFold   实现 集合 划分 ， 然后 依次 执行 fit 训练 模型 ，       和   score   评估 在 评估 集上 的 效果 ， 将 k 次 的 效果 平均 得到 在 该组 超 参数 下 的 平均 效果 。 最后 从 多组 超 参数 中       选择 出 最佳 的 超 参数 重新 训练 模型 ， 在 测试 集上 评估 结果 。       & quot ; & quot ; & quot ;       best _ err       =       1       best _ params       =       {     &# 39 ; depth &# 39 ;     :     0     ,       &# 39 ; min _ samples _ leaf &# 39 ;     :       1     ,       &# 39 ; criterion &# 39 ;     :       &# 39 ; entropy &# 39 ;     }       k       =       5       clf       =       DecisionTreeClassifier     ( )         ###   YOUR   CODE ,   定义   kfold   对象         ###   END   YOUR   CODE         for       depth       in       range     (     1     ,     10     ) :               for       min _ samples _ leaf       in       [     1     ,     3     ,     10     ,     30     ] :                       for       criterion       in       [     &# 39 ; entropy &# 39 ;     ,       &# 39 ; gini &# 39 ;     ] :                                                       ###   YOUR   CODE ， 实现 K - Fold 算法                                 ###   END   YOUR   CODE         print     (     &# 39 ; Best   error :       % . 4f     ,   depth =     % d     ,   min _ samples _ leaf =     % d     ,   criterion =     % s     &# 39 ;     .     format     (     best _ err     ,       best _ params     [     &# 39 ; depth &# 39 ;     ] ,       best _ params     [     &# 39 ; min _ samples _ leaf &# 39 ;     ] ,       best _ params     [     &# 39 ; criterion &# 39 ;     ] ) )         ###   YOUR   CODE ,   在 上述 最佳 超 参数 下 重新 在 训练 集上 训练 模型 ， 并 在 测试 集上 评估 效果         ###   END   YOUR   CODE        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/ml-in-action.html"},
      
      
      {"title": "第10.0讲：强化学习简介", "text": "    Table   of   Contents           关于           马尔科夫 决策 过程           MDP 的 例子           确定性 环境           随机 策略                   哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程           状态值 函数           动作 值 函数           HJB 方程           确定性 环境 的 HJB 方程                           动态 规划           值 迭代           压缩 映像 原理                   策略 迭代           两种 迭代法 的 对比           环境 未知 的 问题                   强化 学习 的 解释           强化 学习 的 应用           有 注意力 的 图像识别 系统                   思考 与 实践                 关于       强化 学习 近年 大火 ， 最早 是因为 AlphaGo 使用 强化 学习 打败 人类 围棋 冠军 引发 的 。 在 那 之后 ， 强化 学习 在 工业 场景 应用 越来越 多 ， 原来 很多 做 搜索 、 推荐 、 广告 等 一直 在 用 监督 学习 的 业务 ， 也 开始 使用 强化 学习 来 优化 用户 体验 和 平台 收益 了 。 强化 学习 实际上 很 早就 提出 了 ， 事实上 ， 强化 学习 来源于 控制论 。 控制论 之 父 叫做   维纳   ， 学过 信号处理 的 可能 知道 他 ， 维纳滤波 就是 用 他 的 名字 命名 的 。 控制论 最早 源于 航天 ， 人们 要 控制 火箭 发射装置 ， 将 火箭 发射 到 地球 之外 ； 控制论 还 源于 机器人 控制 ， 人们 需要 用 算法 对 机器人 的 行为 进行 控制 。 所以 ， 很多 讲 强化 学习 的 文献 也 会 说 强化 学习 是 在 优化 控制策略 。       在 这 篇文章 中 ， 您 将 学习 到           强化 学习 是 什么 ？ 可以 干什么 ？       马尔科夫 决策 过程 ( MDP ) 的 重要 概念       哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程       已知 环境 下 HJB 方程 的 求解 算法 ： 值 迭代 和 策略 迭代           我 期望 您 至少 有 ：           高中数学 水平 且 年满 18 岁 ， 部分 内容 需要 你 了解 监督 学习 ， 你 可以 通过 本 教程 前面 的 章节 进行 学习 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           马尔科夫 决策 过程       考虑 下面 的 迷宫 游戏 问题 ， 你 控制 一个 机器人 在 一个二维 迷宫 中 运动 ， 迷宫 中有 正常 的 陆地 、 火坑 、 石柱 、 钻石 。 你 可以 控制 机器人 上下左右 运动 ， 机器人 不能 走 到 迷宫 外面 ， 一次 最 多 只能 运动 一步 ， 如果 不 小心 掉 到 火坑 中 ， 游戏 结束 ， 如果 找到 了 钻石 ， 那么 可以 得到 奖励 ， 并且 游戏 结束 ！ 由于 你 的 控制 是 通过 语音指令 控制 ， 机器人 有 一定 概率 会 判断 出错 。 比如 你 说 让 机器人 往 左 走 ， 机器人 有 一定 概率 会 往右 走 ， 所以 机器人 的 移动 和 你 的 指令 之间 并 不是 完美 匹配 的 。 你 的 目标 是 通过 设计 策略 ， 让 机器人 尽快 地 找到 钻石 ， 获得 奖励 。               上述 问题 有 几个 关键 要素 ：           状态 ： 机器人 所处 的 位置 是 有限 的 ， 我们 把 每 一个 位置 称作 一个 状态 ， 那么 一共 有 15 个 状态 （ 有 一个 是 石柱 ， 机器人 无法 到达 这个 位置 ） 。 其中 两个 是 火坑 ， 一个 是 钻石 ， 由于 机器人 进入 这些 状态 就 会 结束 游戏 ， 我们 称为 终态 。 我们 用 S 来 表示 状态 ， 那么 S 可以 有 15 个 取值 ， 我们 一次 用 数字 标识 这 15 个 状态 ， 那么 $ S   \ \ in   \ \ { 1 , 2 , ... , 15   \ \ }   $ 。       动作 ： 在 每个 不是 终态 的 状态 下 ， 我们 都 有 4 个 控制 动作 ， 上 、 下 、 左 、 右 ， 我们 也 可以 将 这些 动作 依次 编号 为 1 到 4 ， 用 A 表示 动作 ， 那么 $ A   \ \ in   \ \ { 1 , 2 , 3 , 4   \ \ } $ 。       转移 概率 ： 因为 我们 是 通过 声音 控制 机器人 的 ， 所以 机器人 可能 听错 ， 可以 认为 是 语音 识别 技术 尚 不 成熟 的 原因 。 那么 在 某个 状态 S ， 采取 动作 A 之后 ， 机器人 到达 的 状态 并 不是 完全 确定 的 。 例如 当 机器人 在 左上角 时 ， 采取 “ 右 ” 这个 动作 时 ， 机器人 也 有 一定 概率 会 向下 移动 ， 进入状态 5 （ 假设 状态 按照 从左到右 顺序 编号 ） 。 为了 描述 机器人 的 这种 不 确定 运动 ， 可以 用 一个 转移 概率 来 表示 。 我们 用 P ( S ' | S ,   A ) 表示 在 状态 S 下 ， 采取 动作 A 的 条件 下 ， 机器人 进入状态 S ' 的 概率 。 例如 在 刚才 这个 例子 中 ， P ( S ' = 5 | S = 1 ,   A = 右 ) 就 表示 在 状态 1 （ 也 就是 左上角 ） ， 采取 动作 右 的 条件 下 ， 进入状态 5 （ 也 就是 第二行 第一个 状态 ） 的 概率 。 所以 ， 这个 转移 概率 描述 的 是 机器人 所 处 状态 在 我们 的 控制 下 变化 的 规律 。       回报 ： 在 机器人 每 一步 运动 到 下 一个 状态 时 ， 环境 会 给 我们 一个 奖励 或者 惩罚 ， 例如 如果 进入 火坑 游戏 就 会 结束 ， 而 拿到 钻石 就 会 得到 奖励 ， 这种 奖励 或者 惩罚 我们 用 数量 来 量化 。 我们 可以 用 正数 表示 奖励 ， 负数 表示 惩罚 ， 这 就是 回报 。 一般 情况 下 ， 回报 可能 跟 状态 和 动作 都 有 关系 ， 所以 我们 用 一个 函数 来 表示     R ( S ,   A ,   S ' ) ， 它 表示 在 在 状态 S 下 采取 动作 A 到达 状态 S ' 时 ， 获得 的 回报 。 注意 ， 一般 情况 下 ， 回报 是 在 每 一个 动作 执行 后 跳转 到 新 的 状态 就 会 收到 的 ， 而 不是 只有 最后 到达 终点 时才 有 。 在 这个 例子 中 ， 只有 最终 的 那 一步 才 有 回报 ， 可以 认为 其他 的 每 一步 的 回报 为 0 。 你 也 可以 给 其他 的 步骤 给 一个 负 的 回报 ， 负 的 回报 可以 解释 为 每 一步 付出 的 成本 ， 例如 时间 成本 、 机器人 的 能量消耗 等等 。       马尔科夫 性 ： 描述 环境 的   转移 概率   只 跟 当前 状态 和 动作 有关 ， 而 与 之前 经过 的 状态 和 动作 无关 ， 这种 性质 叫做 马尔科夫 性 （ 因为 是 一个 叫 马尔科夫 的 人 最早 提出 来 的 ） 。 在 这个 例子 中 ， 机器人 在 我们 的 控制 下要 进入 的 状态 的 概率 只 与 当前 的 状态 和 我们 的 控制 动作 有关 ， 而 与 在 这 之前 机器人 经历 过 的 状态 无关 。 例如 ， 当 机器人 处于 状态 1 时 ， 那么 接下来 ， 采取 动作 \" 右 \" 后 ， 状态 变到 2 还是 5 的 概率 与 机器人 到达 状态 1 之间 的 所有 事情 都 无关 ！ 这种 无关 ， 并 不是 说 最终 获得 的 回报 与 之前 的 状态 无关 ， 而是 说 当前 这 一步 的   转移 概率   与 之前 的 状态 无关 ！ 并 不是 所有 的 决策 事情 都 有 马尔科夫 性 的 ， 比如 股票 ， 今天 涨跌 不但 与 昨天 有关 ， 还 与 之前 的 多天 有关 。 如果 我们 用 $ S _ t ,   A _ t $ 分别 代表 t 时刻 的 状态 和 采用 的 动作 ， 那么 马尔科夫 性 可以 表示 为           $ $     P ( S _ { t + 1 } | S _ t ,   A _ t ,   S _ { t - 1 } ,   A _ { t - 1 } ,   ... , S _ 1 ,   A _ 1 )   =   P ( S _ { t + 1 } | S _ t ,   A _ t )     $ $       这个 公式 的 左边 是 在 前面 一系列 的 状态 和 动作 的 条件 下 ， 下 一步 转移 到 状态 $ S _ { t + 1 } $ 的 概率 ， 右边 是 在 当前 状态 $ S _ t $ 下 ， 采取 动作 $ A _ t $ 后 ， 转移 到 状态 $ S _ { t + 1 } $ 的 概率 ，   而 不管 更 前面 的 状态 和 动作 是 什么   。 这 两者 相等 ， 就是说 转移 概率 只 跟 当前 状态 和 动作 有关 ， 而 与 之前 经过 的 状态 和 动作 无关 ， 即 上面 说 的 马尔科夫 性 。       马尔科夫 性 也 叫 无 记忆性 ， 想象 一个 得 了 一种 奇怪 失忆症 的 病人 ， 他 每天 醒来时 ， 就 会 将 以前 发生 的 事情 全部 忘记 。 马尔科夫 性 就 像 一个 不 记事 的 系统 ， 这种 系统 只是 对 实际 问题 的 一种 理想 近似 ， 可以 简化 数学模型 。               上述 问题 可以 抽象 为 上述 4 个 要素 和 1 个 性质 的 普遍 问题 ， 因为 环境 具有 马尔科夫 性 ， 我们 需要 做出 决策 对 机器人 进行 控制 ， 所以 叫做 马尔科夫 决策 过程 ( Markov   decision   process ， MDP ) 。 对于 这 类 问题 ， 我们 在 每 一步 决定 要 采取 那个 动作 的 策略 可以 用 一个 策略 函数 来 表示 $ \ \ pi ( A   |   S ) $ ， 这个 函数 的 意义 是 在 状态 S 下 ， 采用 动作 A 的 概率 。 对于 确定性 的 策略 可以 看做 它 的 一个 特例 ， 即 只有 一个 动作 的 概率 为 1 其他 为 0 . 例如 ， 我们 可以 定义 一个 确定性 策略 如下 ： 如果 右边 能 走 ， 就 往 “ 右 ” ， 如果 右边 不能 走 ， 就 往 “ 下 ” 。 用 数学公式 表示 为       $ $     \ \ pi ( S )   =   \ \ begin { cases }                             \ \ text { 右 } ,   S   \ \ in   \ \ { 1 , 2 , 3 , 6 , 8 , 9 , 10 , 12 , 13 , 14   \ \ }   \ \ \ \                             \ \ text { 下 } ,   S   \ \ in   \ \ { 4 , 7 , 11   \ \ }                             \ \ end { cases }     $ $       MDP 问题 的 目标 是 找到 这样 一个 策略 ， 在 这个 策略 下 ， 总 的 期望 折扣 回报 最大化 ！ 总 期望 折扣 回报 定义 如下       $ $     R   =   E   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t ) ,   \ \ gamma   \ \ in   ( 0 ,   1 ]     $ $       也 就是 在 这个 策略 下 ， 将 所有 获得 的 回报 打个 折 之后 全部 加 起来 。 越往后 的 折扣 越大 ， 这 是因为 我们 更 关心 离 当前 时间 更近 的 回报 。 这个 回报 越大 ， 说明 这个 策略 越 好 ， 使得 回报 最大 的 策略 就是 最优 策略 。 因为 环境 有 一定 的 随机性 ， 在 这个 策略 下 实际 的 回报 是 随机 的 ， 所以 我们 对 回报 求 期望 ， 得到 期望 折扣 回报 。       以 上述 迷宫 为例 ， 我们 假定 游戏 结束 后 回报 全部 为 0 ， 状态 也 不 改变 了 。 那么 上述 求和 只到 游戏 结束 ， 假设 折扣 因子 为 1 ， 跳 到 火坑 的 回报 为 - 1 ， 找到 钻石 的 回报 为 + 1 ， 其他 情况 回报 为 0 。 那么 一个 找到 钻石 的 策略 的 总 回报 就是 1 ！ 而 跳 到 火坑 的 策略 的 总 回报 为 - 1 .   由于 环境 的 随机性 ， 实际 的 策略 下 这 两种 情况 都 有 可能 ， 所以 一般 期望 回报 在 - 1 到 1 之间 。 一个 策略 越 好 ， 那么 期望 回报 会 更 接近 1 ， 期望 回报 最大 的 策略 就是 我们 要 寻找 的 最优 策略 。       MDP 的 例子       设想 你 在 跟 朋友 玩 斗地主 （ 一种 流行 的 扑克牌 游戏 ， 你 可以 类比 于 所有 你 熟悉 的 牌类 游戏 ） ， 影响 你 每 一步 出牌 的 信息 包括 每个 人 手中 的 剩下 的 牌 的 张数 ， 每 一个 人 已经 出过 的 牌 ， 还有 你 自己 目前 手中 的 牌 ， 这些 信息 构成 了 一个 状态 ( S ) 。 你 的 出牌 动作 可以 从 所有 可能 的 出牌 方式 中 选择 一个 ， 比如 3 带 1 或者 一对 A 都 是 一个 动作 ( A ) 。 可以 想象 ， 状态 数目 和 动作 数目 都 非常 大 ， 但是 不用 担心 ， 计算机 很 容易 处理 大量 的 状态 和 动作 的 任务 ， 但是 前提 是 你 要 把 MDP 中 的 所有 元素 定义 清楚 。 你 每次 出 牌 之后 ， 到 你 下 一轮 出牌 时 ， 完成 了 一个 状态 转移 ， 转移 到 的 状态 就是 下 一轮 出牌 时 ， 每个 人 剩下 的 牌 的 张数 ， 每个 人 已经 出过 的 牌 ， 还有 你 自己 手中 当时 的 牌 。 由于 你 无法 决定 其他人 的 出牌 ， 所以 对 你 的 某 一个 出 牌 动作 ， 转移 到 的 状态 也 是 不 确定 的 ， 存在 着 转移 概率 ( P ) 。 但是 这个 转移 概率 在 当前 的 状态 和 你 的 动作 给定 的 情况 下 ， 而 与 之前 的 状态 和 动作 无关 ， 因为 根据 我们 对 状态 的 定义 ， 它 已经 包含 了 这些 信息 ， 所以 不用 管 之前 的 状态 了 ， 这 表明 这个 任务 满足 马尔科夫 性 。 我们 的 目标 是 要 最终 赢得 这 一局 牌 ， 可以 将 赢 了 的 回报 ( R ) 定义 为 + 1 ， 而 将 输 了 的 回报 定义 为 - 1 ， 其他 每 一步 的 回报 都 是 0 。 那么 玩 斗地主 游戏 的 过程 具有 上述 4 个 要素 和 1 个 性质 ， 因此 在 计算机 设计 斗地主 算法 时 ， 就 可以 用 马尔科夫 决策 过程 来 描述 ！       设想 你 是 一只 AlphaGo ， 正在 和 人类 下围棋 ， 影响 你 每 一步 落子 的 信息 就是 当前 的 棋局 状态 ( S ) ， 而 你 每 一个 落子 动作 ( A ) 都 是从 所有 可能 的 落子 位置 中 选择 的 一个 。 从 你 当前 落子 到 下 一步 落子 时 ， 状态 发生 了 转移 ， 又 有 你 不 知道 对手 会下 哪 一步 ， 所以 存在 状态 转移 概率 ( P ) 。 这个 转移 概率 只 与 当前 的 棋局 状态 和 你 的 落子 动作 有关 ， 所以 具有 马尔科夫 性 。 我们 的 目标 是 最终 要 赢得 这 一局 牌 ， 每 一局 牌 结束 时 都 会 有 一个 回报 ( R ) 。 因此 ， AlphaGo 下围棋 的 过程 具有 上述 4 个 要素 和 1 个 性质 ， 因此 可以 用 马尔科夫 决策 过程 ( MDP ) 来 描述 。 事实上 ， 在 Google 最新版 的 AlphaGo 程序 中 ， 正是 通过 MDP 来 对 下围棋 这个 过程 进行 建模 的 。       设想 你 是 一个 航天 控制系统 设计 工程师 ， 你 想 让 火箭 按照 预期 将 卫星 发射 到 预定 轨道 当中 。 火箭 在 每 一个 时刻 上 所处 的 位置 和 速度 共同 构成 了 一个 状态 ( S ) ， 而 你 设计 的 控制算法 每个 时刻 所 给出 的 推动 策略 ( 加速 喷火 还是 减速 喷火 等等 ) 就是 一个 动作 ( A ) 。 这个 问题 中 ， 状态 是 连续 的 ， 动作 既 可以 是 有限 的 也 可以 是 连续 的 。 每 一个 动作 执行 后 ， 由于 受到 环境 的 干扰 （ 阻力 ， 控制 误差 等等 因素 ） ， 下 一个 时刻 火箭 的 状态 存在 着 一个 转移 概率 ( P ) 。 这个 转移 概率 在 当前 时刻 的 状态 和 动作 给定 的 情况 下 与 之前 火箭 的 状态 和 控制 动作 无关 ， 这 表明 任务 满足 马尔科夫 性 。 我们 的 目标 是 用 最少 的 能量消耗 将 火箭 发射 到 预定 轨道 ， 每 一步 的 回报 ( R ) 是 负 的 ， 因为 每 一步 消耗 了 燃料 ， 总 的 回报 就是 负 的 消耗 总 能量 。 最大化 总 回报 就是 最小化 能量消耗 ， 因此 上述 火箭 控制系统 可以 用 马尔科夫 决策 过程 来 描述 。       这样 的 例子 还有 很多 ， 可以 看到 ， 很多 实际 的 决策 和 控制 问题 ， 都 可以 看做 马尔科夫 决策 过程 ( MDP ) ， 因此 MDP 应用 非常 广泛 ， 从 斗地主 到 火箭 发射 ， 几乎 无所不在 。       确定性 环境       假设 我们 可以 直接 控制 上述 机器人 ， 那么 在 某个 状态 S ， 采用 动作 A 后 ， 机器人 到达 的 状态 S ' 就是 确定 的 ， 也 就是 转移 概率 P ( S ' | S ,   A ) 只 可能 取 0 和 1 两个 值 ！ 这种 情况 下 ， 我们 说 环境 是 确定 的 ， 这 就是 我们 正常 的 迷宫 游戏 。       在 环境 是 确定 的 ， 没有 随机 干扰 情况 下 ， 我们 很 容易 看出 ， 下图 所示 的 一个 策略 就是 最佳 策略 ， 箭头 所指 的 方向 就是 在 当前 状态 下 ， 应该 采取 的 动作 。 因为 在 这个 路劲 上 ， 除了 最终 的 状态 ， 其他 状态 回报 都 是 0 ， 只有 到达 最终 状态 的 时候 有 一个 + 1 的 回报 ， 所以 这个 策略 的 总 回报 等于 1 。 不难想象 ， 这个 策略 并 不是   唯一   最优 的 策略 ， 例如 在 这条 路径 上 往回 走 一段 路程 之后 再 往前走 ， 总 回报 还是 等于 1 ， 因为 中间 所有 的 状态 的 回报 都 是 0 。       如果 我们 想要 得到 最短 的 路径 ， 该 怎么办 呢 ？ 方法 很 简单 ， 我们 可以 让 除了 到达 终态 的 步骤 外 ， 其他 每 一步 的 回报 为 - 0.1 即可 ！ 虽然 这样一来 ， 但是 只有 最短 路径 的 策略 可以 获得 最大 总 回报 ， 容易 计算 出 最大 总 回报 等于 0.5 。 你 可以 把 每 一步 回报 为 负数 看做 成本 ， 例如 你 控制 的 机器 要 消耗 电 ， 不必要 的 步骤 就是 在 浪费 你 的 时间 ， 增加 时间 成本 。 在 最 短 路径 找到 钻石 实际上 是 在 说 用 最少 的 成本 得到 最大 的 回报 ！       另外 一个 方法 是 让 折扣 因子 小于 1 ， 比如 取 0.9 ， 每 一步 的 回报 还 保持 为 0 不变 ， 那么 路径 越长 ， 最后 得到 的 那个 回报 就 会 被 衰减 得 更 多 ， 只有 最短 路径 才能 得到 最大 回报 ， 此时 最大 回报 为       $ $     R   =   0   +   0   +   0   +   0   +   0   +   0.9 ^ 6   =   0.531     $ $       折扣 因子 实际上 是 再 将 未来 的 回报 折现 到 现在 ， 想想 给 你 的 回报 是 很多年 后 的 1 万块 钱 ， 那么 同样 是 一万块 钱 ， 年限 不同 决定 了 其 价值 也 不 相同 （ 想想 一下 2000 年 的 1 万元 和 2010 年 的 1 万元 的 价值 明显 是 不 一样 的 ， 因为 货币 会 贬值 ） 。 为了 统一 衡量 这些 不同 年限 的 回报 ， 可以 将 他们 折现 到 当下 ， 看 他们 在 现在 值 多少 钱 。   折扣 因子 就是 折现 率 ， 它 将 未来 的 回报 折现 到 现在 便于 统一 比较   ！       因此 ， 从 这个 例子 来看 ， 在 对 实际 问题 建模 的 时候 ， 合理 设计 回报 也 是 一个 很 重要 的 事情 。               随机 策略       这个 简单 的 例子 我们 很 容易 利用 上帝 视角 ， 发现 最佳 的 策略 $ \ \ pi ^   *   $ 使得 总 回报 最大 ， 就是 上图 中 的 最 短 路径 。 但是 ， 设想 一下 ， 我们 是 图 中 那个 机器人 ， 身在 此 山中 ， 云深 不知 处 ， 无法 开 上帝 视角 ， 不 知道 上下左右 分别 是 什么 坑 ， 只 知道 自己 所处 的 位置 是否 有 坑 ， 因此 就 不 知道 采取 某个 动作 之后 会 得到 什么 ， 除非 我们 去 尝试 一下 。 在 这种 情况 下 ， 我们 该 采用 什么 策略 呢 ？ 没有 太好 的 办法 ， 只有 采用 随机 策略 去 尝试 。       所谓 随机 策略 就是 每个 状态 下 ， 选择 上下左右 四个 动作 的 概率 都 不 为 0 ， 比如 在 对 环境 一无所知 的 情况 下 ， 可以 将 每个 方向 的 概率 都 设为 一样 。 当 你 经过 尝试 或者 通过 其他 途径 获得 了 一些 关于 环境 的 信息 ， 你 也 可以 让 某个 方向 上 的 概率 更大 一些 ， 例如 有人 告诉 我 ， 钻石 就 在 最 右下角 ， 那么 我们 可以 不用 采用 那么 随机 的 策略 ， 让 向 右 和 向下 这 两个 动作 的 概率 更大 一些 。 这 表明 ， 我们 知道 的 信息 越 多 ， 可以 采用 的 策略 就 越 不用 那么 随机 。 如果 我们 对 环境 十分 清楚 ， 例如 我们 知道 每 一个 状态 下 每 一个 动作 执行 后 会 转移 到 哪个 状态 ( 即 知道 转移 概率 P ) ， 并且 知道 哪些 状态 是 坑 ， 哪些 是 钻石 ( 也 即 是 知道 了 回报 函数 R ) ， 也 就是 开 了 上帝 视角 ， 知道 环境 的   转移 概率   和 环境 的   回报 函数   ， 那么 我们 原则上 可以 计算 出 这个 最佳 的 策略 。 那么 问题 来 了 ，   在 环境 的 转移 概率 和 回报 函数 已知 的 情况 下   ， 怎么 求解 最佳 策略 呢 ？       哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程       在 回答 上述 问题 之前 ， 我们 先 来 介绍 一个 方程 ， 叫做 哈密顿 - 雅克 比 - 贝尔曼 ( HJB ) 方程 ， 很 明显 这个 方程 跟 三个 人 有关 ， 这 三个 人 都 是 很 有名 的 数学家 ， 哈密顿 还是 个 数学 物理学家 ， 是 哈密顿 力学 的 创始人 。 这个 方程 刻画 了 最优 策略 要 满足 的 充分 必要条件 ， 一旦 这个 方程 求解 出来 了 ， 那么 最优 策略 也 就 知道 了 。 在 介绍 这个 方程 之前 ， 我们 先 来 介绍 两个 在 强化 学习 中 非常 重要 的 概念 ， 状态值 函数 和 动作 值 函数 ， 熟悉 这 两个 概念 对 以后 的 学习 十分 重要 ， 我们 会 不断 的 碰到 这 两个 概念 。       状态值 函数       当 我们 知道 环境 的 状态 转移 概率 时 ， 那么 很 直接 的 想法 是 查看 四周 ， 找到 最好 的 一个 状态 ， 然后 选择 可以 到达 这个 最好 的 状态 的 动作 。 例如 在 状态 8 的 时候 ， 周围 的 状态 只有 3 个 ， 12 是 火坑 ， 5 和 9 是 普通 的 状态 ， 显然 12 这个 状态 不能 跳 ， 状态 9 看起来 比 5 好 ， 因为 它 离 钻石 更近 一些 。 但是 在 复杂 的 问题 中 ， 我们 很 难用 这样 一个 简单 的 法则 来 表达 状态 是 好 还是 坏 ， 因为 在 复杂 的 问题 中 ， 随机性 可能 比较 大 ， 最 短距离 可能 并 不是 最佳 的 。 例如 状态 6 比 2 到达 钻石 的 距离 短 ， 但是 如果 状态 6 到达 10 的 转移 概率 很 低 很 低 ， 那么 状态 2 可能 比 6 更好 。 这 表明 最短 路径 并 不是 一个 很 好 的 评估 一个 状态 指标 。 那么 如何 精确 评估 一个 状态 好 还是 坏 呢 ？ 答案 就是 状态值 函数 。       定义 ： 状态值 函数 V ( S ) 是从 状态 S 出发 ， 所 能 获得 的 最大 期望 回报 ！       $ $     V ( S )   =   \ \ max   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S   \ \ right ]     $ $       我们 假设 环境 是 确定性 的 ， 没有 随机性 ， 这样 可以 省去 求 期望 的 步骤 ， 便于 理解 状态值 函数 。 对于 终止 状态 ， 我们 定义 它们 的 状态值 函数 值恒 等于 0 。 假设 折扣 因子 等于 1 ， 每 一步 跳转 的 回报 函数 都 为 - 0.1 ， 仍然 假定 环境 是 确定 的 。 我们 来看 一下 从 状态 13 出发 ， 可能 到达 的 状态 有 3 个 ， 分别 是 12 、 9 、 14 ， 我们 来 计算 一下 这 三个 状态 的 状态值 函数 。 因为 12 是 火坑 ， 是 终止 状态 ， 所以 V ( 12 ) = 0 。 对于 状态 9 ， 最快 可以 通过 3 步 到达 钻石 ， 获得 的 总 回报 是 V ( 9 ) = - 0.1 - 0.1 + 1 = 0.8 。 而 状态 14 最快 一步 就 能 到达 钻石 ， 所以 总 回报 就是 这 一步 的 回报 V ( 14 ) = 1 . 很 明显 ， V ( 14 ) & gt ; V ( 9 ) & gt ; V ( 12 ) ， 所以 这 3 个 状态 中 ， 状态 14 最好 ， 这 跟 我们 的 直觉 一致 。   这 表明 状态值 函数 可以 评估 两个 不同 状态 的 好坏   。               状态值 函数 的 求 最大值 操作 表明 采用 的 是 最优 策略 $ \ \ pi ^   *   $ 下 的 总 折扣 回报 ， 对于 某个 具体 的 策略 $ \ \ pi $ ， 还 可以 定义 策略 值 函数 为 ： 从 状态 S 出发 ， 采用 策略 $ \ \ pi $ 选择 动作 ， 所 能 获得 的 期望 回报 ！       $ $     V ^ { \ \ pi } ( S )   =   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ t   \ \ sim   \ \ pi   \ \ right ]     $ $       $ V ^ { \ \ pi } ( S ) $ 和 $ V ( S ) $ 都 是从 状态 S 出发 获得 的 最大 期望 回报 ， 不同 的 是 $ V ^ { \ \ pi } ( S ) $ 要求 后面 采取 的 策略 是 给定 的 策略 $ \ \ pi $ ， 而 $ V ( S ) $ 要求 的 是 最好 的 策略 。 假设 从 状态 S = 13 出发 ， 策略 $ \ \ pi _ 1 $ 是 一直 往上走 ， 如果 上面 不能 走 就 往右 走 （ 下图 绿色 箭头 ） 。 策略 $ \ \ pi _ 2 $ 是 一直 往右 走 ， 右边 走 不了 就 往下走 ( 下图 黄色 箭头 ) 。 在 策略 1 下 ， 机器人 会 跳 到 火坑 里面 （ 这里 还是 认为 是 确定性 环境 ， 每 一步 回报 为 - 0.1 ， 火坑 回报 - 1 ， 钻石 回报 + 1 ） ， 所以 $ V ^ { \ \ pi _ 1 } ( 13 ) = - 1.2 $ ， 而 在 策略 2 下 ， 机器人 将 找到 钻石 ， 所以 $ V ^ { \ \ pi _ 2 } ( 13 ) = 0.9 $ 。 显然 在 状态 13 下 ， 策略 2 更好 ， 对应 的 策略 值 函数 也 越 大 。 因此 ，   策略 值 函数 $ V ^ { \ \ pi } ( S ) $ 可以 用来 评估 一个 策略 好不好   。               状态值 函数 $ V ( S ) $ 与 策略 值 函数 $ V ^ { \ \ pi } ( S ) $ 不同 的 是 ， 前者 采用 的 是 最佳 策略 ， 而 后者 采用 的 是 具体 的 某个 策略 $ \ \ pi $ 。 两者 的 关系 是 ， $ V ( S ) $ 是 采用 最佳 策略 $ \ \ pi   ^   *   $ 下 的 策略 值 函数 ， 即 $ V ( S )   =   V ^ { \ \ pi   ^   *   } ( S ) $ 。       动作 值 函数       前面 的 状态值 函数 可以 用来 评估 一个 状态 好不好 ， 那么 在 一个 给定 的 状态 下 ， 怎么 评估 某个 动作 好不好 呢 ？ 答案 是 动作 值 函数 ！       定义 ： 动作 值 函数 Q ( S ,   A ) 是从 状态 S 出发 ， 采用 动作 A 后 ， 所 能 获得 的 最大 期望 回报 ！ 动作 值 函数 也 称 Q 函数 。       $ $     Q ( S ,   A )   =   \ \ max   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ 1 = A   \ \ right ]     $ $               求和 包括 两 部分 ， 第一 部分 是 第一步 采用 动作 A 后 调到 状态 S ' 所 获得 的 单步 回报 $ R ( S ,   A ,   S ' ) $ ， 第二 部分 是从 S ' 开始 采用 最优 策略 所 获得 的 最大 回报 ， 这 一部分 可以 用 状态 S ' 的 值 函数 表示 $ V ( S ' ) $ 。 所以 总 回报 为 $ R ( S ,   A ,   S ' )   +   \ \ gamma   V ( S ' ) $ 。 考虑 环境 的 随机性 后 ， 需要 对 所有 的 可能 状态 S ' 求 期望 ， 于是 有 如下 关系       $ $     Q ( S ,   A )   =   \ \ sum _ { s ' }   P ( s ' |   S ,   A ) [   R ( S ,   A ,   s ' )   +   \ \ gamma   V ( s ' )   ]     $ $       以 确定性 环境 为例 （ 如上图 所示 ） ， 设 单步 回报 为 - 0.1 ， 在 初始状态 S 时 ， 可以 有 3 个 动作 ， 分别 是 上 、 左 、 右 ， 当 第一步 采用 动作 上后 ， 所 获得 的 回报 包括 两 部分 ， 第一 部分 是 执行 动作 上后 调到 状态 S ' 的 回报 R ( S , 上 , S ' ) ， 第二 部分 是从 S ' 开始 采用 最优 策略 所 获得 的 回报 ， 这 正是 状态值 函数 的 定义 ， 所以 这部分 回报 是 V ( S ' ) 。 综上 可 得 动作 值 函数 $ Q ( S , 上 )   =   R ( S , 上 , S ' )   +   \ \ gamma   V ( S ' ) $ 。       根据 值 函数 的 定义 ， 对 初始状态 S ， 其值 函数 V ( S ) 是 之后 每 一步 都 采用 最优 策略 带来 的 总 回报 期望值 。 而 动作 值 函数 Q ( S ,   A ) 要求 第一步 必须 采用 动作 A ， 从 第二步 开始 才 采用 最优 策略 ， 如果 第一步 采用 的 动作 A 就是 最优 策略 的 动作 ， 那么 动作 值 函数 就 等于 状态值 函数 ！ 也 就是       $ $     V ( S )   =   \ \ max _ a   Q ( S ,   a )     $ $       并且 可以 得到 最优 动作 $ A ^   *   =   \ \ arg \ \ max _ a   Q ( S ,   a ) $ 。               这 两个 关系 表明 ， 在 环境 已知 的 情况 下 ， 即 转移 概率 P 和 回报 函数 R 已知 ， Q 和 V 可以 互相转化 ， 只 要求 出 其中 一个 ， 另 一个 也 就 求 出来 了 。       同理 可以 定义 在 具体 策略 $ \ \ pi $ 下 的 动作 值 函数 为 ： 从 状态 S 出发 ， 采用 动作 A 后 ， 然后 在 以后 的 决策 中 采用 策略 $ \ \ pi $ 选择 动作 ， 所 能 获得 的 期望 回报 ！       $ $     Q ^ { \ \ pi } ( S ,   A )   =   E   \ \ left [   \ \ sum _ { t = 1 } ^ { \ \ infty }   \ \ gamma ^ { t - 1 }   R ( S _ t ,   A _ t ,   S '   _   t )   |   S _ 1 = S ,   A _ 1 = A ,   A _ t   \ \ sim   \ \ pi   \ \ right ]     $ $       与 动作 值 函数 的 差别 在于 从 第二步 开始 ， 动作 按照 策略 $ \ \ pi $ 给出 而 不是 最优 策略 。       HJB 方程       利用 动作 值 函数 可以 评估 一个 动作 的 好坏 ， 给出 最优 策略 。 因此 ， 如果 我们 能够 求出 Q 函数 ， 那么 最优 策略 也 就 出来 了 ； 如果 能够 求 出 状态值 函数 ， 根据 前面 这 两个 值 函数 的 关系 ， 也 可以 得到 Q 函数 ， 从而 得到 最优 策略 。 那么 怎么 求 出 状态值 函数 呢 ？ 答案 是 利用 值 函数 的 递推 关系 ， 构建 HJB 方程 。       利用 状态值 函数 和 动作 值 函数 的 两个 关系 可 得       $ $     \ \ begin { align }     V ( s )   & amp ; =   \ \ max _ a   Q ( s ,   a )   \ \ \ \               & amp ; =   \ \ max _ a   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )   ]     \ \ end { align }     $ $       最后 一个 式子 就是 HJB 方程 ， 也 有 叫 贝尔曼 方程 的 。 这个 式子 中 ， 未知 的 是 状态值 函数 V ( s ) ， 已知 的 是 环境 的 状态 转移 概率 P 和 回报 R ， 由于 存在 求 max 操作 ， 使得 它 是 关于 状态值 函数 的 非线性 方程 ！ 如果 我们 从 这个 方程 中 求解 出 状态值 函数 V ( s ) ， 那么 最优 策略 的 问题 就 引刃 而解 了 ！       确定性 环境 的 HJB 方程       如果 环境 是 确定性 的 ， 即 采用 某个 动作 后 转移 到 的 状态 是 唯一 的 ， 在 迷宫 的 例子 中 ， 相当于 我 可以 直接 控制 机器人 的 运动 ， 而 不是 通过 语音 来 间接 控制 。 那么 上述 HJB 方程 的 求 期望 步骤 可以 省略 ， HJB 方程 变为       $ $     V ( s )   =   \ \ max _ a   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )     $ $       s ' 是 在 状态 s 下 采用 动作 a 后 转移 到 的 状态 ， 在 确定性 迷宫 问题 中 ， 考虑 s = 14 的 例子 ， HJB 方程 就是说 ( 记住   V ( 15 ) = 0 )       $ $     V ( 14 )   =   \ \ max   \ \ { \ \ gamma   V ( 13 ) ,   \ \ gamma   V ( 10 ) ,   1   \ \ }     $ $       对 每 一个 状态 s ， 都 可以 写出 上述 类似 的 非线性 方程 ， 我们 可以 得到 14 个 这样 的 非线性 方程 构成 的 非 线性方程组 。 那么 怎么 求解 这种 非线性 方程 呢 ？ 可能 有些 对 算法 比较 熟悉 的 同学 已经 从 上述 表达式 看 出来 了 ， 这 就是 动态 规划 ！       动态 规划       采用 动态 规划 求解 HJB 方程 有 两类 方法 ， 分别 是 值 迭代 和 策略 迭代 。 在 一定 条件 下 ， 他们 都 能 收敛 到 最优 解 。       值 迭代       值 迭代 的 基本 思想 是 ， 将 HJB 方程 看做 如下 函数 的 不动点       $ $     f ( V )   =   \ \ max _ a   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ( s ' )   ]     $ $       V 是 一个 向量 ， 包含 多个 元素 ， 这个 函数 是 一个多 变量 非线性 函数 。 函数 f 的 不动点 是 指 满足 方程 $ f ( x )   =   x $ 的 解 。 根据   压缩 映像 原理   ， 如果 函数 f 是 压缩 映象 ， 那么 对 任何 初始值 $ V _ 0 $ ， 可以 不断 地 应用 函数 f 迭代 下去 $ V _ { k + 1 }   =   f ( V _ { k } ) $ ， $ V _ k $ 必 收敛 于 函数 f 的 不动点 ！ 而 f 的 不动点 就是 满足 HJB 方程 的 状态值 函数 ， 所以 有值 迭代 算法           初始化 $ V _ 0 ( i ) = 0 ,   i = 1 , 2 , ... $       利用 HJB 方程 迭代   $ V _ { k + 1 } ( s )   =   \ \ max _ a   R ( s ,   a ,   s ' )   +   \ \ gamma   V _ { k } ( s ' ) $       重复 第 2 步 直到 收敛 ！           当 $ \ \ gamma & lt ; 1 $ 时 ， 函数 f 一定 是 压缩 映像 ， 上述 算法 必 收敛 于 不动点 ！ 不动点 算法 也 是 一个 常用 的 求解 非线性 方程 或 线性方程 的 算法 。 下面 简单 介绍 一下 它 的 原理 。       压缩 映像 原理         注意 ： 理解 本 部分 需要 本科 及 以上 数学 功底 ， 高中数学 能力 的 请 跳 过 ， 接受 上述 结论 即可 ！         一个 定义 在 巴拿赫 空间 B 上 的 自 映射 f : B   →   B   是 压缩 映象 是 说 ， 对 任意 两个 $ x ,   y   \ \ in   B $ ， 有       $ $     d (   f ( x ) ,   f ( y )   )       \ \ le   \ \ gamma   d (   x ,   y   ) ,   0   & lt ;   \ \ gamma   & lt ;   1     $ $       d 是 距离 测度 ， 也就是说 在 这个 映射 下 ， 像 的 距离 比原 像 的 距离 短 ， 就 像 在 压缩 一样 ， 所以 叫做 压缩 映像 ( 也 说 映射 ) 。       压缩 映象 有个 不动点 定理 ， 说 如果 f 是 巴拿赫 空间 的 压缩 映象 ， 那么 必 存在 唯一 的 不动点 x 满足 不动点 方程 $ f ( x )   =   x $ 。 存在 性 我 就 不 证明 了 ， 可以 简单 解释一下 为什么 会 收敛 到 这个 不动点 。 假设 $ x _ 0 $ 是 某个 初始 点 ， $ x _ k   =   f ( x _ { k - 1 } ) $ ， $ x ^   *   $ 是 不动点 ， 那么 根据 压缩 映象 的 定义 ， 对 任意 正整数 k 有       $ $     | |   x _ { k }   -   x ^   *   | |   =       | |   f ( x _ { k - 1 } )   -   f ( x ^   * )   | |   \ \ \ \     & lt ;   \ \ gamma   | |   x _ { k - 1 }   -   x ^   *   | |   & lt ;   ...   & lt ;   \ \ gamma ^ k   | | x _ 0   -   x ^   *   | |   \ \ \ \     \ \ rightarrow   0   ( k   \ \ rightarrow   \ \ infty )     $ $       有 兴趣 的 同学 不妨 利用 类似 的 技巧 证明 一下 值 迭代 在 $ \ \ gamma & lt ; 1 $ 是 收敛 的 。       这个 性质 可以 用来 求解 非线性 方程 ， 下面 是 一个 简单 的 计算 $ \ \ sqrt { n } $ 例子 。 计算机 只会 加减乘除 ， 其他 数学 运算 都 要 表示 为 这 四则运算 才能 计算 ， 那么 怎么 计算 $ \ \ sqrt { n } $ 。 计算方法 有 很多 ， 这里 介绍 利用 压缩 映像 不动点 的 性质 的 计算方法 。 $ \ \ sqrt { n } $ 可以 看做 方程 $ x ^ 2   =   2 $ 的 解 ， 这个 方程 等价 于   $ x   =   0.5 ( n / x   +   x ) $ 。 因此 ， x 是 函数 $ f ( x )   =   0.5 ( n / x   +   x ) $ 的 不动点 。 容易 验证 f 是 非线性 函数 ， 且 是 压缩 映像 。 那么 就 可以 令 x0 = 1 ， 不断 地 应用 $ x _ k   =   f ( x _ { k - 1 } ) $ 迭代 下去 就 可以 了 ， 最终 就 会 收敛 到 $ \ \ sqrt { n } $ ！       策略 迭代       策略 迭代 是 另外 一种 求解 HJB 这个 线性方程 的 方法 ， 因为 这个 非线性 方程 的 所有 非线性 来自 于求 最大值 ， 前面 说 过 ， 如果 我们 把 策略 固定 ， HJB 方程 的 求 最大值 就 消失 了 ， 变成 线性方程 ！       $ $     V ^ { \ \ pi } ( s )   =   \ \ sum _ { s ' }   P ( s ' |   s ,   a ) [   R ( s ,   a ,   s ' )   +   \ \ gamma   V ^ { \ \ pi } ( s ' )   ]     $ $       线性方程 很 容易 解决 ， 从 小学 就 开始 学了 ， 不停 地 代换 消元 即可 ， 这 就是 高斯消 元法 。 这个 过程 ， 称作 策略 评估 ， 因为 计算出来 的 是 某个 策略 下 的 值 函数 ， 值 函数 可以 来 评估 策略 的 好坏 ， 所以 叫 策略 评估 。       当 $ V ^ { \ \ pi } $ 计算 好 了 之后 ， 我们 又 可以 通过 它 得到 一个 新 的 策略       $ $     \ \ pi ' ( s )   =   \ \ arg \ \ max _ a   Q ^ { \ \ pi } ( s ,   a )     $ $       因为 Q 函数 和 V 函数 可以 知一求 二 ， 前面 知道 了 V 函数 ， 利用 V 和 Q 的 关系 很 容易 得到 Q ， 然后 就 可以 计算 这个 新 的 策略 了 。 可以 证明 这个 策略 不会 比 之前 的 策略 差 ， 也 就是 对 所有 状态 s ， 两个 策略 的 值 函数 满足 $ V ^ { \ \ pi } ( s )   \ \ le   V ^ { \ \ pi ' } ( s ) $ ， 因此 只要 不断 地 迭代 下去 ， 也 可以 得到 最优 的 策略 ！ 这 一步 叫做 策略 提升 ， 因为 新 的 策略 效果 提升 啦 ！       综合 这 两步 ， 我们 就 得到 策略 迭代 算法 ：           初始化 一个 策略 $ \ \ pi $       通过 解 线性方程 计算 策略 $ \ \ pi $ 的 值 函数 V 和 Q       从 Q 函数 中 得到 新 的 策略 ， 更新 到 $ \ \ pi $ 中       重复   2 - 3   直到 策略 收敛 ！           两种 迭代法 的 对比       这 两种 迭代法   本质 都 是 在 求解 HJB 方程   ， 得到 值 函数 的 值 ， 因为 状态 是 有限 的 ， 所以 值 函数 就是 一个 向量 。 得到 值 函数 之后 ， 就 可以 得到 最优 策略 了 。 值 迭代 只有 一个 迭代 ， 反复 使用 HJB 方程 进行 迭代 ， 直到 收敛 就 可以 了 。 而 策略 迭代 先 固定 策略 ， 接 线性方程 得到 值 函数 ， 然后 利用 值 函数 来 提升 策略 ， 这 两个 步骤 反复 迭代 ， 直到 找到 最优 策略 。 策略 迭代 通常 的 迭代 次数 会 比值 迭代 要少 ， 但是 内部 接 线性方程 耗时 会 比较 多 ， 两种 迭代 方法 都 在 一定 条件 下 可以 收敛 到 最佳 策略 。 在 实践 部分 ， 我们 将 实现 这 两个 算法 ， 初始 代码 和 环境 已经 准备 好 了 ， 你 只 需要 是 想 这 两个 算法 就行 。       环境 未知 的 问题       前面 我们 讲 到 ， 如果 环境 已知 ， 也 就是 转移 概率 P 和 回报 函数 R 已知 ， 我们 可以 通过 求解 HJB 方程 得到 值 函数 ， 进而 得到 最优 策略 。 但是 如果 我们 不 知道 环境 会 对 我们 做出 如何 反馈 ， 就 像 身 在 迷宫 中 的 机器人 ， 看不到 全貌 。 那么 我们 该 如何 得到 值 函数 和 最优 策略 呢 ？ 答案 是 通过 蒙特卡罗 模拟 ， 估计 出 环境 的 转移 概率 P 和 回报 函数 R 。 因为 状态 是 有限 的 ， 动作 也 是 有限 的 ， 所以 只要 用 很多 个 机器人 采用 完全 随机 的 策略 进行 尝试 ， 那么 根据 尝试 的 结果 ， 可以 估计 出 转移 概率 和 回报 函数 。 假设 随机 尝试 了 很多很多 次 ， 每 一次 采取 动作 都 会 得到 一个 四元组 ( S ' ,   A ' ,   S ' ,   r ) 。 例如 在 迷宫 问题 中 ， 从 动作 1 开始 ， 采用 向 右 ( A = 4 ) ， 到达 状态 2 ， 环境 回报 为 - 1 . 那么 这个 四元组 就是 ( 1 , 4 , 2 , - 1 ) 。 当 得到 很多 这样 的 四元组 后 ， 就 可以 统计 每 一对 ( S ,   A ) 转移 到 S ' 的 次数 N ( S ,   A ,   S ' ) 和 遇到 的 所有 ( S ,   A ) 的 次数 N ( S ,   A ) ， 从而 得到 概率 和 回报       $ $     P ( S ' | S ,   A )   =   \ \ frac {   N ( S ,   A ,   S ' )   } { \ \ sum _ { s ' }   N ( S ,   A ,   s ' ) }   \ \ \ \     R ( S ,   A ,   S ' )   =   r     $ $       例如 ， 从 状态 1 出发 ， 采用 动作 4 ( 向 右 ) ， 有 90 次 转移 到 了 状态 2 ， 有 10 次 转移 到 了 状态 5 ， 所以 可以 估计 出 P ( 1 , 4 , 2 ) = 0.9 ,   P ( 1 , 4 , 5 ) = 0.1 ,   P ( 1 , 4 ,   其他 状态 ) = 0 。 这些 可能 的 状态 转移 带来 的 回报 都 是 - 1 ， 所以 R ( 1 , 4 , 所有 状态 ) = - 1 。       一旦 我们 通过 上述 模拟 方法 得到 对 环境 的 估计 ， 那么 就 可以 采用 上述 动态 规划 方法 求解 出值 函数 ， 进而 得到 最优 策略 ！       强化 学习 的 解释       强化 学习 和 传统 的 监督 学习 方式 有所不同 ， 它 没有 一个 很强 的 监督 信号 告诉 模型 ， 要 拟合 一个 什么样 的 函数 ？ 也 不 像 无 监督 学习 完全 没有 反馈 ， 强化 学习 有 一个 弱 的 反馈 信号 告诉 你 ， 你 采取 的 动作 是 对 的 还是 错 的 ， 就 像 一只 被 训练 接 铁饼 的 小狗 ， 如果 它 不 接住 铁饼 ， 将会 受到 一个 惩罚 ， 相反 ， 如果 它 接住 了 铁饼 ， 或者 主动 去 追 铁饼 ， 将会 收到 一个 奖励 。 这种 反馈 机制 ， 让 小狗 虽然 一 开始 不 知道 要 干什么 ， 但是 经过 不断 的 尝试 - 失败 ， 不断 地 探索 ， 它 将 最终 知道 自己 要 追逐 的 目标 是 什么 。 而 监督 学习 就是 有 一个 老师 ， 先 示范 一下 告诉 小狗 要 做 什么 ， 然后 让 小狗 跟着 做 。 无 监督 学习 则 既 没有 老师 ， 也 没有 反馈 ， 所以 小狗 也 不 知道 要 干 啥 ， 它 只会 做 自己 熟悉 的 事情 。 从 这个 角度 来看 ， 强化 学习 是 在 无 监督 学习 和 监督 学习 中间 的 一种 半 监督 学习 。       从 实现 来看 ， 强化 学习 与 监督 学习 不同 的 是 ， 它 有 动作 ！ 也 就是 有 主观 能动性 ， 监督 学习 则 是 被动 地 接受 老师 教给 的 知识 ， 没有 充分利用 主官 能动性 。 监督 学习 从 一 开始 就 知道 学习 的 目标 精确 的 是 什么 ， 比如 做 人脸识别 ， 你 一 开始 就 知道 这张 图片 是不是 人脸 ， 模型 要 做 的 就是 建立 一个 函数 输出 是 人脸 还 是不是 人脸 。 而 强化 学习 是 要 建模 策略 函数 ， 在 一 开始 是 不 知道 哪个 策略 是 对 的 还是 错 的 ， 只有 尝试 之后 ， 收到 环境 的 反馈 后 ， 才 知道 这种 尝试 是否 正确 。 这种 反馈 是 通过 和 环境 交互 得到 的 ， 而 不是 我们 “ 误差 冒泡 ” 的 算法 中 给出 的 。 拿 人脸识别 来说 ， 监督 学习 就是 已经 有 很多 标注 的 人脸 和 非 人脸 照片 ， 你 去 训练 一个 模型 ， 拟合 这个 输入 和 输出 。 而 强化 学习 则 是 你 不 知道 这个 任务 是 干什么 ， 只是 给 你 图片 ， 可以 和 我 互动 ， 我 不会 直接 告诉 你 答案 ， 你 可以 通过观察 我 的 表情 来 得到 一部分 关于 猜 对 还是 猜错 的 信息 。       从 应用 场景 来看 ， 监督 学习 主要 是 来 做 预测 的 ， 而 强化 学习 主要 是 来 做 决策 的 。 监督 学习 可以 先 预测 ， 然后 根据 预测 的 结果 信息 然后 进行 决策 。 而 强化 学习 则 直接 给出 策略 。 因此 从 这种 角度 来看 ， 似乎 强化 学习 更加 直接 。 例如 在 做 搜索 排序 ， 监督 学习 会 拟合 一个 得分 ， 然后 你 根据 这个 得分 从大到 小 排序 ， 或者 在 加上 一些 其他 策略 。 但是 显然 这种 不是 最优 的 ， 比如 得分 很 高 的 都 是 一些 非常 相似 的 东西 ， 都 排 在 前面 是 没什么 意义 的 。 而 强化 学习 直接 拟合 排序 策略 ， 将 监督 学习 人工 决策 的 过程 也 干 了 ， 因此 可能 得到 更好 的 效果 。       强化 学习 与 深度 学习 的 关系 ： 实际上 强化 学习 和 深度 学习 是 对 机器 学习 算法 两个 完全 不同 的 分类 方式 。 强化 学习 是 与 监督 学习 和 无 监督 学习 一个 范畴 的 ， 而 深度 学习 则 是从 学习 的 模型 层面 上 进行 划分 ， 是 与 决策树 、 浅层 模型 一个 范畴 的 。 强化 学习 是 一种 学习 的 通用 方法 ， 不 限制 用 什么 模型 ， 而 深度 学习 时 特指 用 某类 模型 而 没有 说用 什么 学习 方法 。 强化 学习 可以 和 深度 学习 模型 结合 起来 ， 就是 深度 强化 学习 ， AlphaGo 的 创始人 之一 曾 在 ICML 大会 上 说 AI = DL ( 深度 学习 )   +   RL ( 强化 学习 ) ， 可见 强化 学习 和 深度 学习 的 重要 地位 。       强化 学习 的 应用       有 注意力 的 图像识别 系统       在 监督 学习 中 ， 我们 说 到 一个 模型 就是 在 拟合 一个 函数 ， 输入 是 一些 特征 ， 图像 的 数字 表示 就是 它 的 特征 ， 图像 在 计算机 中 都 是 用 很多 数字 表示 ， 如果 是 黑白 图像 ， 每 一个 像素 是 一个 数值 ， 数值 大小 表示 明暗 。 图像识别 就是 在 拟合 这样 一个 函数 ， 输入 是 图像 ， 输出 是 图像 的 类别 。 对于 一张 高清 图片 ， 往往 需要 降低 分辨率 ， 减少 计算 量 ， 或者 分成 很多 子块 ， 进行 分类 ， 然后 再 把 字块 识别 的 结果 融合 起来 。 那么 怎么 选择 子快 呢 ？ 最 简单 的 方法 是 随机 按照 不同 位置 不同 分辨率 选出 很多 子块 ， 一种 更好 的 方案 是 第一个 子块 可以 根据 整个 图像 的 低分辨率 选取 ， 接下来 每次 选取 的 位置 根据 之间 看到 的 所有 子块 共同 决定 ， 这样 每次 选择 哪个 字块 就可以看 做 马尔科夫 决策 过程 。 状态 ( S ) 是 我 之前 看过 的 所有 图片 ， 动作 ( A ) 是 我 选择 的 字块 的 位置 （ 假设 字块 的 大小 被 固定 ） ， 回报 ( R ) 是 我 最终 的 识别 是否 准确 ， 识别 对 了 回报 为 1 ， 否则 为 0 。 这 就是 Google 在 2014 年 再 论文   Recurrent   Models   of   Visual   Attention   中 所用 的 方案 。 他 把 每次 查看 的 字块 的 机制 叫做 注意力 机制 ， 就 像 我们 人来 识别 图片 中 的 内容 时 ， 目光 的 焦点 会 在 图片 中 不断 跳动 ， 每 一次 跳动 就可以看 做 我们 的 一次 决策 过程 。             < ! - -   ###   超越 经验 的 推荐 算法     ###   阿里巴巴 鲁班 系统           ###   AlphaGo     ###   AI 玩游戏     - - >       思考 与 实践           考虑 如下 图 所示 的 MDP 问题 ， S 是 初始状态 ， G 是 终止 状态 ， 对于 非 终止 状态 ， 每个 状态 可以 采取 两个 动作 ： 左 或者 右 ， 每 一步 的 回报 都 是 - 1 。 其中 中间 有 一个 状态 ， 采取 的 动作 和 实际 运动 方向 是 相反 的 ， 也 就是 动作 是 向 左 ， 而 实际 运动 是 向 右 ， 其他 两个 状态 正常 。 一个 随机 策略 $ \ \ pi $ 定义 如下 ： 每 一次 都 随机 地以 概率 p 选择 动作 “ 右 ” ， 以 概率 ( 1 - p ) 选择 动作 “ 左 ” 。 试 推导 $ V ^ { \ \ pi } ( S ) $ 的 最大值 和 此时 的 概率 p 的 值 。 参考答案 ： p = 0.59 ,   V = - 11.6 。                       编程 实现 值 迭代 和 策略 迭代 算法           构建 一个 简单 的 模拟 环境 ， 有 nS 个 状态 ， 0 ， 1 ， ... ， nS - 1 ； 其中 nS - 1 是 终止 状态 。 该 环境 下 一共 两个 动作 ： 0 向 左 运动 ， 1 向 右 运动 ， 每个 动作 都 有 概率 p0 不动 ， p1 的 概率 会往 反 方向 运动 ,   1 - p0 - p1 概率 正常 运动 。                   0       1       2       ...       9                       ← · →       ← · →       ← · →       ← · →       终点                           import       numpy       as       np         nS       =       10       nA       =       2         # 不要 改 这个 参数       Done       =       nS       -       1       p0       =       0.1       p1       =       0.1       P       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       #   转移 概率       R       =       np     .     zeros     ( (     nS     ,       nA     ,       nS     ) )       -       1.0       #   回报 都 是 - 1       gamma       =       1         #   环境 构建       for       s       in       range     (     nS     ) :               if       s       = =       Done     :       #   终止 态 转移 概率 都 为 0                       continue               for       a       in       range     (     nA     ) :                       inc       =       a       *       2       -       1       #   步长                                       P     [     s     ,       a     ,       s     ]       + =       p0       #   不 动                       P     [     s     ,       a     ,       max     (     0     ,       s       -       inc     ) ]       + =       p1       #   反 方向                       P     [     s     ,       a     ,       max     (     0     ,       s       +       inc     ) ]       + =       1       -       p0       -       p1       #   正常 运动             V       =       np     .     zeros     (     nS     )       #   值 迭代       for       it       in       range     (     1000     ) :                 ##   YOUR   CODE   HERE                 ##   END               pass         print       &# 39 ; iteral   steps : &# 39 ;     ,       it       print       V                 #   策略 迭代         pi       =       np     .     zeros     (     nS     ,       dtype     =     int     )       # 初始 策略 全部 往 左         for       it       in       range     (     100     ) :               V       =       np     .     zeros     (     nS     )               #   策略 评估 ， 解 线性方程                 ##   YOUR   CODE   HERE                 ##   END                 #   策略 提升                 ##   YOUR   CODE   HERE                 ##   END         print       &# 39 ; pi   = &# 39 ;     ,       pi       print       &# 39 ; V   = &# 39 ;     ,       V        ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/basic-rl.html"},
      
      
      {"title": "第11.0讲：时间差分学习", "text": "    Table   of   Contents           关于           蒙特卡罗 方法                 关于       强化 学习 近年 大火 ， 最早 是因为 AlphaGo 使用 强化 学习 打败 人类 围棋 冠军 引发 的 。 在 那 之后 ， 强化 学习 在 工业 场景 应用 越来越 多 ， 原来 很多 做 搜索 、 推荐 、 广告 等 一直 在 用 监督 学习 的 业务 ， 也 开始 使用 强化 学习 来 优化 用户 体验 和 平台 收益 了 。 强化 学习 实际上 很 早就 提出 了 ， 事实上 ， 强化 学习 来源于 控制论 。 控制论 之 父 叫做   维纳   ， 学过 信号处理 的 可能 知道 他 ， 维纳滤波 就是 用 他 的 名字 命名 的 。 控制论 最早 源于 航天 ， 人们 要 控制 火箭 发射装置 ， 将 火箭 发射 到 地球 之外 ； 控制论 还 源于 机器人 控制 ， 人们 需要 用 算法 对 机器人 的 行为 进行 控制 。 所以 ， 很多 讲 强化 学习 的 文献 也 会 说 强化 学习 是 在 优化 控制策略 。       在 这 篇文章 中 ， 您 将 学习 到           环境 未知 情况 下 的 更好 的 学习 方法 ？       蒙特卡罗 方法 和 时间 差分 学习       SARSA ,   Q - learning       on - policy   与   off - policy   学习           我 期望 您 至少 有 ：           高中数学 水平 且 年满 18 岁 ， 部分 内容 需要 你 了解 监督 学习 ， 和 强化 学习 基本概念 ， 你 可以 通过 本 教程 前面 的 章节 进行 学习 。       如果 你 需要 完成 实践 部分 ， 需要 有 基本 的   python   知识 ， 你 可以 通过   python 快速 入门   快速 了解 python 如何 使用 。           蒙特卡罗 方法       在 上 一 讲 中 ， 我们 说 到 ， 对于 一个 MDP 问题 ， 如果 环境 已知 ， 也 就是 知道 环境 的 转移 概率 P 和 回报 函数 R ， 可以 通过 求解 贝尔曼 方程 得到 值 函数 ， 从而 得到 最优 策略 ， 求解 的 方法 有 两种 ， 分别 是 值 迭代 和 策略 迭代 。 如果 环境 未知 ， 我们 提到 一种 通过 随机 尝试 的 方法 ， 先 估计 出 环境 ， 然后 转化 为 环境 已知 问题 求解 。 这种 随机 尝试 效率 很差 ， 需要 尝试 很 多次 ， 并且 随机 尝试 本身 就 有 成本 ， 而且 需要 等到 尝试 很 多次 之后 ， 才能 得到 一个 比较 好 的 策略 。 那么 ， 能 不能 在 每 一次 尝试 之后 ， 都 能 将 策略 提升 到 一个 更好 的 策略 呢 ？ 因为 每 一次 尝试 ， 环境 的 反馈 都 会 提供 一部分 对 环境 的 信息 ， 如果 我们 能 利用 好 这部分 信息 ， 就 有 可能 从中 获取 到 有 价值 的 信息 来 提升 我们 的 策略 。       还是 以 走 迷宫 为例 （ 如图所示 ） ， 并且 考虑 确定性 迷宫 这种 简单 情况 。 我们 的 目标 是 估计 出 每个 状态 s 下 ， 采取 每 一个 动作 a 的 动作 值 函数 Q ， 即 从 s 出发 ， 第一步 采取 动作 a 所能 获得 的 最大 回报 。 那么 ， 一个 简单 的 想法 是 ， 从 某个 状态 s 出发 ， 例如 状态 1 ， 如果 某 一次 行动 ， 采用 动作 a = 向 右 获得 了 钻石 ， 那么 动作 a = 向 右 更 有 可能 有 较大 的 Q ； 相反 如果 a = 向下 掉 到 了 火坑 ， 那么 动作 a = 向下 更 有 可能 有 较 小 的 Q 。 这 表明 ， 从 某 一次 的 尝试 中 ， 已经 能够 获得 一些 关于 环境 的 信息 ， 虽然 信息量 没有 多到 足以 完全 确定 最优 策略 的 地步 ， 但是 这个 信息 已经 可以 用来 更新 我们 的 策略 了 。 例如 ， 在 以后 的 尝试 中 ， 可以 给 动作 a = 向 右 以 更 大 的 概率 ， 而 给 动作 a = 向下 以 更 小 的 概率 。                     为什么 估计 动作 值 函数 而 不是 状态值 函数 ？         我们 的 目标 是 得到 动作 值 函数 Q ( s ,   a ) 而 不是 状态值 函数 V ( s ) ， 因为 只有 状态值 函数 V ( s ) 的 情况 下 ， 我们 还 需要 知道 转移 概率 P 才能 得到 动作 值 函数 ， 反过来 则 简单 得 多 。 在 环境 未知 ， 也 就是 转移 概率 你 无法 知道 ， 所以 要 估计 动作 值 函数           如果 我们 从 状态 s 出发 ， 采用 动作 a 尝试 了 n 次 ， 每 一次 的 总 回报 为 $ G _ i $ ， 那么 容易 估计 出 期望 回报       $ $     Q _ n ( s ,   a )   =   \ \ frac { 1 } { n } \ \ sum _ { i = 1 } ^ n   G _ i     $ $       想象 一下 ， 这 n 次 行动 是 一次 执行 的 ， 上 式 需要 这 n 次 尝试 完全 结束 后 ， 才能 估计 Q ， 事实上 可以 把 上式 改写 为 递归 形式       $ $     Q _ n ( s ,   a )   =   \ \ frac { 1 } { n } ( G _ n   +   ( n - 1 )   Q _ { n - 1 } ( s ,   a ) )   \ \ \ \     =   Q _ { n - 1 } ( s ,   a )   +   \ \ frac { 1 } { n } \ \ left (   G _ n   -   Q _ { n - 1 } ( s ,   a )   \ \ right )     $ $       也 就 说 ， 每 一次 尝试 实际 得到 的 回报 可以 用来 调整 之前 对 Q 函数 的 估计 ， 每 一次 对 Q 函数 的 改变 就是 上 一次 估计 的 误差 $ G _ n   -   Q _ { n - 1 } ( s ,   a ) $ 乘 上 一个 系数 $ \ \ alpha   =   \ \ frac { 1 } { n } $ 。       这种 利用 经验 从 未知 环境 中 估计值 函数 的 方法 叫做 蒙特卡罗 法 。               梯度 下降              ", "tags": "tutorial/ml", "url": "/wiki/tutorial/ml/td-learning.html"},
      
      
      
        
        
      
      {"title": "第0讲: 关于《Python教程》", "text": "    Table   of   Contents               Python 据说 已经 列入 中学 信息学 课程 中 ， 加上 人工智能 ， python 已经 成为 目前 仅次于 Java 的 编程语言 了 。 Python 简单 方便 ， 但是 对于 很多 人 ， 入门 还是 比较 困难 ， 所以 写 了 这样 一个 简单 的 教程 ， 帮助 大家 入门 Python 。  ", "tags": "tutorial/python", "url": "/wiki/tutorial/python/about.html"},
      
      
      {"title": "第1.0讲：Python初探", "text": "    Table   of   Contents           关于           为什么 编程           机器 适合 做 什么                   python 的 第一个 程序           安装 本地 python 环境           WINDOWS                   运行 python 程序 的 方法           命令行                         关于       如果 你 没有 什么 编程 基础 ， 想 学习 python 编程 ， 那么 本 教程 很 适合 你 。       为什么 编程       初次 听到 编程 的 时候 ， 我 认为 是 一件 很 高端 很 困难 的 事情 ， 相信 很多 人 都 有 类似 的 感受 。 编程语言 是 什么 ？ 人 与 人 之间 通过 语言 和 文字 进行 交流 ， 我 把 我 的 想法 通过 语言 告诉 你 ， 或者 通过 文字 告诉 你 。 但是 ， 一台 机器 ， 我们 怎么 把 自己 的 想法 告诉 他 呢 ？ 到 目前为止 ， 能够 直接 理解 自然语言 的 机器 尚未 制造 出来 ， 一些 可以 理解 自然语言 的 机器 最终 也 是 利用 编程语言 将 处理 自然语言 的 逻辑 告诉 机器 。 所以 ， 编程语言 是 目前 能够 直接 和 计算机 进行 交流 最 重要 的 方式 。       机器 适合 做 什么       设想 你 正在 浏览 一个 图片 类 的 网站 ， 你 想 把 页面 上 的 所有 图片 都 保存 下来 ， 或者 你 想 把 整个 网站 所有 的 图片 都 下载 下来 。 显然 一个 一个 用 鼠标 点击 下载 十分 费时费力 ， 那么 我们 能 不能 告诉 机器 自动 帮 我 把 所有 图片 都 下载 下来 呢 ？ 显然 目前 通过 自然语言 是 做 不到 的 ， 如果 你 对 他 说 ， “ 嘿 ， 快 帮 我 把 这个 网站 上 的 图片 都 下 下来 ！ ” ， 它 是 不会 理睬 你 的 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       设想 你 的 老板 要 你 统计 每天 公司 内 的 业务 报表 ， 并 把 分析 结果 发送 邮件 给 他 。 分析 过程 是 如此 的 程序化 ， 每 一天 做 得 事情 都 是 如此 的 相似 。 你 感叹 道 ： “ 为啥 让 我 天天 做 一些 没有 技术含量 的 事情 ！ ” 这是 ， 你 想象 如果 电脑 每天 能够 自动 把 这事 给 干 了 ， 就 可以 喝 喝茶 聊聊天 就 把 工作 给 完成 了 该 多 好 。 但是 ， 机器 仍然 不会 理睬 你 ， 它 不 懂用 意念 交流 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       设想 你 在 毕业 季 寻找 工作 ， 每天 都 关注 学校 BBS 或者 就业 论坛 中新 的 招聘会 信息 和 内 推 信息 。 为了 不错 过 每 一次 机会 ， 你 不得不 每隔 半小时 刷 一下 论坛 ， 有时候 一不小心 忘记 了 ， 就 把 一个 好 机会 给 偷偷 错过 了 ， 十分 懊恼 。 这时 ， 你 想 如果 有 新 的 信息 机器 就 能 直接 通知 你 就 好 了 。 但是 ， 机器 仍然 不会 理睬 你 ， 它 不 懂用 电话 交流 。 你 需要 通过 一种 他 能 理解 的 语言 告诉 它 ， 它 才 知道 要 干 啥 ， 这种 语言 就是 编程语言 。       实际上 ， 编程 未来 将 成为 每 一个 人 的 基本技能 ， 就 像 现在 的 英语 一样 。 你 不 需要 深入 理解 计算机 、 理解 逻辑 门 ， 只 需要 熟悉 一门 语言 ， 就 可以 操纵 机器 为所欲为 ， 为什么 不 试试 python 呢 ？       python 的 第一个 程序       编程语言 是 用来 和 机器 交流 的 ， 那么 我们 先来 让 机器 给 我们 打个招呼 如何 ？ 如果 我们 跟 机器 用 自然语言 说 ： “ Hello ， 机器 ！ ” ， 它 听不懂 ， 但是 我们 可以 用 python 让 机器 跟 我们 打个招呼 ： “ Hello ,   World ! ” 。 用 Python 让 机器 输出 \" Hello ,   World ! \" 非常简单 ， 你 只 需写 上 一条 语句 即可 ！               print     (     & quot ; Hello ,   World ! & quot ;     )                       Hello ,   World !               你 可以 通过 在线 python 运行 环境 ， 先 动手 试试 ， 在 后面 将 告诉 你 怎么 在 本地 安装 python 环境 。 你 可以 试着 修改 双引号 里面 的 内容 ， 替换成 任何 你 想 让 机器 说 的话 。 赶紧 动手 试试 吧 。         runoob 的 python 在线 运行 环境         安装 本地 python 环境       安装 python 环境 的 步骤 取决于 你 的 操作系统 ， 对 不同 的 系统 我 将 分别 介绍       WINDOWS       首先 去 python 官方网站 下载安装 包 ：   https : / / www . python . org / downloads /   。 目前 python 有 2.7 和 3.6 两个 版本 ， 这 两个 版本 都 可以 ， 一般 下载 2.7 的 版本 即可 。               下载 下来 ， 按照 正常 的 软件 安装 即可 。       运行 python 程序 的 方法       命令行       用 记事本 或者 将 python 代码 文件 保存 到 一个 文本文件 ，  ", "tags": "tutorial/python", "url": "/wiki/tutorial/python/intro.html"},
      
      
      
      
      
      
        
      
      {"title": "Flink", "text": "    Table   of   Contents           关于           主要 内容                 关于             https : / / www . infoq . cn / article / saL - HR2JTSBHObfscdiW         淘宝 用户 增长 的   5 + 1   个 策略           主要 内容           淘宝 用户 增长 ：       用户 增长 公式 ： MAU 拆解 。 MAUt   =   新增 用户   +   MAUt - 1   *   ( 1 - lambda )   +   流失 用户   *   r       新增 用户 ： 纯 增量       活跃 用户 留存       流失 用户 召回               三纵两横       智能 投放     三方 流量 ， 即 站 外 流量     目标 ： 高质量 MAU 增量     传统 的 广告投放 ： 用户 请求   - & gt ;   广告主 竞价   - & gt ;   决定 是否 投放     在 传统 的 广告投放 基础 上 加入 用户 点击 广告 后 的 跟踪 反馈 信号 （ 例如 转化 等 特定 行为 ） ， 反馈 信号 由 广告主 反馈 给 平台 用于 优化 平台 的 投放 策略   OCPx ， OCPC     流量 筛选 ， 筛选 优质 流量 ， 降低 渠道 成本     对 人 的 认识 。 识别 增长 的 目标 用户 ， 什么 是 目标 用户 ？ 高 LTV 的 用户 ， LTV 建模     对 素材 的 认识 。 高效 的 素材 个性化 推荐 ， 怎样才能 ？ 提升 素材 的 曝光 效率 ， 一个 基本 条件 就是 素材 数量 。 海量 素材 ， 素材 生成 ， 海量 的 元素 （ 商品 、 品牌 ） ， 经验丰富 的 设计师 = & gt ; 高效率 的 的 模板 ； 高效 的 NLP 算法 = & gt ; 高效 的 文案 ；   = & gt ;   海量 素材     素材 筛选 ： 高效 的 个性化 推荐 算法 ， u2anything     对于 允许 广告主 自助 推荐 的 渠道 接入 个性化 推荐 算法 ， 曝光 点击率 10 % +     对于 不 允许 自主 推荐 的 渠道 ， 利用 海量 的 素材 赋能 渠道 推荐 系统 。 简单 来说 就是 加 海量 物料     流量 分发     素材 推荐   & amp ;   素材 模型 优化 ？     矩阵 优势     二方 引流 ： 利用 自己 平台 上 的 多个 产品 ， 互相 引流 ， 形成 流量 矩阵     拉承 一体     触达 后 的 引入 是 优化 空间 最大 的 环节     登录 率 优化     收口 流量 ： 统一 数据 、 统一 行为     用户 信息 总线 服务 ， 将 所有 行为 （ 站外 行为 ， 比如 看到 一个 券 ） 写入 总线 ， 在 需要 的 时候 进行 消费 （ APP 内 首页 也 能 看到 这个 券 ） ， 对 用户 进行 引导     长 周期 运营     长 生命周期 运营 ： 以 数据 为 基础 ， 结合 算法 的 能力 ， 驱动 用户 成长 的 运营 方式     关注点 从 行为 漏斗 效率 到 生命周期 的 跃迁 ， 希望 用户 不断 地 向上 发展     全 链路 触达 ： 在 用户 动线 的 每 一个 节点 上 设置 触发 点 ， 目标 不是 驱动 用户 进入 漏斗 的 下 一个 节点 ， 而是 驱动 用户 生命周期 的 跃迁     首先 定义 用户 生命周期 节点 ， 整合 用户 行为 数据 ， 利用 算法 能力 计算 出 用户 在 生命周期 中所处 的 位置 ， 并 预测 到 下 一 状态 跃迁 的 概率 。 当时 别 用户 在 跃迁 边界 时 ， 干预 系统 触发 商品 、 券 等 信息 ， 加速 用户 跃迁 。 例如 ， 当 预测 到 用户 长期 没有 核心 行为 ， 即将 流失 的 时候 ， 使用 定向 商品 刺激 用户 ， 产生 购买 行为 ， 「 临门一脚 」 。     平台 提效     以 技术 的 方式 代替 人工 ， 把 两周 的 事情 提效 到 15 分钟     除了 开发 提效 工具 ， 还 需要 将 这些 工具 变成 系统 ， 把 用户 增长 理论 实体化 ， 流程化     将 「 上卷 和 下 钻 」 分析 工具化 ； AB 测试工具 化     工具 做 完 了 ， 但是 用 得 人 并 不会 主动 使用 或者 正确 使用     解决之道 ： 通过 系统 保证 ， 将 这些 工具 默认 集成 到 产品 中 ， 成为 默认 选项     数据 为 王     用户 增长 团队 的 能力 升级     解释 趋势 和 关注 分布     关注 趋势 ： 选择 一个 核心 指标 ， 建立 其 在 时间 维度 的 图标 ， 关注 趋势 变化 。 试图 通过 各种 手段 解释 变动 的 原因 ， 沉淀 方法论     关注 分布 ： 关注 核心 指标 在 各个 维度 上 的 分布 ， 发现 不 均匀分布 的 维度 ， 找到 解决 不 均匀分布 的 方案 ， 提出 假设 ， 大胆 试验      ", "tags": "user-growth", "url": "/wiki/user-growth/taobao2019.html"},
      
      
      
      
      
        
      
      {"title": "github pages使用指南", "text": "  Github   Pages   使用指南  ", "tags": "web", "url": "/wiki/web/github-pages.html"},
      
      
      {"title": "wordpress开发指南", "text": "    Table   of   Contents           分类目录           为 分类目录 添加 元 数据                         分类目录       为 分类目录 添加 元 数据       钩子               / / 新建 分类目录       add _ action ( &# 39 ; category _ add _ form _ fields &# 39 ; ,   &# 39 ; cat _ add _ form _ fields _ cb &# 39 ; ) ;       / / 编辑 分类目录       add _ action (   &# 39 ; category _ edit _ form _ fields &# 39 ; ,   &# 39 ; cat _ edit _ form _ fields _ cb &# 39 ; ) ;         / /   保存       add _ action (   &# 39 ; edited _ category &# 39 ; ,   &# 39 ; cat _ form _ save _ fields _ cb &# 39 ; ) ;       add _ action (   &# 39 ; create _ category &# 39 ; ,   &# 39 ; cat _ form _ save _ fields _ cb &# 39 ; ) ;                 创建 和 更新 元 数据 函数               get _ term _ meta (   $ term _ id ,   $ meta _ key ,   $ single ) ;       add _ term _ meta ( $ term _ id ,   $ meta _ key ,   $ meta _ value ) ;       update _ term _ meta ( $ term _ id ,   $ meta _ key ,   $ meta _ value ) ;        ", "tags": "web", "url": "/wiki/web/wordpress.html"},
      
      
      {"title": "在github pages中使用simiki指南", "text": "  起因       开始 使用 simiki 是因为 一篇 博客 — —   程序员 的 知识 管理   ，     这篇 博客 对 我 启发 很大 。 是 的 ， 程序员 或者 一般 的 工程师 经常 需要 配置 一些 开发 环境 之类 的     工作 ， 如果 能够 将 这些 过程 记录下来 ， 日后 再 配置 的 时候 会 减少 很多 不必要 的 时间 浪费 。     此外 ， 如果 能 将 平常 一些 琐碎 的 知识 记录下来 也 是 不错 的 。 使用 simiki 可以 将 这些 工作 通过     wiki 来 实现 ， 并且 可以 将 数据 保存 在 本地 ， 不用 担心 数据 丢失 之类 的 风险 。         请 不要 FORK 我 的 WIKI ！ ！     因为 这个 WIKI 里面 的 内容 大多 是 我 原创 的 ， 请 不要 FORK ， 谢谢 。       工具 准备       为了 使用 simiki ， 需要 准备 好 基本 环境 。               安装 python ， 不同 的 平台 安装 方式 不同 ， 都 很 容易 。               安装 simiki 库 及其 依赖 库 ， 我 使用 pip 进行 安装 ， 只 需要 一条 命令   pip   install   simiki   即可 。               注册 github 账户 ， 并且 创建   & lt ; username & gt ; . github . io   代码 项目 。 完成 之后 ， 你 应该 可以 通过 该           子 域名 访问 到 自己 的 page 页面 ， 具体 细节 请 参考   官方 文档   。               环境 配置 过程       在 github 中 创建   wiki   项目 ， 并 创建   gh - pages   分支 [ 1 ] 。             git   clone   git @ github . com : tracholar / wiki . git   git   checkout   - b   gh - pages   git   rm   - rf   .               切换 到 master 分支 初始化 simiki ， 生成 content 和 themes 目录 和 几个 文件 。 并 在 output 目录 生成 静态 文件 。             git   checkout   master   simiki   init   simiki   g               部署       windows 中 的 部署       为了 部署 方便 ， 在 master 分钟 中将 output 目录 过滤 掉 ，     在 . gitignore 文件 中 添加 一行             output               即可 。 此外 ， 需要 将 output 目录 push 到 gh - pages 分支 ， 可以 利用 我 写 的 一个     批处理 脚本   deploy . bat   。     这个 脚本 提供 两个 功能 ， 初始化 和 部署 。             deploy   [ option ]           - i     初始化           message     以 message 作为 git   commit 信息 提交 并 推送 到 github               如果 你 要 使用 请 把 脚本 中库 的 URL 改成 你 的 URL 。     初始化 之前 请 删除 前面 生成 的 output 目录 ， 然后 执行 以下 命令 。             deploy   - i               然后 重新 提交 部署             deploy   init - version               然后 就 可以 在 你 的 项目 页面 中 看到 wiki 了 ， 对 我 来说 是   tracholar . github . io / wiki 。           Tips :   注意 文件夹 和 文件 命名 统一 用 小写 ， 否则 你 会 后悔 ， 因为 windows 不 区分 大小写     而 linux 是 区分 的 。           linux / MAC   中 的 部署       还 没试 过 ， 等 试过 之后 再 写 ， 你 可以 参考   官方 指南       使用   ghp - import   部署 ， 更 方便 ， 只是 不 支持 windows 。       公司 发了 mac ， 我 又 写 了 个 bash 脚本   deploy . sh   。     使用 方法 跟 windows 上 的 一样 ， 不过 要   chmod   + x   deploy . sh   给 脚本 增加 执行 权限 。     当然 ， 你 也 需要 把 git 仓库 的 地址 改成 你 自己 的 地址 。       我 的 效果       请 访问   https : / / tracholar . github . io / wiki   观看 。         请 不要 FORK 我 的 WIKI ！ ！     因为 这个 WIKI 里面 的 内容 大多 是 我 原创 的 ， 请 不要 FORK ， 谢谢 。       [ 1 ]   https : / / help . github . com / articles / creating - project - pages - manually /    ", "tags": "web", "url": "/wiki/web/simiki.html"},
      
      
]};