---
title: "第03.0讲：机器学习建模实战"
layout: page
date: 2018-02-06
---
[TOC]

## 关于
本讲内容将通过一个例子，深入理解机器学习建模的整个过程。在这一讲中，你将学习到：

1. 什么是过拟合与泛化？为什么要划分训练集、测试集、验证集。
2. 什么是交叉验证？
3. 如何评估一个分类模型的好坏？
4. 方差-偏差分解
5. 决策树的减枝与泛化

学习本讲，希望你

1. 至少有高中数学水平。
2. 了解决策树，如果还不了解，可以参看[决策树模型](/wiki/tutorial/ml/intro-dt.html)
2. 如果你需要完成实践部分，需要有基本的 python 知识，你可以通过[python快速入门](/wiki/tutorial/ml/intro-python.html)快速了解python如何使用。

本讲所用的数据集还是采用鸢尾花(iris)数据集，你可以从UCI网站上下载<https://archive.ics.uci.edu/ml/datasets/Iris>。如果已经安装了 scikit-learn，那么可以利用提供的dataset接口直接调用。鸢尾花数据集是著名的统计学家 Fisher 提供的。下面我们采样少量的数据看一看。

|   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target  
|---|---|---|---|---|---  
91 | 6.1 | 3.0 | 4.6 | 1.4 | 1  
77 | 6.7 | 3.0 | 5.0 | 1.7 | 1  
99 | 5.7 | 2.8 | 4.1 | 1.3 | 1  
65 | 6.7 | 3.1 | 4.4 | 1.4 | 1  
14 | 5.8 | 4.0 | 1.2 | 0.2 | 0  
108 | 6.7 | 2.5 | 5.8 | 1.8 | 2  
142 | 5.8 | 2.7 | 5.1 | 1.9 | 2  
127 | 6.1 | 3.0 | 4.9 | 1.8 | 2  
24 | 4.8 | 3.4 | 1.9 | 0.2 | 0  
2 | 4.7 | 3.2 | 1.3 | 0.2 | 0


该数据集的每一条记录代表一个样本，每一个样本有4个属性变量：

- sepal length (cm) 萼片长度
- sepal width (cm) 萼片宽度
- petal length (cm) 花瓣长度
- petal width (cm) 花瓣宽度

每一个样本有1个目标变量target，target有3个取值，每一种取值的意义如下：

- 0： setosa 山鸢尾
- 1： versicolor 变色鸢尾
- 2： virginica 维吉尼亚鸢尾

这个数据集一共有150个样本，这三种花都有50个样本。上面显示出来的只是随机选取的一部分数据。每一种鸢尾花的图片如下，从左到右分别是 setosa,versicolor,virginica

![Iris](/wiki/static/images/iris.png)

## 分类准确率
在上一讲中，我们说到决策树生成算法，每一步都是穷举所有可能的分裂规则，利用信息增益准则找到最佳规则，将最佳规则加入决策树中，持续这个步骤知道满足停止准则。在前面，我们介绍了三个停止准则，分别是：如果集合中只有一类样本，就停止分裂；如果集合中样本个数少于某个阈值，就停止分裂；如果树的深度达到某个阈值，就停止分裂。显然，如果停止准则不同，那么我们得到的决策树也不同，这么多决策树如何评估哪一个好，哪一个不好呢？

以上述鸢尾花分类任务为例，分类得好和坏可以通过分类转确率来衡量，分类准确率=分类正确的样本数目/总样本数目。如下图所示，分别是3棵决策树，深度分别为0、1、2，叶子节点中的3个数字分别代表训练样本中3类花的样本数目。第一棵决策树只有一个叶子节点，也就是对所有样本都预测同一个值（训练样本中最多的那一类，在这个例子中3类样本一样更多，任意一个预测类别都一样），假设预测为第0类，那么准确率只有1/3。第二棵决策树深度为1，根据花瓣长度是否小于2.45将特征空间划分为两部分，每一部分对应一个叶子节点，左边的叶子节点预测为第0类，而右边的叶子节点预测为第1类（由于第1类样本和第2类样本一样多，所以预测为第1类和第2类效果是一样的）。因此，分到左子树的50个样本都会预测为第0类，都预测准确，而分到右子树的100个样本只有一半预测准确，总的预测准确样本数是100，准确率为2/3。随着决策树深度的加深，训练样本预测的准确率会越来越高，知道每个叶子节点上都只有一类样本，那么训练样本的预测准确率将达到100%！如果以训练集上的准确率为评估指标，那么显然越深的决策树预测效果越好。而且可以想象，对任何一个训练集，总可以不断地将决策树加深，直到每个叶子节点上都只有一类样本。那么，训练集上的准确率越高是否就代表模型的预测能力越好呢？

![决策树](/wiki/static/images/dt-03.png)


## 交叉验证
我们前面讲过，机器学习就是从数据中自动发现一些有意义的规律。这些规律是有意义的，意味着可以用来指导实践，用来对没有见到过的数据进行预测。以鸢尾花为例，我们希望通过机器学习从已经观测到的那150个样本，找到一些关于4个属性与类别的规律，这样我们就可以通过这4个属性对没有见过的鸢尾花预测它的类别了。我们用X表示用来预测的属性，也就是特征，用Y表示预测的目标类别，也就是标签，那么在所有可能的数据中，可以用一个联合概率分布P(X, Y)来表示X和Y的概率关系，它的意义是在所有的数据中特征等于X且类别为Y的概率。这些所有可能的数据构成的集合我们称作总体，P(X, Y)就是总体的概率分布，已经观测到的数据 D={(x,y)| (x,y)~P(X, Y)} 只是总体的一个采样结果，它们只是总体很少的一部分并且通常假设这些样本是独立选取的。你可以理解为1个样本是否选取与其他样本没有关系，因此样本间统计独立，又因为他们都来自同一个总体，所以他们是独立同分布(i.i.d)的！机器学习的目标就是从这有限的观测数据D中学习到关于总体的规律。

![经验和总体](/wiki/static/images/cross_validate01.png)

因此，我们当然不希望模型学到的只是训练集上看到的有限的规律，甚至只有少数样本表现出来的假规律。这也是因为机器学习采用的是不完全归纳法，所以存在被经验误导的可能性，我们希望模型学到这种错误规律的风险尽可能小，这样模型才是有价值的，所以我们应该在全量的总体上来评估模型的效果才靠谱。以鸢尾花任务为例，确切来说就是要求模型在未知的来自同一个总体的样本上，分类准确率尽可能高。如果我们用函数h(x)表示训练好的模型，对于样本的特征为x，模型的预测结果$\hat{y} = h(x)$。用示性函数$I(h(x) = y)$表示模型是否预测正确，预测正确结果为1，预测错误结果为0.那么模型在训练集上的准确率为

$$
\hat{P} _ c = E_{(x, y) \sim D} I(h(x) = y)
$$

上述准确率并不是一个评估模型效果好坏的指标，而应该用模型在总体上的准确率

$$
P _ c = E_{(x, y) \sim P(x, y)} I(h(x) = y)
$$

这两个准确率的唯一区别就是求期望是在观测数据上还是在总体上计算的结果。在总体上的期望值才有意义，才能表达模型在未知数据上的预测效果。但是，这个期望值我们无法求，因为总体对我们来说是未知的，那怎么办呢？一个简单的方法是，将观测数据划分成两部分，一部分用来训练模型，而用另外一部分计算准确率作为模型在总体上准确率的估计值。这样划分的两个集合我们叫做训练集和测试集，在训练集上训练模型，而在测试集上评估模型的效果。

有了测试集后，就可以用测试集上的预测效果作为模型在总体上的预测效果的一个较好的估计。前面说过，在训练集上，深度越深的决策树准确率约高，但是在测试集上的效果就不见得。那么我们能不能通过测试集上的效果来选择最佳的模型呢？答案是不能的！模型的建立过程中不能涉及到任何测试集上的信息，否则测试集上的评估结果就不能很好地反应出模型真实的性能。为了从很多模型中选择一个最好的，我们还需要将训练集继续划分成训练集和验证集！用训练集训练多个模型，用验证集选出最好的一个模型，测试集只能用于评估！但是在实际建模任务中，我们会用真实的未观测数据来评估模型，比如训练好一个推荐的模型，上线之后运行一段时间来验证实际效果，因此也有只将训练集划分成训练集和验证集两个集合的做法，而用线上效果来评估模型。而在学术论文当中，为了与同行比较效果，那么就需要划分成3个集合，用验证集选择模型，而用测试集报告本论文方法的效果与同行进行比较。曾经有学者在选择模型用到了测试集的数据，最后被发现了，这是严重的学术不端行为！

将训练集划分成训练集和验证集选择模型的方法也叫做交叉验证(Cross Validate)，交叉验证还有一些其他方法，这里再介绍两种：k折叠和留一法。k折叠是为了解决训练集数目少的问题，如果训练集数目很大，上述简单的划分就可以了，但是如果训练集较小，某一次划分带来的统计波动很大，使得这种验证方法不稳定。为了解决这个问题，可以将训练集随机划分成k个相等的集合，用其中k-1个集合训练模型，而用剩下一个计算评估指标，但是这个评估指标并不是最终选择模型的指标。选择评估集合可以有k种不同的选择方式（k个集合任何一个都可以作为评估集合），因此可以用相同的超参数（我们将决策树深度、每个叶子节点最少样本数目等等这种在训练模型之前就需要确定的参数叫做模型的超参数）训练k次，可得到k个准确率，然后用这k个准确率的平均值作为最终的评估指标来选择**不同的超参数**。

![k折叠交叉验证](/wiki/static/images/k-fold.png)

留一法可以看做k等于训练样本数目的特殊情况，也就是每次只留下一个样本来评估。在k折叠交叉验证中，k越大，那么评估集上估计的统计误差就越小，因为评估指标是k个评估结果的平均值！但是计算消耗的资源就越多，留一法是评估指标最接近总体上的评估指标的，但是计算消耗的资源也最多，一般应根据训练集合的大小来选择合适的k值，一般k取3到10是比较合理的。

以鸢尾花任务为例，为了选择最佳的决策树深度这个超参数，我们可以利用5折叠交叉验证。对每一个深度，利用5折叠交叉验证计算出k折叠平均准确率，平均准确率最高的那个深度值就是最佳的！然后我们可以将深度设置为这个最佳值，在全量训练样本中重新训练决策树模型，作为最终的预测模型！

## 过拟合与决策树减枝






## 离散特征和缺失值的处理



## 决策树的其他分裂准则


## 思考与实践
