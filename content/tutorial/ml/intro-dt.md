---
title: "第2.0讲：决策树模型"
layout: page
date: 2018-02-06
---
[TOC]

## 关于
本讲内容将通过一个例子，深入理解决策树模型。在这一讲中，你将学习到：

1. 什么是决策树模型？
2. 构建决策树模型的算法是怎么实现的？

学习本讲，希望你

1. 至少有高中数学水平。
2. 如果你需要完成实践部分，需要有基本的 python 知识，你可以通过[python快速入门](/wiki/tutorial/ml/intro-python.html)快速了解python如何使用。

## 回顾决策树
在上一讲中，我们简单了解了一下决策树的基本概念。如下图所示，是我们上一讲通过数据分析，设计出来的简单规则模型对应的决策树。决策树首先是一颗树，树由很多节点构成。这些节点分为两类，中间节点（椭圆形）和叶子节点（方形）。中间节点代表一条规则，叶子节点代表模型的决策输出。有多少个叶子结点，就代表有多少条规则。这个决策树实际上代表3条规则，每条规则可以用一个 IF-THEN 条件语句表示：

![决策树](/wiki/static/images/dt-01.png)

1. IF 花瓣长度(petal length) < 2.8, THEN sentosa
2. IF 花瓣长度(petal length) >= 2.8 AND 花瓣长度(petal length) < 4.9, THEN versicolor
3. IF 花瓣长度(petal length) >= 4.9, THEN virginica

这三个规则，共同定义了一个分段函数！

$$
y =
\begin{cases}
0, \text{petal length} \lt 2.8 \\\\
1, \text{petal length} \in [2.8, 4.9) \\\\
2, \text{petal length} \ge 4.9
\end{cases}
$$

对于决策树，我们先定义几个基本概念，便于后面表述：

- 子节点：和节点相连的后继节点，比如节点(petal length < 4.9)是节点(petal length < 2.8)的子节点
- 父节点：当前节点是子节点的父节点。比如节点(petal length < 2.8)是节点(petal length < 4.9)的父节点
- 边：两个相连节点中间的叫边，这条边有方向，从父节点指向子节点。
- 中间节点：有子节点的节点，它不直接输出预测结果的节点，对应一个规则
- 叶子节点：没有子节点的节点，直接输出预测结果的节点，对应一个复杂的规则，通常是多个规则的组合
- 根节点：没有父节点的节点
- 路径：从根节点出发，沿着子节点移动，到达叶子节点时所经历的所有节点序列就是一条路径。路径上边的数量叫做路径长度，实际上也等于中间节点的数目。
- 深度：最大的路径长度，比如上述决策树深度为2.
- 规则：一个可以判定真假的表达式就叫规则，比如花瓣长度(petal length) < 2.8
- 组合规则：多个规则通过且(AND)和或(OR)连接起来的语句叫做组合规则，比如 花瓣长度(petal length) >= 2.8 AND 花瓣长度(petal length) < 4.9

## 决策树生成算法
在前面一讲，我们通过数据可视化分析，找到了针对花瓣长度(petal length)的3个规则。寻找这些规则的过程能不能够自动化完成呢？如果可以的话，那么决策树就可以自动化地生成了，不用再去做数据分析了。答案是肯定的，这就是决策树生成算法。

假设我们对花瓣长度设计规则，一个规则可以看做将它的取值划分成两个区间，关键是找到分割点的值。最简单的方法是，随机取一个分割点。

![随机分割](/wiki/static/images/dt-02.png)

如图所示，是两个不同的分割方式，第一种采用的分割点是2，在训练样本中，三个类别的样本数目都是50个，通过分割后，落到左边子节点的样本数目分别是[50, 0, 0]，即只有第1类的样本，看起来区分性还不错，把第一类完美地识别出来了；落到右边子节点的样本数目分别是[0, 50, 50]，第2类和第3类暂时还无法区分，不过不用担心，我们可以沿着右子节点继续分割下去就可能把这两类也分开来。第二种分割方式采用的分割点是4，相比于第一种分割方式，它把11个第2类样本也放到左边了，看起来区分性没第一种好。那么问题来了，怎么衡量一种分割方式比另外一种好呢？

### 信息熵
信息熵可以用来度量一个概率分布$\\{p_i, i=1,2,...\\}$的不确定度，熵的定义是

$$
H(\\{p_i\\}) = -\sum_i p_i \log p_i
$$

为什么熵可以度量不确定度呢？我们来看看最简单的一个例子，假设我们抛一枚硬币，如果硬币是均匀的，那么正面和反面出现的概率都是0.5，我们计算一下熵$H = - 0.5\log 0.5 - 0.5\log0.5=\log 2 = 1$（这里为方便记，对数的底取为2）。如果这个硬币不那么均匀，假设正面朝上的概率为0.9，反面朝上的概率为0.1，我们再来计算一下熵 $H =  - 0.9\log 0.9 - 0.1\log0.1=0.427$。熵变小了！直观来看是不确定性减小了！因为抛一枚均匀的硬币，确实很难猜测它是正面还是反面，但是如果非常不均匀的硬币，正面朝上的概率是0.9，那么我们有很大的把握猜测它是正面！如果我们把熵H随着正面朝上概率p画一个函数图像，可以看到它在0.5处取最大值，直观理解是均匀的硬币最难猜，相反在0和1处取最小值，直观理解是只有一面的硬币最好猜！

![熵](/wiki/static/images/entropy_plot.svg)


信息熵的另外一个解释是，要描述一个事情所需要的最少比特数。比特是计算表达数据量的一个单位，计算机中都是用0和1表示数据，1位这样的0/1单元就是1比特。所以这句话也可以这样理解，如果我要用计算机存储这样一个事件，最少要用的数据量。对于一个均匀硬币，我要记录结果是正面还是反面，我只要用0表示反面，1表示正面，这样只需要1个比特（恰好等于熵）就可以记录结果了。但是如果一枚非常不均匀的硬币，正面朝上的概率为1，那么我们根本不需要记录就可以知道它的结果肯定是正面，对应的熵为0！

### 信息增益准则
利用信息熵，我们可以度量每一种分割方法的好坏。因为熵的意义是不确定度，那么我们计算分割前后这种不确定度的减少量，不确定度的减少越多，说明分割后越好区分每一类，所以分割越好。
