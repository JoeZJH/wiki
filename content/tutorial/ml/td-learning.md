---
title: "第11.0讲：时间差分学习"
layout: page
date: 2018-03-30
---
[TOC]

## 关于
强化学习近年大火，最早是因为AlphaGo使用强化学习打败人类围棋冠军引发的。在那之后，强化学习在工业场景应用越来越多，原来很多做搜索、推荐、广告等一直在用监督学习的业务，也开始使用强化学习来优化用户体验和平台收益了。强化学习实际上很早就提出了，事实上，强化学习来源于控制论。控制论之父叫做[维纳](https://en.wikipedia.org/wiki/Norbert_Wiener)，学过信号处理的可能知道他，维纳滤波就是用他的名字命名的。控制论最早源于航天，人们要控制火箭发射装置，将火箭发射到地球之外；控制论还源于机器人控制，人们需要用算法对机器人的行为进行控制。所以，很多讲强化学习的文献也会说强化学习是在优化控制策略。

在这篇文章中，您将学习到

1. 环境未知情况下的更好的学习方法？
2. 蒙特卡罗方法和时间差分学习
3. SARSA, Q-learning
4. on-policy 与 off-policy 学习

我期望您至少有：

1. 高中数学水平且年满18岁，部分内容需要你了解监督学习，和强化学习基本概念，你可以通过本教程前面的章节进行学习。
2. 如果你需要完成实践部分，需要有基本的 python 知识，你可以通过[python快速入门](/wiki/tutorial/ml/intro-python.html)快速了解python如何使用。

## 蒙特卡罗方法
在上一讲中，我们说到，对于一个MDP问题，如果环境已知，也就是知道环境的转移概率P和回报函数R，可以通过求解贝尔曼方程得到最优策略，求解的方法有两种，分别是值迭代和策略迭代。如果环境未知，我们提到一种通过随机尝试的方法，先估计出环境，然后转化为环境已知问题求解。这种随机尝试效率很差，需要尝试很多次，并且随机尝试本身就有成本，而且需要等到尝试很多次之后，才能得到一个比较好的策略。那么，能不能在每一次尝试之后，都能将策略提升到一个更好的策略呢？因为每一次尝试，环境的反馈都会提供一部分对环境的信息，如果我们能利用好这部分信息，就有可能从中获取到有价值的信息来提升我们的策略。
