---
layout: page
title: "推荐系统评论快报-20年07期"
date: 2020-07-01
---
[TOC]

## AutoInt
- 论文：AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks，CIKM 2019
- 基本思想：用self-attention来学习特征交叉
- 代码：<https://github.com/DeepGraphLearning/RecommenderSystems>

### 主要内容
#### 模型

<img src="/wiki/static/images/autoint-01.png" style="max-width:400px" />

- 将所有的特征当做稀疏特征，投影到同一个emb低维空间，得到向量序列e1,...,eM
    1. 对单值特征，直接emb
    2. 单值连续值特征，emb后乘以连续值
    3. 对多值特征，emb后取平均

<img src="/wiki/static/images/autoint-02.png" style="max-width:300px" />

- 交互层，将上述每个emb依次看做query，利用self-attention实现特征间的交互，
  利用多头Attention每个向量都可得到H个变换后的向量，拼接得到交叉特征$(\tilde{e_m} = \tilde{e_m}^1 \oplus ... \oplus \tilde{e_m}^H)$

<img src="/wiki/static/images/autoint-03.png" style="max-width:200px" />
<img src="/wiki/static/images/autoint-04.png" style="max-width:200px" />
<img src="/wiki/static/images/autoint-05.png" style="max-width:400px" />

- 将多头变化后的向量$(\tilde{e_m})$跟原始向量$(e_m)$用残差方式融合得到最终向量。
$$
e_m^{Res} = ReLU(\tilde{e_m} + W_{Res} e_m)
$$

- 进过上述交叉层后，每个$(e_m)$都得到一个融合特征交叉后的$(e_m^{Res})$。
  concat后进入逻辑回归层，预测概率即可。

#### 试验
- 实验结果显示，相比于其他实现交叉的方法，在离线指标上有显著提升。
- 残差对结果提升较大，self-attention层数不用太多2-4层即可。
- 交互层对模型训练性能影响还比较大。

<img src="/wiki/static/images/autoint-06.png" style="max-width:400px" />
<img src="/wiki/static/images/autoint-07.png" style="max-width:600px" />


### 评论
- 利用self-attention来做特征间的交叉是个不错的想法，但是self-attention的
  物理意义应该是用其他特征向量来平滑中间特征向量，直观上来看有点说不太通。至少
  在这个点上，可以做一些改进，比如$(\tilde{e_m}^h)$不是用所有特征向量的线性组合，
  而是用特征间内积的线性组合，可能更能表示特征间的交叉。
- 另外，由于不同的特征的emb尺寸是一样的，可能会影响高基数特征的表达能力，因为
  这个尺寸大家必须相同，所以不会特别大，而很多id特征的基数很高，如果比较重要的话，
  提高这部分特征的emb尺寸是有一定价值的。但是在autoint中，强行让所有的emb尺寸都相同，
  从理论上来分析，可能会有一定的负面效果。当然，最终以试验结果为准，炼丹大分部都是玄学。
- 实际上，可以对self-attention稍加改造就可以不需要让每个特征的emb尺寸保持一致。
  在计算内积的时候增加一个投影即可，所以维度不匹配的地方都可以通过一个投影来实现维度匹配。
- 复杂度从实验结果来看，在合理的范围内，可以在实际场景中试试。


## 噪声、正则与泛化
- 公众号文章：[泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练](https://mp.weixin.qq.com/s/b6dTrFgwCjpusWdclB6UXw)
- 作者苏剑林写过很多不错的技术文章，而且文笔很好，思考得也有深度，值得我们学习。

### 主要内容

