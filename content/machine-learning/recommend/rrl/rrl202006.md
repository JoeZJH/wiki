---
layout: page
title: "推荐系统评论快报-20年06期"
date: 2020-06-01
---
[TOC]


## bayes方法的不确定性建模
- 论文：What Uncertainties Do We Need in Bayesian Deep Learning
  for Computer Vision，NIPS2017
- 推荐理由：跟我之前想得同时建模均值和方差挺像的，不需要有不确定性的标注，
  就能建模。但是本文无疑思考得要深刻得多，这种不确定性是系统固有的，可以建模
  预测，还有由于数据有限带来的估计偏差，本文通过贝叶斯方法来估计。并且可以
  同时估计这两种不确定性。虽然这篇文章是CV任务，但是这种不确定性建模方法原则
  上可以应用到很多任务当中。
  
### 主要内容
- 对于回归问题，$(y = f_0(x) + \epsilon)$，f0是X能解释的部分，而$(\epsilon)$是
  不能解释的固有随机性。这种固有的随机性带来的不确定性叫做 aleatoric 不确定性。
  另一方面，在收集数据的时候，数据本身也可能存在误差，比如测量误差等。
  这也算作aleatoric不确定性的一部分。这种不确定性无法通过增加训练样本
  的方式来消除。
- 另一种不确定性来自于通过有限数据集训练的模型f与真实f0之间的误差，叫做
  epistemic不确定性，它可以通过增加训练样本来消除。
- aleatoric不确定性的建模：对于回归问题，同时建模均值函数和方差函数
$$
Y \sim N(f_0(X), \sigma_0^2(X))
$$
- 对于分类问题，将对数几率建模成高斯分布
$$
logit_i \sim N(f_{0i}(X), \sigma_{0i}^2(X)) \\\\
P(Y=i) = softmax(logit)[i]
$$
- epistemic不确定性的建模：由于它是由于训练数据不充分带来的误差，可以通过
  贝叶斯方法来估计。将模型参数全部贝叶斯化，



## 用不确定性来加权多任务损失函数
- 论文：Multi-task learning using uncertainty to weigh losses 
  for scene geometry and semantics，CVPR 2018
- 推荐理由：多任务损失函数的权重是个超参数，本文提供一种简单的方法，并且有
  不错的理论逻辑。对其他做多任务场景，有一定借鉴意义。