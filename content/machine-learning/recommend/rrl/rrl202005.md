---
layout: page
title: "推荐系统评论快报-20年05期"
date: 2020-05-30
---
[TOC]

# 关于
- 收集一下近期推荐系统相关的一些热点文章，及其相关的论文。
  在此基础上，做简要阅读和评论，重点精读的文章单独写一篇笔记。

# 召回相关
## NCF vs MF
- 论文：Neural Collaborative Filtering vs. Matrix Factorization Revisited
- 作者 Steffen Rendle 是 FM 的发明人，绝对大佬
- 知乎讨论<https://www.zhihu.com/question/396722911>

### 主要观点
- 比较了基于矩阵分解的方法和目前基于MLP的NCF方法，
  指出在合适的超参数选择下，矩阵分解的方法比NCF更好
- 虽然MLP可以近似任意函数，但是作者证实他确很难学到点积！
  很难学到的意思是，需要很高的模型容量以及很多数据才能学到
- MLP太慢了，不如点积，因为点击可以用近似索引
- 作者不是说MLP不好，而是希望不要被MLP能近似任意函数误导，
  MLP需要较大数据和较低emb维度，才能拟合得好相似函数


### 详细内容
- 两种学习相似函数的模式，点积和MLP

![点积和MLP](/wiki/static/images/mf-ncf-01.png)

- NCF原始论文(2017)用MLP和GMF两个分数来表示相似（模型结构见下图），GMF是用加权的内积
$$
\phi^{GMF}(p, q) = \sigma(w^T(p \bigodot q))
$$

![GMF-MLP](/wiki/static/images/ncf-001.png)

- NCF原始论文里面，GMF没有对权重w正则，是导致不稳定的因素。
  因为p和q有正则，会导致p和q倾向于减小，如果w同时增大对应的倍率，
  那么损失函数实际上不变。另外，w的信息实际上可以让p和q来学到，
  所以增加w这个参数实际上并没有增加模型容量。
  
- MLP难拟合内积，误差界是 $( O(d^4/\epsilon^2) )$
- 上述误差界来自于论文， Learning Polynomials with Neural Networks，
  这篇论文的主要结论见后文
- 试验评估指标：hit rate（即召回率），NDCG

### 评论
- MLP难以学到点积这个点比较有意义，这是不是说明了在DNN时代，
  手动做一些交叉还是能拿到一些收益的？例如，W&D中用wide来记忆
  用户历史行为跟item的交叉。在DeepFM中用FM来交叉emb向量，对于更上层的
  向量，实际上也可以发现对他们做交叉也能有一些收益。
- 虽然GMF那种方式没有增加模型容量，但是如果将元素乘法的向量放到MLP里面，
  还是能增加模型容量的。并且多了显式交叉的信息。
- 实际上目前工业界是拿点积做召回（如DSSM），MLP用来做精排，点积确实快，
  但是限制了模型容量
- 召回的模型评估指标：hit rate，MRR，NDCG 都可以，hit rate侧重召回率，
  后两个侧重排序。

## NN学习多项式
- Learning Polynomials with Neural Networks，2014
- 本文是一篇偏理论的文章，主要是在上一篇论文中提到的一个结论，
  但是我认为这篇文章里面的一些结论，对设计网络的人来说，
  还有有一些启发价值的。

### 主要结论
- 任何多项式都可以通过线性组合随机初始化的充分多个神经元，来任意逼近。
  也就是说，含有一层隐层的MLP可以拟合任何多项式。神经元的个数需要$(O(n^{2d}))，
  n是输入变量个数，即输入的维度。

#### Representation Theorem
- 简单表述：在一个有限的范围内，可以用指数函数任意逼近d阶多项式。
  在逼近误差为$(\epsilon)$的时候，需要$(m = O(n^{2d}/\epsilon^2))$个神经元。
  
#### 用梯度下降优化
- 用隐层数目为$(m = O(n^{2d}/\epsilon^2))$的单隐层神经网络，

## LightGCN
- Xiangnan He, Kuan Deng ,Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang(2020). LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation
