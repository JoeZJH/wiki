---
title: "CS231N - Convolutional Neural Networks for Visual Recognition"
layout: page
date: 2016-07-01
---
[TOC]

## 关于
李菲菲在Stanford开的课程，见<http://cs231n.stanford.edu/>


## BP 算法与计算图
一个图节点实现`forward`计算激活函数和`backward`计算梯度，图的变对应于变量。

### 高效的BP算法

LeeCun 1998 的论文中给出了BP算法的一些trick：

- 采用随机梯度下降，更快：考虑对样本的10次复制，随机梯度相当于训练了10次，而批量梯度下降只有1次！
结果通常更好：可以跳出鞍点，也可以更大概率跳出局部最优。可以随时间进一步训练！online learning。
对于SGD，理论上最优学习率随时间线性下降！
- 每一次epoch重新打散样本！
- 归一化输入，均值接近0通常收敛更快！同样，输出也尽可能是0均值。去相关，归一化方差，会加快收敛。
- 基于上一个原则，tanh比sigmoid好。
- 目标，匹配输出激活函数
- 初始化权值，权值要使得激活函数工作在线性区，（这样才好学，否则梯度为0）目标是让输入输出的方差相同（都为1）。
当输入方差为1的时候，输出方差为

$$
\sigma_{y_i} = (\sum_j w_{ij}^2)^{1/2}
$$

为了保证输出方差也为1，那么

$$
\sigma_w = m^{-1/2}
$$

对于部分连接的网络（如CNN，DTNN），m应该是连接的节点个数。

- 学习率，自动调整衰减。动量机制，减少震荡。

$$
\Delta w(t+1) = \yita \frac{\partial E_{t+1}}{\partial w} + \mu \Delta w(t)
$$

- 自适应学习率


### 自动微分
四种计算梯度的方法：

1. 手动推导梯度的公式，然后编码实现：易错，费时
2. 数值微分（有限差分）：简单实现，但是效率低，而且不精确
3. 符号微分（Mathematica, Maple)：表达式通常会比较复杂，存在 expression swell 的问题，而且对表达式形式有要求（close form）
4. 自动微分：可以达到机器精度，和理想的渐进性能只差一个常数因子（性能牛逼！）




### 参考

1. Automatic differentiation in machine learning: a survey, 2015
2. Efficient BP, LeeCun 1998
3. <https://cs231n.github.io/optimization-2/>
4. [Stochastic Gradient Tricks](https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/)


## 神经网络历史
### 感知器
Frank Rosenblatt 1957

$$
y = f(w x + b) \\\\
f(z) = 1 when z>0 else 0
$$

更新权值

$$
w_i(t+1) = w_i(t) + \alpha (d_j - y_j(t))x_{j, i}
$$

相当于下述损失函数 + SGD优化（注意这里label是+1，-1和上面有区别，这里只是便于表达）

$$
\max(0, - d_j * y_j)
$$

### 三层神经网络
Rumelhart et al. 1986，BP算法

### RBM深度网络
Hinton 2006

### 第一个强结果
Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition George Dahl,
Dong Yu, Li Deng, Alex Acero, 2010 MSR

Imagenet classification with deep convolutional neural networks
Alex Krizhevsky, Ilya Sutskever, **Geoffrey E Hinton**, 2012


### 激活函数
- sigmoid: 将结果映射到[0,1]之间，有概率解释。问题在于饱和将梯度变为0了，非0均值。而非0均值，导致梯度被限制在各分量全为正或者全为负的区域。
导致收敛变慢。`exp`函数计算复杂度较大。

- tanh: 解决了0均值的问题
- ReLU: 在正值区不饱和，计算效率较高，但是还是不是0均值，负向梯度为0
- Leaky ReLU = max(0.1x, x),解决负向梯度饱和问题，
- Maxout = max(w1 x + b1, w2 x + b2), 基本解决上述问题，但是参数变多了 double
- ELU


$$
x > 0: x \\\\
x < 0: \alpha (\exp(x) - 1)
$$


中心化：减去图像均值（AlexNet），减去每个通道的均值（VGGNet）

初始化：
“Xavier initialization” [Glorot et al., 2010]： $(n_{in} Var(w) = 1)$



对于 ReLU，因为一半恒为0，因此有一个0.5因子。$(\frac{1}{2} n_{in} Var(w) = 1)$ He et al., 2015

论文：

1. Understanding the difficulty of training deep feedforward neural networks, Glorot and Bengio, 2010
2. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015


Batch Normalize[Ioffe and Szegedy, 2015]：归一化到标准正态分布，然后让它自己学一个纺射变换。通常插入在全连接层和激活函数之间。


### CNN
1. 1998，LeNet-5, LeCun
2. LeNet-5: Gradient-based learning applied to document recognition，1998，LeCun, Bottou, Bengio, Haffner
3. AlexNet: ImageNet Classification with Deep Convolutional Neural Networks，Hinton 2012,
4. ZFNet:
4. VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition
4. GoogLeNet: Going Deeper with Convolutions
5. ResNet


## 目标检测
任务：分类 + 定位

Location as Regression: 输入图片，输出4个坐标！（非常简单）

姿势估计：Toshev and Szegedy, “DeepPose: Human Pose Estimation via Deep Neural Networks”, CVPR 2014

sliding window: Overfeat:Integrated Recognition, Localization and Detection using Convolutional Networks,  ICLR 2014
