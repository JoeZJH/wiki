---
title: "2018-CVPR论文集"
layout: page
date: 2019-03-08
---
[TOC]

## 关于
CVPR2018论文简读笔记

## SENet
- 论文: Squeeze-and-Excitation Networks, 中科院软件所
- ILSVRC 2017 冠军模型
- 代码: https://github.com/hujie-frank/SENet
- 相关解读: <https://zhuanlan.zhihu.com/p/32733549>
- 主要创新点是在普通的卷积层后面增加了一个模块: Squeeze-and-Excitation block

![Squeeze-and-Excitation block](/wiki/static/images/squeeze-excitatioin.png)

- 模块解读
    1. 对输入 H'xW'xC 的特征图, 首先进行正常的卷积层变换成 HxWxC
    2. 然后对每一个通道(即C这个维度)加权, 用来提升重要的特征通道, 抑制不重要的特征通道, 作者称之为 feature recalibration, 其实就是通道维度的attention机制嘛
    3. 通道权重计算
        1. squeeze操作: 将 HxWxC 特征图通过全局平均池化,变成 1x1xC 的向量
        2. excitation: 通过一个单隐层的神经网络变成一个recalibration后的C维向量
        
        
## shuffle-net
- face++
- 略, 有精度笔记, 关键点是通道shuffle, 将通道间的全连接变成部分连接, 又通过通道shuffle, 避免通道间的完全隔离

## Learning Transferable Architectures for Scalable Image Recognition
- Googlebrain
- 创新点: 用算法学习模型结构, 先用个小数据集搜索一个比较好的结构,然后在大数据集上重新训练
- NASNet搜索空间 B. Zoph and Q. V. Le. Neural architecture search with rein- forcement learning. In International Conference on Learning Representations, 2017.
    - 用一个RNN来输出神经网络每一层的参数序列(见图),即动作A,RNN输出的是动作序列
    - RNN的参数当做策略函数的参数
    - 用当前层之前层的超参数作为状态S?
    - 模型的层数超过一个阈值就停止
    - 用RNN输出的参数作为模型超参数的NN在验证集上的准确率作为回报R
    - 采用策略梯度算法REINFORCE对RNN的参数进行优化

![NASNet](/wiki/static/images/nasnet.png)

## MobileNetV2
- 论文: MobileNetV2: Inverted Residuals and Linear Bottlenecks
- Google
- 创新点: 新的模块层, 先用一个1x1卷积层升维, 然而利用深度分离卷积在每一个通道上单独做卷积, 然后又用1x1卷积降维, 还增加了残差模块
- TensorFlow-slim 官方实现

## Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
- JD、微软、澳大利亚大学etc


   