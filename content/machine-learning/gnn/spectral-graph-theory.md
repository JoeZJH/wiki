---
title: "谱图理论-Spectral Graph Theory"
layout: page
date: 2019-11-05
---
[TOC]

# 关于
- 课程 Spectral Graph Theory  的学习笔记
- <http://www.cs.yale.edu/homes/spielman/561/>
- python 图代码库 <https://networkx.github.io/> <https://networkx.github.io/documentation/stable/>

# 导言
- 图的邻接矩阵 $(A_G(u, v))$
- 图的操作：
    - diffusion operator，扩散操作：每一个时刻，某个顶点上的东西，均匀扩散到邻居顶点，不保留任何物质
        - $(D_G)$表示顶点的度矩阵，那么考虑归一化（这是为了得到扩散概率）邻接矩阵$(W_G = D_G^{-1} A_G)$ 。它的每一行$(W_G(u, \cdot))$代表从顶点u通过一次扩散操作转移到其他顶点的物质的比例。
            - 另一种解释：W可以看做一个概率转移矩阵，从u转移到v的概率
        - 用向量p表示初始每个顶点物质的数量，那么进过一次扩散操作后，每个顶点物质的数量为 $(p W_G)$
            - 概率解释：一次扩散操作后，每个定点上的物质的数量是所有定点的量通过一次转移，到达该顶点的量的总和
    - 拉普拉斯矩阵：$(L_G = D_G - A_G)$
        - 半正定矩阵 $(x^T L_G x  = \sum_{(u,v) \in E} (x(u) - x(v))^2)$
        - 二次型的意义是图上数量分布的平滑性
- 谱论 
    - 实对称矩阵，有n个特征值和对应的特征向量。线性代数的基础知识，不多介绍
    - 瑞利商 $(\frac{x^T M x}{x^T x})$
- 拉普拉斯矩阵的特征向量，假设特征值从小到大排序 $(\lambda_1 \le \lambda_2 \le \cdot \lambda_n)$
    - L的特征值大于等于0
    - 0 是特征值，对应的特征向量是全1向量
    - 特征值0的代数重数代表图可以划分为互不联通的子图的数目
    - $(\lambda_2 > 0)$ 当且仅当图是联通的。
        - 个人理解，一个互相联通的图可以用一个拉普拉斯矩阵表示出来，且存在一个非0向量使得这个Lx=0
        - 两个互不联通的图看做一个图的时候，由于两个图节点间没有任何连接，因此对应的总拉普拉斯矩阵应该是
        $$
        L = \begin{bmatrix}
        L_1 & O \\\\
        O & L_2
        \end{bmatrix}
        $$
        - 因此，显然可以构造两个不同的向量$([v_1, O], [O, v_2])$ 使得Lv=0，其中v1和v2分别是子矩阵的0空间中的任意向量。
    

# 拉普拉斯矩阵
- 全1向量是特征值0对应的特征向量
- $(\lambda_2)$ 对应的特征向量是满足下列条件的向量

$$
\min_x x^T Lx \\\\
s.t. ||x||^2 = 1 \\\\
1^T x = 0
$$

- $(\lambda_3)$ 对应的特征向量是满足于跟上述x正交且满足上述条件的解y
- $(\lambda_2)$ 跟问题「通过切割最少的边，将图切分成两个子图」有密切关系
- 顶点集合的子集S的边界(boundary)定义为S与剩下定点的所有相连的边的集合

$$
\partial S = \\{(u,v) \in E| u \in S, v\notin S \\}
$$

- isoperimetric ratio 定义为S的边界集合大小与S顶点数目之比
$$
\theta(S) = \frac{|\partial S|}{|S|}
$$

- isoperimetric number 定义为上述比值的最小值，要求S的大小不超过所有顶点数的一半
$$
\theta_G = \min_{|S| \le n/2} \theta(S)
$$

- 定理(下界)：对每一个子集 $(S \subset V)$， 
$$
\theta(S) \ge \lambda_2 (1-s)
$$
s=|S|/|V|是顶点数目比率，因为S的数目不超过V的一半，所以$(s \le 1/2)$，所以
$$
\theta_G \ge \lambda_2 /2
$$
 
> 定理的证明，关键是理解 $(x^T L x)$ 的几何意义。取一个特殊的向量x，x是子集S的示性函数，即xi代表第i个定点是否在S中，如果在则xi=1，否则xi=0。
> 那么 $( (Lx)_i , i \in S)$ 的意义则为第i个顶点与S外的其他定点的边的数目，把S中的所有i求和即得到边界$(\partial S)$ 的大小了。
> 所以 $(\theta(S) = \frac{x^T L x}{x^T x})$
> 显然上式是不小于$(\lambda_2)$ 的，但是还有更准确的下界。
> 将x按照两个子空间分解（0特征值的空间，即全1向量空间；正交补空间，即 $(x^T i = 0)$）
> $(x = y + s)$，$(y^T 1 = 0)$，s则为每一维都是s的向量，且$(s_i=|S|/|V| )$
> 所以继续推导有（注意s是L的特征向量，Ls=0）
> $$
> \theta(S) = \frac{x^T L x}{x^T x} \\\\
>           = \frac{y^T L y}{x^T x}   \ge \lambda_2 \frac{y^T y}{x^T x} \\\\
>           = \lambda_2 \frac{y^T y}{y^T y + s^T s}  =  \lambda_2 (1-s)
> $$


# FAQ

## 为什么特征值0的代数重数代表图可以划分为互不联通的子图的数目

两个互不联通的图看做一个图的时候，由于两个图节点间没有任何连接，因此对应的总拉普拉斯矩阵应该是
$$
L = \begin{bmatrix}
L_1 & O \\\\
O & L_2
\end{bmatrix}
$$

因此，显然可以构造两个不同的向量$([v_1, O], [O, v_2])$ 使得Lv=0，其中v1和v2分别是子矩阵的0空间中的任意向量。

显然，如果可以划分成k个子图，那么0特征值的代数重数就是k。



## 对拉普拉斯矩阵做线性变换有什么特殊含义吗


## 拉普拉斯矩阵的特征值和特征向量有什么意义

