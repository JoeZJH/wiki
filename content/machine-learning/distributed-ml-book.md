---
title: "《分布式机器学习：算法、理论与实践》读书笔记"
layout: page
date: 2020-03-28
---

[TOC]

# 关于
- 刘铁岩写的书


# 第1章 绪论
- 分布式机器学习系统与传统分布式系统的区别
    - 天生具有一定的鲁棒性和容错性，比如丢失一些数据和执行顺序不一定要跟单机版严格一致
    - 除了加速计算外，模型精度是一个重要的追求目标，不能为了加速计算而降低模型的精度


# 第2章 机器学习基础
- 大部分内容略，只记录几个点
- boosting方法扛过拟合：及时训练集的精度不在提升，继续boost还是可以提升在测试集上的效果，因为继续boost提升了置信度（间隔理论）。影像中周志华在科大那次讲座刚好讲到过这个事情，还提出了一个理论解释



# 第3章 分布式机器学习框架
- 略，见后面具体章节，本章节是个综述性质


# 第4章 单机优化之确定性算法
- 凸函数：二阶导大于0
- 强凸：二阶导有下界
- lipschitz连续：一阶导有上界，不能跳变太多，梯度不能太大
- beta光滑：二阶导有上界，梯度不能变化太快了

## 一阶确定性算法
- 梯度下降：
    - beta光滑，则次线性收敛O(1/T)
    - alpha强凸，则一阶收敛
- 投影次梯度下降：解决非光滑，有约束
    1. 先按次梯度下降，次梯度任选一个就行
    2. 投影到可行域
- 近端梯度下降，参见近似梯度法/投影梯度法，投影算子
- Frank-Wolfe算法：相比投影次梯度法是在最后再投影会可行域，该算法在下降方向选取的时候就考虑了约束区域。好处是在投影计算较为困难时，比投影次梯度方便
    1. 计算梯度
    2. 在可行域内搜索到与负梯度方向夹角最小的下降方向
    3. 使用动量策略更新下降方向
- Nesterov（尼斯特洛夫，看起来是个俄国人）加速：很多地方都有用到
    - 对更新量外推
- 坐标下降：每次梯度下降一个变量

## 二阶确定性算法
- 牛顿法
- 拟牛顿法：用迭代的方式更新海森矩阵，避免牛顿法直接求海森矩阵的困难


## 对偶方法



# 第5章 单机优化之随机性算法


# 第6章 数据与模型并行


# 第7章 通信机制



# 第8章 数据与模型聚合



# 第9章 分布式机器学习算法



# 第10章 分布式机器学习理论


# 第11章 分布式机器学习系统


# 第12章 结语



# FAQ

## 分布式机器学习系统与其他分布式系统的区别是啥
- 天生具有一定的鲁棒性和容错性，比如丢失一些数据和执行顺序不一定要跟单机版严格一致
- 除了加速计算外，模型精度是一个重要的追求目标，不能为了加速计算而降低模型的精度


## 投影次梯度法 vs 近端梯度下降的收敛速度上有什么区别
