---
title: "自然语言处理入门"
layout: page
date: 2020-03-01
---

[TOC]

# 关于
- 《自然语言处理入门》何晗，HanLP作者
- 主要内容
    - 中文分词
    - 序列标注与分词
    - CRF
    - 词性标注
    - 命名实体识别
    - 信息抽取
    - 文本聚类
    - 文本分类
    - 依存句法分析
    - 深度学习与NLP
- HanLP主要代码是通过Java实现的，方便集成到线上环境；同时支持python，其实底层还是通过python来调用Java包

# 第1章 新手上路
- 基于规则的专家系统
    - 波特词干算法，由一些列规则组成，规则有优先级之分
- 基于统计的学习方法
- 语料库
    - 中文分词：人名日报
    
# 第2章 词典分词
- 词典：HanLP自带了千万级词典，同时提供mini版
- 切分算法
    - 完全切分，即遍历所有连续字序列，跟词典中的词进行匹配。输出所有可能的词
    ```python
    def fully_segment(text, dic):
        """
        text: 文本序列
        dic: set 集合，包含词典中所有的词    
        """
        word_list = []
        for i in range(len(text)):
            for j in range(len(text)):
                word = text[i:j]
                if word in dic:
                    word_list.append(word)
        return word_list
    ```
    - 正向最长匹配：即认为越长的词优先级越高，优先匹配长词。从前往后扫叫做正向最长匹配，从后往前扫叫逆向最长匹配
    ```python
    def forward_segment(text, dic):
      word_list = []
          for i in range(len(text)):
              for j in range(len(text)):
                  word = text[i:j]
                  if word in dic:
                      word_list.append(word)
          return word_list
    ```
    - 逆向最长匹配：总的而言，逆向最长匹配效果好于正向，孙茂松教授论文
    - 双向最长匹配：汉语中单字词数量远小于非单字词
        1. 同时执行正向和逆向最长匹配，若两者词数不一样，选择词数更好的那一个
        2. 否则返回单字最少的那个。
        3. 其他情况返回你想最长匹配
    - 性能：
        - Python set 比用java 的treemap慢
        - 正向和逆向性能差不多，但是java的不好说，可能因为垃圾回收机制，正向快一些？
- 字典树 trie 树，前缀树；避免了前缀的重复比较，比简单的二叉树匹配要快
    - 由于子节点较多，所以孩子节点集合要用字典或者map来实现
- 首字散列其余二分的字典树：在逆向和双向匹配是，大约比Java的treemap快一倍的样子
    - 首字：用字符散列函数来查询孩子，子节点个数很大，例如65535
    - 其余的节点，通过二分查找来查询孩子
- 前缀树的妙用：优化到1000万字每秒
    - 出发点，如果「自然」不在词典中，那么所有以「自然」开头的词都不在词典中。比普通实现的全切分可以快5-7倍
- 双数组字典树：略，进一步加速2-3倍

- 中文分词的评估
    1. 将切分区间标记为起止下标[i,j]
    2. 统计预测的区间集合A跟标注样本的区间集合B的交集，计算precision和recall
    $$
    P = \frac{|A \union B|}{|A|} \\\\
    R = \frac{|A \union B|}{|B|}
    $$
- 评估指标
    - P，R，F1
    - Roov，即不在词典内的词召回率
    - Riv，即在词典内的词召回率，因为存在歧义的问题，所以无法做到100%
- 最长匹配算法，在MSR语料上的P为89%，R为95%，F192%，Roov2.58%,Riv97% 

- 字典树的其他应用
    - 停用词过滤，敏感词过滤
    - 繁简体转化，需要按词进行转换
    - 拼音转换，需要按词进行转换

# 第3章 二元语法与中文分词
- 基本思想是，利用语言模型这个先验知识，来对分词的歧义情况消歧义。例如，可以解决「商品和服务」的两种分法「商品 和 服务」 「商品 和服 务」，前者的概率较高
- 语言模型通过以词为单位的二元语言模型来建模。举例
    1. p(商品 和 服务) = p(商品|BOS) p(和|商品) p(服务|和) p(EOS|服务)
    2. p(商品 和服 务) = p(商品|BOS) p(和服|商品) p(务|和服) p(EOS|务)
- 上述条件概率通过统计得到 p(w1 | w0) = #(w0 w1)/#(w0)
- 数据稀疏和平滑，高阶语言模型用低阶平滑，例如2元模型用1元平滑
$$
p(w1|w0) = \lambda p(w1|w0) + (1-\lambda) p(w1)
$$

- 分词语料库：人民日报，MSR
- 分词的每一种切分（在这种方法中要求最小粒度要在词典中），都可以看做词图上的一条路径，所以分词就变成找到一条概率最大的路径。当用log处理概率后，就可以认为是一个最短路径问题。viterbi译码算法
- 与用户词典的融合
    - 低优先级，用户词典合并：例如，统计分词后结果「商品 和 服务 员」，如果「服务员」出现在用户词典中，那么会合并为「商品 和 服务员」
    - 高优先级，干预词网生成，一元词频由用户提供，二元词频直接由一元词频伪造（搞成相同）。分出概率较高，一般只在一定要分出的场景下才用
- 问题，OOV 新词；因为OOV和新词都不在词典中，所以新词无法召回

# 第4章 隐马尔科夫模型与序列标注
- 中文分词作为一种序列标注问题，每个词可以属于的类别
    - B，词首
    - E，词尾
    - M，词中
    - S，单字成词

# FAQ
## 为什么逆向最长匹配通常好于正向最长匹配

## 为什么字典树相比普通的二叉树要快
- 例如，比较「你好」，普通二叉树每个字都要比较多次；而字典树「你」只比较一次！

## 为什么二元语言模型要求切分的最小粒度要在词典中
- 因为不在词典中，没法通过词频计算概率
