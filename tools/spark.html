<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Spark - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#tools">tools</a>&nbsp;»&nbsp;Spark</div>
</div>
<div class="clearfix"></div>
<div id="title">Spark</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">安装</a></li>
<li><a href="#worker">启动主机和worker</a><ul>
<li><a href="#spark-shell">Spark shell</a></li>
</ul>
</li>
<li><a href="#sparkcontext">SparkContext</a></li>
<li><a href="#rdd">RDD</a></li>
<li><a href="#spark-sqlcontext">Spark SQLContext</a><ul>
<li><a href="#dataframe">DataFrame</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">安装</h2>
<p>从Spark官网下载安装包，然后解压即可。非常简单</p>
<h2 id="worker">启动主机和worker</h2>
<p>进入spark目录，然后运行脚本</p>
<div class="hlcode"><pre>./sbin/start-master.sh
</pre></div>


<p>即可。进程会在后台运行，你可以通过 <a href="http://localhost:8080">http://localhost:8080</a> 进行监控。</p>
<p>启动worker的脚本是</p>
<div class="hlcode"><pre>./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT
</pre></div>


<p>其中IP和PORT可以在监控页面看到。</p>
<p>关闭worker很简单，直接关闭worker运行的shell或者ctr + c中断即可。
关闭主机需要运行脚本</p>
<div class="hlcode"><pre>./sbin/stop-master.sh
</pre></div>


<h3 id="spark-shell">Spark shell</h3>
<p>启动scala版的shell命令为<code>./bin/spark-shell</code>，python版的命令为<code>./bin/pyspark</code></p>
<h2 id="sparkcontext">SparkContext</h2>
<p>sc是spark的入口，通过<code>SparkConf</code>来创建它。</p>
<div class="hlcode"><pre><span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;FromPostgreSql&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[4]&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.executor.memory&quot;</span><span class="o">,</span> <span class="s">&quot;2g&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">sparkConf</span><span class="o">)</span>
</pre></div>


<p>使用<code>sc.stop()</code>方法停止SparkContext。</p>
<h2 id="rdd">RDD</h2>
<ul>
<li>RDD，全称为Resilient Distributed Datasets，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。</li>
<li>in-memory cache. <code>cache()</code></li>
<li>RDD 常用操作<ul>
<li><code>count()</code></li>
<li><code>foreach</code>, <code>map</code>, <code>flatMap</code>, <code>filter</code>,</li>
</ul>
</li>
</ul>
<h2 id="spark-sqlcontext">Spark SQLContext</h2>
<p>Spark SQL支持的语法</p>
<div class="hlcode"><pre><span class="k">SELECT</span> <span class="p">[</span><span class="k">DISTINCT</span><span class="p">]</span> <span class="p">[</span><span class="k">column</span> <span class="k">names</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="n">wildcard</span><span class="p">]</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">kesypace</span> <span class="n">name</span><span class="p">.]</span><span class="k">table</span> <span class="n">name</span>
<span class="p">[</span><span class="k">JOIN</span> <span class="n">clause</span> <span class="k">table</span> <span class="n">name</span> <span class="k">ON</span> <span class="k">join</span> <span class="n">condition</span><span class="p">]</span>
<span class="p">[</span><span class="k">WHERE</span> <span class="n">condition</span><span class="p">]</span>
<span class="p">[</span><span class="k">GROUP</span> <span class="k">BY</span> <span class="k">column</span> <span class="n">name</span><span class="p">]</span>
<span class="p">[</span><span class="k">HAVING</span> <span class="n">conditions</span><span class="p">]</span>
<span class="p">[</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="k">column</span> <span class="k">names</span> <span class="p">[</span><span class="k">ASC</span> <span class="o">|</span> <span class="n">DSC</span><span class="p">]]</span>
</pre></div>


<p>如果使用join进行查询，则支持的语法为：</p>
<div class="hlcode"><pre><span class="k">SELECT</span> <span class="k">statement</span>
<span class="k">FROM</span> <span class="k">statement</span>
<span class="p">[</span><span class="k">JOIN</span> <span class="o">|</span> <span class="k">INNER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="n">SEMI</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="k">OUTER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">RIGHT</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">RIGHT</span> <span class="k">OUTER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">FULL</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">FULL</span> <span class="k">OUTER</span> <span class="k">JOIN</span><span class="p">]</span>
<span class="k">ON</span> <span class="k">join</span> <span class="n">condition</span>
</pre></div>


<h3 id="dataframe">DataFrame</h3>
<p>Spark DataFrame的设计灵感正是基于R与Pandas。
我们通过外部Json文件创建一个DataFrame：</p>
<div class="hlcode"><pre><span class="k">val</span> <span class="n">dataFrame</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">load</span><span class="o">(</span><span class="s">&quot;/example/data.json&quot;</span><span class="o">,</span> <span class="s">&quot;json&quot;</span><span class="o">)</span>
<span class="n">dataFrame</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
</pre></div>


<ol>
<li>Spark in Action [BOOK] <a href="https://zhangyi.gitbooks.io/spark-in-action">https://zhangyi.gitbooks.io/spark-in-action</a></li>
</ol>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-07-05 </span></div>

        </div>
        <div id="footer">
            <span>
                Copyright © 2016 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/tracholar/wiki" target="_blank"> github </a>.
            </span>
        </div>
        
    </body>
</html>