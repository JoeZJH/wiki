<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Spark - tracholar's personal knowledge wiki</title>
        <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
        <meta name="description" content="A wiki website of tracholar when I learned new knowledgy and technics."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#tools">tools</a>&nbsp;»&nbsp;Spark</div>
</div>
<div class="clearfix"></div>
<div id="title">Spark</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">安装</a></li>
<li><a href="#worker">启动主机和worker</a><ul>
<li><a href="#spark-shell">Spark shell</a></li>
</ul>
</li>
<li><a href="#sparkcontext">SparkContext</a></li>
<li><a href="#rdd">RDD</a><ul>
<li><a href="#rdd_1">RDD 操作</a></li>
<li><a href="#rdd_2">RDD持久化</a></li>
<li><a href="#_2">理解闭包</a></li>
<li><a href="#kv">KV值操作</a></li>
<li><a href="#_3">通用的变换</a></li>
<li><a href="#action">Action</a></li>
</ul>
</li>
<li><a href="#_4">共享变量</a></li>
<li><a href="#spark">提交spark任务</a></li>
<li><a href="#spark-streaming">Spark Streaming</a></li>
<li><a href="#spark-sqlcontext">Spark SQLContext，</a><ul>
<li><a href="#dataframe">DataFrame</a></li>
</ul>
</li>
<li><a href="#mllib">MLlib</a><ul>
<li><a href="#sparkml">spark.ml 包</a><ul>
<li><a href="#_5">基础类</a></li>
<li><a href="#_6">特征提取</a></li>
<li><a href="#_7">特征变换</a></li>
<li><a href="#_8">特征选择</a></li>
<li><a href="#orgapachesparkmlclassification">分类 org.apache.spark.ml.classification</a></li>
<li><a href="#orgapachesparkmlregression">回归 org.apache.spark.ml.regression</a></li>
<li><a href="#orgapachesparkmlclustering">聚类 org.apache.spark.ml.clustering</a></li>
<li><a href="#_9">协同过滤</a></li>
<li><a href="#dataframe_1">DataFrame</a></li>
</ul>
</li>
<li><a href="#sparkmllib">spark.mLlib</a></li>
<li><a href="#_10">基本数据结构</a></li>
<li><a href="#_11">模型评估</a></li>
</ul>
</li>
<li><a href="#tips">TIPS</a><ul>
<li><a href="#log4j">使用log4j</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">安装</h2>
<p>从Spark官网下载安装包，然后解压即可。非常简单</p>
<h2 id="worker">启动主机和worker</h2>
<p>进入spark目录，然后运行脚本</p>
<div class="hlcode"><pre><span></span>./sbin/start-master.sh
</pre></div>


<p>即可。进程会在后台运行，你可以通过 <a href="http://localhost:8080">http://localhost:8080</a> 进行监控。</p>
<p>启动worker的脚本是</p>
<div class="hlcode"><pre><span></span>./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IsP:PORT
</pre></div>


<p>其中IP和PORT可以在监控页面看到。</p>
<p>关闭worker很简单，直接关闭worker运行的shell或者ctr + c中断即可。<br />
关闭主机需要运行脚本</p>
<div class="hlcode"><pre><span></span>./sbin/stop-master.sh
</pre></div>


<h3 id="spark-shell">Spark shell</h3>
<p>启动scala版的shell命令为<code>./bin/spark-shell</code>，python版的命令为<code>./bin/pyspark</code></p>
<h2 id="sparkcontext">SparkContext</h2>
<p>sc是spark的入口，通过<code>SparkConf</code>来创建它。</p>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;FromPostgreSql&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[4]&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.executor.memory&quot;</span><span class="o">,</span> <span class="s">&quot;2g&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkCsontext</span><span class="o">(</span><span class="n">sparkConf</span><span class="o">)</span>
</pre></div>


<p>对了，目前spark只支持的scala版本是2.10.x，所以用2.11.x版本可能会出错。</p>
<p>使用<code>sc.stop()</code>方法停止SparkContext。貌似不执行stop，本地用<code>sbt run</code>运行时会出现错误信息，<br />
但是提交jar方式运行没问题。<br />
参考<a href="https://stackoverflow.com/questions/28362341/error-utils-uncaught-exception-in-thread-sparklistenerbus">https://stackoverflow.com/questions/28362341/error-utils-uncaught-exception-in-thread-sparklistenerbus</a>.</p>
<ul>
<li>issue<ul>
<li>使用<code>sbt run</code>方式运行任务，如果涉及到<code>saveAsTextFile</code>操作时，会出错，原因未知。</li>
</ul>
</li>
</ul>
<h2 id="rdd">RDD</h2>
<ul>
<li>RDD，全称为Resilient Distributed Datasets，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。</li>
<li>in-memory cache. <code>cache()</code></li>
<li>RDD 常用操作<ul>
<li><code>count()</code></li>
<li><code>foreach</code>, <code>map</code>, <code>flatMap</code>, <code>filter</code>,</li>
</ul>
</li>
<li>并行化容器，可以通过<code>SparkContext.parallelize</code> 方法创建分布式便于并行计算的数据结构。也可以用来将scala的容器转换为RDD结构的tips</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">,</span><span class="mi">6</span><span class="o">,</span><span class="mi">7</span><span class="o">)</span>
<span class="k">val</span> <span class="n">distData</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>
</pre></div>


<ul>
<li>从外部数据库创建，支持本地文件系统，HDFS，Cassandra， HBase， Amazon S3， 等。<br />
  支持的文件格式包括文本文件， SequenceFiles，其他Hadoop输入格式。<br />
  其中文本格式可以通过<code>SparkContext.textFile(URI [, partition_number])</code>方法创建RDD。<ul>
<li>支持本地文件和网络文件的URI，"/home/user/path-to-file", "hdfs://path-to-file"</li>
<li>支持文件夹，压缩文件，通配符等方式。例如"/path-to-file/*.gz", "/path-to-file/directory"</li>
<li>指定分区数目，每一个分区是64MB，默认创建一个分区。</li>
<li>也可以通过 <code>SparkContext.wholeTextFiles</code> 读取一个目录下的所有文本文件，返回的是 (filename, content)，<br />
  而<code>textFile</code> 则返回所有的行</li>
<li>其他Hadoop输入格式可以使用 <code>SparkContext.hadoopRDD</code> 方法。</li>
<li>其他基于 <code>org.apache.hadoop.mapreduce</code> API 的输入格式可以通过  <code>SparkContext.newAPIHadoopRDD</code> 方法创建</li>
<li><code>RDD.saveAsObjectFile</code> 和 <code>SparkContext.objectFile</code> 支持保存RDD为简单的序列化java对象。</li>
</ul>
</li>
</ul>
<h3 id="rdd_1">RDD 操作</h3>
<ul>
<li>支持两种操作 map， reduce</li>
<li>变换：从一个已经存在的数据创建新的数据，如 <code>map</code>, <code>reduce</code>, <code>reduceByKey</code>。所有的变换操作都是惰性求值，而且不保存<br />
  中间结果。如果重新计算，中间结果也会重新计算。如果需要保存中间结果可以通过<code>RDD.persist()</code>方法指明保存该RDD。</li>
<li>传递函数给spark，不同的语言不同<ul>
<li>scala中可以通过以下几种方式<ul>
<li>匿名函数</li>
<li>单例模式对象的一个静态方法</li>
<li>一个类的实例对象的一个成员方法，这种情况需要传递整个对象过去。同样，如果函数应用了外部的对象的一个域，那么也需要传递整个对象。<br />
  为了避免这个问题，可以创建该域的一个本地拷贝。</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field</span> <span class="k">=</span> <span class="s">&quot;Hello&quot;</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// 修改后的doStuff 函数</span>
<span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field_</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="n">field</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field_</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>


<div class="hlcode"><pre><span></span>- java,  `org.apache.spark.api.java.function` 对象，或者java 8 的lambda表达式
- python， lambda表达式，本地函数，模块的顶级函数，对象的方法
</pre></div>


<ul>
<li>重新分区，<code>repartition</code>会重新分配所有数据，如果是降低分区数目，可以用<code>coalesce</code>，它会避免移动所有数据，<br />
  而只是移动丢弃的分区的数据，参考<a href="https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce">stackoverflow的讨论</a>。</li>
</ul>
<h3 id="rdd_2">RDD持久化</h3>
<p>持久化的两个方法 <code>.cache()</code>和<code>.persist(StorageLevel.SOME_LEVEL)</code>，存储级别有：</p>
<ul>
<li>MEMORY_ONLY ： 默认级别，以 deserialized Java objects 保存在内存（JVM），内存放不下的部分每次也是重新计算</li>
<li>MEMORY_AND_DISK ： 保存在内存，放不下的放在磁盘</li>
<li>MEMORY_ONLY_SER ： 序列化后再保存在内存，放不下重新计算</li>
<li>MEMORY_AND_DISK_SER ：与上一个术语差异在于放不下的放磁盘</li>
<li>DISK_ONLY ： 只放磁盘</li>
<li>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. ： 多保存一个备份</li>
<li>OFF_HEAP (experimental) ： Store RDD in serialized format in Tachyon</li>
</ul>
<p>在python中都是用pickle序列化，只有这一种。<br />
手动移除cache的方法是 <code>RDD.unpersist()</code>，如果不手动移除，Spark 也会自动处理cache的。</p>
<h3 id="_2">理解闭包</h3>
<ul>
<li>在RDD的foreach中，对外部变量的引用实际上是复制了该对象到executor中，然后引用executor中的那个对像，所以不会改变本想引用的那个对象。<br />
  可以使用<code>Accumulator</code>来实现改变主对象。</li>
<li>输出RDD到stdout，同样存在一个问题，在foreach和map中的prinln是输出到executor的stdout。可以通过<code>RDD.collect().foreach(println)</code>方法实现，<br />
  如果该只是打印一部分，可以通过<code>RDD.take(100).foreach(println)</code> 来实现。</li>
</ul>
<h3 id="kv">KV值操作</h3>
<ul>
<li>由于KV类型可以是很多不同类型，通用的操作不多，最常用的是 <code>shuffle</code> 操作，例如 grouping 和 aggregating by key。</li>
<li>在spark中通过创建Tuple2对象实现K-V，例如在下述代码中</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
</pre></div>


<p>注意，在使用自定义的对象作为key的时候，需要确保<code>.equals()</code>方法与<code>hashCode()</code>方法兼容。</p>
<h3 id="_3">通用的变换</h3>
<ul>
<li>map(func)</li>
<li>filter(func)</li>
<li>flatMap(func), 相当于先做map，然后做flat操作</li>
<li>mapPartitions(func)，map到每一个分区</li>
<li>mapPartitionsWithIndex(func)， 带有index的版本</li>
<li>sample， 采样</li>
<li>union，并集</li>
<li>intersection，交集</li>
<li>distinc, 去重</li>
<li>groupByKey，输入(K,V)，输出(K, Iter<V>)</li>
<li>reduceByKey(func)，输入(K,V)</li>
<li>aggregateByKey</li>
<li>sortByKey</li>
<li><code>join(otherDataset [, numTasks])</code>,  (K,V), (K,W) -&gt; (K, (V,W))  </li>
<li>cogroup</li>
<li>cartesian 笛卡尔积？</li>
<li>pipe</li>
<li>coalesce</li>
<li>repartition<br />
略</li>
</ul>
<h3 id="action">Action</h3>
<ul>
<li>reduce</li>
<li>collect</li>
<li>count</li>
<li>first</li>
<li>take(n)</li>
<li>takeSample</li>
<li>takeOrdered</li>
<li>saveAsTextFile(path)</li>
<li>saveAsSequenceFile(path), java and scala</li>
<li>countByKey，对每一个key单独计数</li>
<li>foreach(func)</li>
</ul>
<h2 id="_4">共享变量</h2>
<ul>
<li>broadcast变量，不同的executor共享</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">broadcastVar</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span>
<span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
</pre></div>


<p>优点在于，不同于简单复制，可以采用P2P协议来提升在多个节点之间复制的性能！对于很大的共享对象，性能提升很明显！<br />
<a href="https://stackoverflow.com/questions/26884871/advantage-of-broadcast-variables">https://stackoverflow.com/questions/26884871/advantage-of-broadcast-variables</a></p>
<ul>
<li>Accumulator,</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="s">&quot;My Accumulator&quot;</span><span class="o">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span> <span class="o">+=</span> <span class="n">x</span><span class="o">)</span>
<span class="n">accum</span><span class="o">.</span><span class="n">value</span>
</pre></div>


<p>一般需要实现自己的AccumulatorParam子类，</p>
<div class="hlcode"><pre><span></span><span class="k">object</span> <span class="nc">VectorAccumulatorParam</span> <span class="k">extends</span> <span class="nc">AccumulatorParam</span><span class="o">[</span><span class="kt">Vector</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">zero</span><span class="o">(</span><span class="n">initialValue</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Vector</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Vector</span><span class="o">.</span><span class="n">zeros</span><span class="o">(</span><span class="n">initialValue</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="k">def</span> <span class="n">addInPlace</span><span class="o">(</span><span class="n">v1</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">,</span> <span class="n">v2</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Vector</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">v1</span> <span class="o">+=</span> <span class="n">v2</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// Then, create an Accumulator of this type:</span>
<span class="k">val</span> <span class="n">vecAccum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="o">(</span><span class="k">new</span> <span class="nc">Vector</span><span class="o">(...))(</span><span class="nc">VectorAccumulatorParam</span><span class="o">)</span>
</pre></div>


<h2 id="spark">提交spark任务</h2>
<p>使用 bin/spark-submit 脚本提交，语法</p>
<div class="hlcode"><pre><span></span>./bin/spark-submit <span class="se">\</span>
  --class &lt;main-class&gt; <span class="se">\</span>
  --master &lt;master-url&gt; <span class="se">\</span>
  --deploy-mode &lt;deploy-mode&gt; <span class="se">\</span>
  --conf &lt;key&gt;<span class="o">=</span>&lt;value&gt; <span class="se">\</span>
  ... <span class="c1"># other options</span>
  &lt;application-jar&gt; <span class="se">\</span>
  <span class="o">[</span>application-arguments<span class="o">]</span>
</pre></div>


<blockquote>
<p>For Python applications, simply pass a .py file in the place of <application-jar> instead of a JAR, and add Python .zip, .egg or .py files to the search path with --py-files.</p>
</blockquote>
<h2 id="spark-streaming">Spark Streaming</h2>
<p>简单地说，就是用来从其他地方拉数据的。<br />
输入数据流 =&gt; Spark streaming =&gt; batches of input data =&gt; Spark engine =&gt; batches of processed data</p>
<h2 id="spark-sqlcontext">Spark SQLContext，</h2>
<ul>
<li>从SparkContext创建</li>
</ul>
<div class="hlcode"><pre><span></span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">SQLContext</span>
<span class="k">val</span> <span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span> <span class="c1">// An existing SparkContext.</span>
<span class="k">val</span> <span class="n">sqlContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">SQLContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</pre></div>


<ul>
<li>使用<code>.sql</code>函数进行SQL查询，Spark SQL支持的语法</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">SELECT</span> <span class="p">[</span><span class="k">DISTINCT</span><span class="p">]</span> <span class="p">[</span><span class="k">column</span> <span class="k">names</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="n">wildcard</span><span class="p">]</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">kesypace</span> <span class="n">name</span><span class="p">.]</span><span class="k">table</span> <span class="n">name</span>
<span class="p">[</span><span class="k">JOIN</span> <span class="n">clause</span> <span class="k">table</span> <span class="n">name</span> <span class="k">ON</span> <span class="k">join</span> <span class="n">condition</span><span class="p">]</span>
<span class="p">[</span><span class="k">WHERE</span> <span class="n">condition</span><span class="p">]</span>
<span class="p">[</span><span class="k">GROUP</span> <span class="k">BY</span> <span class="k">column</span> <span class="n">name</span><span class="p">]</span>
<span class="p">[</span><span class="k">HAVING</span> <span class="n">conditions</span><span class="p">]</span>
<span class="p">[</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="k">column</span> <span class="k">names</span> <span class="p">[</span><span class="k">ASC</span> <span class="o">|</span> <span class="n">DSC</span><span class="p">]]</span>
</pre></div>


<p>如果使用join进行查询，则支持的语法为：</p>
<div class="hlcode"><pre><span></span><span class="k">SELECT</span> <span class="k">statement</span>
<span class="k">FROM</span> <span class="k">statement</span>
<span class="p">[</span><span class="k">JOIN</span> <span class="o">|</span> <span class="k">INNER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="n">SEMI</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">LEFT</span> <span class="k">OUTER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">RIGHT</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">RIGHT</span> <span class="k">OUTER</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">FULL</span> <span class="k">JOIN</span> <span class="o">|</span> <span class="k">FULL</span> <span class="k">OUTER</span> <span class="k">JOIN</span><span class="p">]</span>
<span class="k">ON</span> <span class="k">join</span> <span class="n">condition</span>
</pre></div>


<p>-</p>
<h3 id="dataframe">DataFrame</h3>
<p>Spark DataFrame的设计灵感正是基于R与Pandas。<br />
我们通过外部Json文件创建一个DataFrame：</p>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">dataFrame</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">load</span><span class="o">(</span><span class="s">&quot;/example/data.json&quot;</span><span class="o">,</span> <span class="s">&quot;json&quot;</span><span class="o">)</span>
<span class="n">dataFrame</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
</pre></div>


<blockquote>
<p>With a SQLContext, applications can create DataFrames from an existing RDD, from a Hive table, or from data sources.</p>
</blockquote>
<div class="hlcode"><pre><span></span><span class="c1">// Create the DataFrame</span>
<span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="o">(</span><span class="s">&quot;examples/src/main/resources/people.json&quot;</span><span class="o">)</span>

<span class="c1">// Show the content of the DataFrame</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// age  name</span>
<span class="c1">// null Michael</span>
<span class="c1">// 30   Andy</span>
<span class="c1">// 19   Justin</span>

<span class="c1">// Print the schema in a tree format</span>
<span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">// |-- age: long (nullable = true)</span>
<span class="c1">// |-- name: string (nullable = true)</span>

<span class="c1">// Select only the &quot;name&quot; column</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// name</span>
<span class="c1">// Michael</span>
<span class="c1">// Andy</span>
<span class="c1">// Justin</span>

<span class="c1">// Select everybody, but increment the age by 1</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">df</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">),</span> <span class="n">df</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// name    (age + 1)</span>
<span class="c1">// Michael null</span>
<span class="c1">// Andy    31</span>
<span class="c1">// Justin  20</span>

<span class="c1">// Select people older than 21</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">df</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">)</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// age name</span>
<span class="c1">// 30  Andy</span>

<span class="c1">// Count people by age</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">).</span><span class="n">count</span><span class="o">().</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// age  count</span>
<span class="c1">// null 1</span>
<span class="c1">// 19   1</span>
<span class="c1">// 30   1</span>
</pre></div>


<ul>
<li>直接在文件上运行SQL！</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="o">)</span>
</pre></div>


<ul>
<li>注册UDF</li>
</ul>
<div class="hlcode"><pre><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">())</span>
</pre></div>


<h2 id="mllib">MLlib</h2>
<ul>
<li>不同的包的特点，推荐<code>spark.ml</code><ul>
<li><code>spark.mllib</code> contains the original API built on top of RDDs. 在2.0版本不在支持新特性了，不再维护。</li>
<li><code>spark.ml</code> provides higher-level API built on top of <code>DataFrames</code> for constructing ML pipelines.</li>
</ul>
</li>
</ul>
<h3 id="sparkml">spark.ml 包</h3>
<h4 id="_5">基础类</h4>
<ul>
<li>基于DataFrame，借助于抽象，将模型抽象为三个基本类，estimators（实现fit方法）, transformers（实现transform方法）, pipelines</li>
<li>一个正常的模型应该同时实现 <code>fit</code> 和 <code>transform</code> 两个方法</li>
<li><code>transform</code> 将生成一个新的DataFrame，包含了预测的结果</li>
<li><code>fit</code> 的DataFrame需要包含两列 featuresCol 和 labelCol 默认名字为 label</li>
<li>
<p><code>transform</code> 之前的DataFrame需要包含一列 featuresCol，默认名字为features，输出三列（依赖于参数），三列有默认名字，都可以通过setter函数进行设置。</p>
<ul>
<li>predictedCol 预测的标签，默认名字为 <code>prediction</code></li>
<li>rawPredictedCol 预测的裸数据？向量？逻辑回归是<code>wx</code>貌似，默认名字为 <code>rawPrediction</code></li>
<li>probabilityCol 预测的概率，默认名字为 <code>probability</code></li>
</ul>
</li>
<li>
<p>模型参数封装类 <code>Param</code>，他的一个常用子类是 <code>ParamMap</code>，实现了Map接口，可以通过 <code>get, put</code>进行操作。<br />
在2.0版本开始，Spark对Estimators和Transformers提供统一的参数API。</p>
</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">paramMap</span> <span class="k">=</span> <span class="nc">ParamMap</span><span class="o">(</span><span class="n">lr</span><span class="o">.</span><span class="n">maxIter</span> <span class="o">-&gt;</span> <span class="mi">20</span><span class="o">)</span>
  <span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="n">lr</span><span class="o">.</span><span class="n">maxIter</span><span class="o">,</span> <span class="mi">30</span><span class="o">)</span> <span class="c1">// Specify 1 Param.  This overwrites the original maxIter.</span>
  <span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="n">lr</span><span class="o">.</span><span class="n">regParam</span> <span class="o">-&gt;</span> <span class="mf">0.1</span><span class="o">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">threshold</span> <span class="o">-&gt;</span> <span class="mf">0.55</span><span class="o">)</span> <span class="c1">// Specify multiple Params.</span>
</pre></div>


<div class="hlcode"><pre><span></span><span class="n">paramMap</span> <span class="o">=</span> <span class="p">{</span><span class="n">lr</span><span class="o">.</span><span class="n">maxIter</span><span class="p">:</span> <span class="mi">20</span><span class="p">}</span>
<span class="n">paramMap</span><span class="p">[</span><span class="n">lr</span><span class="o">.</span><span class="n">maxIter</span><span class="p">]</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># Specify 1 Param, overwriting the original maxIter.</span>
<span class="n">paramMap</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">lr</span><span class="o">.</span><span class="n">regParam</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">threshold</span><span class="p">:</span> <span class="mf">0.55</span><span class="p">})</span> <span class="c1"># Specify multiple Params.</span>
</pre></div>


<ul>
<li><code>pipeline</code> 将不同模型（transform）堆叠起来，类似于sklearn里面的pipeline。<br />
pipeline保存了一个Array[PipelineStage]，可以通过<code>.setStage(Array[_ &lt;: PipelineStage])</code>函数进行设置。<br />
pipeline实现了estimator的fit接口和transformer的transform接口。</li>
<li>模型持久化 <code>save, load</code></li>
<li><code>PipelineStage</code>抽象类，啥也没干？？？？？？？？！！！！！<code>transformer</code>还是它的子类！！</li>
<li>
<p><code>UnaryTransformer</code> 单列转换对象，是transformer的子抽象类，也实现了pipelinestage接口。<br />
  有两个变量<code>inputCol</code>和<code>outputCol</code>代表输入输出列的名字。<br />
  有几个常用的实例，例如Tokenizer，HashingTF等。</p>
</li>
<li>
<p>模型的保存和加载，利用类的静态方法<code>.load</code>加载(MLReader的实现)，而用实例的<code>.save</code>方法（MLWriter的实现）保存模型到文件。</p>
</li>
<li>
<p>模型评估 <code>Evaluator</code>(实现<code>evaluate(dataFrame)</code>方法)， <code>RegressionEvaluator</code>回归， <code>BinaryClassificationEvaluator</code>二元分类，<br />
<code>MulticlassClassificationEvaluator</code> 多元分类。</p>
<ul>
<li><code>BinaryClassificationEvaluator</code> 除了<code>evaluate</code>方法之外，还有几个重要的属性和属性setter。标签列名<code>labelCol</code>，度量名称 <code>metricName</code> 默认为areaUnderROC，即AUC。<code>rawPredictionCol</code> 预测结果列名。以及相应的setter和getter。</li>
<li><code>MulticlassClassificationEvaluator</code>，三个属性 <code>labelCol</code>，<code>metricName</code> （supports "f1" (default), "precision", "recall", "weightedPrecision", "weightedRecall"），<code>predictionCol</code></li>
<li><code>RegressionEvaluator</code>，三个属性  <code>labelCol</code>，<code>metricName</code> （"rmse" (default): root mean squared error， "mse": mean squared error， "r2": R2 metric， "mae": mean absolute error），<code>predictionCol</code></li>
</ul>
</li>
<li>
<p>交叉验证选择模型超参数。交叉验证 <code>CrossValidator</code> 类，有4个基本方法</p>
<ul>
<li><code>.setEstimator</code></li>
<li><code>.setEvaluator</code></li>
<li><code>.setEstimatorParamMaps(paramGrid)</code> 参数网络</li>
<li><code>.setNumFolds(k)</code> k-fold交叉验证的参数k<br />
同是他也是一个estimator，调用它的<code>fit</code>方法训练模型，返回训练好的模型CrossValidatorModel或模型序列。<br />
他也是一个transformer，调用<code>transform</code>方法直接执行多个transform。</li>
</ul>
</li>
<li>
<p>训练集和测试集的分割 <code>TrainValidationSplit</code>与交叉验证类类似，取代<code>.setNumFolds</code>的是函数<code>.setTrainRatio(ratio)</code>。</p>
</li>
<li>
<p>参数网格可以通过 <code>ParamGridBuilder</code>对象创建，他有三个方法，<code>addGrid(param, values:Array)</code>添加一个参数网格，<br />
<code>baseOn(paramPair)</code>设置指定参数为固定值，<code>build()</code>方法返回一个<code>Array[ParamMap]</code>数组</p>
</li>
</ul>
<h4 id="_6">特征提取</h4>
<ul>
<li>TF-IDF(HashingTF and IDF)，传统的词统计是通过维护一个查找的词典（hash表或者查找树实现），<br />
HashTF则是直接通过对特征计算hash函数映射到低维索引。还可以通过第二个hash函数确定是否存在冲突。<br />
有什么优势？？？使用的类<code>HashingTF, IDF, Tokenizer</code></li>
<li>Word2Vec，低维词向量学习，对应的类：<code>Word2Vec</code></li>
<li>CountVectorizer，直接统计：<code>CountVectorizer</code></li>
</ul>
<h4 id="_7">特征变换</h4>
<ul>
<li>Tokenizer：将文本转换为一个一个的词。例如中文分词就算一个，对于英文可以简单的用空白字符分割就行。可用的类有：<ul>
<li><code>Tokenizer</code> 常规Tokenizer</li>
<li><code>RegexTokenizer</code> 正则式Tokenizer</li>
</ul>
</li>
<li>StopWordsRemover：停止词的移除。<code>StopWordsRemover</code>，可以通过<code>setCaseSensitive</code>设置大小写敏感，<br />
和<code>setStopWords(value: Array[String])</code>设置停止词词典。</li>
<li>n-gram：将输入的一串词转换为n-gram。<code>NGram</code></li>
<li>Binarizer：通过阈值将数值特征变成二值特征。类<code>Binarizer</code>，主要方法：<code>setThreshold</code></li>
<li>PCA：PCA 降维。<code>PCA</code>，方法<code>setK</code></li>
<li>PolynomialExpansion：将特征展开为多项式特征，实现特征交叉。<code>x1,x2-&gt;x1^2,x2^2,x1x2</code>。<code>PolynomialExpansion</code>方法:<code>setDegree</code></li>
<li>DCT：离散余弦变换。<code>DCT</code></li>
<li><code>StringIndexer</code> 将字符串类型的变量（或者label）转换为索引序号，序号会按照频率排序，不是字典序。对于不在词典的string，默认抛出异常，也可以通过<code>setHandleInvalid("skip")</code>直接丢弃。</li>
<li><code>IndexToString</code> 和<code>StringIndexer</code>配合使用可以让字符串类型的变量的处理变得透明，这个是将index变成原来的字符串</li>
<li><code>OneHotEncoder</code>：将单个数字转换为0-1编码的向量。<code>1-&gt;(0,1,0,0)</code>。常用在类别特征的变换。</li>
<li><code>VectorIndexer</code>：将输入向量中的类别特征自动编码为index。比较高端，需要学习一下！<code>setMaxCategories(4)</code>表示特征的值数目超过4个就认为是连续特征，否则认为需要编码。</li>
<li><code>Normalizer</code>：归一化。需要指定p-norm的值<code>setP</code>。按照p范数归一化，默认为2。可以用在输出概率或score时归一化？</li>
<li><code>StandardScaler</code>：标准化特征到方差为1，也可以将均值设为0. 方法：<code>setWithStd(bool), setWithMean(bool)</code></li>
<li><code>MinMaxScaler</code>：归一化到0-1之间。也可以指定min和max</li>
<li><code>MaxAbsScaler</code>：@since(2.0.0)，除以最大值的绝对值，从而将特征归一化到[-1,1]</li>
<li><code>Bucketizer</code>：分桶。方法<code>setSplits(splits)</code>来设置分割点，分割点需要严格递增。</li>
<li><code>ElementwiseProduct</code>：对输入向量乘以一个权值。方法<code>setScalingVec</code>设置权值。</li>
<li><code>SQLTransformer</code>：让你用SQL语句进行变换特征。例子：</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">sqlTrans</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SQLTransformer</span><span class="o">().</span><span class="n">setStatement</span><span class="o">(</span>
  <span class="s">&quot;SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__&quot;</span><span class="o">)</span>
</pre></div>


<ul>
<li><code>VectorAssembler</code>：将多个特征合并到一个向量，也可以合并向量。通过<code>setInputCols(Array[String]</code>设置要合并的列。</li>
<li><code>QuantileDiscretizer</code>：首先对特征采样，然后根据采样值将特征按照等量分桶（等频离散化）。基于采样，所以每次不同。方法<code>setNumBuckets(i:Int)</code>设置桶的个数。</li>
</ul>
<h4 id="_8">特征选择</h4>
<ul>
<li><code>VectorSlicer</code>：通过slice选择特征，人工指定indices。方法<code>setIndices</code>设置选择的indices。字符串indices通过<code>setNames</code>方法设置索引。</li>
<li><code>RFormula</code>：通过R模型公式选择特征，例如<code>clicked ~ country + hour</code>，输出列是默认是公式的响应变量名字。它会对字符串one-hot编码，对数值列转换为double类型。需要人工指定哪些特征。</li>
<li><code>ChiSqSelector</code>：通过卡方独立性检验来选择特征。方法<code>setNumTopFeatures</code>指定要选择的卡方值前多少个。</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">labelIndexer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StringIndexer</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setInputCol</span><span class="o">(</span><span class="s">&quot;label&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setOutputCol</span><span class="o">(</span><span class="s">&quot;indexedLabel&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">fit</span><span class="o">(</span><span class="n">training</span><span class="o">)</span>
<span class="k">val</span> <span class="n">rf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">RandomForestClassifier</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setPredictionCol</span><span class="o">(</span><span class="s">&quot;indexedPrediction&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setLabelCol</span><span class="o">(</span><span class="s">&quot;indexedLabel&quot;</span><span class="o">)</span>
<span class="n">setRFParam</span><span class="o">(</span><span class="n">rf</span><span class="o">,</span> <span class="n">param</span><span class="o">)</span>
<span class="k">val</span> <span class="n">labelConverter</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">IndexToString</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setInputCol</span><span class="o">(</span><span class="s">&quot;indexedPrediction&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setOutputCol</span><span class="o">(</span><span class="s">&quot;prediction&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setLabels</span><span class="o">(</span><span class="n">labelIndexer</span><span class="o">.</span><span class="n">labels</span><span class="o">)</span>
<span class="k">val</span> <span class="n">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">labelIndexer</span><span class="o">,</span> <span class="n">rf</span><span class="o">,</span> <span class="n">labelConverter</span><span class="o">))</span>
</pre></div>


<h4 id="orgapachesparkmlclassification">分类 <code>org.apache.spark.ml.classification</code></h4>
<ul>
<li>逻辑回归：<code>LogisticRegression</code>，参数<code>maxIter, regParam, elasticNetParam</code> 分别是最大迭代次数、正则项系数、elastic网的参数。</li>
<li>决策树：<code>DecisionTreeClassifier</code></li>
<li>随机森林：<code>RandomForestClassifier</code></li>
<li>GBDT：<code>GBTClassifier</code></li>
<li>多层感知器（全连接神经网络）：<code>MultilayerPerceptronClassifier</code>，<code>setLayers</code>指定每层的节点数目。<strong>有没有预训练？需要研究一下！！！</strong></li>
<li>One-vs-All：<code>OneVsRest</code>，将二分类变成多分类模型，采用One-vs-all策略。方法<code>setClassifier</code>设置二分类器。</li>
<li>朴素贝叶斯：<code>NaiveBayes</code>，</li>
</ul>
<h4 id="orgapachesparkmlregression">回归 <code>org.apache.spark.ml.regression</code></h4>
<ul>
<li>线性回归：<code>LinearRegression</code></li>
<li>广义线性回归：<code>GeneralizedLinearRegression</code> @since(2.0.0)</li>
<li>决策树回归：<code>DecisionTreeRegressor</code></li>
<li>随机森林回归：<code>RandomForestRegressor</code></li>
<li>GBDT回归：<code>GBTRegressor</code></li>
<li>Survival regression：<code>AFTSurvivalRegression</code>，什么东西？</li>
<li>Isotonic 回归：<code>IsotonicRegression</code>，什么东西？</li>
</ul>
<h4 id="orgapachesparkmlclustering">聚类 <code>org.apache.spark.ml.clustering</code></h4>
<ul>
<li><code>KMeans</code>，K-means聚类，通过<code>setK</code>设置类的数目。<a href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf">K-means++的分布式实现</a>。</li>
<li><code>LDA</code>：Latent Dirichlet allocation</li>
<li><code>BisectingKMeans</code>：Bisecting k-means 聚类，@since(2.0.0) 不懂？</li>
<li><code>GaussianMixture</code>：GMM 模型。@since(2.0.0)</li>
</ul>
<h4 id="_9">协同过滤</h4>
<ul>
<li><code>ALS</code>：ALS算法，2.0才加到ml包里面，之前在mllib包。</li>
</ul>
<h4 id="dataframe_1">DataFrame</h4>
<p>DataFrame相当于 RDD[Row]，而Row相当于一个可以包含各种不同数据的Seq。<br />
DataFrame通过collect函数之后就是Array[Row]</p>
<p>通过工厂方法<code>SQLContext.createDataFrame</code>创建DataFrame，可以从一下几个数据源创建</p>
<ul>
<li>从<code>List(label, FeatureVector)</code>序列创建</li>
<li>从 <code>JavaRDD</code>创建</li>
<li>从 <code>RDD</code> 创建</li>
<li>从 <code>List[Row]</code> 创建</li>
<li>从 <code>RDD[Row]</code> 创建</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">training</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">(</span><span class="nc">Seq</span><span class="o">(</span>
  <span class="o">(</span><span class="mf">1.0</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="mf">0.0</span><span class="o">,</span> <span class="mf">1.1</span><span class="o">,</span> <span class="mf">0.1</span><span class="o">)),</span>
  <span class="o">(</span><span class="mf">0.0</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="mf">2.0</span><span class="o">,</span> <span class="mf">1.0</span><span class="o">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">)),</span>
  <span class="o">(</span><span class="mf">0.0</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="mf">2.0</span><span class="o">,</span> <span class="mf">1.3</span><span class="o">,</span> <span class="mf">1.0</span><span class="o">)),</span>
  <span class="o">(</span><span class="mf">1.0</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="mf">0.0</span><span class="o">,</span> <span class="mf">1.2</span><span class="o">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">))</span>
<span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;label&quot;</span><span class="o">,</span> <span class="s">&quot;features&quot;</span><span class="o">)</span>
</pre></div>


<p>spark的DataFrame每一列可以存储向量！甚至图像！任意值都行！！</p>
<ul>
<li>
<p>SQL操作</p>
<ul>
<li>select(col1, col2, ...) 选取部分列</li>
<li>sample 采样</li>
<li>sort 排序</li>
<li>unionAll 融合其他表</li>
<li>orderBy</li>
<li>limit</li>
<li>join 内连接</li>
<li>groupyBy</li>
<li>filter(sql表达式)</li>
</ul>
</li>
<li>
<p>lazy val rdd 对象，可以通过RDD接口操作</p>
</li>
<li>
<p>df.sqlContext 可以访问创建该DataFrame 的SQLContext对象，rdd.sparkContext 可以访问创建RDD的SparkContext对象。</p>
</li>
<li>
<p>保存到磁盘</p>
</li>
</ul>
<div class="hlcode"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">转换操作</span> <span class="o">}</span> <span class="o">.</span><span class="n">saveAsTextFile</span><span class="o">(</span><span class="n">filepath</span><span class="o">)</span>
</pre></div>


<h3 id="sparkmllib">spark.mLlib</h3>
<ul>
<li>LogisticRegressionWithLBFGS</li>
<li>LogisticRegressionModel， 要<code>model.clearThreshold</code> predict才会输出概率，否则输出的是判决后的值</li>
</ul>
<h3 id="_10">基本数据结构</h3>
<ul>
<li>Vector, 可以通过工厂对象<code>Vectors</code>创建，普通向量<code>Vectors.dense</code>，稀疏向量<code>Vectors.sparse</code>，通过<code>.toArray</code>方法转换为<code>Array[Double]</code></li>
<li>LabeledPoint, 二元组 <code>(label:Double, features: Vector)</code></li>
<li>Matrix， 可以通过工厂对象<code>Matrices</code>创建，普通矩阵 <code>Matrices.dense</code>，稀疏矩阵<code>Matrices.sparse</code></li>
<li>RowMatrix，前面的向量和矩阵都是存在单机中，这种和下面的矩阵是分布式存储的。</li>
<li>IndexedRowMatrix，indexedrow是(long, vector)的包装使得index是有意义的</li>
<li>CoordinateMatrix，</li>
<li>BlockMatrix</li>
</ul>
<div class="hlcode"><pre><span></span><span class="k">val</span> <span class="n">rows</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">Vector</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// an RDD of local vectors</span>
<span class="c1">// Create a RowMatrix from an RDD[Vector].</span>
<span class="k">val</span> <span class="n">mat</span><span class="k">:</span> <span class="kt">RowMatrix</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">RowMatrix</span><span class="o">(</span><span class="n">rows</span><span class="o">)</span>


<span class="k">val</span> <span class="n">rows</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">IndexedRow</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// an RDD of indexed rows</span>
<span class="c1">// Create an IndexedRowMatrix from an RDD[IndexedRow].</span>
<span class="k">val</span> <span class="n">mat</span><span class="k">:</span> <span class="kt">IndexedRowMatrix</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">IndexedRowMatrix</span><span class="o">(</span><span class="n">rows</span><span class="o">)</span>
</pre></div>


<h3 id="_11">模型评估</h3>
<p>包名<code>org.apache.spark.mllib.evaluation</code></p>
<ul>
<li>两分类 BinaryClassificationMetrics</li>
<li>多分类 MulticlassMetrics</li>
</ul>
<h2 id="tips">TIPS</h2>
<h3 id="log4j">使用log4j</h3>
<div class="hlcode"><pre><span></span><span class="k">package</span> <span class="nn">org.apache.log4j</span><span class="o">;</span>

  <span class="n">public</span> <span class="k">class</span> <span class="nc">Logger</span> <span class="o">{</span>

    <span class="c1">// Creation &amp; retrieval methods:</span>
    <span class="n">public</span> <span class="n">static</span> <span class="nc">Logger</span> <span class="n">getRootLogger</span><span class="o">();</span>
    <span class="n">public</span> <span class="n">static</span> <span class="nc">Logger</span> <span class="n">getLogger</span><span class="o">(</span><span class="nc">String</span> <span class="n">name</span><span class="o">);</span>

    <span class="c1">// printing methods:</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">trace</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">debug</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">info</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">warn</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">error</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">fatal</span><span class="o">(</span><span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>

    <span class="c1">// generic printing method:</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">log</span><span class="o">(</span><span class="nc">Level</span> <span class="n">l</span><span class="o">,</span> <span class="nc">Object</span> <span class="n">message</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// 例子</span>
<span class="k">import</span> <span class="nn">org.apache.log4j.Logger</span>
<span class="k">val</span> <span class="n">log</span> <span class="k">=</span> <span class="nc">Logger</span><span class="o">.</span><span class="n">getLogger</span><span class="o">(</span><span class="n">getClass</span><span class="o">.</span><span class="n">getName</span><span class="o">)</span>
<span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="o">(</span><span class="s">&quot;info&quot;</span><span class="o">)</span>
</pre></div>


<ul>
<li>
<p>如果对RDD操作里面有随机的因素在里面，那么每次操作会不一样！！</p>
</li>
<li>
<p>Spark in Action [BOOK] <a href="https://zhangyi.gitbooks.io/spark-in-action">https://zhangyi.gitbooks.io/spark-in-action</a></p>
</li>
<li>Spark Programming Guide <a href="https://spark.apache.org/docs/latest/programming-guide.html">https://spark.apache.org/docs/latest/programming-guide.html</a></li>
</ul>
</div>
<div>
    <img src="/wiki/static/images/support-qrcode.png" alt="支持我" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<div id="content-footer">created in <span class="create-date date"> 2016-07-05 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'Spark',
  owner: 'tracholar',
  repo: 'wiki',
  oauth: {
    client_id: '0cc0476e504b5e70ae7c',
    client_secret: 'ab98e39ef79469040057eba9c6b2b543b84c72ee',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2018 tracholar.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>